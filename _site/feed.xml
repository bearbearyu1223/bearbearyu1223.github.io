<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2025-08-10T16:37:46-07:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">🍒 Han’s Generative AI Quest</title><subtitle>I chronicle my captivating journey through Generative AI, sharing insights,  breakthroughs, and learnings from my enthralling side projects in the field. 
</subtitle><entry><title type="html">Study Notes: Stanford CS336 Language Modeling from Scratch [4]</title><link href="http://localhost:4000/cs336/2025/08/10/cs336-gpt2-regex-for-pretokenization-explaind.html" rel="alternate" type="text/html" title="Study Notes: Stanford CS336 Language Modeling from Scratch [4]" /><published>2025-08-10T00:00:00-07:00</published><updated>2025-08-10T00:00:00-07:00</updated><id>http://localhost:4000/cs336/2025/08/10/cs336-gpt2-regex-for-pretokenization-explaind</id><content type="html" xml:base="http://localhost:4000/cs336/2025/08/10/cs336-gpt2-regex-for-pretokenization-explaind.html"><![CDATA[<h2 id="demystifying-gpt-2s-pre-tokenization-how-one-regex-pattern-handles-the-worlds-languages">Demystifying GPT-2’s Pre-Tokenization: How One Regex Pattern Handles the World’s Languages</h2>

<p>While working on <strong>Assignment 1</strong> of <em>Stanford’s CS336: Language Modeling from Scratch</em>, I came across a deceptively simple — yet remarkably powerful — regex pattern used in the pre-tokenization stage of the BPE algorithm.</p>

<p>I thought it would be worthwhile to share my notes and walk through how this single pattern can handle text from <strong>multiple languages, scripts, and symbol sets</strong> with precision.</p>

<p>You can find my full BPE assignment implementation here:</p>
<ul>
  <li><strong>BPE training algorithm:</strong> <a href="https://github.com/bearbearyu1223/assignment1-basics/blob/main/cs336_basics/bpe.py">bpe.py</a></li>
  <li><strong>Tokenizer class:</strong> <a href="https://github.com/bearbearyu1223/assignment1-basics/blob/main/cs336_basics/tokenizer.py">tokenizer.py</a></li>
</ul>

<hr />

<h3 id="-how-to-run-the-bpe-training-process">🔧 How to Run the BPE Training Process</h3>

<ol>
  <li><strong>Clone the repository</strong>
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>git clone https://github.com/bearbearyu1223/assignment1-basics.git
<span class="nb">cd </span>assignment1-basics
</code></pre></div>    </div>
  </li>
  <li>
    <p><strong>Set up the local development environment</strong><br />
Follow the instructions in the <a href="https://github.com/bearbearyu1223/assignment1-basics/blob/main/developer_guide.md">developer_guide.md</a>.</p>
  </li>
  <li><strong>Run BPE training</strong>
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>uv run cs336_basics/train_bpe_example.py
</code></pre></div>    </div>
    <p>This will train a BPE tokenizer using the <code class="language-plaintext highlighter-rouge">TinyStoriesV2-GPT4-train.txt</code> dataset, with 10,000 vocabulary size and with special token <code class="language-plaintext highlighter-rouge">"&lt;|endoftext|&gt;"</code>.</p>
  </li>
</ol>

<hr />

<h3 id="-how-to-test-the-tokenizer">🧪 How to Test the Tokenizer</h3>

<p>Run:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>uv run pytest tests/test_train_bpe.py
</code></pre></div></div>

<p>This will validate the tokenizer’s functionality and ensure the pre-tokenization regex behaves as expected.</p>

<h3 id="-the-gpt-2-split-pattern">📜 The GPT-2 Split Pattern</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">regex</span>

<span class="n">GPT2_SPLIT_PATTERN</span> <span class="o">=</span> <span class="sa">r</span><span class="s">"""'(?:[sdmt]|ll|ve|re)| ?\p{L}+| ?\p{N}+| ?[^\s\p{L}\p{N}]+|\s+(?!\S)|\s+"""</span>
</code></pre></div></div>

<p>This single (but mighty) regex is responsible for splitting text into meaningful segments — <strong>words, numbers, punctuation, symbols, whitespace</strong> — in a way that is consistent across languages and scripts.</p>

<hr />

<h3 id="-pattern-breakdown">🔍 Pattern Breakdown</h3>

<h4 id="1-contractions">1. Contractions</h4>
<pre><code class="language-regex">'(?:[sdmt]|ll|ve|re)
</code></pre>
<p>Matches <strong>common English contractions</strong> starting with an apostrophe:<br />
<code class="language-plaintext highlighter-rouge">'s, 'd, 'm, 't, 'll, 've, 're</code></p>

<p><strong>Examples:</strong></p>
<ul>
  <li><code class="language-plaintext highlighter-rouge">"don't"</code> → <code class="language-plaintext highlighter-rouge">["don", "'t"]</code></li>
  <li><code class="language-plaintext highlighter-rouge">"we're"</code> → <code class="language-plaintext highlighter-rouge">["we", "'re"]</code></li>
</ul>

<hr />

<h4 id="2-letters-any-language">2. Letters (Any Language)</h4>
<pre><code class="language-regex"> ?\p{L}+
</code></pre>
<p>Matches <strong>letters</strong> from <strong>any Unicode language</strong> (with optional leading space).</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">\p{L}</code> = Unicode “Letter” category</li>
  <li>Covers: English, Chinese, Arabic, accented characters, and more.</li>
</ul>

<p><strong>Examples:</strong></p>
<ul>
  <li><code class="language-plaintext highlighter-rouge">"hello world"</code> → <code class="language-plaintext highlighter-rouge">["hello", " world"]</code></li>
  <li><code class="language-plaintext highlighter-rouge">"café 北京"</code> → <code class="language-plaintext highlighter-rouge">["café", " 北京"]</code></li>
</ul>

<hr />

<h4 id="3-numbers-any-script">3. Numbers (Any Script)</h4>
<pre><code class="language-regex"> ?\p{N}+
</code></pre>
<p>Matches <strong>numbers</strong> from any writing system (with optional leading space).</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">\p{N}</code> = Unicode “Number” category</li>
  <li>Covers: Arabic numerals (<code class="language-plaintext highlighter-rouge">0–9</code>), Roman numerals (<code class="language-plaintext highlighter-rouge">Ⅰ, Ⅱ, Ⅲ</code>), Arabic-Indic (<code class="language-plaintext highlighter-rouge">٠١٢</code>), etc.</li>
</ul>

<p><strong>Examples:</strong></p>
<ul>
  <li><code class="language-plaintext highlighter-rouge">"I have 5 items"</code> → <code class="language-plaintext highlighter-rouge">["I", " have", " 5", " items"]</code></li>
  <li><code class="language-plaintext highlighter-rouge">"Ⅲ winners"</code> → <code class="language-plaintext highlighter-rouge">["Ⅲ", " winners"]</code></li>
</ul>

<hr />

<h4 id="4-punctuation--symbols">4. Punctuation / Symbols</h4>
<pre><code class="language-regex"> ?[^\s\p{L}\p{N}]+
</code></pre>
<p>Matches <strong>punctuation or symbols</strong> (with optional leading space).</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">[^\s\p{L}\p{N}]</code> = NOT whitespace, NOT letters, NOT numbers</li>
  <li>Captures: <code class="language-plaintext highlighter-rouge">!@#$%^&amp;*()_+-=[]{}|;:'",./&lt;&gt;?</code></li>
</ul>

<p><strong>Examples:</strong></p>
<ul>
  <li><code class="language-plaintext highlighter-rouge">"Wow!!!"</code> → <code class="language-plaintext highlighter-rouge">["Wow", "!!!"]</code></li>
  <li><code class="language-plaintext highlighter-rouge">" $100"</code> → <code class="language-plaintext highlighter-rouge">[" $", "100"]</code></li>
</ul>

<hr />

<h4 id="5-trailing-whitespace">5. Trailing Whitespace</h4>
<pre><code class="language-regex">\s+(?!\S)
</code></pre>
<p>Matches <strong>whitespace at the end</strong> of text or before more whitespace.<br />
This ensures trailing spaces are preserved as tokens.</p>

<hr />

<h4 id="6-general-whitespace">6. General Whitespace</h4>
<pre><code class="language-regex">\s+
</code></pre>
<p>Matches <strong>any remaining whitespace</strong>.</p>

<hr />

<h3 id="-testing-the-pattern">🛠 Testing the Pattern</h3>

<p>Here’s a helper function to test how this regex splits different inputs.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">test_regex</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">description</span><span class="o">=</span><span class="s">""</span><span class="p">):</span>
    <span class="s">"""Test the regex pattern and display results clearly"""</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"</span><span class="se">\n</span><span class="si">{</span><span class="s">'='</span><span class="o">*</span><span class="mi">60</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"TEST: </span><span class="si">{</span><span class="n">description</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"INPUT: '</span><span class="si">{</span><span class="n">text</span><span class="si">}</span><span class="s">'"</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"</span><span class="si">{</span><span class="s">'='</span><span class="o">*</span><span class="mi">60</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>

    <span class="n">matches</span> <span class="o">=</span> <span class="n">regex</span><span class="p">.</span><span class="n">findall</span><span class="p">(</span><span class="n">GPT2_SPLIT_PATTERN</span><span class="p">,</span> <span class="n">text</span><span class="p">)</span>

    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"TOKENS (</span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">matches</span><span class="p">)</span><span class="si">}</span><span class="s">):"</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">token</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">matches</span><span class="p">,</span> <span class="mi">1</span><span class="p">):</span>
        <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"  </span><span class="si">{</span><span class="n">i</span><span class="si">:</span><span class="mi">2</span><span class="n">d</span><span class="si">}</span><span class="s">: </span><span class="si">{</span><span class="nb">repr</span><span class="p">(</span><span class="n">token</span><span class="p">)</span><span class="si">}</span><span class="s">"</span><span class="p">)</span> 

    <span class="n">reconstructed</span> <span class="o">=</span> <span class="s">''</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">matches</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"</span><span class="se">\n</span><span class="s">RECONSTRUCTION CHECK: </span><span class="si">{</span><span class="s">'✓ PASS'</span> <span class="k">if</span> <span class="n">reconstructed</span> <span class="o">==</span> <span class="n">text</span> <span class="k">else</span> <span class="s">'✗ FAIL'</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">matches</span>
</code></pre></div></div>

<hr />

<h3 id="-real-world-test-cases">🧪 Real-World Test Cases</h3>

<p>Below are <strong>diverse examples</strong> — from contractions to Unicode scripts, punctuation to code.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">test_cases</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">(</span><span class="s">"I can't believe it's working!"</span><span class="p">,</span> <span class="s">"Basic contractions"</span><span class="p">),</span>
    <span class="p">(</span><span class="s">"You're right, they'll see we've done it."</span><span class="p">,</span> <span class="s">"Multiple contractions"</span><span class="p">),</span>
    <span class="p">(</span><span class="s">"Hello 世界! Café français 🌍"</span><span class="p">,</span> <span class="s">"Unicode letters and emoji"</span><span class="p">),</span>
    <span class="p">(</span><span class="s">"I have 5 cats, ٧ dogs, and Ⅲ birds."</span><span class="p">,</span> <span class="s">"Various number systems"</span><span class="p">),</span>
    <span class="p">(</span><span class="s">"Wait... What?!? $100.50 (seriously)!!!"</span><span class="p">,</span> <span class="s">"Complex punctuation"</span><span class="p">),</span>
    <span class="p">(</span><span class="s">"  Multiple   spaces   everywhere  "</span><span class="p">,</span> <span class="s">"Multiple spaces"</span><span class="p">),</span>
    <span class="p">(</span><span class="s">"She's got $1,000.50 in café № ٧... Amazing!!! 🚀"</span><span class="p">,</span> <span class="s">"Complex mixed text"</span><span class="p">),</span>
    <span class="p">(</span><span class="s">"'s'd'm't'll've're"</span><span class="p">,</span> <span class="s">"Contraction edge cases"</span><span class="p">),</span>
    <span class="p">(</span><span class="s">"!@#$%^&amp;*()_+-=[]{}|;:"</span><span class="p">,.</span><span class="o">/&lt;&gt;</span><span class="err">?</span><span class="s">", "</span><span class="n">Pure</span> <span class="n">punctuation</span><span class="s">"),
    ("</span>   \<span class="n">t</span>\<span class="n">n</span>  <span class="s">", "</span><span class="n">Pure</span> <span class="n">whitespace</span><span class="s">"),
    ("", "</span><span class="n">Empty</span> <span class="n">string</span><span class="s">"),
    ("</span><span class="n">a</span> <span class="mi">1</span> <span class="err">!</span> <span class="s">'", "Single characters"),
    ("我有3只猫，很可爱！", "Chinese with numbers"),
    ("مرحبا بالعالم ١٢٣", "Arabic text with numbers"),
    ("def hello_world(): return '</span><span class="n">Hello</span><span class="p">,</span> <span class="n">World</span><span class="err">!</span><span class="s">'", "Code-like text"),
    ("Visit https://example.com or email test@domain.co.uk", "URLs and emails"),
]

for text, description in test_cases:
    test_regex(text, description)
</span></code></pre></div></div>

<p>Running these cases produces token lists that <strong>perfectly reconstruct the original text</strong>, see the test results below:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    ============================================================
    TEST: Basic contractions
    INPUT: 'I can't believe it's working!'
    ============================================================
    TOKENS (8):
       1: 'I'
       2: ' can'
       3: "'t"
       4: ' believe'
       5: ' it'
       6: "'s"
       7: ' working'
       8: '!'
    
    RECONSTRUCTION CHECK: ✓ PASS
    
    ============================================================
    TEST: Multiple contractions
    INPUT: 'You're right, they'll see we've done it.'
    ============================================================
    TOKENS (12):
       1: 'You'
       2: "'re"
       3: ' right'
       4: ','
       5: ' they'
       6: "'ll"
       7: ' see'
       8: ' we'
       9: "'ve"
      10: ' done'
      11: ' it'
      12: '.'
    
    RECONSTRUCTION CHECK: ✓ PASS
    
    ============================================================
    TEST: Unicode letters and emoji
    INPUT: 'Hello 世界! Café français 🌍'
    ============================================================
    TOKENS (6):
       1: 'Hello'
       2: ' 世界'
       3: '!'
       4: ' Café'
       5: ' français'
       6: ' 🌍'
    
    RECONSTRUCTION CHECK: ✓ PASS
    
    ============================================================
    TEST: Various number systems
    INPUT: 'I have 5 cats, ٧ dogs, and Ⅲ birds.'
    ============================================================
    TOKENS (12):
       1: 'I'
       2: ' have'
       3: ' 5'
       4: ' cats'
       5: ','
       6: ' ٧'
       7: ' dogs'
       8: ','
       9: ' and'
      10: ' Ⅲ'
      11: ' birds'
      12: '.'
    
    RECONSTRUCTION CHECK: ✓ PASS
    
    ============================================================
    TEST: Complex punctuation
    INPUT: 'Wait... What?!? $100.50 (seriously)!!!'
    ============================================================
    TOKENS (11):
       1: 'Wait'
       2: '...'
       3: ' What'
       4: '?!?'
       5: ' $'
       6: '100'
       7: '.'
       8: '50'
       9: ' ('
      10: 'seriously'
      11: ')!!!'
    
    RECONSTRUCTION CHECK: ✓ PASS
    
    ============================================================
    TEST: Multiple spaces
    INPUT: '  Multiple   spaces   everywhere  '
    ============================================================
    TOKENS (7):
       1: ' '
       2: ' Multiple'
       3: '  '
       4: ' spaces'
       5: '  '
       6: ' everywhere'
       7: '  '
    
    RECONSTRUCTION CHECK: ✓ PASS
    
    ============================================================
    TEST: Complex mixed text
    INPUT: 'She's got $1,000.50 in café № ٧... Amazing!!! 🚀'
    ============================================================
    TOKENS (17):
       1: 'She'
       2: "'s"
       3: ' got'
       4: ' $'
       5: '1'
       6: ','
       7: '000'
       8: '.'
       9: '50'
      10: ' in'
      11: ' café'
      12: ' №'
      13: ' ٧'
      14: '...'
      15: ' Amazing'
      16: '!!!'
      17: ' 🚀'
    
    RECONSTRUCTION CHECK: ✓ PASS
    
    ============================================================
    TEST: Contraction edge cases
    INPUT: ''s'd'm't'll've're'
    ============================================================
    TOKENS (7):
       1: "'s"
       2: "'d"
       3: "'m"
       4: "'t"
       5: "'ll"
       6: "'ve"
       7: "'re"
    
    RECONSTRUCTION CHECK: ✓ PASS
    
    ============================================================
    TEST: Pure punctuation
    INPUT: '!@#$%^&amp;*()_+-=[]{}|;:",./&lt;&gt;?'
    ============================================================
    TOKENS (1):
       1: '!@#$%^&amp;*()_+-=[]{}|;:",./&lt;&gt;?'
    
    RECONSTRUCTION CHECK: ✓ PASS
    
    ============================================================
    TEST: Pure whitespace
    INPUT: '   	
      '
    ============================================================
    TOKENS (1):
       1: '   \t\n  '
    
    RECONSTRUCTION CHECK: ✓ PASS
    
    ============================================================
    TEST: Empty string
    INPUT: ''
    ============================================================
    TOKENS (0):
    
    RECONSTRUCTION CHECK: ✓ PASS
    
    ============================================================
    TEST: Single characters
    INPUT: 'a 1 ! ''
    ============================================================
    TOKENS (4):
       1: 'a'
       2: ' 1'
       3: ' !'
       4: " '"
    
    RECONSTRUCTION CHECK: ✓ PASS
    
    ============================================================
    TEST: Chinese with numbers
    INPUT: '我有3只猫，很可爱！'
    ============================================================
    TOKENS (6):
       1: '我有'
       2: '3'
       3: '只猫'
       4: '，'
       5: '很可爱'
       6: '！'
    
    RECONSTRUCTION CHECK: ✓ PASS
    
    ============================================================
    TEST: Arabic text with numbers
    INPUT: 'مرحبا بالعالم ١٢٣'
    ============================================================
    TOKENS (3):
       1: 'مرحبا'
       2: ' بالعالم'
       3: ' ١٢٣'
    
    RECONSTRUCTION CHECK: ✓ PASS
    
    ============================================================
    TEST: Code-like text
    INPUT: 'def hello_world(): return 'Hello, World!''
    ============================================================
    TOKENS (11):
       1: 'def'
       2: ' hello'
       3: '_'
       4: 'world'
       5: '():'
       6: ' return'
       7: " '"
       8: 'Hello'
       9: ','
      10: ' World'
      11: "!'"
    
    RECONSTRUCTION CHECK: ✓ PASS
    
    ============================================================
    TEST: URLs and emails
    INPUT: 'Visit https://example.com or email test@domain.co.uk'
    ============================================================
    TOKENS (15):
       1: 'Visit'
       2: ' https'
       3: '://'
       4: 'example'
       5: '.'
       6: 'com'
       7: ' or'
       8: ' email'
       9: ' test'
      10: '@'
      11: 'domain'
      12: '.'
      13: 'co'
      14: '.'
      15: 'uk'
    
    RECONSTRUCTION CHECK: ✓ PASS
</code></pre></div></div>

<hr />

<h3 id="-why-this-matters">💡 Why This Matters</h3>

<p>BPE pre-tokenization <strong>sets the stage</strong> for the tokenizer’s merge rules to apply. A well-designed split pattern ensures:</p>

<ul>
  <li><strong>Language independence</strong> — Works with English, Arabic, Chinese, emoji, etc.</li>
  <li><strong>Symbol awareness</strong> — Keeps punctuation and symbols intact.</li>
  <li><strong>Whitespace fidelity</strong> — Preserves exact spacing for reversible tokenization.</li>
  <li><strong>Downstream accuracy</strong> — Reduces surprises during model training or inference.</li>
</ul>

<hr />

<h3 id="-takeaways">🚀 Takeaways</h3>

<ul>
  <li>This regex is a <strong>core building block</strong> of GPT-2’s tokenization process.</li>
  <li>It’s <strong>language-agnostic</strong>, <strong>Unicode-friendly</strong>, and <strong>precise</strong> in splitting.</li>
  <li>Understanding it helps when <strong>building custom tokenizers</strong> or adapting GPT-2 BPE to new domains.</li>
</ul>

<p>If you’re working with <strong>LLMs, tokenization, or multilingual NLP</strong>, knowing the details behind this pattern will help you <strong>debug</strong>, <strong>customize</strong>, and <strong>optimize</strong> your preprocessing pipeline.</p>]]></content><author><name>[&quot;Han Yu&quot;]</name></author><category term="cs336" /><summary type="html"><![CDATA[Demystifying GPT-2’s Pre-Tokenization: How One Regex Pattern Handles the World’s Languages]]></summary></entry><entry><title type="html">Study Notes: Stanford CS336 Language Modeling from Scratch [3]</title><link href="http://localhost:4000/cs336/2025/07/26/cs336-note-train-bpe-tinystories.html" rel="alternate" type="text/html" title="Study Notes: Stanford CS336 Language Modeling from Scratch [3]" /><published>2025-07-26T00:00:00-07:00</published><updated>2025-07-26T00:00:00-07:00</updated><id>http://localhost:4000/cs336/2025/07/26/cs336-note-train-bpe-tinystories</id><content type="html" xml:base="http://localhost:4000/cs336/2025/07/26/cs336-note-train-bpe-tinystories.html"><![CDATA[<h1 id="building-a-bpe-tokenizer-from-scratch-train-the-tokenizer-using-tinystories-dataset">Building a BPE Tokenizer from Scratch: Train the Tokenizer using TinyStories Dataset</h1>

<p>Ever wondered how modern language models like GPT break down text into tokens? In this note, I will share how to build a Byte Pair Encoding (BPE) tokenizer from scratch and train it on the <a href="https://arxiv.org/abs/2305.07759">TinyStories Dataset</a>. We will see how BPE achieves impressive compression ratios.</p>

<h2 id="what-is-bpe-tokenization">What is BPE Tokenization?</h2>

<p>Byte Pair Encoding (BPE) is a compression algorithm that’s become the backbone of modern tokenization. Here’s how it works:</p>

<ol>
  <li><strong>Start with bytes</strong>: Every character becomes its byte representation (0-255)</li>
  <li><strong>Find frequent pairs</strong>: Look for the most common pair of adjacent tokens</li>
  <li><strong>Merge and repeat</strong>: Replace the most frequent pair with a new token, then repeat</li>
</ol>

<h3 id="a-simple-example">A Simple Example</h3>

<p>Let’s say we have the word “hello” appearing many times in our text:</p>
<ul>
  <li>Initially: <code class="language-plaintext highlighter-rouge">h-e-l-l-o</code> (5 tokens)</li>
  <li>If “l-l” is the most frequent pair, merge it: <code class="language-plaintext highlighter-rouge">h-e-ll-o</code> (4 tokens)</li>
  <li>If “e-ll” becomes frequent, merge it: <code class="language-plaintext highlighter-rouge">h-ell-o</code> (3 tokens)</li>
</ul>

<p>This process creates a vocabulary that efficiently represents common patterns in your text. Check out <a href="https://bearbearyu1223.github.io/cs336/2025/07/22/cs336-note-simple-bpe.html">my previous post</a> for a brief introduction.</p>

<h2 id="the-tinystories-dataset">The TinyStories Dataset</h2>

<p>We’ll train our tokenizer on <a href="https://arxiv.org/abs/2305.07759">TinyStories</a>, a fascinating dataset of short stories written using only words that 3-4 year olds typically understand. These stories were generated by GPT-3.5 and GPT-4, making them perfect for experimenting with tokenization.</p>

<h3 id="downloading-the-data">Downloading the Data</h3>

<p>First, let’s download the TinyStories froom Huggingface:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="err">!</span><span class="n">mkdir</span> <span class="o">-</span><span class="n">p</span> <span class="n">data</span>
<span class="err">!</span><span class="n">cd</span> <span class="n">data</span>

<span class="err">!</span><span class="n">wget</span> <span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">huggingface</span><span class="p">.</span><span class="n">co</span><span class="o">/</span><span class="n">datasets</span><span class="o">/</span><span class="n">roneneldan</span><span class="o">/</span><span class="n">TinyStories</span><span class="o">/</span><span class="n">resolve</span><span class="o">/</span><span class="n">main</span><span class="o">/</span><span class="n">TinyStoriesV2</span><span class="o">-</span><span class="n">GPT4</span><span class="o">-</span><span class="n">train</span><span class="p">.</span><span class="n">txt</span>
<span class="err">!</span><span class="n">wget</span> <span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">huggingface</span><span class="p">.</span><span class="n">co</span><span class="o">/</span><span class="n">datasets</span><span class="o">/</span><span class="n">roneneldan</span><span class="o">/</span><span class="n">TinyStories</span><span class="o">/</span><span class="n">resolve</span><span class="o">/</span><span class="n">main</span><span class="o">/</span><span class="n">TinyStoriesV2</span><span class="o">-</span><span class="n">GPT4</span><span class="o">-</span><span class="n">valid</span><span class="p">.</span><span class="n">txt</span>

<span class="err">!</span><span class="n">wget</span> <span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">huggingface</span><span class="p">.</span><span class="n">co</span><span class="o">/</span><span class="n">datasets</span><span class="o">/</span><span class="n">stanford</span><span class="o">-</span><span class="n">cs336</span><span class="o">/</span><span class="n">owt</span><span class="o">-</span><span class="n">sample</span><span class="o">/</span><span class="n">resolve</span><span class="o">/</span><span class="n">main</span><span class="o">/</span><span class="n">owt_train</span><span class="p">.</span><span class="n">txt</span><span class="p">.</span><span class="n">gz</span>
<span class="err">!</span><span class="n">gunzip</span> <span class="n">owt_train</span><span class="p">.</span><span class="n">txt</span><span class="p">.</span><span class="n">gz</span>
<span class="err">!</span><span class="n">wget</span> <span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">huggingface</span><span class="p">.</span><span class="n">co</span><span class="o">/</span><span class="n">datasets</span><span class="o">/</span><span class="n">stanford</span><span class="o">-</span><span class="n">cs336</span><span class="o">/</span><span class="n">owt</span><span class="o">-</span><span class="n">sample</span><span class="o">/</span><span class="n">resolve</span><span class="o">/</span><span class="n">main</span><span class="o">/</span><span class="n">owt_valid</span><span class="p">.</span><span class="n">txt</span><span class="p">.</span><span class="n">gz</span>
<span class="err">!</span><span class="n">gunzip</span> <span class="n">owt_valid</span><span class="p">.</span><span class="n">txt</span><span class="p">.</span><span class="n">gz</span>

<span class="err">!</span><span class="n">cd</span> <span class="p">..</span>
</code></pre></div></div>

<h2 id="challenge-parallelizing-pre-tokenization">Challenge: Parallelizing Pre-tokenization</h2>

<p>The TinyStories dataset is big (over 2GB), which presents a challenge for tokenizer training. We need to:</p>
<ol>
  <li>Process the file in parallel for speed</li>
  <li>Ensure we don’t split tokens incorrectly at chunk boundaries</li>
</ol>

<h3 id="solution-smart-chunking-with-special-tokens">Solution: Smart Chunking with Special Tokens</h3>

<p>Our solution uses special tokens (like <code class="language-plaintext highlighter-rouge">&lt;|endoftext|&gt;</code>) as natural boundaries for splitting the file.</p>

<p><strong>Simple Example</strong>: Let’s say we have a text file containing: “Hello<code class="language-plaintext highlighter-rouge">&lt;SPLIT&gt;</code>World<code class="language-plaintext highlighter-rouge">&lt;SPLIT&gt;</code>How<code class="language-plaintext highlighter-rouge">&lt;SPLIT&gt;</code>Are<code class="language-plaintext highlighter-rouge">&lt;SPLIT&gt;</code>You”, special split token is <code class="language-plaintext highlighter-rouge">&lt;SPLIT&gt;</code>, and we want to divide the text into 3 chunks.</p>

<p>Here’s one implementation for intelligent file chunking as shared in the <a href="https://github.com/stanford-cs336/assignment1-basics/blob/main/cs336_basics/pretokenization_example.py">cs336 lecture notes</a>:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">BinaryIO</span>

<span class="k">def</span> <span class="nf">find_chunk_boundaries</span><span class="p">(</span> <span class="nb">file</span><span class="p">:</span> <span class="n">BinaryIO</span><span class="p">,</span>
      <span class="n">desired_num_chunks</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
      <span class="n">split_special_token</span><span class="p">:</span> <span class="nb">bytes</span><span class="p">)</span><span class="o">-&gt;</span><span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">]:</span>
  <span class="s">"""
  Chunk the file into parts that can be counted independently.
  May return fewer chunks if the boundaries end up overlapping.
  """</span>
  <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">split_special_token</span><span class="p">,</span> <span class="nb">bytes</span><span class="p">),(</span>
      <span class="s">"Must represent special token as a bytestring"</span>
  <span class="p">)</span>

  <span class="c1"># Get total file size in bytes
</span>  <span class="nb">file</span><span class="p">.</span><span class="n">seek</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">os</span><span class="p">.</span><span class="n">SEEK_END</span><span class="p">)</span>
  <span class="n">file_size</span> <span class="o">=</span> <span class="nb">file</span><span class="p">.</span><span class="n">tell</span><span class="p">()</span>
  <span class="nb">file</span><span class="p">.</span><span class="n">seek</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

  <span class="n">chunk_size</span> <span class="o">=</span> <span class="n">file_size</span> <span class="o">//</span> <span class="n">desired_num_chunks</span>

  <span class="c1"># Initial guesses for chunk boundary locations, uniformly spaced
</span>  <span class="c1"># Chunks start on previous index, don't include last index
</span>  <span class="n">chunk_boundaries</span> <span class="o">=</span> <span class="p">[</span><span class="n">i</span> <span class="o">*</span> <span class="n">chunk_size</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">desired_num_chunks</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)]</span>
  <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Initial guess of the chunk boundaries: </span><span class="si">{</span><span class="n">chunk_boundaries</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
  <span class="n">chunk_boundaries</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">file_size</span>

  <span class="n">mini_chunk_size</span> <span class="o">=</span> <span class="mi">4096</span>  <span class="c1"># Read ahead by 4k bytes at a time
</span>
  <span class="k">for</span> <span class="n">bi</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">chunk_boundaries</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>
      <span class="n">initial_position</span> <span class="o">=</span> <span class="n">chunk_boundaries</span><span class="p">[</span><span class="n">bi</span><span class="p">]</span>
      <span class="nb">file</span><span class="p">.</span><span class="n">seek</span><span class="p">(</span><span class="n">initial_position</span><span class="p">)</span>  <span class="c1"># Start at boundary guess
</span>      <span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
          <span class="n">mini_chunk</span> <span class="o">=</span> <span class="nb">file</span><span class="p">.</span><span class="n">read</span><span class="p">(</span><span class="n">mini_chunk_size</span><span class="p">)</span>  <span class="c1"># Read a mini chunk
</span>
          <span class="c1"># If EOF, this boundary should be at the end of the file
</span>          <span class="k">if</span> <span class="n">mini_chunk</span> <span class="o">==</span> <span class="sa">b</span><span class="s">""</span><span class="p">:</span>
              <span class="n">chunk_boundaries</span><span class="p">[</span><span class="n">bi</span><span class="p">]</span> <span class="o">=</span> <span class="n">file_size</span>
              <span class="k">break</span>

          <span class="c1"># Find the special token in the mini chunk
</span>          <span class="n">found_at</span> <span class="o">=</span> <span class="n">mini_chunk</span><span class="p">.</span><span class="n">find</span><span class="p">(</span><span class="n">split_special_token</span><span class="p">)</span>
          <span class="k">if</span> <span class="n">found_at</span> <span class="o">!=</span> <span class="o">-</span><span class="mi">1</span><span class="p">:</span>
              <span class="n">chunk_boundaries</span><span class="p">[</span><span class="n">bi</span><span class="p">]</span> <span class="o">=</span> <span class="n">initial_position</span> <span class="o">+</span> <span class="n">found_at</span>
              <span class="k">break</span>
          <span class="n">initial_position</span> <span class="o">+=</span> <span class="n">mini_chunk_size</span>

  <span class="c1"># Make sure all boundaries are unique, but might be fewer than desired_num_chunks
</span>  <span class="k">return</span> <span class="nb">sorted</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">chunk_boundaries</span><span class="p">))</span>
</code></pre></div></div>

<h3 id="testing-our-chunking-algorithm">Testing Our Chunking Algorithm</h3>

<p>Let’s see how this works with a concrete example:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">io</span>
<span class="k">def</span> <span class="nf">demonstrate_chunk_boundaries</span><span class="p">():</span>
    <span class="s">"""Demonstrate how to use find_chunk_boundaries with a practical example."""</span>

    <span class="c1"># Create sample data - our example text
</span>    <span class="n">sample_text</span> <span class="o">=</span> <span class="s">"Hello&lt;SPLIT&gt;World&lt;SPLIT&gt;How&lt;SPLIT&gt;Are&lt;SPLIT&gt;You"</span>
    <span class="n">sample_bytes</span> <span class="o">=</span> <span class="n">sample_text</span><span class="p">.</span><span class="n">encode</span><span class="p">(</span><span class="s">'utf-8'</span><span class="p">)</span>

    <span class="k">print</span><span class="p">(</span><span class="s">"=== Original Data ==="</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Text: </span><span class="si">{</span><span class="n">sample_text</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Bytes: </span><span class="si">{</span><span class="n">sample_bytes</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Total size: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">sample_bytes</span><span class="p">)</span><span class="si">}</span><span class="s"> bytes"</span><span class="p">)</span>
    <span class="k">print</span><span class="p">()</span>

    <span class="c1"># Create a file-like object from our sample data
</span>    <span class="n">file_obj</span> <span class="o">=</span> <span class="n">io</span><span class="p">.</span><span class="n">BytesIO</span><span class="p">(</span><span class="n">sample_bytes</span><span class="p">)</span>

    <span class="c1"># Define our split token
</span>    <span class="n">split_token</span> <span class="o">=</span> <span class="sa">b</span><span class="s">"&lt;SPLIT&gt;"</span>
    <span class="n">desired_chunks</span> <span class="o">=</span> <span class="mi">3</span>

    <span class="k">print</span><span class="p">(</span><span class="s">"=== Finding Chunk Boundaries ==="</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Desired number of chunks: </span><span class="si">{</span><span class="n">desired_chunks</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Split token: </span><span class="si">{</span><span class="n">split_token</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
    <span class="k">print</span><span class="p">()</span>

    <span class="c1"># Find the chunk boundaries
</span>    <span class="n">boundaries</span> <span class="o">=</span> <span class="n">find_chunk_boundaries</span><span class="p">(</span><span class="n">file_obj</span><span class="p">,</span> <span class="n">desired_chunks</span><span class="p">,</span> <span class="n">split_token</span><span class="p">)</span>

    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Final boundaries: </span><span class="si">{</span><span class="n">boundaries</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Number of chunks created: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">boundaries</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
    <span class="k">print</span><span class="p">()</span>

    <span class="c1"># Demonstrate how to use the boundaries to read chunks
</span>    <span class="k">print</span><span class="p">(</span><span class="s">"=== Reading Chunks ==="</span><span class="p">)</span>
    <span class="n">file_obj</span><span class="p">.</span><span class="n">seek</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># Reset file pointer
</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">boundaries</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>
        <span class="n">start_pos</span> <span class="o">=</span> <span class="n">boundaries</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
        <span class="n">end_pos</span> <span class="o">=</span> <span class="n">boundaries</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span>
        <span class="n">chunk_size</span> <span class="o">=</span> <span class="n">end_pos</span> <span class="o">-</span> <span class="n">start_pos</span>

        <span class="c1"># Read the chunk
</span>        <span class="n">file_obj</span><span class="p">.</span><span class="n">seek</span><span class="p">(</span><span class="n">start_pos</span><span class="p">)</span>
        <span class="n">chunk_data</span> <span class="o">=</span> <span class="n">file_obj</span><span class="p">.</span><span class="n">read</span><span class="p">(</span><span class="n">chunk_size</span><span class="p">)</span>
        <span class="n">chunk_text</span> <span class="o">=</span> <span class="n">chunk_data</span><span class="p">.</span><span class="n">decode</span><span class="p">(</span><span class="s">'utf-8'</span><span class="p">)</span>

        <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Chunk </span><span class="si">{</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="si">}</span><span class="s">:"</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"  Position: bytes </span><span class="si">{</span><span class="n">start_pos</span><span class="si">}</span><span class="s">-</span><span class="si">{</span><span class="n">end_pos</span><span class="o">-</span><span class="mi">1</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"  Size: </span><span class="si">{</span><span class="n">chunk_size</span><span class="si">}</span><span class="s"> bytes"</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"  Content: '</span><span class="si">{</span><span class="n">chunk_text</span><span class="si">}</span><span class="s">'"</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"  Raw bytes: </span><span class="si">{</span><span class="n">chunk_data</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
        <span class="k">print</span><span class="p">()</span>
</code></pre></div></div>

<p>Running this demonstration:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">demonstrate_chunk_boundaries</span><span class="p">()</span>
</code></pre></div></div>

<p><strong>Output:</strong></p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>=== Original Data ===
Text: Hello&lt;SPLIT&gt;World&lt;SPLIT&gt;How&lt;SPLIT&gt;Are&lt;SPLIT&gt;You
Bytes: b'Hello&lt;SPLIT&gt;World&lt;SPLIT&gt;How&lt;SPLIT&gt;Are&lt;SPLIT&gt;You'
Total size: 47 bytes

=== Finding Chunk Boundaries ===
Desired number of chunks: 3
Split token: b'&lt;SPLIT&gt;'

Initial guess of the chunk boundaries: [0, 15, 30, 45]
Final boundaries: [0, 17, 37, 47]
Number of chunks created: 3

=== Reading Chunks ===
Chunk 1:
  Position: bytes 0-16
  Size: 17 bytes
  Content: 'Hello&lt;SPLIT&gt;World'
  Raw bytes: b'Hello&lt;SPLIT&gt;World'

Chunk 2:
  Position: bytes 17-36
  Size: 20 bytes
  Content: '&lt;SPLIT&gt;How&lt;SPLIT&gt;Are'
  Raw bytes: b'&lt;SPLIT&gt;How&lt;SPLIT&gt;Are'

Chunk 3:
  Position: bytes 37-46
  Size: 10 bytes
  Content: '&lt;SPLIT&gt;You'
  Raw bytes: b'&lt;SPLIT&gt;You'
</code></pre></div></div>

<p>Notice how the algorithm automatically adjusted the boundaries to align with <code class="language-plaintext highlighter-rouge">&lt;SPLIT&gt;</code> tokens, ensuring clean chunk separation.</p>

<h2 id="bpe-training-implementation">BPE Training Implementation</h2>

<p>Now implement the core BPE training algorithm. The implementation shared here handles parallel processing, special tokens, and efficient pair counting.</p>

<h3 id="core-training-function">Core Training Function</h3>

<p>Here’s is my complete BPE training implementation:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">re</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">multiprocessing</span> <span class="k">as</span> <span class="n">mp</span>
<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">defaultdict</span><span class="p">,</span> <span class="n">Counter</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">List</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">,</span> <span class="n">BinaryIO</span>

<span class="c1"># Simplified GPT-2-style regex pattern for pre-tokenization (using standard re module)
</span><span class="n">GPT2_SPLIT_PATTERN</span> <span class="o">=</span> <span class="sa">r</span><span class="s">"""'(?:[sdmt]|ll|ve|re)| ?[a-zA-ZÀ-ÿ]+| ?[0-9]+| ?[^\s\w]+|\s+(?!\S)|\s+"""</span>

<span class="k">def</span> <span class="nf">process_chunk</span><span class="p">(</span><span class="n">args</span><span class="p">):</span>
    <span class="s">"""Process a chunk of the file and return word counts."""</span>
    <span class="n">start</span><span class="p">,</span> <span class="n">end</span><span class="p">,</span> <span class="n">input_path</span><span class="p">,</span> <span class="n">special_tokens</span> <span class="o">=</span> <span class="n">args</span>
    <span class="n">word_counts</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>

    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">input_path</span><span class="p">,</span> <span class="s">'rb'</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="n">f</span><span class="p">.</span><span class="n">seek</span><span class="p">(</span><span class="n">start</span><span class="p">)</span>
        <span class="n">chunk</span> <span class="o">=</span> <span class="n">f</span><span class="p">.</span><span class="n">read</span><span class="p">(</span><span class="n">end</span> <span class="o">-</span> <span class="n">start</span><span class="p">).</span><span class="n">decode</span><span class="p">(</span><span class="s">'utf-8'</span><span class="p">,</span> <span class="n">errors</span><span class="o">=</span><span class="s">'ignore'</span><span class="p">)</span>

        <span class="c1"># Split on special tokens to prevent merging across boundaries
</span>        <span class="k">if</span> <span class="n">special_tokens</span><span class="p">:</span>
            <span class="n">pattern</span> <span class="o">=</span> <span class="s">'|'</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">re</span><span class="p">.</span><span class="n">escape</span><span class="p">(</span><span class="n">token</span><span class="p">)</span> <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">special_tokens</span><span class="p">)</span>
            <span class="n">text_segments</span> <span class="o">=</span> <span class="n">re</span><span class="p">.</span><span class="n">split</span><span class="p">(</span><span class="sa">f</span><span class="s">'(</span><span class="si">{</span><span class="n">pattern</span><span class="si">}</span><span class="s">)'</span><span class="p">,</span> <span class="n">chunk</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">text_segments</span> <span class="o">=</span> <span class="p">[</span><span class="n">chunk</span><span class="p">]</span>

        <span class="k">for</span> <span class="n">segment</span> <span class="ow">in</span> <span class="n">text_segments</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">segment</span> <span class="ow">in</span> <span class="n">special_tokens</span><span class="p">:</span>
                <span class="k">continue</span>  <span class="c1"># Skip special tokens during counting
</span>
            <span class="c1"># Apply GPT-2 regex pattern
</span>            <span class="k">for</span> <span class="n">match</span> <span class="ow">in</span> <span class="n">re</span><span class="p">.</span><span class="n">finditer</span><span class="p">(</span><span class="n">GPT2_SPLIT_PATTERN</span><span class="p">,</span> <span class="n">segment</span><span class="p">):</span>
                <span class="n">token_text</span> <span class="o">=</span> <span class="n">match</span><span class="p">.</span><span class="n">group</span><span class="p">()</span>
                <span class="n">token_bytes</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">token_text</span><span class="p">.</span><span class="n">encode</span><span class="p">(</span><span class="s">'utf-8'</span><span class="p">))</span>
                <span class="n">word_counts</span><span class="p">[</span><span class="n">token_bytes</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>

    <span class="k">return</span> <span class="n">word_counts</span>


<span class="k">def</span> <span class="nf">train_bpe_tokenizer</span><span class="p">(</span><span class="n">input_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">special_tokens</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="n">verbose</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="nb">dict</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">bytes</span><span class="p">],</span> <span class="nb">list</span><span class="p">[</span><span class="nb">tuple</span><span class="p">[</span><span class="nb">bytes</span><span class="p">,</span> <span class="nb">bytes</span><span class="p">]]]:</span>
    <span class="s">"""
    Train a byte-level Byte Pair Encoding (BPE) tokenizer from a text file.

    Args:
        input_path: Path to the input text file containing training data
        vocab_size: Maximum size of the final vocabulary (includes initial bytes + special tokens + merges)
        special_tokens: List of special token strings to include in vocabulary
        verbose: Whether to print training progress information

    Returns:
        vocab: Complete tokenizer vocabulary mapping token IDs to byte sequences
        merges: Ordered list of BPE merge operations performed during training
    """</span>
    <span class="kn">import</span> <span class="nn">time</span>

    <span class="c1"># Initialize vocabulary with bytes 0-255
</span>    <span class="n">vocab</span> <span class="o">=</span> <span class="p">{</span><span class="n">i</span><span class="p">:</span> <span class="nb">bytes</span><span class="p">([</span><span class="n">i</span><span class="p">])</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">256</span><span class="p">)}</span>
    <span class="n">next_id</span> <span class="o">=</span> <span class="mi">256</span>

    <span class="c1"># Add special tokens to vocabulary
</span>    <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">special_tokens</span><span class="p">:</span>
        <span class="n">token_bytes</span> <span class="o">=</span> <span class="n">token</span><span class="p">.</span><span class="n">encode</span><span class="p">(</span><span class="s">'utf-8'</span><span class="p">)</span>
        <span class="n">vocab</span><span class="p">[</span><span class="n">next_id</span><span class="p">]</span> <span class="o">=</span> <span class="n">token_bytes</span>
        <span class="n">next_id</span> <span class="o">+=</span> <span class="mi">1</span>

    <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
        <span class="k">print</span><span class="p">(</span><span class="s">"Step 1: Setting up parallel processing..."</span><span class="p">)</span>

    <span class="c1"># Get chunk boundaries for multiprocessing
</span>    <span class="n">num_processes</span> <span class="o">=</span> <span class="n">mp</span><span class="p">.</span><span class="n">cpu_count</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
        <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Using </span><span class="si">{</span><span class="n">num_processes</span><span class="si">}</span><span class="s"> processes for parallel tokenization"</span><span class="p">)</span>

    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">input_path</span><span class="p">,</span> <span class="s">'rb'</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">special_tokens</span><span class="p">:</span>
            <span class="c1"># Use first special token for chunking boundaries
</span>            <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
                <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Finding chunk boundaries aligned with special token: </span><span class="si">{</span><span class="n">special_tokens</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
            <span class="n">boundaries</span> <span class="o">=</span> <span class="n">find_chunk_boundaries</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">num_processes</span><span class="p">,</span> <span class="n">special_tokens</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">encode</span><span class="p">(</span><span class="s">'utf-8'</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Use simple chunking without special token alignment
</span>            <span class="n">f</span><span class="p">.</span><span class="n">seek</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">os</span><span class="p">.</span><span class="n">SEEK_END</span><span class="p">)</span>
            <span class="n">file_size</span> <span class="o">=</span> <span class="n">f</span><span class="p">.</span><span class="n">tell</span><span class="p">()</span>
            <span class="n">chunk_size</span> <span class="o">=</span> <span class="n">file_size</span> <span class="o">//</span> <span class="n">num_processes</span>
            <span class="n">boundaries</span> <span class="o">=</span> <span class="p">[</span><span class="n">i</span> <span class="o">*</span> <span class="n">chunk_size</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_processes</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)]</span>
            <span class="n">boundaries</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">file_size</span>
            <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
                <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"File size: </span><span class="si">{</span><span class="n">file_size</span><span class="si">:</span><span class="p">,</span><span class="si">}</span><span class="s"> bytes, chunk size: </span><span class="si">{</span><span class="n">chunk_size</span><span class="si">:</span><span class="p">,</span><span class="si">}</span><span class="s"> bytes"</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
        <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Created </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">boundaries</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="si">}</span><span class="s"> chunks for processing"</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="s">"</span><span class="se">\n</span><span class="s">Step 2: Pre-tokenizing text corpus..."</span><span class="p">)</span>

    <span class="c1"># Process chunks in parallel
</span>    <span class="n">chunk_args</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">start</span><span class="p">,</span> <span class="n">end</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">boundaries</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">boundaries</span><span class="p">[</span><span class="mi">1</span><span class="p">:]):</span>
        <span class="n">chunk_args</span><span class="p">.</span><span class="n">append</span><span class="p">((</span><span class="n">start</span><span class="p">,</span> <span class="n">end</span><span class="p">,</span> <span class="n">input_path</span><span class="p">,</span> <span class="n">special_tokens</span><span class="p">))</span>

    <span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="n">time</span><span class="p">()</span>
    <span class="k">with</span> <span class="n">mp</span><span class="p">.</span><span class="n">Pool</span><span class="p">(</span><span class="n">processes</span><span class="o">=</span><span class="n">num_processes</span><span class="p">)</span> <span class="k">as</span> <span class="n">pool</span><span class="p">:</span>
        <span class="n">chunk_results</span> <span class="o">=</span> <span class="n">pool</span><span class="p">.</span><span class="nb">map</span><span class="p">(</span><span class="n">process_chunk</span><span class="p">,</span> <span class="n">chunk_args</span><span class="p">)</span>
    <span class="n">tokenization_time</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start_time</span>

    <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
        <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Pre-tokenization completed in </span><span class="si">{</span><span class="n">tokenization_time</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s"> seconds"</span><span class="p">)</span>

    <span class="c1"># Merge results from all chunks
</span>    <span class="n">word_counts</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
    <span class="n">total_tokens</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">chunk_result</span> <span class="ow">in</span> <span class="n">chunk_results</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">count</span> <span class="ow">in</span> <span class="n">chunk_result</span><span class="p">.</span><span class="n">items</span><span class="p">():</span>
            <span class="n">word_counts</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="o">+=</span> <span class="n">count</span>
            <span class="n">total_tokens</span> <span class="o">+=</span> <span class="n">count</span>

    <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
        <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Found </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">word_counts</span><span class="p">)</span><span class="si">:</span><span class="p">,</span><span class="si">}</span><span class="s"> unique word types"</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Total token count: </span><span class="si">{</span><span class="n">total_tokens</span><span class="si">:</span><span class="p">,</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Most common words:"</span><span class="p">)</span>
        <span class="n">sorted_words</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">word_counts</span><span class="p">.</span><span class="n">items</span><span class="p">(),</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">reverse</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">word_bytes</span><span class="p">,</span> <span class="n">count</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">sorted_words</span><span class="p">[:</span><span class="mi">5</span><span class="p">]):</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="n">word_str</span> <span class="o">=</span> <span class="nb">bytes</span><span class="p">(</span><span class="n">word_bytes</span><span class="p">).</span><span class="n">decode</span><span class="p">(</span><span class="s">'utf-8'</span><span class="p">,</span> <span class="n">errors</span><span class="o">=</span><span class="s">'replace'</span><span class="p">)</span>
                <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"  </span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s">. '</span><span class="si">{</span><span class="n">word_str</span><span class="si">}</span><span class="s">' -&gt; </span><span class="si">{</span><span class="n">count</span><span class="si">:</span><span class="p">,</span><span class="si">}</span><span class="s"> times"</span><span class="p">)</span>
            <span class="k">except</span><span class="p">:</span>
                <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"  </span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s">. </span><span class="si">{</span><span class="n">word_bytes</span><span class="si">}</span><span class="s"> -&gt; </span><span class="si">{</span><span class="n">count</span><span class="si">:</span><span class="p">,</span><span class="si">}</span><span class="s"> times"</span><span class="p">)</span>

    <span class="c1"># Convert to working format for BPE (list of byte values)
</span>    <span class="n">word_freq</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">for</span> <span class="n">word_bytes</span><span class="p">,</span> <span class="n">freq</span> <span class="ow">in</span> <span class="n">word_counts</span><span class="p">.</span><span class="n">items</span><span class="p">():</span>
        <span class="n">word_tokens</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">word_bytes</span><span class="p">)</span>  <span class="c1"># Convert to list of ints
</span>        <span class="n">word_freq</span><span class="p">[</span><span class="nb">tuple</span><span class="p">(</span><span class="n">word_tokens</span><span class="p">)]</span> <span class="o">=</span> <span class="n">freq</span>

    <span class="n">merges</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">pair_index</span> <span class="o">=</span> <span class="p">{}</span>  <span class="c1"># Efficient indexing for pair counting
</span>
    <span class="k">def</span> <span class="nf">update_pair_index</span><span class="p">(</span><span class="n">word_freq</span><span class="p">,</span> <span class="n">pair_index</span><span class="p">):</span>
        <span class="s">"""Update the pair index for efficient counting."""</span>
        <span class="n">pair_index</span><span class="p">.</span><span class="n">clear</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">freq</span> <span class="ow">in</span> <span class="n">word_freq</span><span class="p">.</span><span class="n">items</span><span class="p">():</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">word</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>
                <span class="n">pair</span> <span class="o">=</span> <span class="p">(</span><span class="n">word</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">word</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">])</span>
                <span class="k">if</span> <span class="n">pair</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">pair_index</span><span class="p">:</span>
                    <span class="n">pair_index</span><span class="p">[</span><span class="n">pair</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
                <span class="n">pair_index</span><span class="p">[</span><span class="n">pair</span><span class="p">].</span><span class="n">append</span><span class="p">((</span><span class="n">word</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">freq</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">count_pairs</span><span class="p">(</span><span class="n">pair_index</span><span class="p">):</span>
        <span class="s">"""Count pair frequencies efficiently using the index."""</span>
        <span class="n">pair_counts</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">pair</span><span class="p">,</span> <span class="n">occurrences</span> <span class="ow">in</span> <span class="n">pair_index</span><span class="p">.</span><span class="n">items</span><span class="p">():</span>
            <span class="n">total_count</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">freq</span> <span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">freq</span> <span class="ow">in</span> <span class="n">occurrences</span><span class="p">)</span>
            <span class="n">pair_counts</span><span class="p">[</span><span class="n">pair</span><span class="p">]</span> <span class="o">=</span> <span class="n">total_count</span>
        <span class="k">return</span> <span class="n">pair_counts</span>

    <span class="c1"># BPE training loop
</span>    <span class="n">target_merges</span> <span class="o">=</span> <span class="n">vocab_size</span> <span class="o">-</span> <span class="nb">len</span><span class="p">(</span><span class="n">vocab</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
        <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"</span><span class="se">\n</span><span class="s">Step 3: Training BPE with </span><span class="si">{</span><span class="n">target_merges</span><span class="si">:</span><span class="p">,</span><span class="si">}</span><span class="s"> merges..."</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Initial vocabulary size: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">vocab</span><span class="p">)</span><span class="si">}</span><span class="s"> (256 bytes + </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">special_tokens</span><span class="p">)</span><span class="si">}</span><span class="s"> special tokens)"</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="s">"="</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>

    <span class="n">bpe_start_time</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="n">time</span><span class="p">()</span>

    <span class="k">for</span> <span class="n">merge_num</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">target_merges</span><span class="p">):</span>
        <span class="n">merge_step_start</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="n">time</span><span class="p">()</span>

        <span class="c1"># Update pair index
</span>        <span class="n">update_pair_index</span><span class="p">(</span><span class="n">word_freq</span><span class="p">,</span> <span class="n">pair_index</span><span class="p">)</span>

        <span class="c1"># Count pairs efficiently
</span>        <span class="n">pair_counts</span> <span class="o">=</span> <span class="n">count_pairs</span><span class="p">(</span><span class="n">pair_index</span><span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">pair_counts</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
                <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"No more pairs to merge at step </span><span class="si">{</span><span class="n">merge_num</span> <span class="o">+</span> <span class="mi">1</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
            <span class="k">break</span>

        <span class="c1"># Find most frequent pair (with lexicographic tiebreaking)
</span>        <span class="n">best_pair</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">pair_counts</span><span class="p">.</span><span class="n">items</span><span class="p">(),</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]))[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">best_count</span> <span class="o">=</span> <span class="n">pair_counts</span><span class="p">[</span><span class="n">best_pair</span><span class="p">]</span>

        <span class="c1"># Create new token for merge
</span>        <span class="n">new_token_id</span> <span class="o">=</span> <span class="n">next_id</span>
        <span class="n">next_id</span> <span class="o">+=</span> <span class="mi">1</span>

        <span class="c1"># Get the byte sequences for the tokens being merged
</span>        <span class="n">left_bytes</span> <span class="o">=</span> <span class="n">vocab</span><span class="p">[</span><span class="n">best_pair</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span>
        <span class="n">right_bytes</span> <span class="o">=</span> <span class="n">vocab</span><span class="p">[</span><span class="n">best_pair</span><span class="p">[</span><span class="mi">1</span><span class="p">]]</span>

        <span class="c1"># Record merge as byte sequences
</span>        <span class="n">merges</span><span class="p">.</span><span class="n">append</span><span class="p">((</span><span class="n">left_bytes</span><span class="p">,</span> <span class="n">right_bytes</span><span class="p">))</span>

        <span class="c1"># Update vocabulary - merge the two byte sequences
</span>        <span class="n">vocab</span><span class="p">[</span><span class="n">new_token_id</span><span class="p">]</span> <span class="o">=</span> <span class="n">left_bytes</span> <span class="o">+</span> <span class="n">right_bytes</span>

        <span class="c1"># Update word frequencies by applying merge
</span>        <span class="n">new_word_freq</span> <span class="o">=</span> <span class="p">{}</span>

        <span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">freq</span> <span class="ow">in</span> <span class="n">word_freq</span><span class="p">.</span><span class="n">items</span><span class="p">():</span>
            <span class="n">new_word</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="k">while</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">word</span><span class="p">):</span>
                <span class="k">if</span> <span class="p">(</span><span class="n">i</span> <span class="o">&lt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">word</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span> <span class="ow">and</span>
                    <span class="n">word</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">==</span> <span class="n">best_pair</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="ow">and</span>
                    <span class="n">word</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="n">best_pair</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
                    <span class="n">new_word</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">new_token_id</span><span class="p">)</span>
                    <span class="n">i</span> <span class="o">+=</span> <span class="mi">2</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">new_word</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">word</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
                    <span class="n">i</span> <span class="o">+=</span> <span class="mi">1</span>

            <span class="n">new_word_tuple</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">new_word</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">new_word_tuple</span> <span class="ow">in</span> <span class="n">new_word_freq</span><span class="p">:</span>
                <span class="n">new_word_freq</span><span class="p">[</span><span class="n">new_word_tuple</span><span class="p">]</span> <span class="o">+=</span> <span class="n">freq</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">new_word_freq</span><span class="p">[</span><span class="n">new_word_tuple</span><span class="p">]</span> <span class="o">=</span> <span class="n">freq</span>

        <span class="n">word_freq</span> <span class="o">=</span> <span class="n">new_word_freq</span>
        <span class="n">merge_step_time</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">merge_step_start</span>

        <span class="c1"># Progress logging
</span>        <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
            <span class="k">if</span> <span class="p">(</span><span class="n">merge_num</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">merge_num</span> <span class="o">&lt;</span> <span class="mi">10</span> <span class="ow">or</span> <span class="p">(</span><span class="n">merge_num</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="mi">1000</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">try</span><span class="p">:</span>
                    <span class="n">left_str</span> <span class="o">=</span> <span class="n">left_bytes</span><span class="p">.</span><span class="n">decode</span><span class="p">(</span><span class="s">'utf-8'</span><span class="p">,</span> <span class="n">errors</span><span class="o">=</span><span class="s">'replace'</span><span class="p">)</span>
                    <span class="n">right_str</span> <span class="o">=</span> <span class="n">right_bytes</span><span class="p">.</span><span class="n">decode</span><span class="p">(</span><span class="s">'utf-8'</span><span class="p">,</span> <span class="n">errors</span><span class="o">=</span><span class="s">'replace'</span><span class="p">)</span>
                    <span class="n">merged_str</span> <span class="o">=</span> <span class="p">(</span><span class="n">left_bytes</span> <span class="o">+</span> <span class="n">right_bytes</span><span class="p">).</span><span class="n">decode</span><span class="p">(</span><span class="s">'utf-8'</span><span class="p">,</span> <span class="n">errors</span><span class="o">=</span><span class="s">'replace'</span><span class="p">)</span>
                    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Merge </span><span class="si">{</span><span class="n">merge_num</span> <span class="o">+</span> <span class="mi">1</span><span class="si">:</span><span class="mi">4</span><span class="n">d</span><span class="si">}</span><span class="s">/</span><span class="si">{</span><span class="n">target_merges</span><span class="si">}</span><span class="s">: "</span>
                          <span class="sa">f</span><span class="s">"'</span><span class="si">{</span><span class="n">left_str</span><span class="si">}</span><span class="s">' + '</span><span class="si">{</span><span class="n">right_str</span><span class="si">}</span><span class="s">' -&gt; '</span><span class="si">{</span><span class="n">merged_str</span><span class="si">}</span><span class="s">' "</span>
                          <span class="sa">f</span><span class="s">"(freq: </span><span class="si">{</span><span class="n">best_count</span><span class="si">:</span><span class="p">,</span><span class="si">}</span><span class="s">, time: </span><span class="si">{</span><span class="n">merge_step_time</span><span class="si">:</span><span class="p">.</span><span class="mi">3</span><span class="n">f</span><span class="si">}</span><span class="s">s)"</span><span class="p">)</span>
                <span class="k">except</span><span class="p">:</span>
                    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Merge </span><span class="si">{</span><span class="n">merge_num</span> <span class="o">+</span> <span class="mi">1</span><span class="si">:</span><span class="mi">4</span><span class="n">d</span><span class="si">}</span><span class="s">/</span><span class="si">{</span><span class="n">target_merges</span><span class="si">}</span><span class="s">: "</span>
                          <span class="sa">f</span><span class="s">"</span><span class="si">{</span><span class="n">left_bytes</span><span class="si">}</span><span class="s"> + </span><span class="si">{</span><span class="n">right_bytes</span><span class="si">}</span><span class="s"> -&gt; </span><span class="si">{</span><span class="n">left_bytes</span> <span class="o">+</span> <span class="n">right_bytes</span><span class="si">}</span><span class="s"> "</span>
                          <span class="sa">f</span><span class="s">"(freq: </span><span class="si">{</span><span class="n">best_count</span><span class="si">:</span><span class="p">,</span><span class="si">}</span><span class="s">, time: </span><span class="si">{</span><span class="n">merge_step_time</span><span class="si">:</span><span class="p">.</span><span class="mi">3</span><span class="n">f</span><span class="si">}</span><span class="s">s)"</span><span class="p">)</span>

    <span class="n">bpe_time</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">bpe_start_time</span>

    <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
        <span class="k">print</span><span class="p">(</span><span class="s">"="</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"BPE training completed in </span><span class="si">{</span><span class="n">bpe_time</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s"> seconds"</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Final vocabulary size: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">vocab</span><span class="p">)</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Total merges performed: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">merges</span><span class="p">)</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>

        <span class="c1"># Show compression statistics
</span>        <span class="k">if</span> <span class="n">word_counts</span><span class="p">:</span>
            <span class="n">original_tokens</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="nb">bytes</span><span class="p">(</span><span class="n">word_bytes</span><span class="p">))</span> <span class="k">for</span> <span class="n">word_bytes</span><span class="p">,</span> <span class="n">count</span> <span class="ow">in</span> <span class="n">word_counts</span><span class="p">.</span><span class="n">items</span><span class="p">()</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">count</span><span class="p">))</span>
            <span class="n">compressed_tokens</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">word</span><span class="p">)</span> <span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">count</span> <span class="ow">in</span> <span class="n">word_freq</span><span class="p">.</span><span class="n">items</span><span class="p">()</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">count</span><span class="p">))</span>
            <span class="n">compression_ratio</span> <span class="o">=</span> <span class="n">original_tokens</span> <span class="o">/</span> <span class="n">compressed_tokens</span> <span class="k">if</span> <span class="n">compressed_tokens</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="mf">1.0</span>
            <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Compression ratio: </span><span class="si">{</span><span class="n">compression_ratio</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s">x (from </span><span class="si">{</span><span class="n">original_tokens</span><span class="si">:</span><span class="p">,</span><span class="si">}</span><span class="s"> to </span><span class="si">{</span><span class="n">compressed_tokens</span><span class="si">:</span><span class="p">,</span><span class="si">}</span><span class="s"> tokens)"</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">vocab</span><span class="p">,</span> <span class="n">merges</span>


<span class="k">def</span> <span class="nf">save_tokenizer</span><span class="p">(</span><span class="n">vocab</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">bytes</span><span class="p">],</span> <span class="n">merges</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">tuple</span><span class="p">[</span><span class="nb">bytes</span><span class="p">,</span> <span class="nb">bytes</span><span class="p">]],</span>
                  <span class="n">vocab_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">merges_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>
    <span class="s">"""Save vocabulary and merges to disk files."""</span>
    <span class="kn">import</span> <span class="nn">json</span>
    <span class="kn">import</span> <span class="nn">pickle</span>

    <span class="c1"># Save vocabulary
</span>    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">vocab_path</span><span class="p">,</span> <span class="s">'wb'</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="n">pickle</span><span class="p">.</span><span class="n">dump</span><span class="p">(</span><span class="n">vocab</span><span class="p">,</span> <span class="n">f</span><span class="p">)</span>

    <span class="c1"># Save merges
</span>    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">merges_path</span><span class="p">,</span> <span class="s">'wb'</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="n">pickle</span><span class="p">.</span><span class="n">dump</span><span class="p">(</span><span class="n">merges</span><span class="p">,</span> <span class="n">f</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">load_tokenizer</span><span class="p">(</span><span class="n">vocab_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">merges_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="nb">dict</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">bytes</span><span class="p">],</span> <span class="nb">list</span><span class="p">[</span><span class="nb">tuple</span><span class="p">[</span><span class="nb">bytes</span><span class="p">,</span> <span class="nb">bytes</span><span class="p">]]]:</span>
    <span class="s">"""Load vocabulary and merges from disk files."""</span>
    <span class="kn">import</span> <span class="nn">pickle</span>

    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">vocab_path</span><span class="p">,</span> <span class="s">'rb'</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="n">vocab</span> <span class="o">=</span> <span class="n">pickle</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>

    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">merges_path</span><span class="p">,</span> <span class="s">'rb'</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="n">merges</span> <span class="o">=</span> <span class="n">pickle</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">vocab</span><span class="p">,</span> <span class="n">merges</span>
</code></pre></div></div>

<h2 id="training-on-tinystories-dataset">Training on TinyStories Dataset</h2>

<p>Now let’s use our implementation to train a tokenizer on the TinyStories dataset. Here is one training function to demonstrate all the steps:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">time</span>
<span class="kn">import</span> <span class="nn">os</span>

<span class="k">def</span> <span class="nf">train_bpe_tokentizer_via_dataset</span><span class="p">(</span><span class="n">input_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"="</span> <span class="o">*</span> <span class="mi">80</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"BPE TOKENIZER TRAINING ON TINYSTORIES DATASET"</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"="</span> <span class="o">*</span> <span class="mi">80</span><span class="p">)</span>

    <span class="c1"># Configuration
</span>    <span class="n">vocab_size</span> <span class="o">=</span> <span class="mi">10000</span>
    <span class="n">special_tokens</span> <span class="o">=</span> <span class="p">[</span><span class="s">"&lt;|endoftext|&gt;"</span><span class="p">]</span>

    <span class="c1"># Check if input file exists
</span>    <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="n">exists</span><span class="p">(</span><span class="n">input_path</span><span class="p">):</span>
        <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Error: Input file '</span><span class="si">{</span><span class="n">input_path</span><span class="si">}</span><span class="s">' not found!"</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="s">"Please ensure the TinyStories dataset is in the data/ directory."</span><span class="p">)</span>
        <span class="k">return</span>

    <span class="c1"># Display configuration
</span>    <span class="n">file_size</span> <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="n">getsize</span><span class="p">(</span><span class="n">input_path</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Configuration:"</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"  Input file: </span><span class="si">{</span><span class="n">input_path</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"  File size: </span><span class="si">{</span><span class="n">file_size</span><span class="si">:</span><span class="p">,</span><span class="si">}</span><span class="s"> bytes (</span><span class="si">{</span><span class="n">file_size</span> <span class="o">/</span> <span class="mi">1024</span> <span class="o">/</span> <span class="mi">1024</span><span class="si">:</span><span class="p">.</span><span class="mi">1</span><span class="n">f</span><span class="si">}</span><span class="s"> MB)"</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"  Target vocabulary size: </span><span class="si">{</span><span class="n">vocab_size</span><span class="si">:</span><span class="p">,</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"  Special tokens: </span><span class="si">{</span><span class="n">special_tokens</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"  Verbose logging: Enabled"</span><span class="p">)</span>
    <span class="k">print</span><span class="p">()</span>

    <span class="c1"># Train the tokenizer with verbose output
</span>    <span class="n">overall_start_time</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="n">time</span><span class="p">()</span>
    <span class="n">vocab</span><span class="p">,</span> <span class="n">merges</span> <span class="o">=</span> <span class="n">train_bpe_tokenizer</span><span class="p">(</span>
        <span class="n">input_path</span><span class="o">=</span><span class="n">input_path</span><span class="p">,</span>
        <span class="n">vocab_size</span><span class="o">=</span><span class="n">vocab_size</span><span class="p">,</span>
        <span class="n">special_tokens</span><span class="o">=</span><span class="n">special_tokens</span><span class="p">,</span>
        <span class="n">verbose</span><span class="o">=</span><span class="bp">True</span>  <span class="c1"># Enable detailed logging
</span>    <span class="p">)</span>
    <span class="n">overall_end_time</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="n">time</span><span class="p">()</span>

    <span class="k">print</span><span class="p">(</span><span class="s">"</span><span class="se">\n</span><span class="s">"</span> <span class="o">+</span> <span class="s">"="</span> <span class="o">*</span> <span class="mi">80</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"TRAINING SUMMARY"</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"="</span> <span class="o">*</span> <span class="mi">80</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Total training time: </span><span class="si">{</span><span class="n">overall_end_time</span> <span class="o">-</span> <span class="n">overall_start_time</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s"> seconds"</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Final vocabulary size: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">vocab</span><span class="p">)</span><span class="si">:</span><span class="p">,</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Number of merges performed: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">merges</span><span class="p">)</span><span class="si">:</span><span class="p">,</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Actual vocab size vs target: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">vocab</span><span class="p">)</span><span class="si">}</span><span class="s"> / </span><span class="si">{</span><span class="n">vocab_size</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>

    <span class="c1"># Save the tokenizer
</span>    <span class="n">vocab_path</span> <span class="o">=</span> <span class="s">"tinystories_vocab.pkl"</span>
    <span class="n">merges_path</span> <span class="o">=</span> <span class="s">"tinystories_merges.pkl"</span>

    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"</span><span class="se">\n</span><span class="s">Saving tokenizer to disk..."</span><span class="p">)</span>
    <span class="n">save_tokenizer</span><span class="p">(</span><span class="n">vocab</span><span class="p">,</span> <span class="n">merges</span><span class="p">,</span> <span class="n">vocab_path</span><span class="p">,</span> <span class="n">merges_path</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"  ✓ Vocabulary saved to: </span><span class="si">{</span><span class="n">vocab_path</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"  ✓ Merges saved to: </span><span class="si">{</span><span class="n">merges_path</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>

    <span class="c1"># Detailed vocabulary analysis
</span>    <span class="k">print</span><span class="p">(</span><span class="s">"</span><span class="se">\n</span><span class="s">"</span> <span class="o">+</span> <span class="s">"="</span> <span class="o">*</span> <span class="mi">80</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"VOCABULARY ANALYSIS"</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"="</span> <span class="o">*</span> <span class="mi">80</span><span class="p">)</span>

    <span class="c1"># Count different types of tokens
</span>    <span class="n">byte_tokens</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="mi">1</span> <span class="k">for</span> <span class="n">token_id</span> <span class="ow">in</span> <span class="n">vocab</span><span class="p">.</span><span class="n">keys</span><span class="p">()</span> <span class="k">if</span> <span class="n">token_id</span> <span class="o">&lt;</span> <span class="mi">256</span><span class="p">)</span>
    <span class="n">special_token_count</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">special_tokens</span><span class="p">)</span>
    <span class="n">merged_tokens</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">vocab</span><span class="p">)</span> <span class="o">-</span> <span class="n">byte_tokens</span> <span class="o">-</span> <span class="n">special_token_count</span>

    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Token type breakdown:"</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"  Byte tokens (0-255): </span><span class="si">{</span><span class="n">byte_tokens</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"  Special tokens: </span><span class="si">{</span><span class="n">special_token_count</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"  Merged tokens: </span><span class="si">{</span><span class="n">merged_tokens</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"  Total: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">vocab</span><span class="p">)</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>

    <span class="c1"># Show some vocabulary examples
</span>    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"</span><span class="se">\n</span><span class="s">Byte tokens (first 10):"</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">vocab</span><span class="p">:</span>
            <span class="n">char</span> <span class="o">=</span> <span class="n">vocab</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="n">decode</span><span class="p">(</span><span class="s">'utf-8'</span><span class="p">,</span> <span class="n">errors</span><span class="o">=</span><span class="s">'replace'</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">char</span><span class="p">.</span><span class="n">isprintable</span><span class="p">()</span> <span class="ow">and</span> <span class="n">char</span> <span class="o">!=</span> <span class="s">' '</span><span class="p">:</span>
                <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"  Token </span><span class="si">{</span><span class="n">i</span><span class="si">:</span><span class="mi">3</span><span class="n">d</span><span class="si">}</span><span class="s">: </span><span class="si">{</span><span class="n">vocab</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="s"> -&gt; '</span><span class="si">{</span><span class="n">char</span><span class="si">}</span><span class="s">'"</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"  Token </span><span class="si">{</span><span class="n">i</span><span class="si">:</span><span class="mi">3</span><span class="n">d</span><span class="si">}</span><span class="s">: </span><span class="si">{</span><span class="n">vocab</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="s"> -&gt; </span><span class="si">{</span><span class="nb">repr</span><span class="p">(</span><span class="n">char</span><span class="p">)</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>

    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"</span><span class="se">\n</span><span class="s">Special tokens:"</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">token_str</span> <span class="ow">in</span> <span class="n">special_tokens</span><span class="p">:</span>
        <span class="n">token_bytes</span> <span class="o">=</span> <span class="n">token_str</span><span class="p">.</span><span class="n">encode</span><span class="p">(</span><span class="s">'utf-8'</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">token_id</span><span class="p">,</span> <span class="n">vocab_bytes</span> <span class="ow">in</span> <span class="n">vocab</span><span class="p">.</span><span class="n">items</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">vocab_bytes</span> <span class="o">==</span> <span class="n">token_bytes</span><span class="p">:</span>
                <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"  Token </span><span class="si">{</span><span class="n">token_id</span><span class="si">:</span><span class="mi">3</span><span class="n">d</span><span class="si">}</span><span class="s">: </span><span class="si">{</span><span class="n">vocab_bytes</span><span class="si">}</span><span class="s"> -&gt; '</span><span class="si">{</span><span class="n">token_str</span><span class="si">}</span><span class="s">'"</span><span class="p">)</span>
                <span class="k">break</span>

    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"</span><span class="se">\n</span><span class="s">Most recently merged tokens (last 10):"</span><span class="p">)</span>
    <span class="n">merged_token_ids</span> <span class="o">=</span> <span class="p">[</span><span class="n">tid</span> <span class="k">for</span> <span class="n">tid</span> <span class="ow">in</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">vocab</span><span class="p">.</span><span class="n">keys</span><span class="p">())</span> <span class="k">if</span> <span class="n">tid</span> <span class="o">&gt;=</span> <span class="mi">256</span> <span class="o">+</span> <span class="nb">len</span><span class="p">(</span><span class="n">special_tokens</span><span class="p">)]</span>
    <span class="k">for</span> <span class="n">token_id</span> <span class="ow">in</span> <span class="n">merged_token_ids</span><span class="p">[</span><span class="o">-</span><span class="mi">10</span><span class="p">:]:</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">decoded</span> <span class="o">=</span> <span class="n">vocab</span><span class="p">[</span><span class="n">token_id</span><span class="p">].</span><span class="n">decode</span><span class="p">(</span><span class="s">'utf-8'</span><span class="p">,</span> <span class="n">errors</span><span class="o">=</span><span class="s">'replace'</span><span class="p">)</span>
            <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"  Token </span><span class="si">{</span><span class="n">token_id</span><span class="si">:</span><span class="mi">4</span><span class="n">d</span><span class="si">}</span><span class="s">: </span><span class="si">{</span><span class="n">vocab</span><span class="p">[</span><span class="n">token_id</span><span class="p">]</span><span class="si">}</span><span class="s"> -&gt; '</span><span class="si">{</span><span class="n">decoded</span><span class="si">}</span><span class="s">'"</span><span class="p">)</span>
        <span class="k">except</span><span class="p">:</span>
            <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"  Token </span><span class="si">{</span><span class="n">token_id</span><span class="si">:</span><span class="mi">4</span><span class="n">d</span><span class="si">}</span><span class="s">: </span><span class="si">{</span><span class="n">vocab</span><span class="p">[</span><span class="n">token_id</span><span class="p">]</span><span class="si">}</span><span class="s"> -&gt; (non-UTF8)"</span><span class="p">)</span>

    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"</span><span class="se">\n</span><span class="s">First 10 merge operations:"</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">left</span><span class="p">,</span> <span class="n">right</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">merges</span><span class="p">[:</span><span class="mi">10</span><span class="p">]):</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">left_str</span> <span class="o">=</span> <span class="n">left</span><span class="p">.</span><span class="n">decode</span><span class="p">(</span><span class="s">'utf-8'</span><span class="p">,</span> <span class="n">errors</span><span class="o">=</span><span class="s">'replace'</span><span class="p">)</span>
            <span class="n">right_str</span> <span class="o">=</span> <span class="n">right</span><span class="p">.</span><span class="n">decode</span><span class="p">(</span><span class="s">'utf-8'</span><span class="p">,</span> <span class="n">errors</span><span class="o">=</span><span class="s">'replace'</span><span class="p">)</span>
            <span class="n">merged_str</span> <span class="o">=</span> <span class="p">(</span><span class="n">left</span> <span class="o">+</span> <span class="n">right</span><span class="p">).</span><span class="n">decode</span><span class="p">(</span><span class="s">'utf-8'</span><span class="p">,</span> <span class="n">errors</span><span class="o">=</span><span class="s">'replace'</span><span class="p">)</span>
            <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"  Merge </span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">:</span><span class="mi">2</span><span class="n">d</span><span class="si">}</span><span class="s">: '</span><span class="si">{</span><span class="n">left_str</span><span class="si">}</span><span class="s">' + '</span><span class="si">{</span><span class="n">right_str</span><span class="si">}</span><span class="s">' -&gt; '</span><span class="si">{</span><span class="n">merged_str</span><span class="si">}</span><span class="s">'"</span><span class="p">)</span>
        <span class="k">except</span><span class="p">:</span>
            <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"  Merge </span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">:</span><span class="mi">2</span><span class="n">d</span><span class="si">}</span><span class="s">: </span><span class="si">{</span><span class="n">left</span><span class="si">}</span><span class="s"> + </span><span class="si">{</span><span class="n">right</span><span class="si">}</span><span class="s"> -&gt; (binary)"</span><span class="p">)</span>

    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"</span><span class="se">\n</span><span class="s">Last 10 merge operations:"</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">left</span><span class="p">,</span> <span class="n">right</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">merges</span><span class="p">[</span><span class="o">-</span><span class="mi">10</span><span class="p">:],</span> <span class="nb">len</span><span class="p">(</span><span class="n">merges</span><span class="p">)</span> <span class="o">-</span> <span class="mi">9</span><span class="p">):</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">left_str</span> <span class="o">=</span> <span class="n">left</span><span class="p">.</span><span class="n">decode</span><span class="p">(</span><span class="s">'utf-8'</span><span class="p">,</span> <span class="n">errors</span><span class="o">=</span><span class="s">'replace'</span><span class="p">)</span>
            <span class="n">right_str</span> <span class="o">=</span> <span class="n">right</span><span class="p">.</span><span class="n">decode</span><span class="p">(</span><span class="s">'utf-8'</span><span class="p">,</span> <span class="n">errors</span><span class="o">=</span><span class="s">'replace'</span><span class="p">)</span>
            <span class="n">merged_str</span> <span class="o">=</span> <span class="p">(</span><span class="n">left</span> <span class="o">+</span> <span class="n">right</span><span class="p">).</span><span class="n">decode</span><span class="p">(</span><span class="s">'utf-8'</span><span class="p">,</span> <span class="n">errors</span><span class="o">=</span><span class="s">'replace'</span><span class="p">)</span>
            <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"  Merge </span><span class="si">{</span><span class="n">i</span><span class="si">:</span><span class="mi">2</span><span class="n">d</span><span class="si">}</span><span class="s">: '</span><span class="si">{</span><span class="n">left_str</span><span class="si">}</span><span class="s">' + '</span><span class="si">{</span><span class="n">right_str</span><span class="si">}</span><span class="s">' -&gt; '</span><span class="si">{</span><span class="n">merged_str</span><span class="si">}</span><span class="s">'"</span><span class="p">)</span>
        <span class="k">except</span><span class="p">:</span>
            <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"  Merge </span><span class="si">{</span><span class="n">i</span><span class="si">:</span><span class="mi">2</span><span class="n">d</span><span class="si">}</span><span class="s">: </span><span class="si">{</span><span class="n">left</span><span class="si">}</span><span class="s"> + </span><span class="si">{</span><span class="n">right</span><span class="si">}</span><span class="s"> -&gt; (binary)"</span><span class="p">)</span>

    <span class="c1"># Show file sizes
</span>    <span class="n">vocab_size_bytes</span> <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="n">getsize</span><span class="p">(</span><span class="n">vocab_path</span><span class="p">)</span>
    <span class="n">merges_size_bytes</span> <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="n">getsize</span><span class="p">(</span><span class="n">merges_path</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"</span><span class="se">\n</span><span class="s">Output file sizes:"</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"  Vocabulary file: </span><span class="si">{</span><span class="n">vocab_size_bytes</span><span class="si">:</span><span class="p">,</span><span class="si">}</span><span class="s"> bytes (</span><span class="si">{</span><span class="n">vocab_size_bytes</span> <span class="o">/</span> <span class="mi">1024</span><span class="si">:</span><span class="p">.</span><span class="mi">1</span><span class="n">f</span><span class="si">}</span><span class="s"> KB)"</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"  Merges file: </span><span class="si">{</span><span class="n">merges_size_bytes</span><span class="si">:</span><span class="p">,</span><span class="si">}</span><span class="s"> bytes (</span><span class="si">{</span><span class="n">merges_size_bytes</span> <span class="o">/</span> <span class="mi">1024</span><span class="si">:</span><span class="p">.</span><span class="mi">1</span><span class="n">f</span><span class="si">}</span><span class="s"> KB)"</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"  Total: </span><span class="si">{</span><span class="n">vocab_size_bytes</span> <span class="o">+</span> <span class="n">merges_size_bytes</span><span class="si">:</span><span class="p">,</span><span class="si">}</span><span class="s"> bytes (</span><span class="si">{</span><span class="p">(</span><span class="n">vocab_size_bytes</span> <span class="o">+</span> <span class="n">merges_size_bytes</span><span class="p">)</span> <span class="o">/</span> <span class="mi">1024</span><span class="si">:</span><span class="p">.</span><span class="mi">1</span><span class="n">f</span><span class="si">}</span><span class="s"> KB)"</span><span class="p">)</span>

    <span class="k">print</span><span class="p">(</span><span class="s">"</span><span class="se">\n</span><span class="s">"</span> <span class="o">+</span> <span class="s">"="</span> <span class="o">*</span> <span class="mi">80</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"TRAINING COMPLETED SUCCESSFULLY!"</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"="</span> <span class="o">*</span> <span class="mi">80</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"You can now use the trained tokenizer for encoding/decoding text."</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Load with: vocab, merges = load_tokenizer('</span><span class="si">{</span><span class="n">vocab_path</span><span class="si">}</span><span class="s">', '</span><span class="si">{</span><span class="n">merges_path</span><span class="si">}</span><span class="s">')"</span><span class="p">)</span>
</code></pre></div></div>

<p>To run the training, one can try for example:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">train_bpe_tokentizer_via_dataset</span><span class="p">(</span><span class="n">input_path</span><span class="o">=</span><span class="s">"data/TinyStoriesV2-GPT4-train.txt"</span><span class="p">)</span>
</code></pre></div></div>

<p>And it will output the following info from the training process:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>================================================================================
BPE TOKENIZER TRAINING ON TINYSTORIES DATASET
================================================================================
Configuration:
  Input file: /content/TinyStoriesV2-GPT4-train.txt
  File size: 2,227,753,162 bytes (2124.6 MB)
  Target vocabulary size: 10,000
  Special tokens: ['&lt;|endoftext|&gt;']
  Verbose logging: Enabled

Step 1: Setting up parallel processing...
Using 12 processes for parallel tokenization
Finding chunk boundaries aligned with special token: &lt;|endoftext|&gt;
Initial guess of the chunk boundaries: [0, 185646096, 371292192, 556938288, 742584384, 928230480, 1113876576, 1299522672, 1485168768, 1670814864, 1856460960, 2042107056, 2227753152]
Created 12 chunks for processing

Step 2: Pre-tokenizing text corpus...
Pre-tokenization completed in 66.52 seconds
Found 59,904 unique word types
Total token count: 536,592,162
Most common words:
  1. '.' -&gt; 41,764,519 times
  2. ',' -&gt; 23,284,331 times
  3. ' the' -&gt; 20,828,576 times
  4. ' and' -&gt; 19,475,966 times
  5. ' a' -&gt; 15,063,529 times

Step 3: Training BPE with 9,743 merges...
Initial vocabulary size: 257 (256 bytes + 1 special tokens)
============================================================
Merge    1/9743: ' ' + 't' -&gt; ' t' (freq: 63,482,199, time: 0.273s)
Merge    2/9743: 'h' + 'e' -&gt; 'he' (freq: 63,341,860, time: 0.318s)
Merge    3/9743: ' ' + 'a' -&gt; ' a' (freq: 47,465,635, time: 0.340s)
Merge    4/9743: ' ' + 's' -&gt; ' s' (freq: 32,362,158, time: 0.340s)
Merge    5/9743: ' ' + 'w' -&gt; ' w' (freq: 31,485,643, time: 0.327s)
Merge    6/9743: 'n' + 'd' -&gt; 'nd' (freq: 28,922,386, time: 0.332s)
Merge    7/9743: ' t' + 'he' -&gt; ' the' (freq: 28,915,024, time: 0.320s)
Merge    8/9743: 'e' + 'd' -&gt; 'ed' (freq: 24,836,456, time: 0.317s)
Merge    9/9743: ' ' + 'b' -&gt; ' b' (freq: 22,147,488, time: 0.326s)
Merge   10/9743: ' t' + 'o' -&gt; ' to' (freq: 20,892,273, time: 0.322s)
Merge  100/9743: ' ha' + 'pp' -&gt; ' happ' (freq: 3,147,884, time: 0.251s)
Merge  200/9743: ' s' + 'e' -&gt; ' se' (freq: 1,410,130, time: 0.343s)
Merge  300/9743: ' s' + 'omet' -&gt; ' somet' (freq: 790,510, time: 0.245s)
Merge  400/9743: ' g' + 'ot' -&gt; ' got' (freq: 524,776, time: 0.338s)
Merge  500/9743: ' e' + 'ach' -&gt; ' each' (freq: 369,637, time: 0.321s)
Merge  600/9743: 'l' + 'f' -&gt; 'lf' (freq: 279,566, time: 0.230s)
Merge  700/9743: ' wal' + 'k' -&gt; ' walk' (freq: 221,114, time: 0.237s)
Merge  800/9743: ' do' + 'll' -&gt; ' doll' (freq: 177,602, time: 0.324s)
Merge  900/9743: ' ' + 'G' -&gt; ' G' (freq: 147,699, time: 0.214s)
Merge 1000/9743: 'ec' + 't' -&gt; 'ect' (freq: 127,288, time: 0.233s)
Merge 1100/9743: ' l' + 'ight' -&gt; ' light' (freq: 108,006, time: 0.208s)
Merge 1200/9743: ' d' + 'in' -&gt; ' din' (freq: 92,211, time: 0.225s)
Merge 1300/9743: ' picture' + 's' -&gt; ' pictures' (freq: 80,416, time: 0.318s)
Merge 1400/9743: 'itt' + 'en' -&gt; 'itten' (freq: 68,466, time: 0.235s)
Merge 1500/9743: 'A' + 'my' -&gt; 'Amy' (freq: 59,829, time: 0.306s)
Merge 1600/9743: ' tal' + 'king' -&gt; ' talking' (freq: 53,781, time: 0.330s)
Merge 1700/9743: 'b' + 'all' -&gt; 'ball' (freq: 48,005, time: 0.309s)
Merge 1800/9743: ' k' + 'iss' -&gt; ' kiss' (freq: 43,477, time: 0.318s)
...
Merge 8000/9743: ' mom' + 'mies' -&gt; ' mommies' (freq: 879, time: 0.205s)
Merge 8100/9743: ' cryst' + 'als' -&gt; ' crystals' (freq: 840, time: 0.299s)
Merge 8200/9743: ' playd' + 'ate' -&gt; ' playdate' (freq: 809, time: 0.283s)
Merge 8300/9743: ' support' + 'ing' -&gt; ' supporting' (freq: 778, time: 0.200s)
Merge 8400/9743: ' activ' + 'ity' -&gt; ' activity' (freq: 747, time: 0.300s)
Merge 8500/9743: 'L' + 'izzy' -&gt; 'Lizzy' (freq: 716, time: 0.284s)
Merge 8600/9743: 'er' + 'ing' -&gt; 'ering' (freq: 691, time: 0.311s)
Merge 8700/9743: ' tid' + 'ied' -&gt; ' tidied' (freq: 660, time: 0.308s)
Merge 8800/9743: 'f' + 'lowers' -&gt; 'flowers' (freq: 633, time: 0.295s)
Merge 8900/9743: ' Gra' + 'nd' -&gt; ' Grand' (freq: 609, time: 0.299s)
Merge 9000/9743: ' frustr' + 'ation' -&gt; ' frustration' (freq: 584, time: 0.301s)
Merge 9100/9743: 'amil' + 'iar' -&gt; 'amiliar' (freq: 561, time: 0.205s)
Merge 9200/9743: ' P' + 'retty' -&gt; ' Pretty' (freq: 542, time: 0.310s)
Merge 9300/9743: ' sal' + 'on' -&gt; ' salon' (freq: 521, time: 0.292s)
Merge 9400/9743: ' p' + 'ounced' -&gt; ' pounced' (freq: 502, time: 0.196s)
Merge 9500/9743: ' pops' + 'ic' -&gt; ' popsic' (freq: 485, time: 0.185s)
Merge 9600/9743: ' pain' + 'ful' -&gt; ' painful' (freq: 469, time: 0.298s)
Merge 9700/9743: 'solut' + 'ely' -&gt; 'solutely' (freq: 454, time: 0.308s)
============================================================
BPE training completed in 2731.72 seconds
Final vocabulary size: 10000
Total merges performed: 9743
Compression ratio: 4.07x (from 2,192,422,648 to 538,511,097 tokens)

================================================================================
TRAINING SUMMARY
================================================================================
Total training time: 2898.45 seconds
Final vocabulary size: 10,000
Number of merges performed: 9,743
Actual vocab size vs target: 10000 / 10000

Saving tokenizer to disk...
  ✓ Vocabulary saved to: tinystories_vocab.pkl
  ✓ Merges saved to: tinystories_merges.pkl

================================================================================
VOCABULARY ANALYSIS
================================================================================
Token type breakdown:
  Byte tokens (0-255): 256
  Special tokens: 1
  Merged tokens: 9743
  Total: 10000

Byte tokens (first 10):
  Token   0: b'\x00' -&gt; '\x00'
  Token   1: b'\x01' -&gt; '\x01'
  Token   2: b'\x02' -&gt; '\x02'
  Token   3: b'\x03' -&gt; '\x03'
  Token   4: b'\x04' -&gt; '\x04'
  Token   5: b'\x05' -&gt; '\x05'
  Token   6: b'\x06' -&gt; '\x06'
  Token   7: b'\x07' -&gt; '\x07'
  Token   8: b'\x08' -&gt; '\x08'
  Token   9: b'\t' -&gt; '\t'

Special tokens:
  Token 256: b'&lt;|endoftext|&gt;' -&gt; '&lt;|endoftext|&gt;'

Most recently merged tokens (last 10):
  Token 9990: b' improving' -&gt; ' improving'
  Token 9991: b' nicest' -&gt; ' nicest'
  Token 9992: b' whiskers' -&gt; ' whiskers'
  Token 9993: b' booth' -&gt; ' booth'
  Token 9994: b' Land' -&gt; ' Land'
  Token 9995: b'Rocky' -&gt; 'Rocky'
  Token 9996: b' meadows' -&gt; ' meadows'
  Token 9997: b' Starry' -&gt; ' Starry'
  Token 9998: b' imaginary' -&gt; ' imaginary'
  Token 9999: b' bold' -&gt; ' bold'

First 10 merge operations:
  Merge  1: ' ' + 't' -&gt; ' t'
  Merge  2: 'h' + 'e' -&gt; 'he'
  Merge  3: ' ' + 'a' -&gt; ' a'
  Merge  4: ' ' + 's' -&gt; ' s'
  Merge  5: ' ' + 'w' -&gt; ' w'
  Merge  6: 'n' + 'd' -&gt; 'nd'
  Merge  7: ' t' + 'he' -&gt; ' the'
  Merge  8: 'e' + 'd' -&gt; 'ed'
  Merge  9: ' ' + 'b' -&gt; ' b'
  Merge 10: ' t' + 'o' -&gt; ' to'

Last 10 merge operations:
  Merge 9734: ' impro' + 'ving' -&gt; ' improving'
  Merge 9735: ' nice' + 'st' -&gt; ' nicest'
  Merge 9736: ' wh' + 'iskers' -&gt; ' whiskers'
  Merge 9737: ' bo' + 'oth' -&gt; ' booth'
  Merge 9738: ' L' + 'and' -&gt; ' Land'
  Merge 9739: 'Rock' + 'y' -&gt; 'Rocky'
  Merge 9740: ' meadow' + 's' -&gt; ' meadows'
  Merge 9741: ' St' + 'arry' -&gt; ' Starry'
  Merge 9742: ' imag' + 'inary' -&gt; ' imaginary'
  Merge 9743: ' bo' + 'ld' -&gt; ' bold'

Output file sizes:
  Vocabulary file: 117,701 bytes (114.9 KB)
  Merges file: 109,714 bytes (107.1 KB)
  Total: 227,415 bytes (222.1 KB)

================================================================================
TRAINING COMPLETED SUCCESSFULLY!
================================================================================
You can now use the trained tokenizer for encoding/decoding text.
Load with: vocab, merges = load_tokenizer('tinystories_vocab.pkl', 'tinystories_merges.pkl')
</code></pre></div></div>

<h2 id="using-the-trained-tokenizer">Using the Trained Tokenizer</h2>

<p>Once we have a trained tokenizer, we need a class to encode and decode text. Here’s one complete implementation:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">SimpleBPETokenizer</span><span class="p">:</span>
    <span class="s">"""Simple BPE tokenizer for encoding/decoding text."""</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">vocab</span><span class="p">,</span> <span class="n">merges</span><span class="p">,</span> <span class="n">special_tokens</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">vocab</span> <span class="o">=</span> <span class="n">vocab</span>  <span class="c1"># {token_id: bytes}
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">merges</span> <span class="o">=</span> <span class="n">merges</span>  <span class="c1"># [(left_bytes, right_bytes), ...]
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">special_tokens</span> <span class="o">=</span> <span class="n">special_tokens</span> <span class="ow">or</span> <span class="p">[</span><span class="s">"&lt;|endoftext|&gt;"</span><span class="p">]</span>

        <span class="c1"># Create reverse mapping for decoding
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">id_to_bytes</span> <span class="o">=</span> <span class="n">vocab</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">bytes_to_id</span> <span class="o">=</span> <span class="p">{</span><span class="n">v</span><span class="p">:</span> <span class="n">k</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">vocab</span><span class="p">.</span><span class="n">items</span><span class="p">()}</span>

        <span class="c1"># GPT-2 style regex pattern
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">pattern</span> <span class="o">=</span> <span class="sa">r</span><span class="s">"""'(?:[sdmt]|ll|ve|re)| ?[a-zA-ZÀ-ÿ]+| ?[0-9]+| ?[^\s\w]+|\s+(?!\S)|\s+"""</span>

        <span class="c1"># Build merge rules for faster encoding
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">merge_rules</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">left_bytes</span><span class="p">,</span> <span class="n">right_bytes</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">merges</span><span class="p">):</span>
            <span class="c1"># Find what tokens these bytes correspond to
</span>            <span class="n">left_id</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">bytes_to_id</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="n">left_bytes</span><span class="p">)</span>
            <span class="n">right_id</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">bytes_to_id</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="n">right_bytes</span><span class="p">)</span>
            <span class="n">merged_bytes</span> <span class="o">=</span> <span class="n">left_bytes</span> <span class="o">+</span> <span class="n">right_bytes</span>
            <span class="n">merged_id</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">bytes_to_id</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="n">merged_bytes</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">left_id</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span> <span class="ow">and</span> <span class="n">right_id</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span> <span class="ow">and</span> <span class="n">merged_id</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
                <span class="bp">self</span><span class="p">.</span><span class="n">merge_rules</span><span class="p">[(</span><span class="n">left_id</span><span class="p">,</span> <span class="n">right_id</span><span class="p">)]</span> <span class="o">=</span> <span class="n">merged_id</span>

    <span class="k">def</span> <span class="nf">encode</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">text</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">]:</span>
        <span class="s">"""Encode text to token IDs."""</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">text</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">[]</span>

        <span class="c1"># Handle special tokens
</span>        <span class="n">token_ids</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">remaining_text</span> <span class="o">=</span> <span class="n">text</span>

        <span class="c1"># Split on special tokens first
</span>        <span class="k">for</span> <span class="n">special_token</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">special_tokens</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">special_token</span> <span class="ow">in</span> <span class="n">remaining_text</span><span class="p">:</span>
                <span class="n">parts</span> <span class="o">=</span> <span class="n">remaining_text</span><span class="p">.</span><span class="n">split</span><span class="p">(</span><span class="n">special_token</span><span class="p">)</span>
                <span class="n">new_parts</span> <span class="o">=</span> <span class="p">[]</span>
                <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">part</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">parts</span><span class="p">):</span>
                    <span class="k">if</span> <span class="n">i</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                        <span class="c1"># Add special token
</span>                        <span class="n">special_bytes</span> <span class="o">=</span> <span class="n">special_token</span><span class="p">.</span><span class="n">encode</span><span class="p">(</span><span class="s">'utf-8'</span><span class="p">)</span>
                        <span class="n">special_id</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">bytes_to_id</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="n">special_bytes</span><span class="p">)</span>
                        <span class="k">if</span> <span class="n">special_id</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
                            <span class="n">token_ids</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">special_id</span><span class="p">)</span>
                    <span class="k">if</span> <span class="n">part</span><span class="p">:</span>
                        <span class="n">new_parts</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">part</span><span class="p">)</span>
                <span class="n">remaining_text</span> <span class="o">=</span> <span class="s">''</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">new_parts</span><span class="p">)</span>

        <span class="c1"># Apply regex tokenization
</span>        <span class="k">for</span> <span class="n">match</span> <span class="ow">in</span> <span class="n">re</span><span class="p">.</span><span class="n">finditer</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">pattern</span><span class="p">,</span> <span class="n">remaining_text</span><span class="p">):</span>
            <span class="n">word</span> <span class="o">=</span> <span class="n">match</span><span class="p">.</span><span class="n">group</span><span class="p">()</span>
            <span class="n">word_tokens</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">_encode_word</span><span class="p">(</span><span class="n">word</span><span class="p">)</span>
            <span class="n">token_ids</span><span class="p">.</span><span class="n">extend</span><span class="p">(</span><span class="n">word_tokens</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">token_ids</span>

    <span class="k">def</span> <span class="nf">_encode_word</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">word</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">]:</span>
        <span class="s">"""Encode a single word using BPE merges."""</span>
        <span class="c1"># Start with individual bytes
</span>        <span class="n">word_bytes</span> <span class="o">=</span> <span class="n">word</span><span class="p">.</span><span class="n">encode</span><span class="p">(</span><span class="s">'utf-8'</span><span class="p">)</span>
        <span class="n">tokens</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="c1"># Convert each byte to its token ID
</span>        <span class="k">for</span> <span class="n">byte_val</span> <span class="ow">in</span> <span class="n">word_bytes</span><span class="p">:</span>
            <span class="n">tokens</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">byte_val</span><span class="p">)</span>  <span class="c1"># Byte token IDs are 0-255
</span>
        <span class="c1"># Apply BPE merges
</span>        <span class="k">while</span> <span class="nb">len</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="c1"># Find the best merge to apply
</span>            <span class="n">best_merge</span> <span class="o">=</span> <span class="bp">None</span>
            <span class="n">best_pos</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
            <span class="n">best_merge_priority</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="s">'inf'</span><span class="p">)</span>

            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>
                <span class="n">pair</span> <span class="o">=</span> <span class="p">(</span><span class="n">tokens</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">tokens</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">])</span>
                <span class="k">if</span> <span class="n">pair</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">merge_rules</span><span class="p">:</span>
                    <span class="c1"># Find merge priority (earlier merges have higher priority)
</span>                    <span class="n">merged_bytes</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">id_to_bytes</span><span class="p">[</span><span class="n">tokens</span><span class="p">[</span><span class="n">i</span><span class="p">]]</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">id_to_bytes</span><span class="p">[</span><span class="n">tokens</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]]</span>
                    <span class="k">for</span> <span class="n">j</span><span class="p">,</span> <span class="p">(</span><span class="n">left_bytes</span><span class="p">,</span> <span class="n">right_bytes</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">merges</span><span class="p">):</span>
                        <span class="k">if</span> <span class="n">left_bytes</span> <span class="o">+</span> <span class="n">right_bytes</span> <span class="o">==</span> <span class="n">merged_bytes</span><span class="p">:</span>
                            <span class="k">if</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="n">best_merge_priority</span><span class="p">:</span>
                                <span class="n">best_merge</span> <span class="o">=</span> <span class="n">pair</span>
                                <span class="n">best_pos</span> <span class="o">=</span> <span class="n">i</span>
                                <span class="n">best_merge_priority</span> <span class="o">=</span> <span class="n">j</span>
                            <span class="k">break</span>

            <span class="k">if</span> <span class="n">best_merge</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
                <span class="k">break</span>

            <span class="c1"># Apply the best merge
</span>            <span class="n">new_tokens</span> <span class="o">=</span> <span class="n">tokens</span><span class="p">[:</span><span class="n">best_pos</span><span class="p">]</span>
            <span class="n">new_tokens</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">merge_rules</span><span class="p">[</span><span class="n">best_merge</span><span class="p">])</span>
            <span class="n">new_tokens</span><span class="p">.</span><span class="n">extend</span><span class="p">(</span><span class="n">tokens</span><span class="p">[</span><span class="n">best_pos</span> <span class="o">+</span> <span class="mi">2</span><span class="p">:])</span>
            <span class="n">tokens</span> <span class="o">=</span> <span class="n">new_tokens</span>

        <span class="k">return</span> <span class="n">tokens</span>

    <span class="k">def</span> <span class="nf">decode</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">token_ids</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
        <span class="s">"""Decode token IDs back to text."""</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">token_ids</span><span class="p">:</span>
            <span class="k">return</span> <span class="s">""</span>

        <span class="c1"># Convert token IDs to bytes
</span>        <span class="n">byte_sequences</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">token_id</span> <span class="ow">in</span> <span class="n">token_ids</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">token_id</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">id_to_bytes</span><span class="p">:</span>
                <span class="n">byte_sequences</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">id_to_bytes</span><span class="p">[</span><span class="n">token_id</span><span class="p">])</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># Handle unknown tokens
</span>                <span class="n">byte_sequences</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="sa">b</span><span class="s">'&lt;UNK&gt;'</span><span class="p">)</span>

        <span class="c1"># Concatenate all bytes and decode
</span>        <span class="n">all_bytes</span> <span class="o">=</span> <span class="sa">b</span><span class="s">''</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">byte_sequences</span><span class="p">)</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">all_bytes</span><span class="p">.</span><span class="n">decode</span><span class="p">(</span><span class="s">'utf-8'</span><span class="p">,</span> <span class="n">errors</span><span class="o">=</span><span class="s">'replace'</span><span class="p">)</span>
        <span class="k">except</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">all_bytes</span><span class="p">.</span><span class="n">decode</span><span class="p">(</span><span class="s">'utf-8'</span><span class="p">,</span> <span class="n">errors</span><span class="o">=</span><span class="s">'ignore'</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">tokenize_with_details</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">text</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>
        <span class="s">"""Tokenize text and show detailed breakdown."""</span>
        <span class="n">token_ids</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">encode</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>

        <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Original text: '</span><span class="si">{</span><span class="n">text</span><span class="si">}</span><span class="s">'"</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Length: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">text</span><span class="p">)</span><span class="si">}</span><span class="s"> characters"</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"UTF-8 bytes: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">text</span><span class="p">.</span><span class="n">encode</span><span class="p">(</span><span class="s">'utf-8'</span><span class="p">))</span><span class="si">}</span><span class="s"> bytes"</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Token count: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">token_ids</span><span class="p">)</span><span class="si">}</span><span class="s"> tokens"</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Compression ratio: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">text</span><span class="p">.</span><span class="n">encode</span><span class="p">(</span><span class="s">'utf-8'</span><span class="p">))</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">token_ids</span><span class="p">)</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s">x"</span><span class="p">)</span>
        <span class="k">print</span><span class="p">()</span>

        <span class="k">print</span><span class="p">(</span><span class="s">"Token breakdown:"</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">token_id</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">token_ids</span><span class="p">):</span>
            <span class="n">token_bytes</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">id_to_bytes</span><span class="p">[</span><span class="n">token_id</span><span class="p">]</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="n">token_str</span> <span class="o">=</span> <span class="n">token_bytes</span><span class="p">.</span><span class="n">decode</span><span class="p">(</span><span class="s">'utf-8'</span><span class="p">,</span> <span class="n">errors</span><span class="o">=</span><span class="s">'replace'</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">token_str</span><span class="p">.</span><span class="n">isprintable</span><span class="p">():</span>
                    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"  </span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">:</span><span class="mi">2</span><span class="n">d</span><span class="si">}</span><span class="s">. Token </span><span class="si">{</span><span class="n">token_id</span><span class="si">:</span><span class="mi">4</span><span class="n">d</span><span class="si">}</span><span class="s">: '</span><span class="si">{</span><span class="n">token_str</span><span class="si">}</span><span class="s">' (</span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">token_bytes</span><span class="p">)</span><span class="si">}</span><span class="s"> bytes)"</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"  </span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">:</span><span class="mi">2</span><span class="n">d</span><span class="si">}</span><span class="s">. Token </span><span class="si">{</span><span class="n">token_id</span><span class="si">:</span><span class="mi">4</span><span class="n">d</span><span class="si">}</span><span class="s">: </span><span class="si">{</span><span class="nb">repr</span><span class="p">(</span><span class="n">token_str</span><span class="p">)</span><span class="si">}</span><span class="s"> (</span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">token_bytes</span><span class="p">)</span><span class="si">}</span><span class="s"> bytes)"</span><span class="p">)</span>
            <span class="k">except</span><span class="p">:</span>
                <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"  </span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">:</span><span class="mi">2</span><span class="n">d</span><span class="si">}</span><span class="s">. Token </span><span class="si">{</span><span class="n">token_id</span><span class="si">:</span><span class="mi">4</span><span class="n">d</span><span class="si">}</span><span class="s">: </span><span class="si">{</span><span class="n">token_bytes</span><span class="si">}</span><span class="s"> (binary)"</span><span class="p">)</span>

        <span class="c1"># Verify round-trip
</span>        <span class="n">decoded</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">decode</span><span class="p">(</span><span class="n">token_ids</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"</span><span class="se">\n</span><span class="s">Decoded text: '</span><span class="si">{</span><span class="n">decoded</span><span class="si">}</span><span class="s">'"</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Round-trip successful: </span><span class="si">{</span><span class="n">text</span> <span class="o">==</span> <span class="n">decoded</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">token_ids</span>
</code></pre></div></div>

<p>Let us compose some simple test cases below:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">test_bpe_tokenizer</span><span class="p">():</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"="</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"BPE TOKENIZER SAMPLE TESTS"</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"="</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>

    <span class="c1"># Load the trained tokenizer
</span>    <span class="k">try</span><span class="p">:</span>
        <span class="n">vocab</span><span class="p">,</span> <span class="n">merges</span> <span class="o">=</span> <span class="n">load_tokenizer</span><span class="p">(</span><span class="s">'tinystories_vocab.pkl'</span><span class="p">,</span> <span class="s">'tinystories_merges.pkl'</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"✓ Loaded tokenizer with </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">vocab</span><span class="p">)</span><span class="si">}</span><span class="s"> vocab entries and </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">merges</span><span class="p">)</span><span class="si">}</span><span class="s"> merges"</span><span class="p">)</span>
    <span class="k">except</span> <span class="nb">FileNotFoundError</span><span class="p">:</span>
        <span class="k">print</span><span class="p">(</span><span class="s">"Error: Tokenizer files not found!"</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="s">"Please run the training script first to create 'tinystories_vocab.pkl' and 'tinystories_merges.pkl'"</span><span class="p">)</span>
        <span class="k">return</span>

    <span class="c1"># Create tokenizer instance
</span>    <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">SimpleBPETokenizer</span><span class="p">(</span><span class="n">vocab</span><span class="p">,</span> <span class="n">merges</span><span class="p">)</span>
    <span class="k">print</span><span class="p">()</span>

    <span class="c1"># Example 1: Simple sentence
</span>    <span class="k">print</span><span class="p">(</span><span class="s">"EXAMPLE 1: Simple sentence"</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"-"</span> <span class="o">*</span> <span class="mi">30</span><span class="p">)</span>
    <span class="n">text1</span> <span class="o">=</span> <span class="s">"Hello world! How are you today?"</span>
    <span class="n">tokenizer</span><span class="p">.</span><span class="n">tokenize_with_details</span><span class="p">(</span><span class="n">text1</span><span class="p">)</span>
    <span class="k">print</span><span class="p">()</span>

    <span class="c1"># Example 2: Text with special token
</span>    <span class="k">print</span><span class="p">(</span><span class="s">"EXAMPLE 2: Text with special token"</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"-"</span> <span class="o">*</span> <span class="mi">30</span><span class="p">)</span>
    <span class="n">text2</span> <span class="o">=</span> <span class="s">"Once upon a time&lt;|endoftext|&gt;The end."</span>
    <span class="n">tokenizer</span><span class="p">.</span><span class="n">tokenize_with_details</span><span class="p">(</span><span class="n">text2</span><span class="p">)</span>
    <span class="k">print</span><span class="p">()</span>

    <span class="c1"># Example 3: Repeated words (should compress well)
</span>    <span class="k">print</span><span class="p">(</span><span class="s">"EXAMPLE 3: Repeated words"</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"-"</span> <span class="o">*</span> <span class="mi">30</span><span class="p">)</span>
    <span class="n">text3</span> <span class="o">=</span> <span class="s">"the the the cat cat sat sat on on the the mat mat"</span>
    <span class="n">tokenizer</span><span class="p">.</span><span class="n">tokenize_with_details</span><span class="p">(</span><span class="n">text3</span><span class="p">)</span>
    <span class="k">print</span><span class="p">()</span>

    <span class="c1"># Example 4: Numbers and punctuation
</span>    <span class="k">print</span><span class="p">(</span><span class="s">"EXAMPLE 4: Numbers and punctuation"</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"-"</span> <span class="o">*</span> <span class="mi">30</span><span class="p">)</span>
    <span class="n">text4</span> <span class="o">=</span> <span class="s">"I have 123 apples, 456 oranges, and 789 bananas!"</span>
    <span class="n">tokenizer</span><span class="p">.</span><span class="n">tokenize_with_details</span><span class="p">(</span><span class="n">text4</span><span class="p">)</span>
    <span class="k">print</span><span class="p">()</span>

    <span class="c1"># Example 5: Just encoding/decoding
</span>    <span class="k">print</span><span class="p">(</span><span class="s">"EXAMPLE 5: Simple encode/decode"</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"-"</span> <span class="o">*</span> <span class="mi">30</span><span class="p">)</span>
    <span class="n">text5</span> <span class="o">=</span> <span class="s">"This is a test."</span>
    <span class="n">token_ids</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">.</span><span class="n">encode</span><span class="p">(</span><span class="n">text5</span><span class="p">)</span>
    <span class="n">decoded_text</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">.</span><span class="n">decode</span><span class="p">(</span><span class="n">token_ids</span><span class="p">)</span>

    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Original: '</span><span class="si">{</span><span class="n">text5</span><span class="si">}</span><span class="s">'"</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Token IDs: </span><span class="si">{</span><span class="n">token_ids</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Decoded: '</span><span class="si">{</span><span class="n">decoded_text</span><span class="si">}</span><span class="s">'"</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Match: </span><span class="si">{</span><span class="n">text5</span> <span class="o">==</span> <span class="n">decoded_text</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
    <span class="k">print</span><span class="p">()</span>

    <span class="c1"># Show some vocabulary statistics
</span>    <span class="k">print</span><span class="p">(</span><span class="s">"VOCABULARY STATISTICS"</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"-"</span> <span class="o">*</span> <span class="mi">30</span><span class="p">)</span>
    <span class="n">byte_tokens</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="mi">1</span> <span class="k">for</span> <span class="n">tid</span> <span class="ow">in</span> <span class="n">vocab</span><span class="p">.</span><span class="n">keys</span><span class="p">()</span> <span class="k">if</span> <span class="n">tid</span> <span class="o">&lt;</span> <span class="mi">256</span><span class="p">)</span>
    <span class="n">special_tokens</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="mi">1</span> <span class="k">for</span> <span class="n">tid</span><span class="p">,</span> <span class="n">token_bytes</span> <span class="ow">in</span> <span class="n">vocab</span><span class="p">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="sa">b</span><span class="s">'&lt;|'</span> <span class="ow">in</span> <span class="n">token_bytes</span><span class="p">)</span>
    <span class="n">merged_tokens</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">vocab</span><span class="p">)</span> <span class="o">-</span> <span class="n">byte_tokens</span> <span class="o">-</span> <span class="n">special_tokens</span>

    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Byte tokens (0-255): </span><span class="si">{</span><span class="n">byte_tokens</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Special tokens: </span><span class="si">{</span><span class="n">special_tokens</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Merged tokens: </span><span class="si">{</span><span class="n">merged_tokens</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Total vocabulary: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">vocab</span><span class="p">)</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>

    <span class="c1"># Show some example merged tokens
</span>    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"</span><span class="se">\n</span><span class="s">Sample merged tokens:"</span><span class="p">)</span>
    <span class="n">merged_token_ids</span> <span class="o">=</span> <span class="p">[</span><span class="n">tid</span> <span class="k">for</span> <span class="n">tid</span> <span class="ow">in</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">vocab</span><span class="p">.</span><span class="n">keys</span><span class="p">())</span> <span class="k">if</span> <span class="n">tid</span> <span class="o">&gt;=</span> <span class="mi">257</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">token_id</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">merged_token_ids</span><span class="p">[:</span><span class="mi">10</span><span class="p">]):</span>
        <span class="n">token_bytes</span> <span class="o">=</span> <span class="n">vocab</span><span class="p">[</span><span class="n">token_id</span><span class="p">]</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">decoded</span> <span class="o">=</span> <span class="n">token_bytes</span><span class="p">.</span><span class="n">decode</span><span class="p">(</span><span class="s">'utf-8'</span><span class="p">,</span> <span class="n">errors</span><span class="o">=</span><span class="s">'replace'</span><span class="p">)</span>
            <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"  Token </span><span class="si">{</span><span class="n">token_id</span><span class="si">}</span><span class="s">: '</span><span class="si">{</span><span class="n">decoded</span><span class="si">}</span><span class="s">' (</span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">token_bytes</span><span class="p">)</span><span class="si">}</span><span class="s"> bytes)"</span><span class="p">)</span>
        <span class="k">except</span><span class="p">:</span>
            <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"  Token </span><span class="si">{</span><span class="n">token_id</span><span class="si">}</span><span class="s">: </span><span class="si">{</span><span class="n">token_bytes</span><span class="si">}</span><span class="s"> (binary)"</span><span class="p">)</span>

    <span class="k">print</span><span class="p">(</span><span class="s">"</span><span class="se">\n</span><span class="s">"</span> <span class="o">+</span> <span class="s">"="</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"All examples completed successfully!"</span><span class="p">)</span>
</code></pre></div></div>

<h1 id="bpe-tokenizer-sample-tests">BPE Tokenizer Sample Tests</h1>

<p>Now run our complete test suite:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">test_bpe_tokenizer</span><span class="p">()</span>
</code></pre></div></div>
<p>Based on the training output from the TinyStories dataset, here are the testing results:</p>

<p>✓ Loaded tokenizer with 10000 vocab entries and 9743 merges</p>

<h2 id="example-1-simple-sentence">Example 1: Simple sentence</h2>

<p><strong>Original text:</strong> ‘Hello world! How are you today?’<br />
<strong>Length:</strong> 31 characters<br />
<strong>UTF-8 bytes:</strong> 31 bytes<br />
<strong>Token count:</strong> 8 tokens<br />
<strong>Compression ratio:</strong> 3.88x</p>

<h3 id="token-breakdown">Token breakdown:</h3>
<ol>
  <li>Token 1183: ‘Hello’ (5 bytes)</li>
  <li>Token 1569: ‘ world’ (6 bytes)</li>
  <li>Token 33: ‘!’ (1 bytes)</li>
  <li>Token 2683: ‘ How’ (4 bytes)</li>
  <li>Token 483: ‘ are’ (4 bytes)</li>
  <li>Token 349: ‘ you’ (4 bytes)</li>
  <li>Token 1709: ‘ today’ (6 bytes)</li>
  <li>Token 63: ‘?’ (1 bytes)</li>
</ol>

<p><strong>Decoded text:</strong> ‘Hello world! How are you today?’<br />
<strong>Round-trip successful:</strong> True</p>

<h2 id="example-2-text-with-special-token">Example 2: Text with special token</h2>

<p><strong>Original text:</strong> ‘Once upon a time&lt;|endoftext|&gt;The end.’<br />
<strong>Length:</strong> 37 characters<br />
<strong>UTF-8 bytes:</strong> 37 bytes<br />
<strong>Token count:</strong> 8 tokens<br />
<strong>Compression ratio:</strong> 4.62x</p>

<h3 id="token-breakdown-1">Token breakdown:</h3>
<ol>
  <li>Token 256: ‘<code class="language-plaintext highlighter-rouge">&lt;|endoftext|&gt;</code>’ (13 bytes)</li>
  <li>Token 430: ‘Once’ (4 bytes)</li>
  <li>Token 439: ‘ upon’ (5 bytes)</li>
  <li>Token 259: ‘ a’ (2 bytes)</li>
  <li>Token 398: ‘ time’ (5 bytes)</li>
  <li>Token 410: ‘The’ (3 bytes)</li>
  <li>Token 870: ‘ end’ (4 bytes)</li>
  <li>Token 46: ‘.’ (1 bytes)</li>
</ol>

<p><strong>Decoded text:</strong> ‘&lt;|endoftext|&gt;Once upon a timeThe end.’<br />
<strong>Round-trip successful:</strong> False</p>

<h2 id="example-3-repeated-words">Example 3: Repeated words</h2>

<p><strong>Original text:</strong> ‘the the the cat cat sat sat on on the the mat mat’<br />
<strong>Length:</strong> 49 characters<br />
<strong>UTF-8 bytes:</strong> 49 bytes<br />
<strong>Token count:</strong> 13 tokens<br />
<strong>Compression ratio:</strong> 3.77x</p>

<h3 id="token-breakdown-2">Token breakdown:</h3>
<ol>
  <li>Token 7199: ‘the’ (3 bytes)</li>
  <li>Token 263: ‘ the’ (4 bytes)</li>
  <li>Token 263: ‘ the’ (4 bytes)</li>
  <li>Token 459: ‘ cat’ (4 bytes)</li>
  <li>Token 459: ‘ cat’ (4 bytes)</li>
  <li>Token 1091: ‘ sat’ (4 bytes)</li>
  <li>Token 1091: ‘ sat’ (4 bytes)</li>
  <li>Token 354: ‘ on’ (3 bytes)</li>
  <li>Token 354: ‘ on’ (3 bytes)</li>
  <li>Token 263: ‘ the’ (4 bytes)</li>
  <li>Token 263: ‘ the’ (4 bytes)</li>
  <li>Token 1492: ‘ mat’ (4 bytes)</li>
  <li>Token 1492: ‘ mat’ (4 bytes)</li>
</ol>

<p><strong>Decoded text:</strong> ‘the the the cat cat sat sat on on the the mat mat’<br />
<strong>Round-trip successful:</strong> True</p>

<h2 id="example-4-numbers-and-punctuation">Example 4: Numbers and punctuation</h2>

<p><strong>Original text:</strong> ‘I have 123 apples, 456 oranges, and 789 bananas!’<br />
<strong>Length:</strong> 48 characters<br />
<strong>UTF-8 bytes:</strong> 48 bytes<br />
<strong>Token count:</strong> 19 tokens<br />
<strong>Compression ratio:</strong> 2.53x</p>

<h3 id="token-breakdown-3">Token breakdown:</h3>
<ol>
  <li>Token 73: ‘I’ (1 bytes)</li>
  <li>Token 499: ‘ have’ (5 bytes)</li>
  <li>Token 6314: ‘ 1’ (2 bytes)</li>
  <li>Token 50: ‘2’ (1 bytes)</li>
  <li>Token 51: ‘3’ (1 bytes)</li>
  <li>Token 1836: ‘ apples’ (7 bytes)</li>
  <li>Token 44: ‘,’ (1 bytes)</li>
  <li>Token 9079: ‘ 4’ (2 bytes)</li>
  <li>Token 53: ‘5’ (1 bytes)</li>
  <li>Token 54: ‘6’ (1 bytes)</li>
  <li>Token 5193: ‘ oranges’ (8 bytes)</li>
  <li>Token 44: ‘,’ (1 bytes)</li>
  <li>Token 267: ‘ and’ (4 bytes)</li>
  <li>Token 32: ‘ ‘ (1 bytes)</li>
  <li>Token 55: ‘7’ (1 bytes)</li>
  <li>Token 56: ‘8’ (1 bytes)</li>
  <li>Token 57: ‘9’ (1 bytes)</li>
  <li>Token 3898: ‘ bananas’ (8 bytes)</li>
  <li>Token 33: ‘!’ (1 bytes)</li>
</ol>

<p><strong>Decoded text:</strong> ‘I have 123 apples, 456 oranges, and 789 bananas!’<br />
<strong>Round-trip successful:</strong> True</p>

<h2 id="example-5-simple-encodedecode">Example 5: Simple encode/decode</h2>

<p><strong>Original:</strong> ‘This is a test.’<br />
<strong>Token IDs:</strong> [1531, 431, 259, 2569, 46]<br />
<strong>Decoded:</strong> ‘This is a test.’<br />
<strong>Match:</strong> True</p>

<h2 id="vocabulary-statistics">Vocabulary Statistics</h2>

<p><strong>Byte tokens (0-255):</strong> 256<br />
<strong>Special tokens:</strong> 1<br />
<strong>Merged tokens:</strong> 9743<br />
<strong>Total vocabulary:</strong> 10000</p>

<h3 id="sample-merged-tokens">Sample merged tokens:</h3>
<ul>
  <li>Token 257: ‘ t’ (2 bytes)</li>
  <li>Token 258: ‘he’ (2 bytes)</li>
  <li>Token 259: ‘ a’ (2 bytes)</li>
  <li>Token 260: ‘ s’ (2 bytes)</li>
  <li>Token 261: ‘ w’ (2 bytes)</li>
  <li>Token 262: ‘nd’ (2 bytes)</li>
  <li>Token 263: ‘ the’ (4 bytes)</li>
  <li>Token 264: ‘ed’ (2 bytes)</li>
  <li>Token 265: ‘ b’ (2 bytes)</li>
  <li>Token 266: ‘ to’ (3 bytes)</li>
</ul>

<hr />
<p>All examples completed successfully!</p>]]></content><author><name>[&quot;Han Yu&quot;]</name></author><category term="cs336" /><summary type="html"><![CDATA[Building a BPE Tokenizer from Scratch: Train the Tokenizer using TinyStories Dataset]]></summary></entry><entry><title type="html">Study Notes: Stanford CS336 Language Modeling from Scratch [2]</title><link href="http://localhost:4000/cs336/2025/07/22/cs336-note-simple-bpe.html" rel="alternate" type="text/html" title="Study Notes: Stanford CS336 Language Modeling from Scratch [2]" /><published>2025-07-22T00:00:00-07:00</published><updated>2025-07-22T00:00:00-07:00</updated><id>http://localhost:4000/cs336/2025/07/22/cs336-note-simple-bpe</id><content type="html" xml:base="http://localhost:4000/cs336/2025/07/22/cs336-note-simple-bpe.html"><![CDATA[<h1 id="byte-pair-encoding-bpe-tokenizer-in-a-nutshell">Byte Pair Encoding (BPE) Tokenizer in a Nutshell</h1>
<h2 id="key-terms">Key Terms</h2>

<table>
  <thead>
    <tr>
      <th style="text-align: left">Concept</th>
      <th style="text-align: left">Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">Unicode</td>
      <td style="text-align: left">System that assigns every character a unique codepoint (e.g., ‘A’ → 65)</td>
    </tr>
    <tr>
      <td style="text-align: left">UTF-8</td>
      <td style="text-align: left">A way to encode those codepoints into 1-4 bytes</td>
    </tr>
    <tr>
      <td style="text-align: left">Byte</td>
      <td style="text-align: left">8 bits; one byte can hold values from 0 to 255</td>
    </tr>
    <tr>
      <td style="text-align: left">Tokenization</td>
      <td style="text-align: left">Breaking text corpus input into manageable pieces (tokens) for a model</td>
    </tr>
  </tbody>
</table>

<p>Let us take the following string as a simple example to illustrate the concept.</p>

<h2 id="example-encoding-a">Example: Encoding ‘A😊’</h2>

<h3 id="step-1-get-the-unicode-codepoints">Step 1: Get the Unicode Codepoints</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">text</span> <span class="o">=</span> <span class="s">'A😊'</span>
<span class="n">codepoints</span> <span class="o">=</span> <span class="p">[</span><span class="nb">ord</span><span class="p">(</span><span class="s">'A'</span><span class="p">),</span> <span class="nb">ord</span><span class="p">(</span><span class="s">'😊'</span><span class="p">)]</span>
<span class="k">print</span><span class="p">(</span><span class="n">codepoints</span><span class="p">)</span>  <span class="c1"># [65, 128522]
</span></code></pre></div></div>

<p>Output:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[65, 128522]
</code></pre></div></div>

<h3 id="step-2-utf-8-encoding-turn-codepoints-into-bytes">Step 2: UTF-8 Encoding (Turn Codepoints into Bytes)</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">utf8_bytes</span> <span class="o">=</span> <span class="n">text</span><span class="p">.</span><span class="n">encode</span><span class="p">(</span><span class="s">"utf-8"</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="nb">tuple</span><span class="p">(</span><span class="n">utf8_bytes</span><span class="p">))</span>  <span class="c1"># (65, 240, 159, 152, 138)
</span></code></pre></div></div>

<p>Output:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(65, 240, 159, 152, 138)
</code></pre></div></div>

<p>Here’s what happened:</p>
<ul>
  <li>‘A’ is encoded using one byte: 65</li>
  <li>‘😊’ is encoded using four bytes: [240, 159, 152, 138]</li>
  <li>‘A😊’ is encoded as the sequence [65, 240, 159, 152, 138]</li>
</ul>

<h2 id="why-using-utf-8-for-encoding-is-helpful">Why Using UTF-8 for Encoding is Helpful</h2>

<p>Instead of dealing with hundreds of thousands of possible codepoints (Unicode has more than 150,000 codepoints) or millions of words/subwords in vocabulary, we can model text using sequences of bytes. Each byte can be represented by an integer from 0 to 255, so we only need a vocabulary of size 256 to model input text. This approach is simple and complete—any character in any language can be represented as bytes, eliminating out-of-vocabulary token concerns.</p>

<h2 id="tokenization-spectrum">Tokenization Spectrum</h2>

<table>
  <thead>
    <tr>
      <th style="text-align: left">Tokenization Level</th>
      <th style="text-align: left">Example Tokens</th>
      <th style="text-align: left">Pros</th>
      <th style="text-align: left">Cons</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left"><strong>Word-level</strong></td>
      <td style="text-align: left"><code class="language-plaintext highlighter-rouge">["unbelievable"]</code></td>
      <td style="text-align: left">Human-readable, efficient</td>
      <td style="text-align: left">OOV (out-of-vocabulary) issues</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>Subword-level</strong> (BPE)</td>
      <td style="text-align: left"><code class="language-plaintext highlighter-rouge">["un", "believ", "able"]</code></td>
      <td style="text-align: left">Handles rare words, compact</td>
      <td style="text-align: left">Requires training</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>Byte-level</strong></td>
      <td style="text-align: left"><code class="language-plaintext highlighter-rouge">[117, 110, 98, 101, ...]</code> (bytes)</td>
      <td style="text-align: left">No OOV, simple</td>
      <td style="text-align: left">Longer sequences, less semantic meaning</td>
    </tr>
  </tbody>
</table>

<h2 id="why-subword-tokenization-is-the-middle-ground">Why Subword Tokenization is the Middle Ground</h2>

<p><strong>Subword tokenization</strong> with <strong>Byte Pair Encoding (BPE)</strong> provides a balance between the other approaches:</p>

<ul>
  <li><strong>Word-level tokenization</strong> struggles with rare or unseen words (e.g., “unbelievable” might be unknown even if “believe” is known)</li>
  <li><strong>Byte-level tokenization</strong> avoids unknown token issues but creates long, inefficient sequences</li>
  <li><strong>Subword tokenization</strong> (BPE):
    <ol>
      <li>Breaks rare words into familiar pieces (subwords)</li>
      <li>Retains compactness for common words</li>
      <li>Is learnable from corpus statistics</li>
    </ol>
  </li>
</ul>

<h2 id="byte-pair-encoding-bpe-algorithm-overview">Byte Pair Encoding (BPE) Algorithm Overview</h2>

<p>BPE starts from characters and iteratively <strong>merges the most frequent adjacent pairs</strong> into longer tokens.</p>

<h3 id="example-training-corpus">Example Training Corpus</h3>

<p>Consider this toy training corpus:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>"low"     (5 times)  
"lowest"  (2 times)  
"newest"  (6 times)  
"wider"   (3 times)
</code></pre></div></div>

<p>We want to learn a compact subword vocabulary that reuses frequent patterns like “low” and “est”.</p>

<h3 id="step-by-step-bpe-process">Step-by-Step BPE Process</h3>

<h4 id="step-0-preprocess-as-characters">Step 0: Preprocess as Characters</h4>
<p>Each word is broken into characters with an end-of-word marker <code class="language-plaintext highlighter-rouge">&lt;/w&gt;</code>:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>"l o w &lt;/w&gt;"        (5)
"l o w e s t &lt;/w&gt;"  (2)
"n e w e s t &lt;/w&gt;"  (6)
"w i d e r &lt;/w&gt;"    (3)
</code></pre></div></div>

<h4 id="step-1-count-adjacent-pairs">Step 1: Count Adjacent Pairs</h4>
<p>Compute most frequent adjacent pairs across all words:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>('e', 's') appears 8 times
('s', 't') appears 8 times
('l', 'o') appears 7 times
('o', 'w') appears 7 times
</code></pre></div></div>

<h4 id="step-2-merge-e--s--es">Step 2: Merge ‘e’ + ‘s’ → ‘es’</h4>
<p>Update vocabulary:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>"l o w &lt;/w&gt;"          (5)
"l o w es t &lt;/w&gt;"     (2)
"n e w es t &lt;/w&gt;"     (6)
"w i d e r &lt;/w&gt;"      (3)
</code></pre></div></div>

<h4 id="step-3-merge-es--t--est">Step 3: Merge ‘es’ + ‘t’ → ‘est’</h4>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>"l o w &lt;/w&gt;"         (5)
"l o w est &lt;/w&gt;"     (2)
"n e w est &lt;/w&gt;"     (6)
"w i d e r &lt;/w&gt;"     (3)
</code></pre></div></div>

<h4 id="step-4-merge-l--o--lo-then-lo--w--low">Step 4: Merge ‘l’ + ‘o’ → ‘lo’, then ‘lo’ + ‘w’ → ‘low’</h4>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>"low &lt;/w&gt;"          (5)
"low est &lt;/w&gt;"      (2)
"n e w est &lt;/w&gt;"    (6)
"w i d e r &lt;/w&gt;"    (3)
</code></pre></div></div>

<h4 id="continue-merging">Continue merging…</h4>
<p>Eventually we learn useful building blocks like ‘low’, ‘est’, and ‘new’. After training, “newest” would tokenize to <code class="language-plaintext highlighter-rouge">['new', 'est', '&lt;/w&gt;']</code>.</p>

<h2 id="bpe-implementation">BPE Implementation</h2>

<p>Below is a complete implementation demonstrating the BPE algorithm on the corpus:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>"low low low low low lower lower widest widest widest newest newest newest newest newest newest"
</code></pre></div></div>

<h3 id="key-components">Key Components</h3>

<ol>
  <li><strong>Initialization</strong>: Creates vocabulary with <code class="language-plaintext highlighter-rouge">&lt;|endoftext|&gt;</code> special token and all 256 byte values</li>
  <li><strong>Pre-tokenization</strong>: Splits text on whitespace and converts words to byte tuples</li>
  <li><strong>Pair Frequency Counting</strong>: Counts all adjacent byte pairs across the corpus</li>
  <li><strong>Merging</strong>: Merges the most frequent pair (lexicographically largest in case of ties)</li>
  <li><strong>Tokenization</strong>: Uses learned merges to tokenize new words</li>
</ol>

<h3 id="how-it-works">How It Works</h3>

<ol>
  <li><strong>Pre-tokenization</strong>: Converts <code class="language-plaintext highlighter-rouge">"low low low..."</code> into <code class="language-plaintext highlighter-rouge">{(l,o,w): 5, (l,o,w,e,r): 2, ...}</code></li>
  <li><strong>Merge Selection</strong>: Finds most frequent pairs like <code class="language-plaintext highlighter-rouge">('s','t')</code> and <code class="language-plaintext highlighter-rouge">('e','s')</code>, chooses lexicographically larger <code class="language-plaintext highlighter-rouge">('s','t')</code></li>
  <li><strong>Iterative Merging</strong>: Continues merging until desired number of merges is reached</li>
  <li><strong>Tokenization</strong>: Applies learned merges in order to tokenize new words</li>
</ol>

<h3 id="expected-output">Expected Output</h3>

<p>With 6 merges, the algorithm produces:</p>
<ul>
  <li><strong>Merges</strong>: <code class="language-plaintext highlighter-rouge">['s t', 'e st', 'o w', 'l ow', 'w est', 'n e']</code></li>
  <li><strong>Final vocabulary</strong>: <code class="language-plaintext highlighter-rouge">&lt;|endoftext|&gt;</code>, 256 byte chars, <code class="language-plaintext highlighter-rouge">st</code>, <code class="language-plaintext highlighter-rouge">est</code>, <code class="language-plaintext highlighter-rouge">ow</code>, <code class="language-plaintext highlighter-rouge">low</code>, <code class="language-plaintext highlighter-rouge">west</code>, <code class="language-plaintext highlighter-rouge">ne</code></li>
  <li><strong>“newest” tokenizes as</strong>: <code class="language-plaintext highlighter-rouge">['ne', 'west']</code></li>
</ul>

<p>Below is one implementation for Algorithm 1 of <a href="https://arxiv.org/abs/1508.07909">Sennrich et al. [2016]</a>.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">defaultdict</span><span class="p">,</span> <span class="n">Counter</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">List</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">,</span> <span class="n">Set</span><span class="p">,</span> <span class="n">Union</span>

<span class="k">class</span> <span class="nc">BPEEncoder</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># Initialize vocabulary with special token and 256 byte values
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">vocab</span> <span class="o">=</span> <span class="p">{</span><span class="s">"&lt;|endoftext|&gt;"</span><span class="p">:</span> <span class="mi">0</span><span class="p">}</span>
        <span class="c1"># Add all possible byte values (0-255) to vocabulary
</span>        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">256</span><span class="p">):</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">vocab</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span>

        <span class="bp">self</span><span class="p">.</span><span class="n">merges</span> <span class="o">=</span> <span class="p">[]</span>  <span class="c1"># List of (token1, token2) pairs that were merged
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">merge_tokens</span> <span class="o">=</span> <span class="p">{}</span>  <span class="c1"># Maps (token1, token2) -&gt; new_token_id
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">token_names</span> <span class="o">=</span> <span class="p">{}</span>  <span class="c1"># Maps token_id -&gt; readable name
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">next_token_id</span> <span class="o">=</span> <span class="mi">257</span>

    <span class="k">def</span> <span class="nf">pre_tokenize</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">text</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="p">...],</span> <span class="nb">int</span><span class="p">]:</span>
        <span class="s">"""
        Pre-tokenize text by splitting on whitespace and convert to byte tuples.
        Returns frequency count of each word as tuple of byte integers.
        For example, converts "low low low..." into {(l,o,w): 5, (l,o,w,e,r): 2, ...}
        """</span>
        <span class="n">words</span> <span class="o">=</span> <span class="n">text</span><span class="p">.</span><span class="n">split</span><span class="p">()</span>
        <span class="n">word_freq</span> <span class="o">=</span> <span class="n">Counter</span><span class="p">(</span><span class="n">words</span><span class="p">)</span>

        <span class="c1"># Convert to tuple of byte integers
</span>        <span class="n">byte_word_freq</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">freq</span> <span class="ow">in</span> <span class="n">word_freq</span><span class="p">.</span><span class="n">items</span><span class="p">():</span>
            <span class="n">byte_tuple</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">word</span><span class="p">.</span><span class="n">encode</span><span class="p">(</span><span class="s">'utf-8'</span><span class="p">))</span>
            <span class="n">byte_word_freq</span><span class="p">[</span><span class="n">byte_tuple</span><span class="p">]</span> <span class="o">=</span> <span class="n">freq</span>

        <span class="k">return</span> <span class="n">byte_word_freq</span>

    <span class="k">def</span> <span class="nf">get_pair_frequencies</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">word_freq</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">str</span><span class="p">],</span> <span class="p">...],</span> <span class="nb">int</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">str</span><span class="p">],</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">str</span><span class="p">]],</span> <span class="nb">int</span><span class="p">]:</span>
        <span class="s">"""
        Count frequency of all adjacent token pairs across all words.
        """</span>
        <span class="n">pair_freq</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">freq</span> <span class="ow">in</span> <span class="n">word_freq</span><span class="p">.</span><span class="n">items</span><span class="p">():</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">word</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>
                <span class="n">pair</span> <span class="o">=</span> <span class="p">(</span><span class="n">word</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">word</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">])</span>
                <span class="n">pair_freq</span><span class="p">[</span><span class="n">pair</span><span class="p">]</span> <span class="o">+=</span> <span class="n">freq</span>

        <span class="k">return</span> <span class="nb">dict</span><span class="p">(</span><span class="n">pair_freq</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">merge_pair</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">word_freq</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">str</span><span class="p">],</span> <span class="p">...],</span> <span class="nb">int</span><span class="p">],</span>
                   <span class="n">pair_to_merge</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">str</span><span class="p">],</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">str</span><span class="p">]],</span>
                   <span class="n">new_token</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">str</span><span class="p">],</span> <span class="p">...],</span> <span class="nb">int</span><span class="p">]:</span>
        <span class="s">"""
        Merge the specified pair in all words where it appears.
        """</span>
        <span class="n">new_word_freq</span> <span class="o">=</span> <span class="p">{}</span>

        <span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">freq</span> <span class="ow">in</span> <span class="n">word_freq</span><span class="p">.</span><span class="n">items</span><span class="p">():</span>
            <span class="n">new_word</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="k">while</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">word</span><span class="p">):</span>
                <span class="c1"># Check if current position matches the pair to merge
</span>                <span class="k">if</span> <span class="p">(</span><span class="n">i</span> <span class="o">&lt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">word</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span> <span class="ow">and</span>
                    <span class="n">word</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">==</span> <span class="n">pair_to_merge</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="ow">and</span>
                    <span class="n">word</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="n">pair_to_merge</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
                    <span class="n">new_word</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">new_token</span><span class="p">)</span>
                    <span class="n">i</span> <span class="o">+=</span> <span class="mi">2</span>  <span class="c1"># Skip both tokens of the pair
</span>                <span class="k">else</span><span class="p">:</span>
                    <span class="n">new_word</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">word</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
                    <span class="n">i</span> <span class="o">+=</span> <span class="mi">1</span>

            <span class="n">new_word_freq</span><span class="p">[</span><span class="nb">tuple</span><span class="p">(</span><span class="n">new_word</span><span class="p">)]</span> <span class="o">=</span> <span class="n">freq</span>

        <span class="k">return</span> <span class="n">new_word_freq</span>

    <span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">text</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">num_merges</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
        <span class="s">"""
        Train BPE on the given text for specified number of merges.
        Returns list of merge operations performed.
        """</span>
        <span class="c1"># Pre-tokenize text
</span>        <span class="n">word_freq</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">pre_tokenize</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Initial word frequencies: </span><span class="si">{</span><span class="bp">self</span><span class="p">.</span><span class="n">_format_word_freq</span><span class="p">(</span><span class="n">word_freq</span><span class="p">)</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>

        <span class="n">merges_performed</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="k">for</span> <span class="n">merge_step</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_merges</span><span class="p">):</span>
            <span class="c1"># Get pair frequencies
</span>            <span class="n">pair_freq</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">get_pair_frequencies</span><span class="p">(</span><span class="n">word_freq</span><span class="p">)</span>

            <span class="k">if</span> <span class="ow">not</span> <span class="n">pair_freq</span><span class="p">:</span>
                <span class="k">break</span>

            <span class="c1"># Find most frequent pair (lexicographically largest in case of tie)
</span>            <span class="n">max_freq</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">pair_freq</span><span class="p">.</span><span class="n">values</span><span class="p">())</span>
            <span class="n">most_frequent_pairs</span> <span class="o">=</span> <span class="p">[</span><span class="n">pair</span> <span class="k">for</span> <span class="n">pair</span><span class="p">,</span> <span class="n">freq</span> <span class="ow">in</span> <span class="n">pair_freq</span><span class="p">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="n">freq</span> <span class="o">==</span> <span class="n">max_freq</span><span class="p">]</span>

            <span class="c1"># Sort pairs lexicographically - convert to comparable format
</span>            <span class="k">def</span> <span class="nf">pair_sort_key</span><span class="p">(</span><span class="n">pair</span><span class="p">):</span>
                <span class="k">def</span> <span class="nf">token_to_str</span><span class="p">(</span><span class="n">token</span><span class="p">):</span>
                    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">token</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
                        <span class="k">return</span> <span class="nb">chr</span><span class="p">(</span><span class="n">token</span><span class="p">)</span>
                    <span class="k">return</span> <span class="nb">str</span><span class="p">(</span><span class="n">token</span><span class="p">)</span>
                <span class="k">return</span> <span class="p">(</span><span class="n">token_to_str</span><span class="p">(</span><span class="n">pair</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="n">token_to_str</span><span class="p">(</span><span class="n">pair</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>

            <span class="c1"># Take lexicographically largest (max)
</span>            <span class="n">best_pair</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">most_frequent_pairs</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="n">pair_sort_key</span><span class="p">)</span>

            <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"</span><span class="se">\n</span><span class="s">Merge </span><span class="si">{</span><span class="n">merge_step</span> <span class="o">+</span> <span class="mi">1</span><span class="si">}</span><span class="s">:"</span><span class="p">)</span>
            <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Pair frequencies: </span><span class="si">{</span><span class="bp">self</span><span class="p">.</span><span class="n">_format_pair_freq</span><span class="p">(</span><span class="n">pair_freq</span><span class="p">)</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
            <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Most frequent pair: </span><span class="si">{</span><span class="bp">self</span><span class="p">.</span><span class="n">_format_pair</span><span class="p">(</span><span class="n">best_pair</span><span class="p">)</span><span class="si">}</span><span class="s"> (freq: </span><span class="si">{</span><span class="n">max_freq</span><span class="si">}</span><span class="s">)"</span><span class="p">)</span>

            <span class="c1"># Create new token name
</span>            <span class="n">new_token</span> <span class="o">=</span> <span class="sa">f</span><span class="s">"merge_</span><span class="si">{</span><span class="bp">self</span><span class="p">.</span><span class="n">next_token_id</span><span class="si">}</span><span class="s">"</span>

            <span class="c1"># Perform the merge
</span>            <span class="n">word_freq</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">merge_pair</span><span class="p">(</span><span class="n">word_freq</span><span class="p">,</span> <span class="n">best_pair</span><span class="p">,</span> <span class="n">new_token</span><span class="p">)</span>

            <span class="c1"># Record the merge
</span>            <span class="n">token1_name</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">_token_to_str</span><span class="p">(</span><span class="n">best_pair</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
            <span class="n">token2_name</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">_token_to_str</span><span class="p">(</span><span class="n">best_pair</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
            <span class="n">merge_str</span> <span class="o">=</span> <span class="sa">f</span><span class="s">"</span><span class="si">{</span><span class="n">token1_name</span><span class="si">}</span><span class="s"> </span><span class="si">{</span><span class="n">token2_name</span><span class="si">}</span><span class="s">"</span>
            <span class="n">merges_performed</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">merge_str</span><span class="p">)</span>

            <span class="c1"># Store merge information
</span>            <span class="bp">self</span><span class="p">.</span><span class="n">merges</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">best_pair</span><span class="p">)</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">merge_tokens</span><span class="p">[</span><span class="n">best_pair</span><span class="p">]</span> <span class="o">=</span> <span class="n">new_token</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">vocab</span><span class="p">[</span><span class="n">new_token</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">next_token_id</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">token_names</span><span class="p">[</span><span class="n">new_token</span><span class="p">]</span> <span class="o">=</span> <span class="n">merge_str</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">next_token_id</span> <span class="o">+=</span> <span class="mi">1</span>

            <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"After merge: </span><span class="si">{</span><span class="bp">self</span><span class="p">.</span><span class="n">_format_word_freq</span><span class="p">(</span><span class="n">word_freq</span><span class="p">)</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">merges_performed</span>

    <span class="k">def</span> <span class="nf">tokenize</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">word</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
        <span class="s">"""
        Tokenize a word using the learned BPE merges.
        """</span>
        <span class="c1"># Start with individual bytes as integers
</span>        <span class="n">tokens</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">word</span><span class="p">.</span><span class="n">encode</span><span class="p">(</span><span class="s">'utf-8'</span><span class="p">))</span>

        <span class="c1"># Apply merges in order
</span>        <span class="k">for</span> <span class="n">merge_pair</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">merges</span><span class="p">:</span>
            <span class="n">new_tokens</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="k">while</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">tokens</span><span class="p">):</span>
                <span class="k">if</span> <span class="p">(</span><span class="n">i</span> <span class="o">&lt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span> <span class="ow">and</span>
                    <span class="n">tokens</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">==</span> <span class="n">merge_pair</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="ow">and</span>
                    <span class="n">tokens</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="n">merge_pair</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
                    <span class="c1"># Replace with the merged token name
</span>                    <span class="n">merged_token</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">merge_tokens</span><span class="p">[</span><span class="n">merge_pair</span><span class="p">]</span>
                    <span class="n">new_tokens</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">merged_token</span><span class="p">)</span>
                    <span class="n">i</span> <span class="o">+=</span> <span class="mi">2</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">new_tokens</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">tokens</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
                    <span class="n">i</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="n">tokens</span> <span class="o">=</span> <span class="n">new_tokens</span>

        <span class="c1"># Convert to readable format
</span>        <span class="n">result</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">tokens</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">token</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
                <span class="n">result</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="nb">chr</span><span class="p">(</span><span class="n">token</span><span class="p">))</span>
            <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">token</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span> <span class="ow">and</span> <span class="n">token</span><span class="p">.</span><span class="n">startswith</span><span class="p">(</span><span class="s">'merge_'</span><span class="p">):</span>
                <span class="c1"># Convert back to the original characters this merge represents
</span>                <span class="n">result</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">token_names</span><span class="p">[</span><span class="n">token</span><span class="p">])</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">result</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">token</span><span class="p">))</span>

        <span class="k">return</span> <span class="n">result</span>

    <span class="k">def</span> <span class="nf">_format_word_freq</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">word_freq</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">str</span><span class="p">],</span> <span class="p">...],</span> <span class="nb">int</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
        <span class="s">"""Format word frequency dictionary for readable output."""</span>
        <span class="n">formatted</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">for</span> <span class="n">word_tuple</span><span class="p">,</span> <span class="n">freq</span> <span class="ow">in</span> <span class="n">word_freq</span><span class="p">.</span><span class="n">items</span><span class="p">():</span>
            <span class="n">tokens</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="p">.</span><span class="n">_token_to_str</span><span class="p">(</span><span class="n">token</span><span class="p">)</span> <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">word_tuple</span><span class="p">]</span>
            <span class="n">word_str</span> <span class="o">=</span> <span class="s">'('</span> <span class="o">+</span> <span class="s">','</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span> <span class="o">+</span> <span class="s">')'</span>
            <span class="n">formatted</span><span class="p">[</span><span class="n">word_str</span><span class="p">]</span> <span class="o">=</span> <span class="n">freq</span>
        <span class="k">return</span> <span class="nb">str</span><span class="p">(</span><span class="n">formatted</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_format_pair_freq</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pair_freq</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">str</span><span class="p">],</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">str</span><span class="p">]],</span> <span class="nb">int</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
        <span class="s">"""Format pair frequency dictionary for readable output."""</span>
        <span class="n">formatted</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">for</span> <span class="n">pair</span><span class="p">,</span> <span class="n">freq</span> <span class="ow">in</span> <span class="n">pair_freq</span><span class="p">.</span><span class="n">items</span><span class="p">():</span>
            <span class="n">first</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">_token_to_str</span><span class="p">(</span><span class="n">pair</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
            <span class="n">second</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">_token_to_str</span><span class="p">(</span><span class="n">pair</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
            <span class="n">pair_str</span> <span class="o">=</span> <span class="n">first</span> <span class="o">+</span> <span class="n">second</span>
            <span class="n">formatted</span><span class="p">[</span><span class="n">pair_str</span><span class="p">]</span> <span class="o">=</span> <span class="n">freq</span>
        <span class="k">return</span> <span class="nb">str</span><span class="p">(</span><span class="n">formatted</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_format_pair</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pair</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">str</span><span class="p">],</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">str</span><span class="p">]])</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
        <span class="s">"""Format a pair for readable output."""</span>
        <span class="n">first</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">_token_to_str</span><span class="p">(</span><span class="n">pair</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="n">second</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">_token_to_str</span><span class="p">(</span><span class="n">pair</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
        <span class="k">return</span> <span class="sa">f</span><span class="s">"(</span><span class="si">{</span><span class="n">first</span><span class="si">}</span><span class="s">, </span><span class="si">{</span><span class="n">second</span><span class="si">}</span><span class="s">)"</span>

    <span class="k">def</span> <span class="nf">_token_to_str</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">token</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">str</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
        <span class="s">"""Convert a token to readable string."""</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">token</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
            <span class="k">return</span> <span class="nb">chr</span><span class="p">(</span><span class="n">token</span><span class="p">)</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">token</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span> <span class="ow">and</span> <span class="n">token</span><span class="p">.</span><span class="n">startswith</span><span class="p">(</span><span class="s">'merge_'</span><span class="p">):</span>
            <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">token_names</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="n">token</span><span class="p">,</span> <span class="n">token</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="nb">str</span><span class="p">(</span><span class="n">token</span><span class="p">)</span>

<span class="c1"># Example usage
</span><span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="s">"__main__"</span><span class="p">:</span>
    <span class="c1"># Initialize BPE encoder
</span>    <span class="n">bpe</span> <span class="o">=</span> <span class="n">BPEEncoder</span><span class="p">()</span>

    <span class="c1"># Example corpus
</span>    <span class="n">corpus</span> <span class="o">=</span> <span class="s">"low low low low low lower lower widest widest widest newest newest newest newest newest newest"</span>

    <span class="k">print</span><span class="p">(</span><span class="s">"BPE Training on Corpus:"</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Corpus: </span><span class="si">{</span><span class="n">corpus</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"="</span> <span class="o">*</span> <span class="mi">50</span><span class="p">)</span>

    <span class="c1"># Train with 6 merges
</span>    <span class="n">merges</span> <span class="o">=</span> <span class="n">bpe</span><span class="p">.</span><span class="n">train</span><span class="p">(</span><span class="n">corpus</span><span class="p">,</span> <span class="n">num_merges</span><span class="o">=</span><span class="mi">6</span><span class="p">)</span>

    <span class="k">print</span><span class="p">(</span><span class="s">"</span><span class="se">\n</span><span class="s">"</span> <span class="o">+</span> <span class="s">"="</span> <span class="o">*</span> <span class="mi">50</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"Training Complete!"</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Merges performed: </span><span class="si">{</span><span class="n">merges</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>

    <span class="c1"># Test tokenization
</span>    <span class="k">print</span><span class="p">(</span><span class="s">"</span><span class="se">\n</span><span class="s">"</span> <span class="o">+</span> <span class="s">"="</span> <span class="o">*</span> <span class="mi">50</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"Tokenization Examples:"</span><span class="p">)</span>
    <span class="n">test_words</span> <span class="o">=</span> <span class="p">[</span><span class="s">"newest"</span><span class="p">,</span> <span class="s">"lower"</span><span class="p">,</span> <span class="s">"widest"</span><span class="p">,</span> <span class="s">"low"</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">test_words</span><span class="p">:</span>
        <span class="n">tokens</span> <span class="o">=</span> <span class="n">bpe</span><span class="p">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">word</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"'</span><span class="si">{</span><span class="n">word</span><span class="si">}</span><span class="s">' -&gt; </span><span class="si">{</span><span class="n">tokens</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>

    <span class="c1"># Show final vocabulary (subset)
</span>    <span class="k">print</span><span class="p">(</span><span class="s">"</span><span class="se">\n</span><span class="s">"</span> <span class="o">+</span> <span class="s">"="</span> <span class="o">*</span> <span class="mi">50</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"New Vocabulary (merged tokens only):"</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">token</span><span class="p">,</span> <span class="n">token_id</span> <span class="ow">in</span> <span class="n">bpe</span><span class="p">.</span><span class="n">vocab</span><span class="p">.</span><span class="n">items</span><span class="p">():</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">token</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span> <span class="ow">and</span> <span class="n">token</span><span class="p">.</span><span class="n">startswith</span><span class="p">(</span><span class="s">'merge_'</span><span class="p">):</span>
            <span class="n">description</span> <span class="o">=</span> <span class="n">bpe</span><span class="p">.</span><span class="n">token_names</span><span class="p">[</span><span class="n">token</span><span class="p">]</span>
            <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Token ID </span><span class="si">{</span><span class="n">token_id</span><span class="si">}</span><span class="s">: '</span><span class="si">{</span><span class="n">description</span><span class="si">}</span><span class="s">'"</span><span class="p">)</span>
</code></pre></div></div>

<h2 id="sample-output">Sample Output</h2>

<p>When you run this code, you’ll see output like:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    BPE Training on Corpus:
    Corpus: low low low low low lower lower widest widest widest newest newest newest newest newest newest
    ==================================================
    Initial word frequencies: {'(l,o,w)': 5, '(l,o,w,e,r)': 2, '(w,i,d,e,s,t)': 3, '(n,e,w,e,s,t)': 6}
    
    Merge 1:
    Pair frequencies: {'lo': 7, 'ow': 7, 'we': 8, 'er': 2, 'wi': 3, 'id': 3, 'de': 3, 'es': 9, 'st': 9, 'ne': 6, 'ew': 6}
    Most frequent pair: (s, t) (freq: 9)
    After merge: {'(l,o,w)': 5, '(l,o,w,e,r)': 2, '(w,i,d,e,s t)': 3, '(n,e,w,e,s t)': 6}
    
    Merge 2:
    Pair frequencies: {'lo': 7, 'ow': 7, 'we': 8, 'er': 2, 'wi': 3, 'id': 3, 'de': 3, 'es t': 9, 'ne': 6, 'ew': 6}
    Most frequent pair: (e, s t) (freq: 9)
    After merge: {'(l,o,w)': 5, '(l,o,w,e,r)': 2, '(w,i,d,e s t)': 3, '(n,e,w,e s t)': 6}
    
    Merge 3:
    Pair frequencies: {'lo': 7, 'ow': 7, 'we': 2, 'er': 2, 'wi': 3, 'id': 3, 'de s t': 3, 'ne': 6, 'ew': 6, 'we s t': 6}
    Most frequent pair: (o, w) (freq: 7)
    After merge: {'(l,o w)': 5, '(l,o w,e,r)': 2, '(w,i,d,e s t)': 3, '(n,e,w,e s t)': 6}
    
    Merge 4:
    Pair frequencies: {'lo w': 7, 'o we': 2, 'er': 2, 'wi': 3, 'id': 3, 'de s t': 3, 'ne': 6, 'ew': 6, 'we s t': 6}
    Most frequent pair: (l, o w) (freq: 7)
    After merge: {'(l o w)': 5, '(l o w,e,r)': 2, '(w,i,d,e s t)': 3, '(n,e,w,e s t)': 6}
    
    Merge 5:
    Pair frequencies: {'l o we': 2, 'er': 2, 'wi': 3, 'id': 3, 'de s t': 3, 'ne': 6, 'ew': 6, 'we s t': 6}
    Most frequent pair: (w, e s t) (freq: 6)
    After merge: {'(l o w)': 5, '(l o w,e,r)': 2, '(w,i,d,e s t)': 3, '(n,e,w e s t)': 6}
    
    Merge 6:
    Pair frequencies: {'l o we': 2, 'er': 2, 'wi': 3, 'id': 3, 'de s t': 3, 'ne': 6, 'ew e s t': 6}
    Most frequent pair: (n, e) (freq: 6)
    After merge: {'(l o w)': 5, '(l o w,e,r)': 2, '(w,i,d,e s t)': 3, '(n e,w e s t)': 6}
    
    ==================================================
    Training Complete!
    Merges performed: ['s t', 'e s t', 'o w', 'l o w', 'w e s t', 'n e']
    
    ==================================================
    Tokenization Examples:
    'newest' -&gt; ['n e', 'w e s t']
    'lower' -&gt; ['l o w', 'e', 'r']
    'widest' -&gt; ['w', 'i', 'd', 'e s t']
    'low' -&gt; ['l o w']
    
    ==================================================
    New Vocabulary (merged tokens only):
    Token ID 257: 's t'
    Token ID 258: 'e s t'
    Token ID 259: 'o w'
    Token ID 260: 'l o w'
    Token ID 261: 'w e s t'
    Token ID 262: 'n e'
</code></pre></div></div>

<p>This implementation demonstrates how BPE learns to represent text efficiently by identifying and merging frequently occurring character patterns, creating a vocabulary that balances between the simplicity of byte-level tokenization and the efficiency of word-level tokenization.</p>]]></content><author><name>[&quot;Han Yu&quot;]</name></author><category term="cs336" /><summary type="html"><![CDATA[Byte Pair Encoding (BPE) Tokenizer in a Nutshell Key Terms]]></summary></entry><entry><title type="html">Study Notes: Stanford CS336 Language Modeling from Scratch [1]</title><link href="http://localhost:4000/cs336/2025/07/20/cs336-note-get-started.html" rel="alternate" type="text/html" title="Study Notes: Stanford CS336 Language Modeling from Scratch [1]" /><published>2025-07-20T00:00:00-07:00</published><updated>2025-07-20T00:00:00-07:00</updated><id>http://localhost:4000/cs336/2025/07/20/cs336-note-get-started</id><content type="html" xml:base="http://localhost:4000/cs336/2025/07/20/cs336-note-get-started.html"><![CDATA[<h2 id="introduction">Introduction</h2>

<p>It’s been a while since my last blog post about my generative AI learning journey (January 1, 2024). My day job has demanded significant time and energy, but I don’t want that to derail my passion and curiosity for deep-diving into this field.</p>

<p>This week, I had a realization: instead of endlessly reflecting, thinking, or planning, it’s time to act. Even with just one hour of focused study after work each day, I can make meaningful progress by year-end.</p>

<p>I’ve decided to tackle Stanford’s <a href="https://stanford-cs336.github.io/spring2025/"><strong>CS336: Language Modeling from Scratch</strong></a> at my own pace. I’m grateful that Stanford makes their <a href="https://www.youtube.com/watch?v=SQ3fZ1sAqXI&amp;list=PLoROMvodv4rOY23Y0BoGoBGgQ1zmU_MT_&amp;ab_channel=StanfordOnline"><strong>Lecture Videos</strong></a>, <a href="https://github.com/stanford-cs336"><strong>Lecture Notes &amp; Assignments</strong></a> freely available online—a perfect fit for someone like me who can’t commit to fixed class schedules but still craves structured learning.</p>

<p>To give back to the community, I’ll document and share my learning notes as I progress. I’m curious to see how much I can accomplish by dedicating just an hour a day (or ~10 hours per week) through the end of the year. Can I complete all the lectures and assignments? I know I’ll move slowly, but I’m excited to test my consistency and see what’s possible.</p>

<p>This first post covers <strong>setting up the local development environment</strong>—a small but necessary step to begin engaging with the lecture materials.</p>

<h2 id="setting-up-the-local-dev-environment">Setting Up the Local Dev Environment</h2>

<h3 id="prerequisites">Prerequisites</h3>

<p>Make sure you have Git, Python 3.11, and Node.js installed on your system. You’ll also need <a href="https://github.com/astral-sh/uv"><code class="language-plaintext highlighter-rouge">uv</code></a>, a fast Python package manager.</p>

<h3 id="step-1-clone-the-cs336-repository">Step 1: Clone the CS336 Repository</h3>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>git clone https://github.com/stanford-cs336/spring2025-lectures
<span class="nb">cd </span>spring2025-lectures
</code></pre></div></div>

<h3 id="step-2-set-up-python-virtual-environment-with-uv">Step 2: Set Up Python Virtual Environment with UV</h3>

<p>Install <code class="language-plaintext highlighter-rouge">uv</code> (if not already installed):</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>curl <span class="nt">-LsSf</span> https://astral.sh/uv/install.sh | sh
</code></pre></div></div>

<p>Create a virtual environment using Python 3.11:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>uv venv <span class="nt">--python</span><span class="o">=</span>3.11
</code></pre></div></div>

<p>Activate the environment:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># macOS/Linux</span>
<span class="nb">source</span> .venv/bin/activate
</code></pre></div></div>

<h3 id="step-3-install-python-dependencies">Step 3: Install Python Dependencies</h3>

<blockquote>
  <p>💡 Note: I’m using a MacBook Pro (M4). The <code class="language-plaintext highlighter-rouge">triton</code> package doesn’t support Apple Silicon, so I excluded it during the initial setup.<br />
More info: <a href="https://github.com/triton-lang/triton">Triton GitHub</a></p>
</blockquote>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">grep</span> <span class="nt">-v</span> <span class="s2">"triton"</span> requirements.txt <span class="o">&gt;</span> requirements_no_triton.txt
uv pip <span class="nb">install</span> <span class="nt">-r</span> requirements_no_triton.txt
</code></pre></div></div>

<h3 id="step-4-generate-executable-lecture-content">Step 4: Generate Executable Lecture Content</h3>

<p>Compile a lecture:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python execute.py <span class="nt">-m</span> lecture_01
</code></pre></div></div>

<p>This will generate a trace file at:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>var/traces/lecture_01.json
</code></pre></div></div>

<h3 id="step-5-build-the-local-react-web-app-to-view-lectures">Step 5: Build the Local React Web App to View Lectures</h3>

<p>Install Node.js if needed:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>brew <span class="nb">install </span>node
</code></pre></div></div>

<p>Then build and serve the trace viewer:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">cd </span>trace-viewer
npm <span class="nb">install
</span>npm run dev
</code></pre></div></div>

<p>Open the viewer in your browser:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>http://localhost:&lt;PORT&gt;?trace=var/traces/lecture_01.json
</code></pre></div></div>

<p>It should be something like <img src="/assets/picture/2025_07_20_cs336_note_get_started/cs336_lecture_view.png" alt="this" />.</p>

<p>Then enjoy going through the lecture notes!</p>]]></content><author><name>[&quot;Han Yu&quot;]</name></author><category term="cs336" /><summary type="html"><![CDATA[Introduction]]></summary></entry><entry><title type="html">Supercharge Message Summarization Experience: Parameter-Efficient Fine-Tuning and LLM Assisted Evaluation</title><link href="http://localhost:4000/parameter-efficient-fine-tuning/2024/01/01/PEFT-summarization.html" rel="alternate" type="text/html" title="Supercharge Message Summarization Experience: Parameter-Efficient Fine-Tuning and LLM Assisted Evaluation" /><published>2024-01-01T00:00:00-08:00</published><updated>2024-01-01T00:00:00-08:00</updated><id>http://localhost:4000/parameter-efficient-fine-tuning/2024/01/01/PEFT-summarization</id><content type="html" xml:base="http://localhost:4000/parameter-efficient-fine-tuning/2024/01/01/PEFT-summarization.html"><![CDATA[<center>
<img src="/assets/picture/2024_01_01_PEFT_summarization/cover.png" width="900" height="500" />
</center>
<h2 id="introduction">Introduction</h2>
<p>In today’s business landscape, we are surrounded by a wealth of opportunities to utilize advanced technology powered by AI. Think of large language models(LLMs) as versatile tools in our toolkit: we can <em>customize</em> them for a variety of specific downstream tasks, a process known as <em>fine-tuning</em>. However, a challenge arises in that each fine-tuned model typically maintains the same parameter size as the original. Therefore, managing multiple fine-tuned models requires careful consideration of factors such as accuracy performance, memory management, inference latency, and disk utilization.</p>

<p>Parameter-Efficient Fine-Tuning (PEFT) methods provide an efficient and streamlined approach for adapting pre-trained LLMs, commonly referred to as <em>base models</em>, to a range of specific downstream tasks. These tasks encompass diverse applications, including but not limited to text summarization, question answering, image generation, and text-to-speech synthesis. In contrast to traditional full fine-tuning, which consumes substantial computational resources, PEFT prioritizes the optimization of a significantly smaller parameter subset referred to as “adapters”. These adapters work in tandem with the base model, achieving competitive performance while imposing lower computational and storage demands.</p>

<p>I’ve shared a <a href="https://github.com/bearbearyu1223/llm-fine-tuning-playground/blob/main/finetune_falcon_7b_conversation_summarization.ipynb">Colab notebook</a> demonstrating a resource-efficient PEFT process using <a href="https://arxiv.org/abs/2305.14314">QLoRA</a> and <a href="https://github.com/huggingface/peft">HuggingFace PEFT libraries</a> to fine tune <a href="https://huggingface.co/vilsonrodrigues/falcon-7b-sharded">Falcon-7B-sharded model</a> on <a href="https://huggingface.co/datasets/samsum">SamSum dataset</a> for summarizing “message-like” conversations. It achieves reasonable summarization performance after training for only 5 epochs on an A100 compute instance with a single GPU. Additionally, I’ve employed <code class="language-plaintext highlighter-rouge">GPT-3.5-turbo</code> to assess generated summaries, showcasing a potentially automated evaluation method by formalizing evaluation guidelines into a prompt template. This approach stands in contrast to traditional automated evaluation metrics like ROUGE or BERTScore, which rely on reference summaries.</p>

<p>Furthermore, I will also share some insights and lessons I’ve gained throughout this process, with a particular focus on considerations when leveraging LLMs to develop product experiences related to summarization.</p>

<p>I hope you’ll discover this article both informative and intriguing, igniting your creativity as you explore the development of your unique product experiences and strategies through the use of fine-tuned foundation models.</p>

<p>Enjoy the read, and let your innovation flourish. Happy new year!</p>

<h2 id="fine-tuning-with-model-quantization-and-lora">Fine-Tuning with Model Quantization and LoRA</h2>
<p>Base models such as Claude, T5, Falcon, and Llama2 excel at predicting tokens in sequences, but they <em>struggle with generating responses that align with instructions</em>. Fine-tuning techniques, such as <strong>Supervised Fine-Tuning (SFT)</strong> and <strong>Reinforcement Learning from Human Feedback (RLHF)</strong>, can be employed to bridge these gaps. In this sample project, we’ll explore the application of SFT to Falcon-7B, a 7-billion-parameter causal decoder model trained by TII on 1,500-billion tokens from RefinedWeb with curated corpora, for conversation summarization tasks.</p>

<h3 id="install-and-import-the-required-libraries">Install and Import the Required Libraries</h3>
<p>To get started, one can create a virtual environment and install all the required libraries needed for this sample project. In Colab, this can be done by running a cell containing the following scripts:</p>
<pre><code class="language-Shell">!pip install huggingface_hub==0.19.4
!pip install -q -U trl accelerate git+https://github.com/huggingface/peft.git
!pip install transformers==4.36.0
!pip install datasets==2.15.0 Tokenizers==0.15.0
!pip install -q bitsandbytes wandb
!pip install py7zr
</code></pre>
<p>then the installed libraries can be imported and be used during runtime via:</p>
<pre><code class="language-Python">import torch
import numpy as np
from huggingface_hub import notebook_login
from datasets import load_dataset, concatenate_datasets
from transformers import AutoModelForCausalLM, AutoTokenizer,BitsAndBytesConfig, AutoTokenizer, GenerationConfig
from peft import LoraConfig, get_peft_model, PeftConfig, PeftModel, prepare_model_for_kbit_training, TaskType
from transformers import TrainingArguments
from trl import SFTTrainer, DataCollatorForCompletionOnlyLM
</code></pre>
<h3 id="prepare-the-dataset-for-fine-tuning">Prepare the Dataset for Fine-Tuning</h3>
<p>You can load the <a href="https://huggingface.co/datasets/samsum">SamSum dataset</a> directly using the [Hugging Face Datasets libraries](https://huggingface.co/docs/datasets/index via Python code:</p>

<pre><code class="language-Python">dataset_name = "samsum"
dataset = load_dataset(dataset_name)

train_dataset = dataset['train']
eval_dataset = dataset['validation']
test_dataset = dataset['test']
dataset
</code></pre>
<p>The dataset contains a total of 14,732 samples for training, 818 samples for validation, and 818 samples for testing. A sample of the dataset is displayed below:
<img src="/assets/picture/2024_01_01_PEFT_summarization/dataset.png" alt="sample_data" /></p>

<p>To format the original training dataset into prompts for instruction fine-tuning, you can use the following helper function. For more details, refer to the detailed reference <a href="https://huggingface.co/docs/trl/sft_trainer#format-your-input-prompts">here</a>).</p>

<pre><code class="language-Python">def formatting_prompts_func(example):
    output_texts = []
    for i in range(len(example['id'])):
        instruction = "Summarize this Dialogue."
        input = example['dialogue'][i]
        output = example['summary'][i]
        prompt="### Instruction:\n{instruction}\n\n### Dialogue:\n{input}\n\n### Summary:\n{output}".format(instruction=instruction, input=input, output=output)
        output_texts.append(prompt)
    return output_texts
</code></pre>
<h3 id="set-up-the-configuration-for-fine-tuning">Set up the Configuration for Fine-Tuning</h3>
<p>To reduce VRAM usage during training, you will fine-tune <a href="https://huggingface.co/vilsonrodrigues/falcon-7b-sharded">a resharded version of Falcon-7B</a> in 4-bit precision using <a href="https://arxiv.org/abs/2305.14314">QLoRA</a>. You can use the following code snippet to load the base model and prepare it for the QLoRA experiment:</p>

<pre><code class="language-Python">model_name = "vilsonrodrigues/falcon-7b-sharded"

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_use_double_quant=True,
    bnb_4bit_compute_dtype=torch.float16,
)

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    quantization_config=bnb_config,
    device_map="auto",
    trust_remote_code=True,
)
model.config.use_cache = False
model.config.pretraining_tp = 1
</code></pre>
<p>Based on the QLoRA paper, we will taget all linear transformer block layers as target modules for fine-tuning (also see the discussions on reddit <a href="https://www.reddit.com/r/LocalLLaMA/comments/15sgg4m/what_modules_should_i_target_when_training_using/?rdt=53925">here</a>). You can leverage the following helper function to find these target modules:</p>

<pre><code class="language-Python">def find_target_modules(model):
    # Initialize a Set to Store Unique Layers
    unique_layers = set()

    # Iterate Over All Named Modules in the Model
    for name, module in model.named_modules():
        # Check if the Module Type Contains 'Linear4bit'
        if "Linear4bit" in str(type(module)):
            # Extract the Type of the Layer
            layer_type = name.split('.')[-1]

            # Add the Layer Type to the Set of Unique Layers
            unique_layers.add(layer_type)

    # Return the Set of Unique Layers Converted to a List
    return list(unique_layers)
target_modules = find_target_modules(model)
print(target_modules)
</code></pre>
<p>And in this case, the target modules for fine-tuning will be 
<code class="language-plaintext highlighter-rouge">['dense_4h_to_h', 'dense_h_to_4h', 'query_key_value', 'dense']</code>.</p>

<p>After loading and preparing the base model for QLoRA, you can configure the fine-tuning experiment using the following code:</p>
<pre><code class="language-Python">model = prepare_model_for_kbit_training(model)

lora_alpha = 32 
lora_dropout = 0.1 
lora_rank = 16

peft_config = LoraConfig(
    lora_alpha=lora_alpha,
    lora_dropout=lora_dropout,
    r=lora_rank,
    bias="none",
    task_type="CAUSAL_LM",
    target_modules=[
        "query_key_value",
        "dense",
        "dense_h_to_4h",
        "dense_4h_to_h",
    ]
)

peft_model = get_peft_model(model, peft_config)
peft_model.print_trainable_parameters()
</code></pre>
<p>This configuration will result in an <em>adapter model</em> with <em>32,636,928 trainable parameters</em>, which is only <em>0.47%</em> of the trainable parameters compared to the <em>6,954,357,632 parameters</em> of the base model.</p>
<h3 id="set-up-the-configuration-for-trainig">Set up the Configuration for Trainig</h3>
<p>Load the tokenizer from the pre-trained base model, both the base model, the LoRA config, and the tokenizer will be needed for the SFT trainer.</p>
<pre><code class="language-Python">tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)

if tokenizer.pad_token_id is None:
  tokenizer.pad_token = tokenizer.eos_token
  tokenizer.pad_token_id = tokenizer.eos_token_id
tokenizer.padding_side = "right"
</code></pre>
<p>Below is the configuration used for SFT</p>
<pre><code class="language-Python">output_dir = "falcon_7b_LoRA_r16_alpha32_epoch10_dialogue_summarization_v0.1"
per_device_train_batch_size = 32 #4
gradient_accumulation_steps = 4
gradient_checkpointing=False
optim = "paged_adamw_32bit"
save_steps = 20
logging_steps = 20
learning_rate = 2e-4
max_grad_norm = 0.1
warmup_ratio = 0.01
lr_scheduler_type = "cosine" #"constant"
num_train_epochs = 5
seed=42
max_seq_length = 512

training_arguments = TrainingArguments(
    output_dir=output_dir,
    per_device_train_batch_size=per_device_train_batch_size,
    gradient_accumulation_steps=gradient_accumulation_steps,
    gradient_checkpointing=gradient_checkpointing,
    optim=optim,
    save_steps=save_steps,
    logging_steps=logging_steps,
    learning_rate=learning_rate,
    fp16=True,
    max_grad_norm=max_grad_norm,
    num_train_epochs=num_train_epochs,
    warmup_ratio=warmup_ratio,
    group_by_length=True,
    lr_scheduler_type=lr_scheduler_type,
    push_to_hub=True,
    report_to="wandb"
)

trainer = SFTTrainer(
    model=peft_model,
    train_dataset=train_dataset,
    formatting_func=formatting_prompts_func,
    tokenizer=tokenizer,
    peft_config=peft_config,
    max_seq_length=max_seq_length,
    args=training_arguments,
)
</code></pre>
<p>You can initiate the fine-tuning experiment via</p>
<pre><code class="language-Python">trainer.train()
</code></pre>
<p>The entire training process took approximately 3 hours running on an A100 instance with a single GPU.</p>

<h3 id="model-inference-of-the-fined-tuned-model">Model Inference of the Fined-Tuned Model</h3>
<p>Upon completion of the training process, you can easily share the adapter model by uploading it to Hugging Face’s model repository using the following code:</p>
<pre><code class="language-Python">trainer.push_to_hub() 
</code></pre>
<p>This published adapter model can then be retrieved and used in conjunction with the base model for various summarization tasks, as demonstrated in the reference code snippet below.</p>
<pre><code class="language-Python">PEFT_MODEL = "bearbearyu1223/falcon_7b_LoRA_r16_alpha32_epoch10_dialogue_summarization_v0.1"
config = PeftConfig.from_pretrained(PEFT_MODEL)
peft_base_model = AutoModelForCausalLM.from_pretrained(
    config.base_model_name_or_path,
    return_dict=True,
    quantization_config=bnb_config,
    device_map="auto",
    trust_remote_code=True,
)

peft_model = PeftModel.from_pretrained(peft_base_model, PEFT_MODEL)

# Generate Summarization
def get_summary(dialogue, max_new_tokens=50, max_length=512, verbose=False):
  prompt= "### Instruction:\n{instruction}\n\n### Dialogue:\n{dialogue}\n\n### Summary:\n".format(instruction="Summarize the Dialogue below.", dialogue=dialogue)
  if verbose:
    print(prompt)

  peft_encoding = peft_tokenizer(prompt, truncation=True, return_tensors="pt").to(torch.device("cuda:0"))
  peft_outputs = peft_model.generate(input_ids=peft_encoding.input_ids, generation_config=GenerationConfig(max_length=max_length, do_sample=True,
                                                                                                         max_new_tokens=max_new_tokens,
                                                                                                         pad_token_id = peft_tokenizer.eos_token_id,
                                                                                                         eos_token_id = peft_tokenizer.eos_token_id,
                                                                                                         attention_mask = peft_encoding.attention_mask,
                                                                                                         temperature=0.1, top_k=1, repetition_penalty=30.0, num_return_sequences=1,))
  peft_text_output = peft_tokenizer.decode(peft_outputs[0], skip_special_tokens=True)

  sub = "### Summary:"
  raw_summary = peft_text_output.split(sub)[1]

  return raw_summary
</code></pre>
<p>See an example of a summary generated by the fine-tuned model in comparison to the reference summary crafted by a human below</p>
<pre><code class="language-Python">test_index=6
dialogue=test_dataset[test_index]['dialogue']
summary=test_dataset[test_index]['summary']
peft_output=get_summary(dialogue,verbose=True)

print("Human Summary:")
print(summary)
print("PEFT Summary:")
print(peft_output)
</code></pre>

<table>
  <thead>
    <tr>
      <th>Instruction</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Summarize the Dialogue below.</td>
    </tr>
  </tbody>
</table>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Max: Know any good sites to buy clothes from?
Payton: Sure :) &lt;file_other&gt; &lt;file_other&gt; &lt;file_other&gt; &lt;file_other&gt; &lt;file_other&gt; &lt;file_other&gt; &lt;file_other&gt;
Max: That's a lot of them!
Payton: Yeah, but they have different things so I usually buy things from 2 or 3 of them.
Max: I'll check them out. Thanks. 
Payton: No problem :)
Max: How about u?
Payton: What about me?
Max: Do u like shopping?
Payton: Yes and no.
Max: How come?
Payton: I like browsing, trying on, looking in the mirror and seeing how I look, but not always buying.
Max: Y not?
Payton: Isn't it obvious? ;)
Max: Sry ;)
Payton: If I bought everything I liked, I'd have nothing left to live on ;)
Max: Same here, but probably different category ;)
Payton: Lol
Max: So what do u usually buy?
Payton: Well, I have 2 things I must struggle to resist!
Max: Which are?
Payton: Clothes, ofc ;)
Max: Right. And the second one?
Payton: Books. I absolutely love reading!
Max: Gr8! What books do u read?
Payton: Everything I can get my hands on :)
Max: Srsly?
Payton: Yup :)
</code></pre></div></div>

<table>
  <thead>
    <tr>
      <th>Summary Type</th>
      <th>Summary Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Human</td>
      <td>Payton provides Max with websites selling clothes. Payton likes browsing and trying on the clothes but not necessarily buying them. Payton usually buys clothes and books as he loves reading.</td>
    </tr>
    <tr>
      <td>PEFT</td>
      <td>Payton sends Max some links with online shops where she buys her stuff. Payton likes both fashion items and literature. She reads all kinds of fiction.</td>
    </tr>
  </tbody>
</table>

<h2 id="evaluation-of-summarization-quality">Evaluation of Summarization Quality</h2>
<p>Traditional evaluation methods for summarization tasks rely on metrics like ROUGE and BLEU, which evaluate the generated summaries by comparing them to human-written reference summaries. These metrics assess aspects such as the overlap in n-grams and word sequences, offering a quantitative and also automated assessment of summary quality.</p>

<p>In cases where human reference summaries are unavailable, it becomes imperative to establish well-defined and consistent annotation guidelines for human annotators. Below is a list of criteria we will consider when formulating the annotation guidelines.</p>

<h3 id="metric-1-relevance">Metric 1: Relevance</h3>
<p><strong>Capturing the Essence:</strong> The LLM will assist annotators in evaluating the relevance of a summary. Annotators will evaluate the relevance of a summary on a scale of 1 to 5 (higher score indicates better quality), considering whether the summary effectively extracts important content from the source conversation, avoiding redundancies and excess information. With clear criteria and steps, annotators can confidently assign scores that reflect the summary’s ability to convey essential details.</p>

<h3 id="metric-2-coherence">Metric 2: Coherence</h3>
<p><strong>Creating Clarity:</strong> The LLM will assist annotators in evaluating the coherence of a summary. Annotators will rate summaries from 1 to 5 (higher score indicates better quality), focusing on the summary’s organization and logical flow. Clear guidelines enable annotators to determine how well the summary presents information in a structured and coherent manner.</p>

<h3 id="metric-3-consistency">Metric 3: Consistency</h3>
<p><strong>Factually Sound:</strong> The LLM will assist annotators in evaluating the consistency of a summary. Annotators will assess summaries for factual alignment with the source conversation, rating them from 1 to 5 (higher score indicates better quality). SummarizeMaster ensures that annotators identify and penalize summaries containing factual inaccuracies or hallucinated facts, enhancing the reliability of the evaluation process.</p>

<h3 id="metric-4-fluency">Metric 4: Fluency</h3>
<p><strong>Language Excellence:</strong> The LLM will assist annotators in evaluating the fluency of a summary. Fluency is a critical aspect of summary evaluation. Annotators will assess summaries for grammar, spelling, punctuation, word choice, and sentence structure, assigning scores from 1 to 5 (higher score indicates better quality).</p>

<p>We will transform these instructions into a prompt template for input to <code class="language-plaintext highlighter-rouge">GPT-3.5-turbo</code> to assess the quality of the summaries generated by our fine-tuned model (check out this <a href="https://github.com/bearbearyu1223/llm-fine-tuning-playground/blob/main/model_eval_finetune_falcon_7b_conversation_summarization.ipynb">Colab</a> to run the evaluation). This approach is primarily motivated by the goal of achieving consistency, standardization, and efficiency in the manual evaluation process, which can otherwise be labor-intensive.</p>

<h2 id="lessons-learned">Lessons Learned</h2>
<p>The automated evaluation results for the 818 test samples are shared <a href="https://github.com/bearbearyu1223/llm-fine-tuning-playground/blob/main/results/falcon_7b_LoRA_r16_dialogue_summarization_12_23_2023_results_eval.csv">here</a>. Simple statistical analysis reveals that the fine-tuned LLM demonstrated a reasonable level of performance on the test set when compared to the human-generated summaries (refer to the chart below)</p>
<center>
<img src="/assets/picture/2024_01_01_PEFT_summarization/eval_score.png" />
</center>
<!-- ![eval_score](/assets/picture/2024_01_01_PEFT_summarization/eval_score.png) -->

<p>However, is that truly the situation? Let’s examine a few instances together.</p>

<p>Example 1:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Yaz: Hi babes, you off out Friday, I need to get my freak on!
Tania: Why the hell not, college work is really doing my head in with the deadlines!
Yaz: Seen Sophie lately, her and lover boy are looking well serious!
Tania: Yeah, saw her in the canteen a couple of days ago, she's thinking of breaking up before Uni. Cutting ties etc
Yaz: Well, maybe not so serious after all then!
Tania: Best to go there single, in my opinion!
Yaz: Yeah, not got much choice in the matter. Maybe I'll meet my dream woman there!😀
Tania: Why the hell not?! I can't wait to get out of here and up to Manchester, only 4 months and a bit, now!
Yaz: Christ, that came around quick, just the little matter of A Levels and getting into Exeter to deal with. I need such high grades for my course, medicine is a cut-throat world.
Tania: Hope not! If anyone is suited to becoming a Dr love, it's you, I am positive of it!
Yaz: Hope you're right, babes! Anyway, pub Friday at 8ish?
Tania: Hell, yes! See you!
</code></pre></div></div>

<table>
  <thead>
    <tr>
      <th>Summary - Human Baseline</th>
      <th>Summary - PEFT Baseline</th>
      <th>Relevance<br />Human</th>
      <th>Relevance<br />PEFT Model</th>
      <th>Coherence<br />Human</th>
      <th>Coherence<br />PEFT Model</th>
      <th>Consistency<br />Human</th>
      <th>Consistency<br />PEFT Model</th>
      <th>Fluency<br />Human</th>
      <th>Fluency<br />PEFT Model</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Yaz and Tania will go to a pub on Friday around 8. Sophie is thinking of breaking up with her boyfriend before Uni. Tania is going to Manchester in 4 months. Yaz wants to study medicine in Exeter, so she needs high grades.</td>
      <td>Sophie wants to break up because he boyfriend has been cheating on him. Tania needs good results from exams to be admitted to university. Yaz will see his friends at the bar tonight about 8 pm.</td>
      <td>4</td>
      <td>3</td>
      <td>4</td>
      <td>3</td>
      <td>4</td>
      <td>2</td>
      <td>4</td>
      <td>2</td>
    </tr>
  </tbody>
</table>

<p><strong>Learnings: The LLM-based evaluator is capable of performing a reasonably accurate evaluation.</strong> In this example, it appears that the summary generated by the fine-tuned model does not align with the factual content of the source conversation. The original source conversation does not mention that Sophie’s boyfriend is cheating. Furthermore, there is a factual inaccuracy in the summary, as it is Yaz, not Tania, who requires a high score for admission to Exeter for studying medicine. Additionally, there are some grammar errors that can be improved or corrected as suggested below (e.g., one can further prompt LLM and ask suggestions to improve the fluency of the summary):</p>

<ol>
  <li>
    <p>“Sophie wants to break up because he boyfriend has been cheating on him.”
Correction: “Sophie wants to break up because her boyfriend has been cheating on her.”</p>
  </li>
  <li>
    <p>“Tania needs good results from exams to be admitted to university.”
Correction: “Tania needs good exam results to be admitted to university.”</p>
  </li>
  <li>
    <p>“Yaz will see his friends at the bar tonight about 8 pm.”
Correction: “Yaz will see his friends at the bar tonight at about 8 pm.”</p>
  </li>
</ol>

<p>These quality issues have been identified by the LLM-based evaluator, which rated both consistency and fluency as 2.</p>

<p>Example 2:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Petra: I need to sleep, I can't stand how sleepy I am
Andy: I know, and it's so boring today, nobody's working at the office
Ezgi: I am working! lazy pigs
Petra: I'm sleeping with my eyes open, kill me
Andy: ask the fat woman from HR
Petra: she would kill me on spot without batting an eye
Andy: she always repeats she has a black belt in karate 
Petra: it's hard to believe she can move, but let her have whatever belt she wants
Andy: LOL
Petra: sooooo sleepy
</code></pre></div></div>

<table>
  <thead>
    <tr>
      <th>Summary - Human Baseline</th>
      <th>Summary - PEFT Baseline</th>
      <th>Relevance<br />Human</th>
      <th>Relevance<br />PEFT Model</th>
      <th>Coherence<br />Human</th>
      <th>Coherence<br />PEFT Model</th>
      <th>Consistency<br />Human</th>
      <th>Consistency<br />PEFT Model</th>
      <th>Fluency<br />Human</th>
      <th>Fluency<br />PEFT Model</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Petra is very sleepy at work today, Andy finds the day boring, and Ezgi is working.</td>
      <td>It is difficult for Petra not to fall asleep because of being tired. Andy suggests that Petra should talk about this issue with the lady who works as human resources manager. She might be able to help Petra.</td>
      <td>4</td>
      <td>4</td>
      <td>4</td>
      <td>4</td>
      <td>4</td>
      <td>4</td>
      <td>4</td>
      <td>4</td>
    </tr>
  </tbody>
</table>

<p><strong>Learnings: LLM-based evaluators may not always accurately capture the intended context or nuances from the conversation.</strong> In this example, the human-generated summary outperforms the summary generated by the fine-tuned model. Interestingly, neither the fine-tuned model’s generated summary nor the LLM-based evaluator were able to accurately recognize the sarcasm conveyed in the original conversations, particularly with reference to the “HR lady”.</p>

<p>Example 3:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Finn: Hey
Zadie: Hi there! What's up?
Finn: All fine. You? 
Zadie: Not bad, thanks 
Finn: Look, I was thinking of going to this neighborhood called Elephant and Castle tomorrow, it's apparently full of Latin American stuff. Fancy joining? 
Zadie: Sure! But what's ""stuff""? 😂
Finn: lol So apparently it's a place were random people from ""Latin America"" (meaning fuck knows which countries) started running small businesses and restaurant, and a nice little community was formed
Zadie: Oh cool 
Finn: Then capitalism came and it's all going to be demolished soon, so it's like the last chance to go
Zadie: What a shame :( Yeah, I haven't had Latin American 😂 food for ages so I'm totally up for it
Finn: Can't wait to taste this cuisine of unspecified Latino origin lol
Zadie: 😂😂😂
Finn: But we can specify time and place if and only if you wish 
Zadie: I might be tempted to lol I'd say early evening, 2-ish?
Finn: Yeah, that's fine by me. So most of the places we want to visit are in this Elephant and Castle shopping centre. Shall I see you at the main entrance, wherever that is 
Zadie: 2 o'clock at unspecified main entrance then? Sounds good to mw
Finn: Yer
Zadie: Cool, see you there! And thanks so much for remembering about me 💜💜💜
Finn: Thanks for saying yes to such an ill-defined plan lmao 
Zadie: Ha ha You know I love those
Finn: See you tomorrow then 
Zadie: Yep Call me if you get lost
Finn: I will I will 🤙 byeeee 
Zadie: Toodles
</code></pre></div></div>

<table>
  <thead>
    <tr>
      <th>Summary - Human Baseline</th>
      <th>Summary - PEFT Baseline</th>
      <th>Relevance<br />Human</th>
      <th>Relevance<br />PEFT Model</th>
      <th>Coherence<br />Human</th>
      <th>Coherence<br />PEFT Model</th>
      <th>Consistency<br />Human</th>
      <th>Consistency<br />PEFT Model</th>
      <th>Fluency<br />Human</th>
      <th>Fluency<br />PEFT Model</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Finn and Zadie are going to Elephant and Castle tomorrow at 2. They will meet at the main entrance.</td>
      <td>Tomorrow Finn wants Zadie join him on his trip around London. They meet at 2 pm near the elephant statue outside the Elephant and Castle Shopping Centre. The neighbourhood has been created by immigrants from South America.</td>
      <td>4</td>
      <td>4</td>
      <td>4</td>
      <td>4</td>
      <td>4</td>
      <td>4</td>
      <td>4</td>
      <td>4</td>
    </tr>
  </tbody>
</table>

<p><strong>Learnings: Both the fine-tuned LLM model and the LLM-based evaluator may occasionally treat context derived from their pre-learned world knowledge as factual information. This underscores the importance of exercising caution when incorporating inferred contextual information into the product experience.</strong> In this example, the summary generated by the fine-tuned model is considered to yield “impressive results” because it correctly identifies that “Elephant and Castle” is a place of interest located in London, and it recognizes the presence of an elephant statue near the entrance to the shopping center, both of which are not directly mentioned in the original conversation. <em>Those additional context is derived from the base model’s world knowledge from training</em>.</p>

<p>However, it raises questions about whether we should deem this as a good summary when the LLM leverages its world knowledge to infer information beyond the direct content of the conversation. It’s important to acknowledge that this approach can be a <em>double-edged sword from a product experience perspective</em>. Some users may find it useful and intelligent as it demonstrates the LLM’s ability to provide context and additional information. Still, it can also potentially be problematic in many situations.</p>

<p>Here are some considerations:</p>

<ul>
  <li>
    <p><strong>Cultural Variations and Sensitivity</strong>: Inferring information based on world knowledge can be problematic when dealing with culturally sensitive topics or diverse audiences. Assumptions made by the model may not align with cultural variations or norms, potentially leading to misunderstandings or offense.</p>
  </li>
  <li>
    <p><strong>Privacy and Ethical Concerns</strong>: Inferences based on world knowledge can sometimes delve into personal or sensitive areas, raising ethical concerns.</p>
  </li>
  <li>
    <p><strong>Accuracy and Context</strong>: While the model’s inferences may be impressive, they may not always accurately capture the intended context or nuances since the world knowledge can be updated, and the model may be trained on outdated data. This can result in potential inaccuracies when applying such knowledge to generate summaries.</p>
  </li>
  <li>
    <p><strong>Users’ Preferences and Control</strong>: Users’ preferences for summaries may vary. Some may appreciate the additional context provided by model inference, while others may prefer more straightforward and direct summaries. It’s essential to provide users with transparency and control over how additional context is used to generate summaries. Users should have the option to enable or disable such contextual inferences to align with their preferences.</p>
  </li>
</ul>

<h2 id="my-top-3-takeaways-from-this-intriguing-project">My Top 3 Takeaways from this Intriguing Project!</h2>

<ol>
  <li>
    <p>Fine-Tuning LLM through adaptation has demonstrated itself as an exceptionally efficient and cost-effective method for developing LLM-powered product experiences. It allows companies, even those not creating their own foundation models, to harness this approach and benefit from the latest and most advanced outcomes driven by Generative AI.</p>
  </li>
  <li>
    <p>Leveraging the world knowledge acquired by the base model can indeed lead to “impressive results” for the fine-tuned model. However, it’s essential to bear in mind that this can be a double-edged sword!</p>
  </li>
  <li>
    <p>LLM can serve as a referee much like a human, evaluating generation results from another LLM or a fine-tuned model. Nevertheless, exercise caution when depending on an LLM-based evaluator to shape your product experience, as not all evaluations may be equally reliable!</p>
  </li>
</ol>

<p>I hope my insights resonate with your experience as well!</p>]]></content><author><name>[&quot;Han Yu&quot;]</name></author><category term="Parameter-Efficient-Fine-Tuning" /><summary type="html"><![CDATA[Introduction In today’s business landscape, we are surrounded by a wealth of opportunities to utilize advanced technology powered by AI. Think of large language models(LLMs) as versatile tools in our toolkit: we can customize them for a variety of specific downstream tasks, a process known as fine-tuning. However, a challenge arises in that each fine-tuned model typically maintains the same parameter size as the original. Therefore, managing multiple fine-tuned models requires careful consideration of factors such as accuracy performance, memory management, inference latency, and disk utilization.]]></summary></entry><entry><title type="html">Craft Your Own Kitchen Companion: Leveraging OpenAI Assistant, Image Generation, and Text-to-Speech for a Revolutionary Cooking Experience</title><link href="http://localhost:4000/chatbot/2023/11/21/assistant-api-playground.html" rel="alternate" type="text/html" title="Craft Your Own Kitchen Companion: Leveraging OpenAI Assistant, Image Generation, and Text-to-Speech for a Revolutionary Cooking Experience" /><published>2023-11-21T00:00:00-08:00</published><updated>2023-11-21T00:00:00-08:00</updated><id>http://localhost:4000/chatbot/2023/11/21/assistant-api-playground</id><content type="html" xml:base="http://localhost:4000/chatbot/2023/11/21/assistant-api-playground.html"><![CDATA[<h2 id="tldr">TL;DR</h2>
<p><img src="/assets/picture/2023_11_21_assistant_api_playground/cropped_image.png" alt="page_cover" /></p>

<p>OpenAI unveiled its <a href="https://platform.openai.com/docs/assistants/overview">Assistant API</a> during its very first developer conference-<a href="https://devday.openai.com/">OpenAI DevDay</a>. This marked a significant addition to the suite of tools and models available for building conversational AI, a domain in which I’ve been actively engaged.</p>

<p>Although I have used other frameworks, tools and models to build customized Assistant in the past (check out my previous posts  <a href="https://bearbearyu1223.github.io/chatbot/2023/07/31/food-qna-on-server-llm.html">Food QnA Chatbot : Help Answer Food Related Questions from Your Own Cookbook</a> and <a href="https://bearbearyu1223.github.io/chatbot/2023/08/14/food-qna-on-device-llm.html">Build On-Device QnA with LangChain and Llama2</a>), intrigued by the new capabilities of the Assistant API, I decided to integrate it into a new project. This sample project is a refreshed demonstration of a simple Cooking Assistant, leveraging not only the Assistant API but also incorporating additional functionalities such as <a href="https://platform.openai.com/docs/guides/text-to-speech">Text-to-Speech</a> and <a href="https://platform.openai.com/docs/guides/images?context=node">Image Generation</a>, showcasing a holistic approach to building conversational AI systems with only a few lines of code.</p>

<p>Get inspired by the latest from Open AI DevDay with my project on <a href="https://github.com/bearbearyu1223/assistant_api_playground"><img src="https://img.shields.io/badge/github-%23121011.svg?style=for-the-badge&amp;logo=github&amp;logoColor=white" alt="GitHub" /></a>? Ready to unleash your creativity? Start building your own generative AI-powered experience today, join the forefront of AI innovation, and let us make AI for everyone!</p>

<h2 id="the-crafted-product-experience">The Crafted Product Experience</h2>

<ol>
  <li><strong>Direct Communication</strong>:
    <ul>
      <li>Purposefully address user (in this case, it will be “Han”) directly in all correspondences, ensuring each interaction is clear and personalized.</li>
    </ul>
  </li>
  <li><strong>Resource Utilization</strong>:
    <ul>
      <li>Utilize information solely from user’s provided cookbooks when responding to his/her inquiries, ensuring responses are relevant and accurate.</li>
    </ul>
  </li>
  <li><strong>Requests for Visual Representation</strong>:
    <ul>
      <li><strong>Initial Acknowledgment</strong>: When user requests a visual representation of a recipe, promptly reply with: “Absolutely! A delightful visual preview of the recipe is coming up shortly. Please hold on!”</li>
      <li><strong>Action Step</strong>: Subsequently, craft a detailed and imaginative textual prompt to aid in visualizing the specified recipe.</li>
    </ul>
  </li>
  <li><strong>Recipe Responses</strong>:
    <ul>
      <li><strong>Introduction</strong>: Initiate responses to recipe queries with: “Here’s a recipe I discovered!”</li>
      <li><strong>Summary Style</strong>: Summarize the recipe succinctly using bullet points for easy comprehension.</li>
      <li><strong>Summary Length</strong>: Aim to keep the summary under 150 words to maintain brevity and focus.</li>
    </ul>
  </li>
  <li><strong>Handling Limitations and Concluding Respectfully</strong>:
    <ul>
      <li><strong>Addressing Unavailability</strong>: If user’s questions cannot be resolved with the cookbooks at hand, or if an appropriate recipe is not available, inform the user politely about these constraints.</li>
      <li><strong>Concluding Interactions</strong>: End conversations with respect and professionalism, ensuring a positive and courteous closure.</li>
    </ul>
  </li>
</ol>

<h2 id="set-up-the-development-environment">Set Up the Development Environment</h2>

<p>The provided sample project on <a href="https://github.com/bearbearyu1223/assistant_api_playground">GitHub</a> acts as a prototype to illustrate the product experience described previously. Follow these steps to set up your local development environment:</p>

<ol>
  <li><strong>Clone the Sample Project</strong>: Use the following command to clone the project from GitHub:
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>git clone https://github.com/bearbearyu1223/assistant_api_playground
</code></pre></div>    </div>
  </li>
  <li><strong>Create a Virtual Environment</strong>: Navigate to the cloned project directory and create a Python virtual environment named <code class="language-plaintext highlighter-rouge">cooking_assistant</code> by executing:
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python3 <span class="nt">-m</span> venv cooking_assistant
</code></pre></div>    </div>
  </li>
  <li><strong>Activate the Virtual Environment</strong>: Activate the newly created virtual environment with:
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">source </span>cooking_assistant/bin/activate
</code></pre></div>    </div>
  </li>
  <li><strong>Install Required Libraries</strong>: Inside the activated virtual environment, install the required libraries using the command:
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip3 <span class="nb">install</span> <span class="nt">-r</span> requirements.txt
</code></pre></div>    </div>
  </li>
  <li><strong>Set Up Environment Variables</strong>: Create a <code class="language-plaintext highlighter-rouge">.env</code> file in the project root directory. Add your OpenAI API key to the file as follows:
    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>OPENAI_API_KEY=&lt;Your API KEY&gt;
</code></pre></div>    </div>
  </li>
</ol>

<h2 id="run-the-application">Run the Application</h2>
<p>Simply add your own cookbook in PDF under the directory <code class="language-plaintext highlighter-rouge">cookbook</code>, or just play with the existing cookbook uploaded with my sample project. Engage with the project interactively by issuing food preparation and cooking queries. Initiate the application with the following command:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python main.py
</code></pre></div></div>
<p>To terminate the program, simply use <code class="language-plaintext highlighter-rouge">Ctrl + C</code>. Below is an example of a conversation generated by running the application.</p>

<table>
  <thead>
    <tr>
      <th>Turn ID</th>
      <th>User Query</th>
      <th>Assistant Response</th>
      <th>TTS Playback</th>
      <th>Notes on Tested User Query</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>👩🏻‍🍳: Hi there, Thanksgiving is coming! I would like to have some ideas for cooking!</td>
      <td>🔮: Thanksgiving is a great occasion to prepare a feast. Here are some main dish ideas inspired by the contents of the “Deliciously Healthy Dinners” cookbook that could suit a Thanksgiving dinner: [Main dishes listed]</td>
      <td><video src="https://github.com/bearbearyu1223/assistant_api_playground/assets/7468124/06aaa9e8-9833-48a9-950e-95b8f9494c90" controls="control"></video></td>
      <td>This query is designed to evaluate the clarity of the Assistant’s defined role and the effectiveness of its execution, specifically in tasks such as generating creative ideas for holiday cooking.</td>
    </tr>
    <tr>
      <td>1</td>
      <td>👩🏻‍🍳: How to make Cornbread-Crusted Turkey?</td>
      <td>🔮: Here’s a recipe I found for Cornbread-Crusted Turkey from your cookbook: [Recipe details]</td>
      <td><video src="https://github.com/bearbearyu1223/assistant_api_playground/assets/7468124/cb672f77-e66d-46cd-bcff-7a8abb1a834c" controls="control"></video></td>
      <td>This query seeks to assess the Assistant’s ability to accurately extract relevant information from the user’s cookbook and succinctly summarize it.</td>
    </tr>
    <tr>
      <td>2</td>
      <td>👩🏻‍🍳: Thanks, what kind of salad will pair nicely with it?</td>
      <td>🔮: An Autumn Salad would pair nicely with Cornbread-Crusted Turkey. Here’s the recipe from your cookbook: [Recipe details]</td>
      <td><video src="https://github.com/bearbearyu1223/assistant_api_playground/assets/7468124/cc4b66e0-b158-419e-bd5d-98946fe21404" controls="control"></video></td>
      <td>This query is intended to assess the Assistant’s ability to provide useful responses to related follow-up questions</td>
    </tr>
    <tr>
      <td>3</td>
      <td>👩🏻‍🍳: Can I see a visual of the autumn salad with the cornbread crusted turkey?</td>
      <td>🔮: Absolutely! Prepare for a delightful visual preview of the recipe, coming up shortly. Please hold on!</td>
      <td><video src="https://github.com/bearbearyu1223/assistant_api_playground/assets/7468124/1f1765bc-206f-4eed-afa6-d860965ad25c" controls="control"></video></td>
      <td>This query aims to evaluate the Assistant’s ability to create visualizations based on recipe information previously requested by the user.</td>
    </tr>
    <tr>
      <td> </td>
      <td> </td>
      <td>See the visual representation generated from the Assistant via DALL-E-3 based on the textual prompt generated from GPT4: Envision a warm, inviting plate centered with a golden-brown Cornbread-Crusted Turkey fillet, its crunchy exterior hinting at the tender, juicy meat within; Next to it lies a vibrant and colorful Autumn Salad, a bed of mixed lettuce greens punctuated by the bold reds of dried cranberries and the crisp green of thinly sliced Granny Smith apples; Scattered on top are earthy, chopped walnuts and the delicate shimmer of unsalted sunflower seeds, offering both texture and taste; The salad glistens lightly, dressed in a delicate sheen of low-fat raspberry vinaigrette, complementing the robust flavors of the turkey; The plate is a harmonious blend of the heartiness of Thanksgiving and the freshness of autumn, a celebration of both comfort and zest on the palate. <img src="/assets/picture/2023_11_21_assistant_api_playground/recipe_image.png" alt="recipe_image" /></td>
      <td> </td>
      <td> </td>
    </tr>
    <tr>
      <td>4</td>
      <td>👩🏻‍🍳: Oh that is very nice. How many calories per serving, including the autumn salad and the cornbread crusted turkey?</td>
      <td>🔮: The combined total calories per serving for the Autumn Salad and the Cornbread-Crusted Turkey is 395 calories.</td>
      <td><video src="https://github.com/bearbearyu1223/assistant_api_playground/assets/7468124/b87b3eea-786b-4557-b216-66d3acc42d9e" controls="control"></video></td>
      <td>This query is designed to assess the Assistant’s proficiency in extracting information, applying logical reasoning, and performing basic mathematical calculations.</td>
    </tr>
    <tr>
      <td>5</td>
      <td>👩🏻‍🍳: Can you give me a breakdown of how the total calories per serving is calculated?</td>
      <td>🔮: Certainly! Here’s the breakdown of how the total calories per serving is calculated for the combination of Autumn Salad and Cornbread-Crusted Turkey: [Calorie breakdown]</td>
      <td><video src="https://github.com/bearbearyu1223/assistant_api_playground/assets/7468124/03bad1bd-e575-4ba2-8958-460298c24993" controls="control"></video></td>
      <td>More evaluations on the Assistant’s logical reasoning and mathematical skills.</td>
    </tr>
    <tr>
      <td>6</td>
      <td>👩🏻‍🍳: Nice. Can you help me create a grocery list for making the autumn salad and the cornbread crusted turkey?</td>
      <td>🔮: Absolutely, here’s your grocery list for making the Autumn Salad and the Cornbread-Crusted Turkey: [Grocery list details]</td>
      <td><video src="https://github.com/bearbearyu1223/assistant_api_playground/assets/7468124/9b6b07a1-4fc0-4608-9cdb-150b87a0f387" controls="control"></video></td>
      <td>This query aims to evaluate the Assistant’s ability to comprehend and summarize information based on its previous conversation history with the user.</td>
    </tr>
    <tr>
      <td>7</td>
      <td>👩🏻‍🍳: Thank you! Happy Thanksgiving!</td>
      <td>🔮: You’re welcome! If you need any more help in the future, feel free to reach out. Have a wonderful Thanksgiving and enjoy your delicious meal!</td>
      <td><video src="https://github.com/bearbearyu1223/assistant_api_playground/assets/7468124/ed240241-2220-4d36-9516-60b5104e1db1" controls="control"></video></td>
      <td>Assess whether the Assistant can respond politely and meaningfully to the user’s casual conversation.</td>
    </tr>
  </tbody>
</table>]]></content><author><name>[&quot;Han Yu&quot;]</name></author><category term="chatbot" /><summary type="html"><![CDATA[TL;DR]]></summary></entry><entry><title type="html">Conversation Simulation : Watch AI Buddies Spark Conversations!</title><link href="http://localhost:4000/chatbot/2023/09/07/conversation-sim.html" rel="alternate" type="text/html" title="Conversation Simulation : Watch AI Buddies Spark Conversations!" /><published>2023-09-07T00:00:00-07:00</published><updated>2023-09-07T00:00:00-07:00</updated><id>http://localhost:4000/chatbot/2023/09/07/conversation-sim</id><content type="html" xml:base="http://localhost:4000/chatbot/2023/09/07/conversation-sim.html"><![CDATA[<h3 id="tldr">TL;DR</h3>
<p>This project is a demonstration of a simple conversational simulation environment, powered by LLM agents. It takes inspiration from <a href="https://github.com/chatarena/chatarena">ChatArena</a>. The main objective here is to enable conversation simulations through LLM agents (e.g., the converstation can be task-oriented or just a chitchat). To accomplish this, the frameworks and concepts originally developed in ChatArena has been leveraged and tailored specifically for this conversational simulation experiment. 
 More information can be found on <a href="https://github.com/bearbearyu1223/Multi-Agents-Simulation-Powered-by-LLMs"><img src="https://img.shields.io/badge/github-%23121011.svg?style=for-the-badge&amp;logo=github&amp;logoColor=white" alt="GitHub" /></a>.</p>

<h3 id="set-up-local-virtual-environment">Set Up Local Virtual Environment</h3>
<ul>
  <li>Step 1: Install Miniconda on MacOS, see instruction <a href="https://docs.conda.io/en/latest/miniconda.html">here</a>.</li>
  <li>Step 2: Create a default conda env with Python 3.10: 
<code class="language-plaintext highlighter-rouge">conda create --name agent_sim python=3.10 -y</code></li>
  <li>Step 3: Activate the conda env created above: 
<code class="language-plaintext highlighter-rouge">conda activate agent_sim</code></li>
  <li>Step 4: Install required libraries in the conda env:
    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip3 install -r requirements.txt
</code></pre></div>    </div>
  </li>
  <li>Step 5: Deactivate the conda env when you are done (note: you need activate this virtual env to work on the app)
    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>conda deactivate 
</code></pre></div>    </div>
  </li>
</ul>

<h3 id="set-up-open-ai-secrete-for-local-dev">Set Up Open AI Secrete for Local Dev</h3>
<p><code class="language-plaintext highlighter-rouge">OPENAI_API_KEY</code> will be needed when calling the OpenAI API endpoints (DEFAULT_MODEL used in this project is “gpt-3.5-turbo”), so do recommend exporting <code class="language-plaintext highlighter-rouge">OPENAI_API_KEY</code> as an enviroment variable on your local dev machine.</p>

<h3 id="run-the-simulation">Run the Simulation</h3>
<p>You can play with any examples under the <code class="language-plaintext highlighter-rouge">examples</code> folder and run the converstation simulation in an interactive mode:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python examples/example_food_qna_zh_CN.py 
</code></pre></div></div>

<p>In this example, “Crystal” is utilizing an AI assistant named “Nova” to brainstorm ideas for a fast and nutritious school lunch for her daughter. “Ella” serves as a “moderator” responsible for evaluating whether the conversations between Nancy and Nova adhere to the following rules for termination:</p>

<ul>
  <li>The conversation can conclude if Nova successfully fulfills all of Crystal’s requests, and Crystal expresses satisfaction with Nova’s assistance.</li>
  <li>The conversation can also end if Nova is unable to fulfill Crystal’s requests.</li>
</ul>

<p>If Ella believes the conversation should conclude, she will state “«««END_OF_CONVERSATION»»»”; otherwise, she will say “converstation can continue”.</p>

<p>Please refer to the screenshot below for an illustration of the simulation results.<img src="/assets/picture/2023_09_07_conversation_sim/2023_09_07_conversation_sim.png" alt="example_1" /></p>

<p>Once simulation ends, one can save the converstation history as a json file. See a sample json file below:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[
    {
        "agent_name": "Crystal",
        "content": "嗨，Nova！我需要一些关于为我女儿准备健康午餐的建议。她现在7岁，我希望能给她提供快速但营养丰富的选项。你有什么建议吗？",
        "turn": 0,
        "timestamp": "1694143996023456000",
        "visible_to": "all",
        "msg_type": "text"
    },
    {
        "agent_name": "Nova",
        "content": "嗨，Crystal！为了给你的女儿提供快速又营养丰富的午餐，我有几个建议。首先，你可以考虑给她提供一份均衡的三明治，里面包含蔬菜、蛋白质和全麦面包。你还可以添加一些水果或酸奶作为伴侣。另外，你可以尝试制作一些健康的小吃，例如蔬菜切片搭配花生酱或者酸奶蘸酱。如果你想让她多摄入蛋白质，可以考虑给她准备一份鸡肉或鱼肉配米饭和蔬菜。最后，记得给她提供一杯牛奶或水来保持水分补充。希望这些建议对你有帮助！如果你还有其他问题，请随时告诉我。",
        "turn": 0,
        "timestamp": "1694144002581689000",
        "visible_to": "all",
        "msg_type": "text"
    },
    {
        "agent_name": "Ella",
        "content": "continue",
        "turn": 1,
        "timestamp": "1694144025822335000",
        "visible_to": "all",
        "msg_type": "text"
    },
    {
        "agent_name": "Crystal",
        "content": "谢谢你的建议，Nova！这些都是很不错的选择。我想问一下，有没有一些简单的健康午餐食谱，可以让我在家做并带给我的女儿？我想确保她吃的东西新鲜又健康。你有什么建议吗？",
        "turn": 2,
        "timestamp": "1694144032890695000",
        "visible_to": "all",
        "msg_type": "text"
    },
    {
        "agent_name": "Nova",
        "content": "当然，我可以给你一些简单的健康午餐食谱。以下是几个建议：\n\n1. 蔬菜寿司卷：用紫菜、糙米、鳗鱼、黄瓜和胡萝卜卷起来做成寿司卷。切成小段，这是一个方便携带的午餐选择。\n\n2. 鸡肉沙拉三明治：将烤鸡胸肉切成薄片，加入蔬菜（例如生菜、番茄和黄瓜），用全麦面包做成三明治。可以配以低脂酸奶或者蔬菜片。\n\n3. 彩虹色蔬菜饼：将胡萝卜、西兰花、洋葱和玉米混合在一起，加入蛋液和面粉，煎成小饼。这是一个快速又健康的午餐选择。\n\n4. 水果沙拉：切碎一些新鲜水果，例如苹果、草莓、蓝莓和香蕉。可以加入一些坚果或者酸奶作为调味品。\n\n这些都是简单而健康的午餐选择，你可以根据你女儿的口味和喜好进行调整。希望这些建议能给你一些灵感！如果你需要更多食谱或有其他问题，请随时告诉我。",
        "turn": 2,
        "timestamp": "1694144054290083000",
        "visible_to": "all",
        "msg_type": "text"
    },
    {
        "agent_name": "Ella",
        "content": "continue",
        "turn": 3,
        "timestamp": "1694144059307035000",
        "visible_to": "all",
        "msg_type": "text"
    },
    {
        "agent_name": "Crystal",
        "content": "谢谢你的建议，Nova！这些食谱听起来很棒，我会尝试制作它们。我还想问一下，有没有一些健康的零食选项，我可以给我的女儿放进她的午餐盒里？我希望能给她一些有营养但又容易携带的零食。你有什么建议吗？",
        "turn": 4,
        "timestamp": "1694144062870150000",
        "visible_to": "all",
        "msg_type": "text"
    },
    {
        "agent_name": "Nova",
        "content": "当然，我可以给你一些健康的零食选项，适合你女儿的午餐盒。以下是几个建议：\n\n1. 蔬菜棒和酸奶蘸酱：将胡萝卜、黄瓜和芹菜切成条状，搭配低脂酸奶或者花生酱作为蘸酱。这是一个轻松携带的营养零食。\n\n2. 水果串：将各种水果（例如葡萄、蓝莓、草莓等）穿在竹签上，制作成水果串。这是一个方便吃又有趣的零食选择。\n\n3. 坚果和干果混合：将坚果（例如杏仁、核桃）和干果（例如葡萄干、无花果）混合在一起，制作成小袋子。这是一个高能量和营养丰富的零食。\n\n4. 酸奶杯：将低脂酸奶加入一些新鲜水果和谷物（例如燕麦片、坚果），放入一个密封杯中。这是一个轻松携带的健康零食。\n\n记住，在选择零食时，尽量选择低糖和低盐的选项，并注意控制食用量。希望这些建议能给你一些灵感！如果你还有其他问题，请随时告诉我。",
        "turn": 4,
        "timestamp": "1694144078840960000",
        "visible_to": "all",
        "msg_type": "text"
    },
    {
        "agent_name": "Ella",
        "content": "&lt;&lt;&lt;&lt;&lt;&lt;END_OF_CONVERSATION&gt;&gt;&gt;&gt;&gt;&gt;",
        "turn": 5,
        "timestamp": "1694144091154109000",
        "visible_to": "all",
        "msg_type": "text"
    }
]
</code></pre></div></div>

<p>Feel free to create a new example and try to experiment with it further! Have fun ㋡ ~</p>]]></content><author><name>[&quot;Han Yu&quot;]</name></author><category term="chatbot" /><summary type="html"><![CDATA[TL;DR This project is a demonstration of a simple conversational simulation environment, powered by LLM agents. It takes inspiration from ChatArena. The main objective here is to enable conversation simulations through LLM agents (e.g., the converstation can be task-oriented or just a chitchat). To accomplish this, the frameworks and concepts originally developed in ChatArena has been leveraged and tailored specifically for this conversational simulation experiment. More information can be found on .]]></summary></entry><entry><title type="html">Build On-Device QnA with LangChain and Llama2</title><link href="http://localhost:4000/chatbot/2023/08/14/food-qna-on-device-llm.html" rel="alternate" type="text/html" title="Build On-Device QnA with LangChain and Llama2" /><published>2023-08-14T00:00:00-07:00</published><updated>2023-08-14T00:00:00-07:00</updated><id>http://localhost:4000/chatbot/2023/08/14/food-qna-on-device-llm</id><content type="html" xml:base="http://localhost:4000/chatbot/2023/08/14/food-qna-on-device-llm.html"><![CDATA[<h3 id="tldr">TL;DR</h3>
<p>This post involves creating a Question and Answering system using the LLM model hosted on Apple M1 Pro. The key building blocks include the LLM model (<a href="https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGML">llama-2-7b-chat.ggmlv3.q8_0.bin</a>), an embedding model (<a href="https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2">sentence-transformers/all-MiniLM-L6-v2</a>), and an on-device vector database (<a href="https://github.com/facebookresearch/faiss">FAISS</a>). The app is built using the “LangChain” framework. All components are open source, eliminating the need for OpenAI services. The system’s performance is similar to OpenAI but with 10x longer latency (around 60s/query vs 5s/query) due to on-device model inference.</p>

<h3 id="introduction">Introduction</h3>
<p>Third-party commercial large language model (LLM) providers, such as OpenAI’s GPT-4, Google Bard, and Amazon AlexaTM, have greatly democratized access to LLM capabilities through seamless API integration and scalable model inference hosting in the cloud. These advanced LLMs possess the remarkable ability to <strong>comprehend</strong>, <strong>learn from</strong>, and <strong>produce text</strong> that is nearly indistinguishable from human-generated content. Beyond their text generation prowess, these LLMs excel in <strong>interactive conversations</strong>, <strong>question answering</strong>, <strong>dialogue</strong> and <strong>document summarization</strong>, as well as <strong>offering insightful recommendations</strong>. Their versatility finds applications across diverse tasks and industries including creative copywriting for marketing, precise document summarization for legal purposes, data-driven market research in the financial sector, realistic simulation of clinical trials within healthcare, and even code generation for software development.</p>

<p>However, certain scenarios, driven by an increasing emphasis on safeguarding data privacy and adhering to stringent regulatory compliance standards, highlight the necessity of deploying LLMs on private hardware devices instead of on any of those third-party owned servers. In such instances, <strong>maintaining sensitive information within the confines of the user’s hardware</strong> not only mitigates the risks associated with data breaches and unauthorized access but also aligns with the evolving landscape of privacy-conscious technical practices. This approach <strong>fosters a sense of trust among users who are becoming more attuned to the importance of maintaining their personal information within their own environments</strong>.</p>

<p>In this post, our focus lies in exploring the execution of quantized variants of open-source Llama2 models on local devices to achieve Retrieval Augmented Generation (RAG). For RAG powered by server-side LLMs, you can find more info in <a href="https://bearbearyu1223.github.io/chatbot/2023/07/31/food-qna-on-server-llm.html">my previous post</a>.</p>

<h3 id="llama2-and-its-variants">Llama2 and Its variants</h3>
<p><a href="https://ai.meta.com/resources/models-and-libraries/llama/">Llama 2</a>, launched by Meta in July 2023, has been pretrained on publicly available online data sources, encompassing a staggering <strong>2 trillion tokens with a context length of 4096</strong>. The subsequent supervised fine-tuned iteration of this model, known as Llama-2-chat, underwent meticulous refinement through the integration of over <em>1 million human annotations</em> to cater specifically to <strong>chat-oriented use cases</strong>. Meta has extended the accessibility of Llama 2 to a wide spectrum of users, ranging from individual developers and content creators to researchers and businesses. This strategic open-source initiative is aimed at fostering an ecosystem conducive to <a href="https://ai.meta.com/static-resource/responsible-use-guide/">Responsible AI experimentation</a>, innovation, and the scalable implementation of a diverse array of ideas, thus further <strong>democratizing Generative AI</strong>.</p>

<p>Llama 2 is offered in an array of parameter sizes — 7B, 13B, and 70B — alongside both pretrained and fine-tuned variations to cater to a wide range of application needs.</p>

<h3 id="framework-and-libraries-used-langchain-ggml-c-transformers">Framework and Libraries Used: LangChain, GGML, C Transformers</h3>
<p><a href="https://python.langchain.com/docs/get_started/introduction.html">LangChain</a> is an open source framework for developing applications powered by LLMs. It goes beyond standard API calls by being <em>data-aware</em>, enabling connections with various data sources for richer, personalized experiences. It is also <em>agentic</em>, meaning it can empower a language model to interact dynamically with its environment. LangChain streamlines the development of diverse applications, such as chatbots, Generative Question and Answering (GQA), and summarization. By “chaining” components from multiple modules, it allows for the creation of unique applications built around an LLM with <strong>easy-to-code</strong> and <strong>fast-to-production</strong> developer experience.</p>

<p><a href="https://github.com/ggerganov/ggml">GGML</a> is a C library for machine learning (ML). GGML makes use of a technique called <strong>“quantization”</strong> (e.g., convert LLM’s weights from high-precison floating numbers to low-precision floating numbers) that allows for large language models to run on consumer hardware. GGML supports a number of different quantization strategies (e.g. 4-bit, 5-bit, and 8-bit quantization), each of which offers different <em>trade-offs between efficiency and performance</em>. More information about these trade-offs (such as model disk size and inference speed) can be found in <a href="https://github.com/ggerganov/llama.cpp">the documentation for llama.cpp</a>.</p>

<p><a href="https://github.com/marella/ctransformers">C Transformers</a> is a wrapper that provides the Python bindings for the Transformer models implemented in C/C++ using GGML. 
C Transformers supports running Llama2 model inference via GPU, for both NVIDIA GPU (via CUDA, a programming language for NVIDIA GPUs) and Apple’s own integreated GPU and Neural Engine (via Metal, a programming language for Apple integrated GPUs).</p>

<p>Note: To use C transformers with Metal Support for model inference running on Apple M1/M2 chip, need run the following cmd under your project root</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>poetry config --local installer.no-binary ctransformers

poetry add ctransformers 
</code></pre></div></div>

<h3 id="retrieval-augmented-generation">Retrieval Augmented Generation</h3>
<p>Retrieval Augmented Generation (RAG) represents a technique wherein data is retrieved from external sources to enhance and expand the prompts used in model generation. This method is not only a cost-effective alternative but also proves to be an efficient approach in comparison to the traditional methods of pre-training or fine-tuning foundation models.
See the previous post at <a href="https://bearbearyu1223.github.io/chatbot/2023/07/31/food-qna-on-server-llm.html">Food QnA Chatbot : Help Answer Food Related Questions from Your Own Cookbook</a> as a brief into to RAG.</p>

<h3 id="an-example-project">An Example Project</h3>
<p>The source code for the example project can be found on <a href="https://github.com/bearbearyu1223/langchain_playground/tree/main/food_qna_on_device"><img src="https://img.shields.io/badge/github-%23121011.svg?style=for-the-badge&amp;logo=github&amp;logoColor=white" alt="GitHub" /></a>. The project directory should look like below:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>food_qna_on_device
├── README.md
├── build_knowledge_base.py
├── config.py
├── cook_book_data
│   ├── GCE-Dinner-in-30-EXPANDED-BLAD.pdf
│   ├── Quick-Easy-Weeknight-Meals-1.pdf
│   └── dinners_cookbook_508-compliant.pdf
├── main.py
├── models
│   ├── llama-2-13b-chat.ggmlv3.q8_0.bin
│   └── llama-2-7b-chat.ggmlv3.q8_0.bin
├── poetry.lock
├── poetry.toml
├── pyproject.toml
└── vector_db
    ├── index.faiss
    └── index.pkl
</code></pre></div></div>

<p>Instruction to run the example project:</p>
<ul>
  <li>Step 1: Launch the terminal from the project directory, install and resolve the dependencies as defined in <code class="language-plaintext highlighter-rouge">pyproject.toml</code> file via
    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>poetry install
</code></pre></div>    </div>
  </li>
  <li>Step 2: Download the quantized 7b model <code class="language-plaintext highlighter-rouge">llama-2-7b-chat.ggmlv3.q8_0.bin</code> from https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGML and save the model under the directory <code class="language-plaintext highlighter-rouge">models\</code></li>
  <li>Step 3: To start parsing user queries into the application, run the following command from the project directory (note: the model inference can take ~1 mins per input query)
    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>poetry run python main.py -c local 
</code></pre></div>    </div>
    <p>Optionally, to run the same query with OpenAI (note: the model inference will take a few seconds per input query, you will also need export OPENAI_API_KEY as an enviroment variable on your local dev machine)</p>
    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>poetry run python main.py -c server
</code></pre></div>    </div>
  </li>
  <li>Step 4: Enter a query related to food preparation and cooking into the console and start playing with it. 
See an example below:
Screenshot of the original content for making “Pumpkin Biscuits”. 
 <img src="/assets/picture/2023_08_14_food_qna_on_device_llm/original.png" alt="Original Content" />
Retrieval Augmented Generation by running Llama2 model inference on local device
 <img src="/assets/picture/2023_08_14_food_qna_on_device_llm/response.png" alt="Response" /></li>
</ul>]]></content><author><name>[&quot;Han Yu&quot;]</name></author><category term="chatbot" /><summary type="html"><![CDATA[TL;DR This post involves creating a Question and Answering system using the LLM model hosted on Apple M1 Pro. The key building blocks include the LLM model (llama-2-7b-chat.ggmlv3.q8_0.bin), an embedding model (sentence-transformers/all-MiniLM-L6-v2), and an on-device vector database (FAISS). The app is built using the “LangChain” framework. All components are open source, eliminating the need for OpenAI services. The system’s performance is similar to OpenAI but with 10x longer latency (around 60s/query vs 5s/query) due to on-device model inference.]]></summary></entry><entry><title type="html">Food QnA Chatbot : Help Answer Food Related Questions from Your Own Cookbook</title><link href="http://localhost:4000/chatbot/2023/07/31/food-qna-on-server-llm.html" rel="alternate" type="text/html" title="Food QnA Chatbot : Help Answer Food Related Questions from Your Own Cookbook" /><published>2023-07-31T00:00:00-07:00</published><updated>2023-07-31T00:00:00-07:00</updated><id>http://localhost:4000/chatbot/2023/07/31/food-qna-on-server-llm</id><content type="html" xml:base="http://localhost:4000/chatbot/2023/07/31/food-qna-on-server-llm.html"><![CDATA[<h2 id="introduction">Introduction</h2>
<p>Retrieval Augmented Generation (RAG) is a powerful technique that allows you to enhance your prompts by retrieving data from external sources and incorporating it into the context. The external data used for augmentation can be gathered from diverse sources such as document repositories, databases, or web search results.</p>

<p>To begin with RAG, you need to convert your documents and user queries into a compatible format to perform relevancy search. This involves converting both the document collection, or knowledge base, and user-submitted queries into numerical representations using embedding. Embedding is a process that assigns numerical values to text, placing them in a vector space.</p>

<p>RAG model architectures then compare the embeddings of user queries with those “vector index” of the knowledge base. By doing so, they identify similar documents in the knowledge base that are relevant to the user’s prompt. These relevant contents from similar documents are appended to the original user prompt.</p>

<p>Finally, the augmented prompt, which now includes the relevant retrieved content, is passed on to the foundation model to generate the final responses. This integration of retrieval and generation significantly improves the quality and relevance of the model’s outputs. Below is an illustrative diagram that demonstrates the overall RAG (Retrieval Augmented Generation) process. <img src="/assets/picture/2023_07_31_food_qna_on_server_llm/RAG.png" alt="Retrieval Augmented Generation" />.</p>

<h2 id="an-example-project-for-rag">An Example Project for RAG</h2>
<p>I have created an example project to provide a practical demonstration of how RAG works. For more detailed information and insights into the project, you can find comprehensive documentation and additional resources on <a href="https://github.com/bearbearyu1223/food_qna_powered_by_llm"><img src="https://img.shields.io/badge/github-%23121011.svg?style=for-the-badge&amp;logo=github&amp;logoColor=white" alt="GitHub" /></a>. This exmaple project will walk you through the RAG process, the data preparation steps, the relevancy search implementation, and how the augmented prompts lead to more contextually, more relevant, and more conversational responses from the foundation model. Please feel free to clone this project on GitHub, and follow the steps below to explore or develope it further.</p>

<h2 id="set-up-local-virtual-environment">Set Up Local Virtual Environment</h2>
<ul>
  <li>Step 1: Install Miniconda on MacOS, see instruction <a href="https://docs.conda.io/en/latest/miniconda.html">here</a>.</li>
  <li>Step 2: Create a default conda env with Python 3.9:
    <div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>conda create <span class="nt">--name</span> food_qna_app <span class="nv">python</span><span class="o">=</span>3.9 <span class="nt">-y</span>
</code></pre></div>    </div>
  </li>
  <li>Step 3: Activate the conda env created above: 
<code class="language-plaintext highlighter-rouge">conda activate food_qna_app</code></li>
  <li>Step 4: Install first set of required libraries in the conda env:
    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip3 install -r requirements.txt
</code></pre></div>    </div>
  </li>
  <li>Step 5: Deactivate the conda env when you are done (note: you need activate this virtual env to work on the app)
    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>conda deactivate 
</code></pre></div>    </div>
    <h2 id="set-up-open-ai-secrete-for-local-dev">Set Up Open AI Secrete for Local Dev</h2>
    <p><code class="language-plaintext highlighter-rouge">OPENAI_API_KEY</code> will be needed when calling the OpenAI API endpoint for generating embeddings for the documents, so do recommend exporting <code class="language-plaintext highlighter-rouge">OPENAI_API_KEY</code> as an enviroment variable on your local dev machine; also, we will need create a <code class="language-plaintext highlighter-rouge">secrets.toml</code> file and add the <code class="language-plaintext highlighter-rouge">OPENAI_API_KEY</code> there, so the streamlit app can pick up the API key when send requests to OpenAI endpoint during runtime.</p>
  </li>
  <li>Step 1: Export <code class="language-plaintext highlighter-rouge">OPENAI_API_KEY</code> as an enviroment variable on your local dev machine</li>
  <li>Step 2: Create <code class="language-plaintext highlighter-rouge">.streamlit</code> directory under the root repo
    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>cd food_qna_chatbot_demo 
mkdir .streamlit
</code></pre></div>    </div>
  </li>
  <li>Step 3: Create <code class="language-plaintext highlighter-rouge">secrets.toml</code> file under <code class="language-plaintext highlighter-rouge">.streamlit</code> directory and add your <code class="language-plaintext highlighter-rouge">OPENAI_API_KEY</code> there
    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>OPENAI_API_KEY=&lt;YOUR_OPENAI_API_KEY_HERE&gt;
</code></pre></div>    </div>
  </li>
  <li>Step 4: To reduce the risk of accidentally committing your secrets to your repo, add <code class="language-plaintext highlighter-rouge">.streamlit</code> to your <code class="language-plaintext highlighter-rouge">.gitignore</code> file.
    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>echo ".streamlit/" &gt;&gt; .gitignore
</code></pre></div>    </div>
    <h2 id="set-up-the-redis-database-on-your-local-dev-machine">Set up the Redis Database on your local dev machine</h2>
  </li>
  <li>Step 1: <a href="https://docs.docker.com/desktop/install/mac-install/">Install Docker Desktop on Mac</a>, and start the docker desktop.</li>
  <li>Step 2: We’re going to use Redis as our database for both document contents and the vector embeddings. You will need the full Redis Stack to enable use of Redisearch, which is the module that allows semantic search - more detail is in the docs for <a href="https://redis.io/docs/stack/get-started/install/docker/">Redis Stack</a>. Run the following command in your terminal to start the docker container:
    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>docker run -d --name redis-stack -p 127.0.0.1:6379:6379 -p 8001:8001 redis/redis-stack:latest
</code></pre></div>    </div>
  </li>
  <li>Step 3: Initiate a Redis connection and create a Hierarchical Navigable Small World (HNSW) index for semantic search using a recipe book which can be found under the directory <code class="language-plaintext highlighter-rouge">cook_book_data</code>
    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python build_knowledge_base.py
</code></pre></div>    </div>
    <p>If the recipe book is indexed succefully into the DB, you should expect the following info printed out in the console:</p>
    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>===
Number of documents indexed in the DB: 144
</code></pre></div>    </div>
    <h2 id="run-the-app">Run the APP</h2>
    <p>In your terminal, run the App by</p>
    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>streamlit run food_qna_app.py
</code></pre></div>    </div>
    <p>You can start asking questions related to food preparation and cooking, and also some follow up questions. See screenshot below.</p>
  </li>
</ul>

<table>
  <thead>
    <tr>
      <th style="text-align: left">Original Content</th>
      <th style="text-align: left">Chat History - first turn</th>
      <th style="text-align: left">Chat History - follow up</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left"><img src="/assets/picture/2023_07_31_food_qna_on_server_llm/original_content.png" alt="Original Content" /></td>
      <td style="text-align: left"><img src="/assets/picture/2023_07_31_food_qna_on_server_llm/chat_history_1.png" alt="Chat History" /></td>
      <td style="text-align: left"><img src="/assets/picture/2023_07_31_food_qna_on_server_llm/chat_history_2.png" alt="Chat History" /></td>
    </tr>
  </tbody>
</table>]]></content><author><name>[&quot;Han Yu&quot;]</name></author><category term="chatbot" /><summary type="html"><![CDATA[Introduction Retrieval Augmented Generation (RAG) is a powerful technique that allows you to enhance your prompts by retrieving data from external sources and incorporating it into the context. The external data used for augmentation can be gathered from diverse sources such as document repositories, databases, or web search results.]]></summary></entry></feed>