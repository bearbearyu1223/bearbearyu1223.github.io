<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2023-11-21T17:50:22-08:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">ğŸ’ Hanâ€™s Generative AI Quest</title><subtitle>I chronicle my captivating journey through Generative AI, sharing insights,  breakthroughs, and learnings from my enthralling side projects in the field. 
</subtitle><entry><title type="html">Craft Your Own Kitchen Companion: Leveraging OpenAI Assistant, Image Generation, and Text-to-Speech for a Revolutionary Cooking Experience</title><link href="http://localhost:4000/chatbot/2023/11/21/assistant-api-playground.html" rel="alternate" type="text/html" title="Craft Your Own Kitchen Companion: Leveraging OpenAI Assistant, Image Generation, and Text-to-Speech for a Revolutionary Cooking Experience" /><published>2023-11-21T00:00:00-08:00</published><updated>2023-11-21T00:00:00-08:00</updated><id>http://localhost:4000/chatbot/2023/11/21/assistant-api-playground</id><content type="html" xml:base="http://localhost:4000/chatbot/2023/11/21/assistant-api-playground.html"><![CDATA[<h2 id="tldr">TL;DR</h2>
<p><img src="/assets/picture/2023_11_21_assistant_api_playground/cropped_image.png" alt="page_cover" /></p>

<p>OpenAI unveiled its <a href="https://platform.openai.com/docs/assistants/overview">Assistant API</a> during its very first developer conference-<a href="https://devday.openai.com/">OpenAI DevDay</a>. This marked a significant addition to the suite of tools and models available for building conversational AI, a domain in which Iâ€™ve been actively engaged.</p>

<p>Although I have used other frameworks, tools and models to build customized Assistant in the past (check out my previous posts  <a href="https://bearbearyu1223.github.io/chatbot/2023/07/31/food-qna-on-server-llm.html">Food QnA Chatbot : Help Answer Food Related Questions from Your Own Cookbook</a> and <a href="https://bearbearyu1223.github.io/chatbot/2023/08/14/food-qna-on-device-llm.html">Build On-Device QnA with LangChain and Llama2</a>), intrigued by the new capabilities of the Assistant API, I decided to integrate it into a new project. This sample project is a refreshed demonstration of a simple Cooking Assistant, leveraging not only the Assistant API but also incorporating additional functionalities such as <a href="https://platform.openai.com/docs/guides/text-to-speech">Text-to-Speech</a> and <a href="https://platform.openai.com/docs/guides/images?context=node">Image Generation</a>, showcasing a holistic approach to building conversational AI systems with only a few lines of code.</p>

<p>Get inspired by the latest from Open AI DevDay with my project on <a href="https://github.com/bearbearyu1223/assistant_api_playground"><img src="https://img.shields.io/badge/github-%23121011.svg?style=for-the-badge&amp;logo=github&amp;logoColor=white" alt="GitHub" /></a>? Ready to unleash your creativity? Start building your own generative AI-powered experience today, join the forefront of AI innovation, and let us make AI for everyone!</p>

<h2 id="set-up-the-dev-enviroment">Set Up the Dev Enviroment</h2>
<ul>
  <li>Step 1: Git clone the sample project from <a href="https://github.com/bearbearyu1223/assistant_api_playground"><img src="https://img.shields.io/badge/github-%23121011.svg?style=for-the-badge&amp;logo=github&amp;logoColor=white" alt="GitHub" /></a>.</li>
  <li>Step 2: Create a venv
    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python3 -m venv cooking_assistant
</code></pre></div>    </div>
  </li>
  <li>Step 3: Under the project root, active the venv you have just created,
    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>source cooking_assistant/bin/activate
</code></pre></div>    </div>
  </li>
  <li>Step 4: Installed the required libaries in the venv
    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip3 install -r requirements.txt
</code></pre></div>    </div>
  </li>
  <li>Step 5: create an <code class="language-plaintext highlighter-rouge">.env</code> file under your project root and add your Open AI API Key as following:
    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>OPENAI_API_KEY=&lt;Your API KEY&gt;
</code></pre></div>    </div>
  </li>
</ul>

<h2 id="run-the-application">Run the Application</h2>
<p>You can play with the project in an interactive mode by entering a query related to food preparation and cooking via:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python main.py
</code></pre></div></div>
<p>and simply press <code class="language-plaintext highlighter-rouge">Ctrl + C</code> to exit the program. See a sample converstation created from this application below.</p>

<table>
  <thead>
    <tr>
      <th>Turn ID</th>
      <th>User Query</th>
      <th style="text-align: center">Assistant Response</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>turn 0</td>
      <td>ğŸ‘©ğŸ»â€ğŸ³: Hi there, Thanksgiving is coming! I would like to have some ideas for cooking!</td>
      <td style="text-align: center"><video src="https://github.com/bearbearyu1223/assistant_api_playground/assets/7468124/e3c82408-aa4b-4b25-85ce-a2c35414bb18" controls="controls"> </video></td>
    </tr>
    <tr>
      <td>turn 1</td>
      <td>ğŸ‘©ğŸ»â€ğŸ³: Yes, please</td>
      <td style="text-align: center"><video src="https://github.com/bearbearyu1223/assistant_api_playground/assets/7468124/4388d450-2fbe-4de4-8cb5-c432a4a6c989" controls="controls"> </video></td>
    </tr>
    <tr>
      <td>turn 2</td>
      <td>ğŸ‘©ğŸ»â€ğŸ³: How does the cornbread crusted turkey looks like?</td>
      <td style="text-align: center"><video src="https://github.com/bearbearyu1223/assistant_api_playground/assets/7468124/3b97f276-e739-4f9b-8676-e0af88e25e0e" controls="controls"> </video><br /></td>
    </tr>
    <tr>
      <td>turn 3</td>
      <td>ğŸ‘©ğŸ»â€ğŸ³: That is nice! And how to make cornbread?</td>
      <td style="text-align: center"><video src="https://github.com/bearbearyu1223/assistant_api_playground/assets/7468124/e2b30daf-d11a-4d0b-96b5-7e0dabeb012a" controls="controls"> </video></td>
    </tr>
    <tr>
      <td>Â </td>
      <td>See the visual representation generated by the Assistant bellow:<br /><br /><img src="/assets/picture/2023_11_21_assistant_api_playground/DALL_E_Cornbread_Crusted_Turkey_34.png" alt="page_cover" /></td>
      <td style="text-align: center">Â </td>
    </tr>
    <tr>
      <td>turn 4</td>
      <td>ğŸ‘©ğŸ»â€ğŸ³: No, actually, can you create a grocery list for me, including the recipes for the cornbread crusted turkey and also the cornbread?</td>
      <td style="text-align: center"><video src="https://github.com/bearbearyu1223/assistant_api_playground/assets/7468124/ccaf9d52-041c-4b96-8a23-674d94aa489d" controls="controls"> </video></td>
    </tr>
  </tbody>
</table>]]></content><author><name>[&quot;Han Yu&quot;]</name></author><category term="chatbot" /><summary type="html"><![CDATA[TL;DR]]></summary></entry><entry><title type="html">Conversation Simulation : Watch AI Buddies Spark Conversations!</title><link href="http://localhost:4000/chatbot/2023/09/07/conversation-sim.html" rel="alternate" type="text/html" title="Conversation Simulation : Watch AI Buddies Spark Conversations!" /><published>2023-09-07T00:00:00-07:00</published><updated>2023-09-07T00:00:00-07:00</updated><id>http://localhost:4000/chatbot/2023/09/07/conversation-sim</id><content type="html" xml:base="http://localhost:4000/chatbot/2023/09/07/conversation-sim.html"><![CDATA[<h3 id="tldr">TL;DR</h3>
<p>This project is a demonstration of a simple conversational simulation environment, powered by LLM agents. It takes inspiration from <a href="https://github.com/chatarena/chatarena">ChatArena</a>. The main objective here is to enable conversation simulations through LLM agents (e.g., the converstation can be task-oriented or just a chitchat). To accomplish this, the frameworks and concepts originally developed in ChatArena has been leveraged and tailored specifically for this conversational simulation experiment. 
 More information can be found on <a href="https://github.com/bearbearyu1223/Multi-Agents-Simulation-Powered-by-LLMs"><img src="https://img.shields.io/badge/github-%23121011.svg?style=for-the-badge&amp;logo=github&amp;logoColor=white" alt="GitHub" /></a>.</p>

<h3 id="set-up-local-virtual-environment">Set Up Local Virtual Environment</h3>
<ul>
  <li>Step 1: Install Miniconda on MacOS, see instruction <a href="https://docs.conda.io/en/latest/miniconda.html">here</a>.</li>
  <li>Step 2: Create a default conda env with Python 3.10: 
<code class="language-plaintext highlighter-rouge">conda create --name agent_sim python=3.10 -y</code></li>
  <li>Step 3: Activate the conda env created above: 
<code class="language-plaintext highlighter-rouge">conda activate agent_sim</code></li>
  <li>Step 4: Install required libraries in the conda env:
    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip3 install -r requirements.txt
</code></pre></div>    </div>
  </li>
  <li>Step 5: Deactivate the conda env when you are done (note: you need activate this virtual env to work on the app)
    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>conda deactivate 
</code></pre></div>    </div>
  </li>
</ul>

<h3 id="set-up-open-ai-secrete-for-local-dev">Set Up Open AI Secrete for Local Dev</h3>
<p><code class="language-plaintext highlighter-rouge">OPENAI_API_KEY</code> will be needed when calling the OpenAI API endpoints (DEFAULT_MODEL used in this project is â€œgpt-3.5-turboâ€), so do recommend exporting <code class="language-plaintext highlighter-rouge">OPENAI_API_KEY</code> as an enviroment variable on your local dev machine.</p>

<h3 id="run-the-simulation">Run the Simulation</h3>
<p>You can play with any examples under the <code class="language-plaintext highlighter-rouge">examples</code> folder and run the converstation simulation in an interactive mode:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python examples/example_food_qna_zh_CN.py 
</code></pre></div></div>

<p>In this example, â€œCrystalâ€ is utilizing an AI assistant named â€œNovaâ€ to brainstorm ideas for a fast and nutritious school lunch for her daughter. â€œEllaâ€ serves as a â€œmoderatorâ€ responsible for evaluating whether the conversations between Nancy and Nova adhere to the following rules for termination:</p>

<ul>
  <li>The conversation can conclude if Nova successfully fulfills all of Crystalâ€™s requests, and Crystal expresses satisfaction with Novaâ€™s assistance.</li>
  <li>The conversation can also end if Nova is unable to fulfill Crystalâ€™s requests.</li>
</ul>

<p>If Ella believes the conversation should conclude, she will state â€œÂ«Â«Â«END_OF_CONVERSATIONÂ»Â»Â»â€; otherwise, she will say â€œconverstation can continueâ€.</p>

<p>Please refer to the screenshot below for an illustration of the simulation results.<img src="/assets/picture/2023_09_07_conversation_sim/2023_09_07_conversation_sim.png" alt="example_1" /></p>

<p>Once simulation ends, one can save the converstation history as a json file. See a sample json file below:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[
    {
        "agent_name": "Crystal",
        "content": "å—¨ï¼ŒNovaï¼æˆ‘éœ€è¦ä¸€äº›å…³äºä¸ºæˆ‘å¥³å„¿å‡†å¤‡å¥åº·åˆé¤çš„å»ºè®®ã€‚å¥¹ç°åœ¨7å²ï¼Œæˆ‘å¸Œæœ›èƒ½ç»™å¥¹æä¾›å¿«é€Ÿä½†è¥å…»ä¸°å¯Œçš„é€‰é¡¹ã€‚ä½ æœ‰ä»€ä¹ˆå»ºè®®å—ï¼Ÿ",
        "turn": 0,
        "timestamp": "1694143996023456000",
        "visible_to": "all",
        "msg_type": "text"
    },
    {
        "agent_name": "Nova",
        "content": "å—¨ï¼ŒCrystalï¼ä¸ºäº†ç»™ä½ çš„å¥³å„¿æä¾›å¿«é€Ÿåˆè¥å…»ä¸°å¯Œçš„åˆé¤ï¼Œæˆ‘æœ‰å‡ ä¸ªå»ºè®®ã€‚é¦–å…ˆï¼Œä½ å¯ä»¥è€ƒè™‘ç»™å¥¹æä¾›ä¸€ä»½å‡è¡¡çš„ä¸‰æ˜æ²»ï¼Œé‡Œé¢åŒ…å«è”¬èœã€è›‹ç™½è´¨å’Œå…¨éº¦é¢åŒ…ã€‚ä½ è¿˜å¯ä»¥æ·»åŠ ä¸€äº›æ°´æœæˆ–é…¸å¥¶ä½œä¸ºä¼´ä¾£ã€‚å¦å¤–ï¼Œä½ å¯ä»¥å°è¯•åˆ¶ä½œä¸€äº›å¥åº·çš„å°åƒï¼Œä¾‹å¦‚è”¬èœåˆ‡ç‰‡æ­é…èŠ±ç”Ÿé…±æˆ–è€…é…¸å¥¶è˜¸é…±ã€‚å¦‚æœä½ æƒ³è®©å¥¹å¤šæ‘„å…¥è›‹ç™½è´¨ï¼Œå¯ä»¥è€ƒè™‘ç»™å¥¹å‡†å¤‡ä¸€ä»½é¸¡è‚‰æˆ–é±¼è‚‰é…ç±³é¥­å’Œè”¬èœã€‚æœ€åï¼Œè®°å¾—ç»™å¥¹æä¾›ä¸€æ¯ç‰›å¥¶æˆ–æ°´æ¥ä¿æŒæ°´åˆ†è¡¥å……ã€‚å¸Œæœ›è¿™äº›å»ºè®®å¯¹ä½ æœ‰å¸®åŠ©ï¼å¦‚æœä½ è¿˜æœ‰å…¶ä»–é—®é¢˜ï¼Œè¯·éšæ—¶å‘Šè¯‰æˆ‘ã€‚",
        "turn": 0,
        "timestamp": "1694144002581689000",
        "visible_to": "all",
        "msg_type": "text"
    },
    {
        "agent_name": "Ella",
        "content": "continue",
        "turn": 1,
        "timestamp": "1694144025822335000",
        "visible_to": "all",
        "msg_type": "text"
    },
    {
        "agent_name": "Crystal",
        "content": "è°¢è°¢ä½ çš„å»ºè®®ï¼ŒNovaï¼è¿™äº›éƒ½æ˜¯å¾ˆä¸é”™çš„é€‰æ‹©ã€‚æˆ‘æƒ³é—®ä¸€ä¸‹ï¼Œæœ‰æ²¡æœ‰ä¸€äº›ç®€å•çš„å¥åº·åˆé¤é£Ÿè°±ï¼Œå¯ä»¥è®©æˆ‘åœ¨å®¶åšå¹¶å¸¦ç»™æˆ‘çš„å¥³å„¿ï¼Ÿæˆ‘æƒ³ç¡®ä¿å¥¹åƒçš„ä¸œè¥¿æ–°é²œåˆå¥åº·ã€‚ä½ æœ‰ä»€ä¹ˆå»ºè®®å—ï¼Ÿ",
        "turn": 2,
        "timestamp": "1694144032890695000",
        "visible_to": "all",
        "msg_type": "text"
    },
    {
        "agent_name": "Nova",
        "content": "å½“ç„¶ï¼Œæˆ‘å¯ä»¥ç»™ä½ ä¸€äº›ç®€å•çš„å¥åº·åˆé¤é£Ÿè°±ã€‚ä»¥ä¸‹æ˜¯å‡ ä¸ªå»ºè®®ï¼š\n\n1. è”¬èœå¯¿å¸å·ï¼šç”¨ç´«èœã€ç³™ç±³ã€é³—é±¼ã€é»„ç“œå’Œèƒ¡èåœå·èµ·æ¥åšæˆå¯¿å¸å·ã€‚åˆ‡æˆå°æ®µï¼Œè¿™æ˜¯ä¸€ä¸ªæ–¹ä¾¿æºå¸¦çš„åˆé¤é€‰æ‹©ã€‚\n\n2. é¸¡è‚‰æ²™æ‹‰ä¸‰æ˜æ²»ï¼šå°†çƒ¤é¸¡èƒ¸è‚‰åˆ‡æˆè–„ç‰‡ï¼ŒåŠ å…¥è”¬èœï¼ˆä¾‹å¦‚ç”Ÿèœã€ç•ªèŒ„å’Œé»„ç“œï¼‰ï¼Œç”¨å…¨éº¦é¢åŒ…åšæˆä¸‰æ˜æ²»ã€‚å¯ä»¥é…ä»¥ä½è„‚é…¸å¥¶æˆ–è€…è”¬èœç‰‡ã€‚\n\n3. å½©è™¹è‰²è”¬èœé¥¼ï¼šå°†èƒ¡èåœã€è¥¿å…°èŠ±ã€æ´‹è‘±å’Œç‰ç±³æ··åˆåœ¨ä¸€èµ·ï¼ŒåŠ å…¥è›‹æ¶²å’Œé¢ç²‰ï¼Œç…æˆå°é¥¼ã€‚è¿™æ˜¯ä¸€ä¸ªå¿«é€Ÿåˆå¥åº·çš„åˆé¤é€‰æ‹©ã€‚\n\n4. æ°´æœæ²™æ‹‰ï¼šåˆ‡ç¢ä¸€äº›æ–°é²œæ°´æœï¼Œä¾‹å¦‚è‹¹æœã€è‰è“ã€è“è“å’Œé¦™è•‰ã€‚å¯ä»¥åŠ å…¥ä¸€äº›åšæœæˆ–è€…é…¸å¥¶ä½œä¸ºè°ƒå‘³å“ã€‚\n\nè¿™äº›éƒ½æ˜¯ç®€å•è€Œå¥åº·çš„åˆé¤é€‰æ‹©ï¼Œä½ å¯ä»¥æ ¹æ®ä½ å¥³å„¿çš„å£å‘³å’Œå–œå¥½è¿›è¡Œè°ƒæ•´ã€‚å¸Œæœ›è¿™äº›å»ºè®®èƒ½ç»™ä½ ä¸€äº›çµæ„Ÿï¼å¦‚æœä½ éœ€è¦æ›´å¤šé£Ÿè°±æˆ–æœ‰å…¶ä»–é—®é¢˜ï¼Œè¯·éšæ—¶å‘Šè¯‰æˆ‘ã€‚",
        "turn": 2,
        "timestamp": "1694144054290083000",
        "visible_to": "all",
        "msg_type": "text"
    },
    {
        "agent_name": "Ella",
        "content": "continue",
        "turn": 3,
        "timestamp": "1694144059307035000",
        "visible_to": "all",
        "msg_type": "text"
    },
    {
        "agent_name": "Crystal",
        "content": "è°¢è°¢ä½ çš„å»ºè®®ï¼ŒNovaï¼è¿™äº›é£Ÿè°±å¬èµ·æ¥å¾ˆæ£’ï¼Œæˆ‘ä¼šå°è¯•åˆ¶ä½œå®ƒä»¬ã€‚æˆ‘è¿˜æƒ³é—®ä¸€ä¸‹ï¼Œæœ‰æ²¡æœ‰ä¸€äº›å¥åº·çš„é›¶é£Ÿé€‰é¡¹ï¼Œæˆ‘å¯ä»¥ç»™æˆ‘çš„å¥³å„¿æ”¾è¿›å¥¹çš„åˆé¤ç›’é‡Œï¼Ÿæˆ‘å¸Œæœ›èƒ½ç»™å¥¹ä¸€äº›æœ‰è¥å…»ä½†åˆå®¹æ˜“æºå¸¦çš„é›¶é£Ÿã€‚ä½ æœ‰ä»€ä¹ˆå»ºè®®å—ï¼Ÿ",
        "turn": 4,
        "timestamp": "1694144062870150000",
        "visible_to": "all",
        "msg_type": "text"
    },
    {
        "agent_name": "Nova",
        "content": "å½“ç„¶ï¼Œæˆ‘å¯ä»¥ç»™ä½ ä¸€äº›å¥åº·çš„é›¶é£Ÿé€‰é¡¹ï¼Œé€‚åˆä½ å¥³å„¿çš„åˆé¤ç›’ã€‚ä»¥ä¸‹æ˜¯å‡ ä¸ªå»ºè®®ï¼š\n\n1. è”¬èœæ£’å’Œé…¸å¥¶è˜¸é…±ï¼šå°†èƒ¡èåœã€é»„ç“œå’ŒèŠ¹èœåˆ‡æˆæ¡çŠ¶ï¼Œæ­é…ä½è„‚é…¸å¥¶æˆ–è€…èŠ±ç”Ÿé…±ä½œä¸ºè˜¸é…±ã€‚è¿™æ˜¯ä¸€ä¸ªè½»æ¾æºå¸¦çš„è¥å…»é›¶é£Ÿã€‚\n\n2. æ°´æœä¸²ï¼šå°†å„ç§æ°´æœï¼ˆä¾‹å¦‚è‘¡è„ã€è“è“ã€è‰è“ç­‰ï¼‰ç©¿åœ¨ç«¹ç­¾ä¸Šï¼Œåˆ¶ä½œæˆæ°´æœä¸²ã€‚è¿™æ˜¯ä¸€ä¸ªæ–¹ä¾¿åƒåˆæœ‰è¶£çš„é›¶é£Ÿé€‰æ‹©ã€‚\n\n3. åšæœå’Œå¹²æœæ··åˆï¼šå°†åšæœï¼ˆä¾‹å¦‚æä»ã€æ ¸æ¡ƒï¼‰å’Œå¹²æœï¼ˆä¾‹å¦‚è‘¡è„å¹²ã€æ— èŠ±æœï¼‰æ··åˆåœ¨ä¸€èµ·ï¼Œåˆ¶ä½œæˆå°è¢‹å­ã€‚è¿™æ˜¯ä¸€ä¸ªé«˜èƒ½é‡å’Œè¥å…»ä¸°å¯Œçš„é›¶é£Ÿã€‚\n\n4. é…¸å¥¶æ¯ï¼šå°†ä½è„‚é…¸å¥¶åŠ å…¥ä¸€äº›æ–°é²œæ°´æœå’Œè°·ç‰©ï¼ˆä¾‹å¦‚ç‡•éº¦ç‰‡ã€åšæœï¼‰ï¼Œæ”¾å…¥ä¸€ä¸ªå¯†å°æ¯ä¸­ã€‚è¿™æ˜¯ä¸€ä¸ªè½»æ¾æºå¸¦çš„å¥åº·é›¶é£Ÿã€‚\n\nè®°ä½ï¼Œåœ¨é€‰æ‹©é›¶é£Ÿæ—¶ï¼Œå°½é‡é€‰æ‹©ä½ç³–å’Œä½ç›çš„é€‰é¡¹ï¼Œå¹¶æ³¨æ„æ§åˆ¶é£Ÿç”¨é‡ã€‚å¸Œæœ›è¿™äº›å»ºè®®èƒ½ç»™ä½ ä¸€äº›çµæ„Ÿï¼å¦‚æœä½ è¿˜æœ‰å…¶ä»–é—®é¢˜ï¼Œè¯·éšæ—¶å‘Šè¯‰æˆ‘ã€‚",
        "turn": 4,
        "timestamp": "1694144078840960000",
        "visible_to": "all",
        "msg_type": "text"
    },
    {
        "agent_name": "Ella",
        "content": "&lt;&lt;&lt;&lt;&lt;&lt;END_OF_CONVERSATION&gt;&gt;&gt;&gt;&gt;&gt;",
        "turn": 5,
        "timestamp": "1694144091154109000",
        "visible_to": "all",
        "msg_type": "text"
    }
]
</code></pre></div></div>

<p>Feel free to create a new example and try to experiment with it further! Have fun ã‹¡ ~</p>]]></content><author><name>[&quot;Han Yu&quot;]</name></author><category term="chatbot" /><summary type="html"><![CDATA[TL;DR This project is a demonstration of a simple conversational simulation environment, powered by LLM agents. It takes inspiration from ChatArena. The main objective here is to enable conversation simulations through LLM agents (e.g., the converstation can be task-oriented or just a chitchat). To accomplish this, the frameworks and concepts originally developed in ChatArena has been leveraged and tailored specifically for this conversational simulation experiment. More information can be found on .]]></summary></entry><entry><title type="html">Build On-Device QnA with LangChain and Llama2</title><link href="http://localhost:4000/chatbot/2023/08/14/food-qna-on-device-llm.html" rel="alternate" type="text/html" title="Build On-Device QnA with LangChain and Llama2" /><published>2023-08-14T00:00:00-07:00</published><updated>2023-08-14T00:00:00-07:00</updated><id>http://localhost:4000/chatbot/2023/08/14/food-qna-on-device-llm</id><content type="html" xml:base="http://localhost:4000/chatbot/2023/08/14/food-qna-on-device-llm.html"><![CDATA[<h3 id="tldr">TL;DR</h3>
<p>This post involves creating a Question and Answering system using the LLM model hosted on Apple M1 Pro. The key building blocks include the LLM model (<a href="https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGML">llama-2-7b-chat.ggmlv3.q8_0.bin</a>), an embedding model (<a href="https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2">sentence-transformers/all-MiniLM-L6-v2</a>), and an on-device vector database (<a href="https://github.com/facebookresearch/faiss">FAISS</a>). The app is built using the â€œLangChainâ€ framework. All components are open source, eliminating the need for OpenAI services. The systemâ€™s performance is similar to OpenAI but with 10x longer latency (around 60s/query vs 5s/query) due to on-device model inference.</p>

<h3 id="introduction">Introduction</h3>
<p>Third-party commercial large language model (LLM) providers, such as OpenAIâ€™s GPT-4, Google Bard, and Amazon AlexaTM, have greatly democratized access to LLM capabilities through seamless API integration and scalable model inference hosting in the cloud. These advanced LLMs possess the remarkable ability to <strong>comprehend</strong>, <strong>learn from</strong>, and <strong>produce text</strong> that is nearly indistinguishable from human-generated content. Beyond their text generation prowess, these LLMs excel in <strong>interactive conversations</strong>, <strong>question answering</strong>, <strong>dialogue</strong> and <strong>document summarization</strong>, as well as <strong>offering insightful recommendations</strong>. Their versatility finds applications across diverse tasks and industries including creative copywriting for marketing, precise document summarization for legal purposes, data-driven market research in the financial sector, realistic simulation of clinical trials within healthcare, and even code generation for software development.</p>

<p>However, certain scenarios, driven by an increasing emphasis on safeguarding data privacy and adhering to stringent regulatory compliance standards, highlight the necessity of deploying LLMs on private hardware devices instead of on any of those third-party owned servers. In such instances, <strong>maintaining sensitive information within the confines of the userâ€™s hardware</strong> not only mitigates the risks associated with data breaches and unauthorized access but also aligns with the evolving landscape of privacy-conscious technical practices. This approach <strong>fosters a sense of trust among users who are becoming more attuned to the importance of maintaining their personal information within their own environments</strong>.</p>

<p>In this post, our focus lies in exploring the execution of quantized variants of open-source Llama2 models on local devices to achieve Retrieval Augmented Generation (RAG). For RAG powered by server-side LLMs, you can find more info in <a href="https://bearbearyu1223.github.io/chatbot/2023/07/31/food-qna-on-server-llm.html">my previous post</a>.</p>

<h3 id="llama2-and-its-variants">Llama2 and Its variants</h3>
<p><a href="https://ai.meta.com/resources/models-and-libraries/llama/">Llama 2</a>, launched by Meta in July 2023, has been pretrained on publicly available online data sources, encompassing a staggering <strong>2 trillion tokens with a context length of 4096</strong>. The subsequent supervised fine-tuned iteration of this model, known as Llama-2-chat, underwent meticulous refinement through the integration of over <em>1 million human annotations</em> to cater specifically to <strong>chat-oriented use cases</strong>. Meta has extended the accessibility of Llama 2 to a wide spectrum of users, ranging from individual developers and content creators to researchers and businesses. This strategic open-source initiative is aimed at fostering an ecosystem conducive to <a href="https://ai.meta.com/static-resource/responsible-use-guide/">Responsible AI experimentation</a>, innovation, and the scalable implementation of a diverse array of ideas, thus further <strong>democratizing Generative AI</strong>.</p>

<p>Llama 2 is offered in an array of parameter sizes â€” 7B, 13B, and 70B â€” alongside both pretrained and fine-tuned variations to cater to a wide range of application needs.</p>

<h3 id="framework-and-libraries-used-langchain-ggml-c-transformers">Framework and Libraries Used: LangChain, GGML, C Transformers</h3>
<p><a href="https://python.langchain.com/docs/get_started/introduction.html">LangChain</a> is an open source framework for developing applications powered by LLMs. It goes beyond standard API calls by being <em>data-aware</em>, enabling connections with various data sources for richer, personalized experiences. It is also <em>agentic</em>, meaning it can empower a language model to interact dynamically with its environment. LangChain streamlines the development of diverse applications, such as chatbots, Generative Question and Answering (GQA), and summarization. By â€œchainingâ€ components from multiple modules, it allows for the creation of unique applications built around an LLM with <strong>easy-to-code</strong> and <strong>fast-to-production</strong> developer experience.</p>

<p><a href="https://github.com/ggerganov/ggml">GGML</a> is a C library for machine learning (ML). GGML makes use of a technique called <strong>â€œquantizationâ€</strong> (e.g., convert LLMâ€™s weights from high-precison floating numbers to low-precision floating numbers) that allows for large language models to run on consumer hardware. GGML supports a number of different quantization strategies (e.g. 4-bit, 5-bit, and 8-bit quantization), each of which offers different <em>trade-offs between efficiency and performance</em>. More information about these trade-offs (such as model disk size and inference speed) can be found in <a href="https://github.com/ggerganov/llama.cpp">the documentation for llama.cpp</a>.</p>

<p><a href="https://github.com/marella/ctransformers">C Transformers</a> is a wrapper that provides the Python bindings for the Transformer models implemented in C/C++ using GGML. 
C Transformers supports running Llama2 model inference via GPU, for both NVIDIA GPU (via CUDA, a programming language for NVIDIA GPUs) and Appleâ€™s own integreated GPU and Neural Engine (via Metal, a programming language for Apple integrated GPUs).</p>

<p>Note: To use C transformers with Metal Support for model inference running on Apple M1/M2 chip, need run the following cmd under your project root</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>poetry config --local installer.no-binary ctransformers

poetry add ctransformers 
</code></pre></div></div>

<h3 id="retrieval-augmented-generation">Retrieval Augmented Generation</h3>
<p>Retrieval Augmented Generation (RAG) represents a technique wherein data is retrieved from external sources to enhance and expand the prompts used in model generation. This method is not only a cost-effective alternative but also proves to be an efficient approach in comparison to the traditional methods of pre-training or fine-tuning foundation models.
See the previous post at <a href="https://bearbearyu1223.github.io/chatbot/2023/07/31/food-qna-on-server-llm.html">Food QnA Chatbot : Help Answer Food Related Questions from Your Own Cookbook</a> as a brief into to RAG.</p>

<h3 id="an-example-project">An Example Project</h3>
<p>The source code for the example project can be found on <a href="https://github.com/bearbearyu1223/langchain_playground/tree/main/food_qna_on_device"><img src="https://img.shields.io/badge/github-%23121011.svg?style=for-the-badge&amp;logo=github&amp;logoColor=white" alt="GitHub" /></a>. The project directory should look like below:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>food_qna_on_device
â”œâ”€â”€ README.md
â”œâ”€â”€ build_knowledge_base.py
â”œâ”€â”€ config.py
â”œâ”€â”€ cook_book_data
â”‚   â”œâ”€â”€ GCE-Dinner-in-30-EXPANDED-BLAD.pdf
â”‚   â”œâ”€â”€ Quick-Easy-Weeknight-Meals-1.pdf
â”‚   â””â”€â”€ dinners_cookbook_508-compliant.pdf
â”œâ”€â”€ main.py
â”œâ”€â”€ models
â”‚   â”œâ”€â”€ llama-2-13b-chat.ggmlv3.q8_0.bin
â”‚   â””â”€â”€ llama-2-7b-chat.ggmlv3.q8_0.bin
â”œâ”€â”€ poetry.lock
â”œâ”€â”€ poetry.toml
â”œâ”€â”€ pyproject.toml
â””â”€â”€ vector_db
    â”œâ”€â”€ index.faiss
    â””â”€â”€ index.pkl
</code></pre></div></div>

<p>Instruction to run the example project:</p>
<ul>
  <li>Step 1: Launch the terminal from the project directory, install and resolve the dependencies as defined in <code class="language-plaintext highlighter-rouge">pyproject.toml</code> file via
    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>poetry install
</code></pre></div>    </div>
  </li>
  <li>Step 2: Download the quantized 7b model <code class="language-plaintext highlighter-rouge">llama-2-7b-chat.ggmlv3.q8_0.bin</code> from https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGML and save the model under the directory <code class="language-plaintext highlighter-rouge">models\</code></li>
  <li>Step 3: To start parsing user queries into the application, run the following command from the project directory (note: the model inference can take ~1 mins per input query)
    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>poetry run python main.py -c local 
</code></pre></div>    </div>
    <p>Optionally, to run the same query with OpenAI (note: the model inference will take a few seconds per input query, you will also need export OPENAI_API_KEY as an enviroment variable on your local dev machine)</p>
    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>poetry run python main.py -c server
</code></pre></div>    </div>
  </li>
  <li>Step 4: Enter a query related to food preparation and cooking into the console and start playing with it. 
See an example below:
Screenshot of the original content for making â€œPumpkin Biscuitsâ€. 
 <img src="/assets/picture/2023_08_14_food_qna_on_device_llm/original.png" alt="Original Content" />
Retrieval Augmented Generation by running Llama2 model inference on local device
 <img src="/assets/picture/2023_08_14_food_qna_on_device_llm/response.png" alt="Response" /></li>
</ul>]]></content><author><name>[&quot;Han Yu&quot;]</name></author><category term="chatbot" /><summary type="html"><![CDATA[TL;DR This post involves creating a Question and Answering system using the LLM model hosted on Apple M1 Pro. The key building blocks include the LLM model (llama-2-7b-chat.ggmlv3.q8_0.bin), an embedding model (sentence-transformers/all-MiniLM-L6-v2), and an on-device vector database (FAISS). The app is built using the â€œLangChainâ€ framework. All components are open source, eliminating the need for OpenAI services. The systemâ€™s performance is similar to OpenAI but with 10x longer latency (around 60s/query vs 5s/query) due to on-device model inference.]]></summary></entry><entry><title type="html">Food QnA Chatbot : Help Answer Food Related Questions from Your Own Cookbook</title><link href="http://localhost:4000/chatbot/2023/07/31/food-qna-on-server-llm.html" rel="alternate" type="text/html" title="Food QnA Chatbot : Help Answer Food Related Questions from Your Own Cookbook" /><published>2023-07-31T00:00:00-07:00</published><updated>2023-07-31T00:00:00-07:00</updated><id>http://localhost:4000/chatbot/2023/07/31/food-qna-on-server-llm</id><content type="html" xml:base="http://localhost:4000/chatbot/2023/07/31/food-qna-on-server-llm.html"><![CDATA[<h2 id="introduction">Introduction</h2>
<p>Retrieval Augmented Generation (RAG) is a powerful technique that allows you to enhance your prompts by retrieving data from external sources and incorporating it into the context. The external data used for augmentation can be gathered from diverse sources such as document repositories, databases, or web search results.</p>

<p>To begin with RAG, you need to convert your documents and user queries into a compatible format to perform relevancy search. This involves converting both the document collection, or knowledge base, and user-submitted queries into numerical representations using embedding. Embedding is a process that assigns numerical values to text, placing them in a vector space.</p>

<p>RAG model architectures then compare the embeddings of user queries with those â€œvector indexâ€ of the knowledge base. By doing so, they identify similar documents in the knowledge base that are relevant to the userâ€™s prompt. These relevant contents from similar documents are appended to the original user prompt.</p>

<p>Finally, the augmented prompt, which now includes the relevant retrieved content, is passed on to the foundation model to generate the final responses. This integration of retrieval and generation significantly improves the quality and relevance of the modelâ€™s outputs. Below is an illustrative diagram that demonstrates the overall RAG (Retrieval Augmented Generation) process. <img src="/assets/picture/2023_07_31_food_qna_on_server_llm/Retrieval_Augmented_Generation.png" alt="Retrieval Augmented Generation" />.</p>

<h2 id="an-example-project-for-rag">An Example Project for RAG</h2>
<p>I have created an example project to provide a practical demonstration of how RAG works. For more detailed information and insights into the project, you can find comprehensive documentation and additional resources on <a href="https://github.com/bearbearyu1223/food_qna_powered_by_llm"><img src="https://img.shields.io/badge/github-%23121011.svg?style=for-the-badge&amp;logo=github&amp;logoColor=white" alt="GitHub" /></a>. This exmaple project will walk you through the RAG process, the data preparation steps, the relevancy search implementation, and how the augmented prompts lead to more contextually, more relevant, and more conversational responses from the foundation model. Please feel free to clone this project on GitHub, and follow the steps below to explore or develope it further.</p>

<h2 id="set-up-local-virtual-environment">Set Up Local Virtual Environment</h2>
<ul>
  <li>Step 1: Install Miniconda on MacOS, see instruction <a href="https://docs.conda.io/en/latest/miniconda.html">here</a>.</li>
  <li>Step 2: Create a default conda env with Python 3.9:
    <div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>conda create <span class="nt">--name</span> food_qna_app <span class="nv">python</span><span class="o">=</span>3.9 <span class="nt">-y</span>
</code></pre></div>    </div>
  </li>
  <li>Step 3: Activate the conda env created above: 
<code class="language-plaintext highlighter-rouge">conda activate food_qna_app</code></li>
  <li>Step 4: Install first set of required libraries in the conda env:
    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip3 install -r requirements.txt
</code></pre></div>    </div>
  </li>
  <li>Step 5: Deactivate the conda env when you are done (note: you need activate this virtual env to work on the app)
    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>conda deactivate 
</code></pre></div>    </div>
    <h2 id="set-up-open-ai-secrete-for-local-dev">Set Up Open AI Secrete for Local Dev</h2>
    <p><code class="language-plaintext highlighter-rouge">OPENAI_API_KEY</code> will be needed when calling the OpenAI API endpoint for generating embeddings for the documents, so do recommend exporting <code class="language-plaintext highlighter-rouge">OPENAI_API_KEY</code> as an enviroment variable on your local dev machine; also, we will need create a <code class="language-plaintext highlighter-rouge">secrets.toml</code> file and add the <code class="language-plaintext highlighter-rouge">OPENAI_API_KEY</code> there, so the streamlit app can pick up the API key when send requests to OpenAI endpoint during runtime.</p>
  </li>
  <li>Step 1: Export <code class="language-plaintext highlighter-rouge">OPENAI_API_KEY</code> as an enviroment variable on your local dev machine</li>
  <li>Step 2: Create <code class="language-plaintext highlighter-rouge">.streamlit</code> directory under the root repo
    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>cd food_qna_chatbot_demo 
mkdir .streamlit
</code></pre></div>    </div>
  </li>
  <li>Step 3: Create <code class="language-plaintext highlighter-rouge">secrets.toml</code> file under <code class="language-plaintext highlighter-rouge">.streamlit</code> directory and add your <code class="language-plaintext highlighter-rouge">OPENAI_API_KEY</code> there
    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>OPENAI_API_KEY=&lt;YOUR_OPENAI_API_KEY_HERE&gt;
</code></pre></div>    </div>
  </li>
  <li>Step 4: To reduce the risk of accidentally committing your secrets to your repo, add <code class="language-plaintext highlighter-rouge">.streamlit</code> to your <code class="language-plaintext highlighter-rouge">.gitignore</code> file.
    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>echo ".streamlit/" &gt;&gt; .gitignore
</code></pre></div>    </div>
    <h2 id="set-up-the-redis-database-on-your-local-dev-machine">Set up the Redis Database on your local dev machine</h2>
  </li>
  <li>Step 1: <a href="https://docs.docker.com/desktop/install/mac-install/">Install Docker Desktop on Mac</a>, and start the docker desktop.</li>
  <li>Step 2: Weâ€™re going to use Redis as our database for both document contents and the vector embeddings. You will need the full Redis Stack to enable use of Redisearch, which is the module that allows semantic search - more detail is in the docs for <a href="https://redis.io/docs/stack/get-started/install/docker/">Redis Stack</a>. Run the following command in your terminal to start the docker container:
    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>docker run -d --name redis-stack -p 127.0.0.1:6379:6379 -p 8001:8001 redis/redis-stack:latest
</code></pre></div>    </div>
  </li>
  <li>Step 3: Initiate a Redis connection and create a Hierarchical Navigable Small World (HNSW) index for semantic search using a recipe book which can be found under the directory <code class="language-plaintext highlighter-rouge">cook_book_data</code>
    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python build_knowledge_base.py
</code></pre></div>    </div>
    <p>If the recipe book is indexed succefully into the DB, you should expect the following info printed out in the console:</p>
    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>===
Number of documents indexed in the DB: 144
</code></pre></div>    </div>
    <h2 id="run-the-app">Run the APP</h2>
    <p>In your terminal, run the App by</p>
    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>streamlit run food_qna_app.py
</code></pre></div>    </div>
    <p>You can start asking questions related to food preparation and cooking, and also some follow up questions. See screenshot below.</p>
  </li>
</ul>

<table>
  <thead>
    <tr>
      <th style="text-align: left">Original Content</th>
      <th style="text-align: left">Chat History - first turn</th>
      <th style="text-align: left">Chat History - follow up</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left"><img src="/assets/picture/2023_07_31_food_qna_on_server_llm/original_content.png" alt="Original Content" /></td>
      <td style="text-align: left"><img src="/assets/picture/2023_07_31_food_qna_on_server_llm/chat_history_1.png" alt="Chat History" /></td>
      <td style="text-align: left"><img src="/assets/picture/2023_07_31_food_qna_on_server_llm/chat_history_2.png" alt="Chat History" /></td>
    </tr>
  </tbody>
</table>]]></content><author><name>[&quot;Han Yu&quot;]</name></author><category term="chatbot" /><summary type="html"><![CDATA[Introduction Retrieval Augmented Generation (RAG) is a powerful technique that allows you to enhance your prompts by retrieving data from external sources and incorporating it into the context. The external data used for augmentation can be gathered from diverse sources such as document repositories, databases, or web search results.]]></summary></entry></feed>