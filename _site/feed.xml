<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2026-02-08T16:03:28-08:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">ğŸ’ Hanâ€™s Generative AI Quest</title><subtitle>I chronicle my captivating journey through Generative AI, sharing insights,  breakthroughs, and learnings from my enthralling side projects in the field. 
</subtitle><entry><title type="html">Study Notes: Stanford CS336 Language Modeling from Scratch [15]</title><link href="http://localhost:4000/cs336/2026/02/08/grpo-math-reasoning-lambda-cloud.html" rel="alternate" type="text/html" title="Study Notes: Stanford CS336 Language Modeling from Scratch [15]" /><published>2026-02-08T00:00:00-08:00</published><updated>2026-02-08T00:00:00-08:00</updated><id>http://localhost:4000/cs336/2026/02/08/grpo-math-reasoning-lambda-cloud</id><content type="html" xml:base="http://localhost:4000/cs336/2026/02/08/grpo-math-reasoning-lambda-cloud.html"><![CDATA[<h2 id="training-math-reasoning-models-with-grpo-on-lambda-cloud-with-2xh100s">Training Math Reasoning Models with GRPO on Lambda Cloud with 2xH100s</h2>

<p><a href="https://lambda.ai/service/gpu-cloud"><img src="/assets/picture/2026-02-08-grpo-math-reasoning-lambda-cloud/lambda-labs-logo.svg" alt="Lambda" width="150" style="vertical-align: middle; margin-bottom: 10px;" /></a></p>

<p>Weâ€™ve all read about GRPO (Group Relative Policy Optimization) and have a rough grasp of the theory. But a practical question often remains: how do you actually train a math reasoning model with GRPO?</p>

<p>This post aims to bridge the gap between understanding GRPO on paper and running it on real cloud hardware.</p>

<p>Using <a href="https://huggingface.co/Qwen/Qwen2.5-Math-1.5B">Qwen2.5-Math-1.5B</a> as a concrete example, Iâ€™ll walk through how to improve its math accuracy from ~6% to ~25%â€”a 4Ã— improvementâ€”by training with GRPO on Lambda Cloud using 2Ã— H100 GPUs. Along the way, Iâ€™ll share:</p>

<ul>
  <li>
    <p>How GRPO is implemented in practice</p>
  </li>
  <li>
    <p>How to structure a 2-GPU training setup (policy model + vLLM inference)</p>
  </li>
  <li>
    <p>How to read and reason about GRPO training curves and what signals actually matter</p>
  </li>
</ul>

<p>The goal is not just to explain what GRPO is, but to show how it behaves end-to-end in a real training runâ€”from reward computation, to GPU allocation, to interpreting the final plots.</p>

<p><em>This guide builds on my previous <a href="/cs336/2026/01/25/cs336-reinforcement-learning-for-language-model.html">study notes on reinforcement learning for language models</a>. If terms like â€œpolicy gradientâ€ or â€œadvantageâ€ are unfamiliar, start there first.</em></p>

<h3 id="table-of-contents">Table of Contents</h3>
<ul>
  <li><a href="#training-math-reasoning-models-with-grpo-on-lambda-cloud-with-2xh100s">Training Math Reasoning Models with GRPO on Lambda Cloud with 2xH100s</a>
    <ul>
      <li><a href="#table-of-contents">Table of Contents</a></li>
      <li><a href="#notation">Notation</a></li>
      <li><a href="#why-grpo-for-math-reasoning">Why GRPO for Math Reasoning?</a></li>
      <li><a href="#grpo-intuition-groups-as-your-baseline">GRPO Intuition: Groups as Your Baseline</a>
        <ul>
          <li><a href="#the-group-concept">The â€œGroupâ€ Concept</a></li>
        </ul>
      </li>
      <li><a href="#grpo-vs-pporlhf">GRPO vs PPO/RLHF</a></li>
      <li><a href="#the-algorithm-step-by-step">The Algorithm Step-by-Step</a>
        <ul>
          <li><a href="#algorithm-3-from-cs336-assignment-5-grpo-training-loop">Algorithm 3 from CS336 Assignment 5: GRPO Training Loop</a></li>
          <li><a href="#the-group-normalization-formula">The Group Normalization Formula</a></li>
        </ul>
      </li>
      <li><a href="#key-implementation-details">Key Implementation Details</a>
        <ul>
          <li><a href="#group-normalized-rewards">Group-Normalized Rewards</a></li>
          <li><a href="#three-loss-types">Three Loss Types</a></li>
          <li><a href="#token-level-loss-with-masking">Token-Level Loss with Masking</a></li>
          <li><a href="#training-loop-and-2-gpu-architecture">Training Loop and 2-GPU Architecture</a></li>
        </ul>
      </li>
      <li><a href="#grpo-experiment-on-lambda-cloud-setup-with-2h100-80gb-sxm5">GRPO Experiment on Lambda Cloud Setup with 2Ã—H100 (80GB SXM5)</a>
        <ul>
          <li><a href="#how-two-gpus-work-together-in-a-grpo-training-setup">How Two GPUs Work Together in a GRPO Training Setup?</a></li>
          <li><a href="#step-by-step-setup">Step-by-Step Setup</a></li>
          <li><a href="#troubleshooting">Troubleshooting</a></li>
        </ul>
      </li>
      <li><a href="#interpreting-training-plots">Interpreting Training Plots</a>
        <ul>
          <li><a href="#panel-1-average-reward-per-step">Panel 1: Average Reward per Step</a></li>
          <li><a href="#panel-2-answer-reward-train-vs-val">Panel 2: Answer Reward (Train vs Val)</a></li>
          <li><a href="#panel-3-policy-gradient-loss">Panel 3: Policy Gradient Loss</a></li>
          <li><a href="#panel-4-reward-range-minmaxmean">Panel 4: Reward Range (Min/Max/Mean)</a></li>
        </ul>
      </li>
      <li><a href="#evaluation-results-base-model-vs-grpo-trained">Evaluation Results: Base Model vs GRPO-Trained</a>
        <ul>
          <li><a href="#example-improvements">Example improvements</a></li>
        </ul>
      </li>
      <li><a href="#summary-and-key-takeaways">Summary and Key Takeaways</a></li>
    </ul>
  </li>
</ul>

<h3 id="notation">Notation</h3>

<p>Before diving in, hereâ€™s a quick reference for the mathematical symbols used throughout this guide:</p>

<table>
  <thead>
    <tr>
      <th>Symbol</th>
      <th>Meaning</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>$\pi$</td>
      <td><strong>Policy</strong> â€” the language model being trained</td>
    </tr>
    <tr>
      <td>$\theta$</td>
      <td><strong>Parameters</strong> â€” the model weights</td>
    </tr>
    <tr>
      <td>$\pi_\theta(a \mid s)$</td>
      <td>Probability of generating token $a$ given context $s$, under model with weights $\theta$</td>
    </tr>
    <tr>
      <td>$G$</td>
      <td><strong>Group size</strong> â€” number of responses sampled per question</td>
    </tr>
    <tr>
      <td>$R$</td>
      <td><strong>Reward function</strong> â€” scores each response (e.g., 1 if correct, 0 if wrong)</td>
    </tr>
    <tr>
      <td>$r^{(i)}$</td>
      <td>Reward for the $i$-th response in a group</td>
    </tr>
    <tr>
      <td>$V(s)$</td>
      <td><strong>Value function</strong> â€” estimates expected future reward from state $s$ (used in PPO, not GRPO)</td>
    </tr>
    <tr>
      <td>$A$</td>
      <td><strong>Advantage</strong> â€” how much better a response is compared to baseline</td>
    </tr>
    <tr>
      <td>$\mu_G$, $\sigma_G$</td>
      <td>Mean and standard deviation of rewards within a group</td>
    </tr>
    <tr>
      <td>$\epsilon$</td>
      <td>Small constant (e.g., 1e-6) to prevent division by zero</td>
    </tr>
    <tr>
      <td>$\rho$</td>
      <td><strong>Importance sampling ratio</strong> â€” $\pi_\theta / \pi_{\theta_{old}}$, used for off-policy correction</td>
    </tr>
  </tbody>
</table>

<p><em>Donâ€™t worry if these arenâ€™t all clear yet â€” each will be explained in context as we go.</em></p>

<h3 id="why-grpo-for-math-reasoning">Why GRPO for Math Reasoning?</h3>

<p>Large language models struggle with multi-step math reasoning. They might solve â€œ2+3â€ but fail on â€œIf a train leaves at 2pm traveling 60mph, and another train leaves at 3pm traveling 80mphâ€¦â€œâ€”problems requiring chained logical steps.</p>

<p>GRPO offers a simpler alternative to full RLHF:</p>

<table>
  <thead>
    <tr>
      <th>Approach</th>
      <th>Value Function?</th>
      <th>Complexity</th>
      <th>When to Use</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>RLHF with PPO</strong></td>
      <td>Yes (separate model)</td>
      <td>High</td>
      <td>When you need maximum performance</td>
    </tr>
    <tr>
      <td><strong>GRPO</strong></td>
      <td>No (group statistics)</td>
      <td>Medium</td>
      <td>When you want simplicity + good results</td>
    </tr>
    <tr>
      <td><strong>Vanilla REINFORCE</strong></td>
      <td>No</td>
      <td>Low</td>
      <td>When youâ€™re learning/debugging</td>
    </tr>
  </tbody>
</table>

<p><strong>Key insight:</strong> GRPO uses the diversity of multiple responses to the same question as a â€œnaturalâ€ baseline, eliminating the need to train a separate value network.</p>

<p>The approach was introduced in <a href="https://arxiv.org/abs/2402.03300">DeepSeekMath</a> and later refined in <a href="https://arxiv.org/abs/2501.12948">DeepSeek-R1</a>.</p>

<h3 id="grpo-intuition-groups-as-your-baseline">GRPO Intuition: Groups as Your Baseline</h3>

<h4 id="the-group-concept">The â€œGroupâ€ Concept</h4>

<p>For each question, GRPO samples G different responses from the current model. These responses form a <em>group</em>. Instead of judging each answer in isolation, GRPO compares responses against each other.</p>

<p>If some responses are correct and others are wrong:</p>

<ul>
  <li>The correct ones are better than the group average â†’ they should be <strong>reinforced</strong></li>
  <li>The incorrect ones are worse than the group average â†’ they should be actively <strong>de-emphasized</strong></li>
</ul>

<p>In other words, GRPO does two things at once:</p>

<ul>
  <li>
    <p>Pushes up good responses</p>
  </li>
  <li>
    <p>Pushes down bad responses, without needing an explicit value baseline or a separate critic</p>
  </li>
</ul>

<p>By normalizing rewards within the group, GRPO naturally:</p>

<ul>
  <li>
    <p>Encourages the model to repeat reasoning patterns that work</p>
  </li>
  <li>
    <p>Discourages failure modes and bad reasoning trajectories</p>
  </li>
</ul>

<p>The group itself becomes the <strong>baseline</strong>:
â€œGiven multiple ways I could have answered this question, which ones should I do more oftenâ€”and which ones should I avoid?â€</p>

<p>This relative comparison is what makes GRPO both simple and stable, especially for domains like math reasoning where clear correctness signals exist.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Question: "What is 15 Ã— 7?"                                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ Response 1  â”‚  â”‚ Response 2  â”‚  â”‚ Response 3  â”‚  â”‚ Response 4  â”‚ â”‚
â”‚  â”‚ "105" âœ“     â”‚  â”‚ "105" âœ“     â”‚  â”‚ "112" âœ—     â”‚  â”‚ "107" âœ—     â”‚ â”‚
â”‚  â”‚ reward = 1  â”‚  â”‚ reward = 1  â”‚  â”‚ reward = 0  â”‚  â”‚ reward = 0  â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                                     â”‚
â”‚         Group mean = 0.5        Group std = 0.5                     â”‚
â”‚                                                                     â”‚
â”‚  Advantages:                                                        â”‚
â”‚  Aâ‚ = (1-0.5)/0.5 = +1.0  â† Reinforce!                              â”‚
â”‚  Aâ‚‚ = (1-0.5)/0.5 = +1.0  â† Reinforce!                              â”‚
â”‚  Aâ‚ƒ = (0-0.5)/0.5 = -1.0  â† Discourage!                             â”‚
â”‚  Aâ‚„ = (0-0.5)/0.5 = -1.0  â† Discourage!                             â”‚
â”‚                                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
</code></pre></div></div>
<p><strong>Key insight:</strong> GRPO only learns from <em>diversity</em>. If all G responses were correct (or all wrong), the advantages would be zero and no learning would occur. This is why sampling temperature matters and we need some exploration!</p>

<h3 id="grpo-vs-pporlhf">GRPO vs PPO/RLHF</h3>

<p>Hereâ€™s how GRPO compares to standard RLHF with PPO:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    RLHF with PPO                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚ Policy Model â”‚      â”‚ Value Model  â”‚      â”‚ Reward Model â”‚       â”‚
â”‚  â”‚   (train)    â”‚      â”‚   (train)    â”‚      â”‚  (frozen)    â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚         â”‚                    â”‚                      â”‚               â”‚
â”‚         â–¼                    â–¼                      â–¼               â”‚
â”‚   Generate response   Estimate expected      Score response         â”‚
â”‚         â”‚             return V(s)                   â”‚               â”‚
â”‚         â”‚                    â”‚                      â”‚               â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚                              â–¼                                      â”‚
â”‚                    Advantage = R - V(s)                             â”‚
â”‚                                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         GRPO                                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚ Policy Model â”‚                           â”‚ Reward Model â”‚        â”‚
â”‚  â”‚   (train)    â”‚                           â”‚  (frozen)    â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚         â”‚                                          â”‚                â”‚
â”‚         â–¼                                          â–¼                â”‚
â”‚   Generate G responses                      Score all G             â”‚
â”‚   for same question                         responses               â”‚
â”‚         â”‚                                          â”‚                â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
â”‚                              â–¼                                      â”‚
â”‚                    Advantage = (R - mean) / std                     â”‚
â”‚                    (computed from G siblings)                       â”‚
â”‚                                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
</code></pre></div></div>

<table>
  <thead>
    <tr>
      <th>Aspect</th>
      <th>RLHF/PPO</th>
      <th>GRPO</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Value function</strong></td>
      <td>Trained separately</td>
      <td>Not needed</td>
    </tr>
    <tr>
      <td><strong>Memory</strong></td>
      <td>2 full models</td>
      <td>1 model + reward function</td>
    </tr>
    <tr>
      <td><strong>Baseline</strong></td>
      <td>Learned V(s)</td>
      <td>Group statistics</td>
    </tr>
    <tr>
      <td><strong>Compute</strong></td>
      <td>Higher</td>
      <td>Lower</td>
    </tr>
    <tr>
      <td><strong>Implementation</strong></td>
      <td>Complex</td>
      <td>Simpler</td>
    </tr>
  </tbody>
</table>

<p><strong>Why this matters:</strong> for a 1.5B-parameter model, GRPO saves roughly ~3 GB of VRAM by eliminating the need for a separate value network. This reduction is substantialâ€”especially when running on consumer or constrained GPUsâ€”and often makes the difference between fitting the model comfortably and needing aggressive memory hacks.</p>

<h3 id="the-algorithm-step-by-step">The Algorithm Step-by-Step</h3>

<h4 id="algorithm-3-from-cs336-assignment-5-grpo-training-loop">Algorithm 3 from CS336 Assignment 5: GRPO Training Loop</h4>

<p>Hereâ€™s the complete GRPO algorithm in pseudocode:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Algorithm: GRPO Training

Input: policy Ï€_Î¸, reward function R, training data D, group size G

for step = 1 to n_grpo_steps:

    # Step 1: Sample batch of questions
    Sample questions {qâ‚, qâ‚‚, ..., qâ‚™} from D

    # Step 2: Generate G responses per question
    for each question q:
        Sample {oâ½Â¹â¾, ..., oâ½á´³â¾} ~ Ï€_Î¸(Â· | q)
        Compute rewards {râ½Â¹â¾, ..., râ½á´³â¾} using R

        # Step 3: Group normalization
        Î¼ = mean(râ½Â¹â¾, ..., râ½á´³â¾)
        Ïƒ = std(râ½Â¹â¾, ..., râ½á´³â¾)
        Aâ½â±â¾ = (râ½â±â¾ - Î¼) / (Ïƒ + Îµ)  for i = 1..G

    # Step 4: Store old log-probs for off-policy training
    Store log Ï€_Î¸_old(oâ‚œ | q, o&lt;â‚œ) for all tokens

    # Step 5: Multiple gradient steps (off-policy)
    for epoch = 1 to epochs_per_batch:
        Compute policy gradient loss with clipping
        Update Î¸ using Adam optimizer

Output: trained policy Ï€_Î¸
</code></pre></div></div>

<h4 id="the-group-normalization-formula">The Group Normalization Formula</h4>

<p>The advantage for response i in a group is:</p>

\[A^{(i)} = \frac{r^{(i)} - \mu_G}{\sigma_G + \epsilon}\]

<p>where:</p>
<ul>
  <li>$r^{(i)}$ = reward for response i</li>
  <li>$\mu_G$ = mean reward in the group</li>
  <li>$\sigma_G$ = standard deviation of rewards in the group</li>
  <li>$\epsilon$ = small constant (1e-6) to prevent division by zero</li>
</ul>

<p><strong>Dr. GRPO variant:</strong> Some implementations skip the std normalization:</p>

\[A^{(i)} = r^{(i)} - \mu_G\]

<p>This simpler form works well when rewards are binary (0 or 1).</p>

<h3 id="key-implementation-details">Key Implementation Details</h3>

<h4 id="group-normalized-rewards">Group-Normalized Rewards</h4>

<p>Hereâ€™s the core implementation from <code class="language-plaintext highlighter-rouge">grpo.py</code>:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">compute_group_normalized_rewards</span><span class="p">(</span>
    <span class="n">reward_fn</span><span class="p">,</span>
    <span class="n">rollout_responses</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span>
    <span class="n">repeated_ground_truths</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span>
    <span class="n">group_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">advantage_eps</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-6</span><span class="p">,</span>
    <span class="n">normalize_by_std</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="bp">True</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="nb">dict</span><span class="p">]:</span>
    <span class="s">"""
    Compute rewards normalized by group statistics.

    Args:
        reward_fn: Function that scores response against ground truth
        rollout_responses: All generated responses (n_questions * group_size)
        repeated_ground_truths: Ground truths repeated for each response
        group_size: Number of responses per question (G)
        normalize_by_std: If True, divide by std (standard GRPO)
                          If False, only subtract mean (Dr. GRPO)
    """</span>
    <span class="n">n_groups</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">rollout_responses</span><span class="p">)</span> <span class="o">//</span> <span class="n">group_size</span>

    <span class="c1"># Score all responses
</span>    <span class="n">raw_rewards</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">response</span><span class="p">,</span> <span class="n">ground_truth</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">rollout_responses</span><span class="p">,</span> <span class="n">repeated_ground_truths</span><span class="p">):</span>
        <span class="n">reward_info</span> <span class="o">=</span> <span class="n">reward_fn</span><span class="p">(</span><span class="n">response</span><span class="p">,</span> <span class="n">ground_truth</span><span class="p">)</span>
        <span class="n">raw_rewards</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">reward_info</span><span class="p">[</span><span class="s">"reward"</span><span class="p">])</span>

    <span class="n">raw_rewards</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">raw_rewards</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>

    <span class="c1"># Reshape to (n_groups, group_size) for group-wise operations
</span>    <span class="n">rewards_grouped</span> <span class="o">=</span> <span class="n">raw_rewards</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="n">n_groups</span><span class="p">,</span> <span class="n">group_size</span><span class="p">)</span>

    <span class="c1"># Compute group statistics
</span>    <span class="n">group_means</span> <span class="o">=</span> <span class="n">rewards_grouped</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>  <span class="c1"># (n_groups, 1)
</span>    <span class="n">group_stds</span> <span class="o">=</span> <span class="n">rewards_grouped</span><span class="p">.</span><span class="n">std</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>    <span class="c1"># (n_groups, 1)
</span>
    <span class="c1"># Compute advantages
</span>    <span class="k">if</span> <span class="n">normalize_by_std</span><span class="p">:</span>
        <span class="c1"># Standard GRPO: A = (r - mean) / (std + eps)
</span>        <span class="n">advantages_grouped</span> <span class="o">=</span> <span class="p">(</span><span class="n">rewards_grouped</span> <span class="o">-</span> <span class="n">group_means</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">group_stds</span> <span class="o">+</span> <span class="n">advantage_eps</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># Dr. GRPO: A = r - mean
</span>        <span class="n">advantages_grouped</span> <span class="o">=</span> <span class="n">rewards_grouped</span> <span class="o">-</span> <span class="n">group_means</span>

    <span class="c1"># Flatten back to (rollout_batch_size,)
</span>    <span class="n">advantages</span> <span class="o">=</span> <span class="n">advantages_grouped</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">advantages</span><span class="p">,</span> <span class="n">raw_rewards</span><span class="p">,</span> <span class="n">metadata</span>
</code></pre></div></div>

<table>
  <thead>
    <tr>
      <th>Normalization</th>
      <th>Formula</th>
      <th>When to Use</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Standard GRPO</strong></td>
      <td>A = (r - Î¼) / (Ïƒ + Îµ)</td>
      <td>General case, variable rewards</td>
    </tr>
    <tr>
      <td><strong>Dr. GRPO</strong></td>
      <td>A = r - Î¼</td>
      <td>Binary rewards (0/1), simpler</td>
    </tr>
  </tbody>
</table>

<h4 id="three-loss-types">Three Loss Types</h4>

<p>The implementation supports three policy gradient loss types:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">compute_policy_gradient_loss</span><span class="p">(</span>
    <span class="n">policy_log_probs</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">loss_type</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>  <span class="c1"># "no_baseline", "reinforce_with_baseline", "grpo_clip"
</span>    <span class="n">raw_rewards</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
    <span class="n">advantages</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
    <span class="n">old_log_probs</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
    <span class="n">cliprange</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.2</span><span class="p">,</span>
<span class="p">):</span>
    <span class="k">if</span> <span class="n">loss_type</span> <span class="o">==</span> <span class="s">"no_baseline"</span><span class="p">:</span>
        <span class="c1"># Vanilla REINFORCE: -R * log Ï€(a|s)
</span>        <span class="n">loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">raw_rewards</span> <span class="o">*</span> <span class="n">policy_log_probs</span>

    <span class="k">elif</span> <span class="n">loss_type</span> <span class="o">==</span> <span class="s">"reinforce_with_baseline"</span><span class="p">:</span>
        <span class="c1"># REINFORCE with baseline: -A * log Ï€(a|s)
</span>        <span class="n">loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">advantages</span> <span class="o">*</span> <span class="n">policy_log_probs</span>

    <span class="k">elif</span> <span class="n">loss_type</span> <span class="o">==</span> <span class="s">"grpo_clip"</span><span class="p">:</span>
        <span class="c1"># PPO-style clipping for off-policy stability
</span>        <span class="n">ratio</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="n">policy_log_probs</span> <span class="o">-</span> <span class="n">old_log_probs</span><span class="p">)</span>
        <span class="n">clipped_ratio</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">ratio</span><span class="p">,</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">cliprange</span><span class="p">,</span> <span class="mi">1</span> <span class="o">+</span> <span class="n">cliprange</span><span class="p">)</span>

        <span class="c1"># Take minimum (pessimistic bound)
</span>        <span class="n">loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">torch</span><span class="p">.</span><span class="nb">min</span><span class="p">(</span><span class="n">ratio</span> <span class="o">*</span> <span class="n">advantages</span><span class="p">,</span> <span class="n">clipped_ratio</span> <span class="o">*</span> <span class="n">advantages</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">loss</span>
</code></pre></div></div>

<p><strong>On-Policy vs Off-Policy: Whatâ€™s the Difference?</strong></p>

<p><em>Quick terminology note:</em> In RL for language models, the <strong>policy</strong> ($\pi$) <em>is</em> the language model being trained. The policy parameters ($\theta$) are the model weights. When we write $\pi_\theta(a \mid s)$, we mean â€œthe probability of generating token $a$ given context $s$, according to the model with weights $\theta$.â€ The model defines a probability distribution over actions (next tokens) given states (prompt + tokens so far)â€”thatâ€™s exactly what a policy is.</p>

<p>This distinction matters for understanding when to use each loss type:</p>

<ul>
  <li>
    <p><strong>On-policy</strong>: The policy used to <em>generate</em> the data is the <em>same</em> as the policy being <em>updated</em>. Each batch of rollouts is used for exactly one gradient step, then discarded. Simple but wastefulâ€”you throw away expensive samples after one use.</p>
  </li>
  <li>
    <p><strong>Off-policy</strong>: The policy used to <em>generate</em> the data can be <em>different</em> from the policy being <em>updated</em>. This lets you reuse the same batch of rollouts for multiple gradient steps, extracting more learning signal from each expensive generation.</p>
  </li>
</ul>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>On-Policy (REINFORCE):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Generate withâ”‚ â”€â”€â–º â”‚ One gradient â”‚ â”€â”€â–º â”‚   Discard    â”‚
â”‚    Ï€_Î¸       â”‚     â”‚    step      â”‚     â”‚   rollouts   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Off-Policy (GRPO with clipping):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Generate withâ”‚ â”€â”€â–º â”‚  Multiple    â”‚ â”€â”€â–º â”‚   Then       â”‚
â”‚   Ï€_Î¸_old    â”‚     â”‚ grad steps   â”‚     â”‚  discard     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                     Uses ratio Ï = Ï€_Î¸/Ï€_Î¸_old
                     to correct for policy drift
</code></pre></div></div>

<p>The catch with off-policy: as you update $\theta$, the current policy $\pi_\theta$ drifts away from the old policy $\pi_{\theta_{old}}$ that generated the data. The <strong>importance sampling ratio</strong> $\rho = \pi_\theta(a \mid s) / \pi_{\theta_{old}}(a \mid s)$ corrects for this, but if $\theta$ changes too much, the correction becomes unreliable. Thatâ€™s why <code class="language-plaintext highlighter-rouge">grpo_clip</code> uses PPO-style clippingâ€”it prevents the ratio from getting too large, keeping updates stable even when reusing rollouts.</p>

<p><strong>Comparison table:</strong></p>

<table>
  <thead>
    <tr>
      <th>Loss Type</th>
      <th>Formula</th>
      <th>Pros</th>
      <th>Cons</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">no_baseline</code></td>
      <td>-R Ã— log Ï€</td>
      <td>Simplest</td>
      <td>High variance</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">reinforce_with_baseline</code></td>
      <td>-A Ã— log Ï€</td>
      <td>Lower variance</td>
      <td>On-policy only</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">grpo_clip</code></td>
      <td>-min(ÏA, clip(Ï)A)</td>
      <td>Off-policy stable</td>
      <td>More complex</td>
    </tr>
  </tbody>
</table>

<p><strong>When to use each:</strong></p>
<ul>
  <li><strong>no_baseline</strong>: Debugging, understanding basics</li>
  <li><strong>reinforce_with_baseline</strong>: Default choice, good balance</li>
  <li><strong>grpo_clip</strong>: When reusing rollouts across multiple gradient steps</li>
</ul>

<h4 id="token-level-loss-with-masking">Token-Level Loss with Masking</h4>

<p>GRPO applies the loss only to response tokens, not the prompt:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Token sequence:                                                    â”‚
â”‚                                                                     â”‚
â”‚  [What][is][2+3][?][&lt;think&gt;][I][need][to][add][&lt;/think&gt;][5][&lt;EOS&gt;]  â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”‚
â”‚        PROMPT                        RESPONSE                       â”‚
â”‚        mask = 0                      mask = 1                       â”‚
â”‚                                                                     â”‚
â”‚  Loss is computed ONLY over response tokens (mask = 1)              â”‚
â”‚                                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
</code></pre></div></div>

<p>The <code class="language-plaintext highlighter-rouge">masked_mean</code> function handles this:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">masked_mean</span><span class="p">(</span><span class="n">tensor</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">mask</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="bp">None</span><span class="p">):</span>
    <span class="s">"""Average only over positions where mask == 1."""</span>
    <span class="n">mask_float</span> <span class="o">=</span> <span class="n">mask</span><span class="p">.</span><span class="nb">float</span><span class="p">()</span>
    <span class="n">masked_tensor</span> <span class="o">=</span> <span class="n">tensor</span> <span class="o">*</span> <span class="n">mask_float</span>

    <span class="k">if</span> <span class="n">dim</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
        <span class="c1"># Global mean
</span>        <span class="k">return</span> <span class="n">masked_tensor</span><span class="p">.</span><span class="nb">sum</span><span class="p">()</span> <span class="o">/</span> <span class="n">mask_float</span><span class="p">.</span><span class="nb">sum</span><span class="p">().</span><span class="n">clamp</span><span class="p">(</span><span class="nb">min</span><span class="o">=</span><span class="mf">1e-8</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># Mean along dimension
</span>        <span class="k">return</span> <span class="n">masked_tensor</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">dim</span><span class="p">)</span> <span class="o">/</span> <span class="n">mask_float</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">dim</span><span class="p">).</span><span class="n">clamp</span><span class="p">(</span><span class="nb">min</span><span class="o">=</span><span class="mf">1e-8</span><span class="p">)</span>
</code></pre></div></div>

<p><strong>Why this matters:</strong> Including prompt tokens in the loss would reinforce the model for generating the questionâ€”not what we want! We only want to reinforce good <em>answers</em>.</p>

<h4 id="training-loop-and-2-gpu-architecture">Training Loop and 2-GPU Architecture</h4>

<p>This section uses <strong><a href="https://github.com/vllm-project/vllm">vLLM</a></strong> for fast inference. vLLM is a high-throughput LLM serving engine that uses <strong>PagedAttention</strong> to efficiently manage GPU memory and <strong>continuous batching</strong> to maximize throughput. For GRPO, where we need to generate many responses (G per question) quickly, vLLM can be 10-24x faster than standard Hugging Face <code class="language-plaintext highlighter-rouge">generate()</code>.</p>

<p><strong>Why Separate GPUs?</strong></p>

<p>I used <strong>2Ã— H100 (80GB SXM5)</strong> GPUs from Lambda Labs for the GRPO experiments (~6.38 USD/hour). Even with 80GB per GPU, running both vLLM inference and policy training on the same GPU leads to memory contention. GRPO training has two distinct phases with competing memory requirements:</p>

<ol>
  <li><strong>Rollout generation</strong> (inference): Generate G responses per question using vLLM</li>
  <li><strong>Policy training</strong> (gradient computation): Update weights using the computed advantages</li>
</ol>

<p>These phases have different memory patterns:</p>

<table>
  <thead>
    <tr>
      <th>Phase</th>
      <th>GPU</th>
      <th>Memory Breakdown</th>
      <th>Total</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Rollout (vLLM)</strong></td>
      <td>GPU 0</td>
      <td>Model weights (~3GB) + KV cache (~40-60GB at high utilization)</td>
      <td>~65GB</td>
    </tr>
    <tr>
      <td><strong>Training (Policy)</strong></td>
      <td>GPU 1</td>
      <td>Model weights (~3GB) + Optimizer states (~6GB) + Activations (~2-4GB)</td>
      <td>~12GB</td>
    </tr>
  </tbody>
</table>

<p><strong>Why not share a single 80GB GPU?</strong></p>

<p>While the training phase only uses ~12GB, combining both workloads is problematic:</p>

<ul>
  <li><strong>Peak memory overlap</strong>: vLLMâ€™s KV cache grows dynamically during generation. If training starts while vLLM is generating long sequences, combined memory can exceed 80GB â†’ OOM.</li>
  <li><strong>Memory fragmentation</strong>: vLLM uses PagedAttention which allocates memory in blocks. Frequent allocation/deallocation during training causes fragmentation, reducing effective capacity.</li>
  <li><strong>Throughput loss</strong>: Context switching between inference and training modes adds overhead.</li>
</ul>

<p>The 2-GPU solution is clean: GPU 0 runs vLLM inference exclusively, GPU 1 handles training. After each rollout batch, updated weights are synced from GPU 1 â†’ GPU 0.</p>

<p><strong>GPU Detection and Allocation Logic</strong></p>

<p>The training script detects available GPUs and chooses between two modes:</p>

<table>
  <thead>
    <tr>
      <th>Mode</th>
      <th>GPUs</th>
      <th>Rollout Generation</th>
      <th>Performance</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>2-GPU mode</strong></td>
      <td>2+</td>
      <td>vLLM (fast, dedicated GPU)</td>
      <td>~10-24Ã— faster rollouts</td>
    </tr>
    <tr>
      <td><strong>Single-GPU mode</strong></td>
      <td>1</td>
      <td>HuggingFace <code class="language-plaintext highlighter-rouge">generate()</code></td>
      <td>Slower, but works</td>
    </tr>
  </tbody>
</table>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># From run_grpo.py
</span><span class="kn">import</span> <span class="nn">torch</span>

<span class="n">n_gpus</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="n">device_count</span><span class="p">()</span>
<span class="n">logger</span><span class="p">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s">"Detected </span><span class="si">{</span><span class="n">n_gpus</span><span class="si">}</span><span class="s"> GPU(s)"</span><span class="p">)</span>

<span class="k">if</span> <span class="n">n_gpus</span> <span class="o">&gt;=</span> <span class="mi">2</span><span class="p">:</span>
    <span class="c1"># 2-GPU mode: vLLM on GPU 0, policy training on GPU 1
</span>    <span class="n">use_vllm</span> <span class="o">=</span> <span class="bp">True</span>
    <span class="n">vllm_device</span> <span class="o">=</span> <span class="s">"cuda:0"</span>
    <span class="n">policy_device</span> <span class="o">=</span> <span class="s">"cuda:1"</span>

    <span class="n">vllm_instance</span> <span class="o">=</span> <span class="n">init_vllm</span><span class="p">(</span>
        <span class="n">model_id</span><span class="o">=</span><span class="n">args</span><span class="p">.</span><span class="n">model_name_or_path</span><span class="p">,</span>
        <span class="n">device</span><span class="o">=</span><span class="n">vllm_device</span><span class="p">,</span>
        <span class="n">gpu_memory_utilization</span><span class="o">=</span><span class="mf">0.85</span><span class="p">,</span>
    <span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="c1"># Single-GPU mode: no vLLM, use HuggingFace generate instead
</span>    <span class="n">use_vllm</span> <span class="o">=</span> <span class="bp">False</span>
    <span class="n">policy_device</span> <span class="o">=</span> <span class="s">"cuda:0"</span>
    <span class="n">logger</span><span class="p">.</span><span class="n">warning</span><span class="p">(</span><span class="s">"Only 1 GPU detected. Falling back to HuggingFace generate (slower)."</span><span class="p">)</span>
</code></pre></div></div>

<p><strong>How does PyTorch know which GPU to use?</strong> It doesnâ€™t decide automaticallyâ€”<strong>you specify it in your code</strong>. PyTorch requires explicit device placement using <code class="language-plaintext highlighter-rouge">.to(device)</code>:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Load policy model explicitly on GPU 1
</span><span class="n">policy</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="p">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>
<span class="n">policy</span> <span class="o">=</span> <span class="n">policy</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="s">"cuda:1"</span><span class="p">)</span>  <span class="c1"># â† You specify this
</span>
<span class="c1"># Tensors must also be moved to the same device
</span><span class="n">input_ids</span> <span class="o">=</span> <span class="n">input_ids</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="s">"cuda:1"</span><span class="p">)</span>  <span class="c1"># Data must match model's device
</span></code></pre></div></div>

<p>If you just call <code class="language-plaintext highlighter-rouge">model.cuda()</code> without specifying a device, it defaults to GPU 0. For multi-GPU setups like GRPO, explicit placement (<code class="language-plaintext highlighter-rouge">cuda:0</code>, <code class="language-plaintext highlighter-rouge">cuda:1</code>) is essential to keep workloads separated.</p>

<p><strong>Why the fallback to use HF generate?</strong> vLLM and policy training canâ€™t efficiently share a single GPUâ€”vLLMâ€™s memory management (PagedAttention, continuous batching) conflicts with PyTorchâ€™s training memory patterns. With only 1 GPU, the script disables vLLM entirely and uses HuggingFaceâ€™s simpler <code class="language-plaintext highlighter-rouge">generate()</code> method, which is slower but avoids memory conflicts.</p>

<p><strong>What is HuggingFace <code class="language-plaintext highlighter-rouge">generate()</code>?</strong> <a href="https://huggingface.co/docs/transformers">HuggingFace Transformers</a> is the most popular library for working with pretrained language models. Its <code class="language-plaintext highlighter-rouge">model.generate()</code> method is the standard way to produce text from a modelâ€”it handles tokenization, sampling strategies (greedy, top-k, top-p), and decoding in a straightforward API. While easy to use and compatible with training (same PyTorch model instance), it processes requests one batch at a time without the advanced optimizations (PagedAttention, continuous batching) that make vLLM fast. For GRPO, this means rollout generation takes longer, but it works reliably on a single GPU.</p>

<p><strong>Decision flowchart:</strong></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     GPU Allocation Decision                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                     â”‚
â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”‚
â”‚                    â”‚  torch.cuda.device_count()                     â”‚
â”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚
â”‚                                â”‚                                    â”‚
â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚
â”‚              â–¼                                   â–¼                  â”‚
â”‚       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚       â”‚   1 GPU     â”‚                     â”‚   2+ GPUs   â”‚           â”‚
â”‚       â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜           â”‚
â”‚              â”‚                                   â”‚                  â”‚
â”‚              â–¼                                   â–¼                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚  Single-GPU Mode      â”‚         â”‚    2-GPU Mode         â”‚        â”‚
â”‚  â”‚                       â”‚         â”‚                       â”‚        â”‚
â”‚  â”‚  â€¢ Policy: cuda:0     â”‚         â”‚  â€¢ vLLM: cuda:0       â”‚        â”‚
â”‚  â”‚  â€¢ Rollouts: HF       â”‚         â”‚  â€¢ Policy: cuda:1     â”‚        â”‚
â”‚  â”‚    generate() (slow)  â”‚         â”‚  â€¢ Rollouts: vLLM     â”‚        â”‚
â”‚  â”‚  â€¢ Shared memory      â”‚         â”‚    (10-24Ã— faster)    â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚                                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
</code></pre></div></div>

<p><strong>Memory Profiling</strong></p>

<p>The training loop includes memory logging to help diagnose issues:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">log_gpu_memory</span><span class="p">(</span><span class="n">msg</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s">""</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
    <span class="s">"""Log current GPU memory usage."""</span>
    <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="n">is_available</span><span class="p">():</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="n">device_count</span><span class="p">()):</span>
            <span class="n">allocated</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="n">memory_allocated</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="o">/</span> <span class="mi">1024</span><span class="o">**</span><span class="mi">3</span>
            <span class="n">reserved</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="n">memory_reserved</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="o">/</span> <span class="mi">1024</span><span class="o">**</span><span class="mi">3</span>
            <span class="n">logger</span><span class="p">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s">"GPU </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s"> </span><span class="si">{</span><span class="n">msg</span><span class="si">}</span><span class="s">: </span><span class="si">{</span><span class="n">allocated</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s"> GB allocated, </span><span class="si">{</span><span class="n">reserved</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s"> GB reserved"</span><span class="p">)</span>
</code></pre></div></div>

<p>Sample output during training:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>GPU 0 after vLLM: 62.45 GB allocated, 65.00 GB reserved
GPU 1 after policy: 3.21 GB allocated, 4.50 GB reserved
GPU 1 after optimizer.step(): 9.45 GB allocated, 12.00 GB reserved
</code></pre></div></div>

<p><strong>What do â€œallocatedâ€ and â€œreservedâ€ mean?</strong></p>

<ul>
  <li><strong>Allocated</strong>: Memory currently holding tensors (model weights, activations, gradients). This is the memory your code is <em>actively using</em>.</li>
  <li><strong>Reserved</strong>: Memory that PyTorchâ€™s CUDA allocator has claimed from the GPU but isnâ€™t currently in use. PyTorch reserves extra memory as a â€œpoolâ€ to avoid expensive allocation callsâ€”when you need new tensors, it pulls from this pool instead of asking the GPU driver.</li>
</ul>

<p>The gap between reserved and allocated (e.g., 65 - 62.45 = 2.55 GB on GPU 0) is â€œfreeâ€ memory within PyTorchâ€™s pool. If you see OOM errors even when allocated seems low, check reservedâ€”fragmentation can cause PyTorch to reserve more than needed.</p>

<p><strong>Memory Optimization Techniques</strong></p>

<table>
  <thead>
    <tr>
      <th>Technique</th>
      <th>How It Helps</th>
      <th>Code Reference</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Gradient checkpointing</strong></td>
      <td>Trades compute for memory by recomputing activations during backprop</td>
      <td><code class="language-plaintext highlighter-rouge">policy.gradient_checkpointing_enable()</code></td>
    </tr>
    <tr>
      <td><strong>Sequence truncation</strong></td>
      <td>Limits max context to reduce memory</td>
      <td><code class="language-plaintext highlighter-rouge">--max-seq-length-train 512</code></td>
    </tr>
    <tr>
      <td><strong>Cache clearing</strong></td>
      <td>Frees unused memory between steps</td>
      <td><code class="language-plaintext highlighter-rouge">torch.cuda.empty_cache()</code></td>
    </tr>
    <tr>
      <td><strong>Explicit <code class="language-plaintext highlighter-rouge">del</code></strong></td>
      <td>Removes tensor references immediately</td>
      <td><code class="language-plaintext highlighter-rouge">del logits, outputs</code></td>
    </tr>
    <tr>
      <td><strong>Smaller micro-batches</strong></td>
      <td>Reduces peak memory per step</td>
      <td><code class="language-plaintext highlighter-rouge">--gradient-accumulation-steps</code></td>
    </tr>
  </tbody>
</table>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Enable gradient checkpointing to reduce memory usage
</span><span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">policy</span><span class="p">,</span> <span class="s">'gradient_checkpointing_enable'</span><span class="p">):</span>
    <span class="n">policy</span><span class="p">.</span><span class="n">gradient_checkpointing_enable</span><span class="p">()</span>
    <span class="n">logger</span><span class="p">.</span><span class="n">info</span><span class="p">(</span><span class="s">"Gradient checkpointing enabled"</span><span class="p">)</span>

<span class="c1"># In the training loop, free memory aggressively
</span><span class="k">del</span> <span class="n">log_prob_result</span><span class="p">,</span> <span class="n">mb_policy_log_probs</span><span class="p">,</span> <span class="n">loss</span>
<span class="n">gc</span><span class="p">.</span><span class="n">collect</span><span class="p">()</span>
<span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="n">empty_cache</span><span class="p">()</span>
</code></pre></div></div>

<p><strong>The Training Loop</strong></p>

<p>Hereâ€™s the step-by-step flow of <code class="language-plaintext highlighter-rouge">grpo_train_loop()</code>:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    GRPO Training Iteration                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                     â”‚
â”‚  1. Sample batch of prompts from training data                      â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”‚
â”‚     â”‚ "What is 2+3?", "Solve xÂ²=4", ...       â”‚                     â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚
â”‚                         â–¼                                           â”‚
â”‚  2. Generate G rollouts per prompt (vLLM or HF generate)            â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”‚
â”‚     â”‚ 8 responses per question                â”‚                     â”‚
â”‚     â”‚ Total: n_prompts Ã— 8 responses          â”‚                     â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚
â”‚                         â–¼                                           â”‚
â”‚  3. Score responses with reward function (CPU)                      â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”‚
â”‚     â”‚ r1_zero_reward_fn: extracts answer from â”‚                     â”‚
â”‚     â”‚ text, compares to ground truth â†’ {0, 1} â”‚                     â”‚
â”‚     â”‚ (string processing, no GPU needed)      â”‚                     â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚
â”‚                         â–¼                                           â”‚
â”‚  4. Compute group-normalized advantages                             â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”‚
â”‚     â”‚ A = (r - group_mean) / (group_std + Îµ)  â”‚                     â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚
â”‚                         â–¼                                           â”‚
â”‚  5. Forward pass on policy model                                    â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”‚
â”‚     â”‚ Compute log Ï€_Î¸(token | context)        â”‚                     â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚
â”‚                         â–¼                                           â”‚
â”‚  6. Compute masked policy gradient loss                             â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”‚
â”‚     â”‚ Loss = -A Ã— log Ï€ (response tokens only)â”‚                     â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚
â”‚                         â–¼                                           â”‚
â”‚  7. Backward pass with gradient accumulation                        â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”‚
â”‚     â”‚ Accumulate gradients over micro-batches â”‚                     â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚
â”‚                         â–¼                                           â”‚
â”‚  8. Optimizer step                                                  â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”‚
â”‚     â”‚ AdamW update, gradient clipping         â”‚                     â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚
â”‚                                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
</code></pre></div></div>

<h3 id="grpo-experiment-on-lambda-cloud-setup-with-2h100-80gb-sxm5">GRPO Experiment on Lambda Cloud Setup with 2Ã—H100 (80GB SXM5)</h3>

<h4 id="how-two-gpus-work-together-in-a-grpo-training-setup">How Two GPUs Work Together in a GRPO Training Setup?</h4>

<p>The 2-GPU architecture separates concerns:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Lambda Cloud 2Ã—H100 Setup                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                     â”‚
â”‚   GPU 0 (H100 80GB)              GPU 1 (H100 80GB)                  â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
â”‚   â”‚                 â”‚            â”‚                 â”‚                â”‚
â”‚   â”‚     vLLM        â”‚            â”‚  Policy Model   â”‚                â”‚
â”‚   â”‚   (~65 GB)      â”‚            â”‚    (~3 GB)      â”‚                â”‚
â”‚   â”‚                 â”‚            â”‚                 â”‚                â”‚
â”‚   â”‚  - Fast batched â”‚   sync     â”‚  - Gradients    â”‚                â”‚
â”‚   â”‚    inference    â”‚ â—„â”€â”€â”€â”€â”€â”€â”€â”€â–º â”‚  - Optimizer    â”‚                â”‚
â”‚   â”‚  - KV cache     â”‚  weights   â”‚  - Backprop     â”‚                â”‚
â”‚   â”‚  - Continuous   â”‚            â”‚                 â”‚                â”‚
â”‚   â”‚    batching     â”‚            â”‚                 â”‚                â”‚
â”‚   â”‚                 â”‚            â”‚                 â”‚                â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
â”‚                                                                     â”‚
â”‚   Rollout generation             Policy training                    â”‚
â”‚   (inference only)               (train mode)                       â”‚
â”‚                                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
</code></pre></div></div>

<p><strong>Understanding the hardware:</strong> A Lambda Cloud instance with 2Ã—H100 GPUs also includes a <strong>host CPU</strong> (typically AMD EPYC or Intel Xeon) that orchestrates all work. The GPUs are acceleratorsâ€”the CPU runs your Python code, loads data, and dispatches compute-heavy operations to the GPUs.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Lambda Cloud Instance                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                     â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚   â”‚                      CPU (Host)                              â”‚  â”‚
â”‚   â”‚   â€¢ Runs Python/PyTorch orchestration code                   â”‚  â”‚
â”‚   â”‚   â€¢ Reward calculation (string parsing, regex)               â”‚  â”‚
â”‚   â”‚   â€¢ Advantage computation (simple arithmetic)                â”‚  â”‚
â”‚   â”‚   â€¢ Data loading and preprocessing                           â”‚  â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                          â”‚                                          â”‚
â”‚            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                            â”‚
â”‚            â–¼                           â–¼                            â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
â”‚   â”‚    GPU 0        â”‚         â”‚    GPU 1        â”‚                   â”‚
â”‚   â”‚   (H100 80GB)   â”‚         â”‚   (H100 80GB)   â”‚                   â”‚
â”‚   â”‚                 â”‚         â”‚                 â”‚                   â”‚
â”‚   â”‚  vLLM rollouts  â”‚         â”‚ Policy training â”‚                   â”‚
â”‚   â”‚  (inference)    â”‚         â”‚ (forward/back)  â”‚                   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
â”‚                                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
</code></pre></div></div>

<p><strong>Where does each step run?</strong></p>

<table>
  <thead>
    <tr>
      <th>Step</th>
      <th>Location</th>
      <th>What Happens</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Rollout generation</td>
      <td><strong>GPU 0</strong></td>
      <td>vLLM generates G responses per question</td>
    </tr>
    <tr>
      <td>Reward calculation</td>
      <td><strong>CPU</strong></td>
      <td>String parsingâ€”extract answer, compare to ground truth</td>
    </tr>
    <tr>
      <td>Advantage computation</td>
      <td><strong>CPU</strong></td>
      <td>Simple arithmetic: <code class="language-plaintext highlighter-rouge">(r - Î¼) / (Ïƒ + Îµ)</code></td>
    </tr>
    <tr>
      <td>Policy forward/backward</td>
      <td><strong>GPU 1</strong></td>
      <td>Compute log-probs and gradients</td>
    </tr>
    <tr>
      <td>Optimizer step</td>
      <td><strong>GPU 1</strong></td>
      <td>Update weights with AdamW</td>
    </tr>
    <tr>
      <td>Weight sync</td>
      <td><strong>GPU 0 â† GPU 1</strong></td>
      <td>Copy updated weights to vLLM</td>
    </tr>
  </tbody>
</table>

<p><strong>Benefits:</strong></p>
<ul>
  <li>No memory contention between inference and training</li>
  <li>vLLM can use continuous batching without interruption</li>
  <li>Policy model has dedicated memory for optimizer states</li>
  <li>Stable training with predictable memory usage</li>
</ul>

<h4 id="step-by-step-setup">Step-by-Step Setup</h4>

<p><strong>1. Provision Instance</strong></p>

<p>On Lambda Cloud, select an instance with 2+ GPUs:</p>
<ul>
  <li>2Ã— A100 (80GB each) - recommended</li>
  <li>2Ã— H100 (80GB each) - faster, if available</li>
</ul>

<p><strong>2. SSH and Check GPUs</strong></p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ssh ubuntu@&lt;your-instance-ip&gt;

<span class="c"># Verify GPUs are visible</span>
nvidia-smi <span class="nt">--list-gpus</span>
<span class="c"># Expected: GPU 0: NVIDIA H100 80GB HBM3</span>
<span class="c">#           GPU 1: NVIDIA H100 80GB HBM3</span>
</code></pre></div></div>

<p><strong>3. Install Dependencies</strong></p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Install uv package manager</span>
curl <span class="nt">-LsSf</span> https://astral.sh/uv/install.sh | sh

<span class="c"># Clone and install</span>
git clone https://github.com/bearbearyu1223/qwen_math_grpo.git
<span class="nb">cd </span>qwen_math_grpo
uv <span class="nb">sync</span> <span class="nt">--extra</span> vllm
</code></pre></div></div>

<p><strong>4. Download Dataset</strong></p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>uv run python scripts/download_dataset.py
</code></pre></div></div>

<p><strong>5. Run Training</strong></p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>uv run python scripts/run_grpo.py <span class="se">\</span>
    <span class="nt">--model-name-or-path</span> Qwen/Qwen2.5-Math-1.5B <span class="se">\</span>
    <span class="nt">--rollout-batch-size</span> 16 <span class="se">\</span>
    <span class="nt">--train-batch-size</span> 16 <span class="se">\</span>
    <span class="nt">--gradient-accumulation-steps</span> 8 <span class="se">\</span>
    <span class="nt">--max-seq-length-train</span> 1024 <span class="se">\</span>
    <span class="nt">--n-grpo-steps</span> 200 <span class="se">\</span>
    <span class="nt">--group-size</span> 8 <span class="se">\</span>
    <span class="nt">--output-dir</span> outputs/grpo_model
</code></pre></div></div>

<p><strong>Parameter descriptions:</strong></p>

<table>
  <thead>
    <tr>
      <th>Parameter</th>
      <th>Value</th>
      <th>What It Does</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">--model-name-or-path</code></td>
      <td><code class="language-plaintext highlighter-rouge">Qwen/Qwen2.5-Math-1.5B</code></td>
      <td>Base model to fine-tune (downloaded from HuggingFace)</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">--rollout-batch-size</code></td>
      <td>16</td>
      <td>Number of questions sampled per GRPO step</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">--train-batch-size</code></td>
      <td>16</td>
      <td>Responses processed per gradient accumulation cycle</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">--gradient-accumulation-steps</code></td>
      <td>8</td>
      <td>Micro-batches accumulated before optimizer update</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">--max-seq-length-train</code></td>
      <td>1024</td>
      <td>Truncates prompt+response to this many tokens. Sequences longer than this limit are cut off. Lower values save GPU memory (activations scale with sequence lengthÂ²) but may lose reasoning steps. For math problems, 1024 tokens typically covers the question + full solution.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">--n-grpo-steps</code></td>
      <td>200</td>
      <td>Total training iterations</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">--group-size</code></td>
      <td>8</td>
      <td>Responses generated per question (G in the formula)</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">--output-dir</code></td>
      <td><code class="language-plaintext highlighter-rouge">outputs/grpo_model</code></td>
      <td>Where to save checkpoints and logs</td>
    </tr>
  </tbody>
</table>

<p><strong>How these numbers relate:</strong></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Questions per step:     16  (rollout-batch-size)
                        Ã—
Responses per question:  8  (group-size)
                        â•â•â•
Total responses:       128  generated per GRPO step

Training processes:     16  (train-batch-size)
                        Ã—
Accumulation steps:      8  (gradient-accumulation-steps)
                        â•â•â•
Effective batch:       128  responses per optimizer update
</code></pre></div></div>

<p>The numbers are chosen so all 128 generated responses are used in exactly one optimizer update. If you reduce <code class="language-plaintext highlighter-rouge">rollout-batch-size</code> or <code class="language-plaintext highlighter-rouge">group-size</code>, reduce the training side proportionally to match.</p>

<p><strong>6. Download Results</strong></p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># From your local machine</span>
scp <span class="nt">-r</span> ubuntu@&lt;your-instance-ip&gt;:~/qwen_math_grpo/outputs ./lambda_outputs
</code></pre></div></div>

<h4 id="troubleshooting">Troubleshooting</h4>

<table>
  <thead>
    <tr>
      <th>Problem</th>
      <th>Cause</th>
      <th>Solution</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>CUDA out of memory</strong></td>
      <td>Batch size too large</td>
      <td>Reduce <code class="language-plaintext highlighter-rouge">--rollout-batch-size</code> and <code class="language-plaintext highlighter-rouge">--train-batch-size</code></td>
    </tr>
    <tr>
      <td><strong>Only 1 GPU detected</strong></td>
      <td>vLLM imported before torch</td>
      <td>Check import order in code</td>
    </tr>
    <tr>
      <td><strong>OOM after manual termination of the training process</strong></td>
      <td>Zombie processes holding GPU memory</td>
      <td>Run <code class="language-plaintext highlighter-rouge">nvidia-smi --query-compute-apps=pid --format=csv,noheader \| xargs -I {} kill -9 {}</code></td>
    </tr>
    <tr>
      <td><strong>vLLM weight load fails</strong></td>
      <td>Wrong vLLM version</td>
      <td>Ensure vLLM 0.6.x or 0.7.x (pinned in pyproject.toml)</td>
    </tr>
  </tbody>
</table>

<p><strong>Memory-saving parameters:</strong></p>

<table>
  <thead>
    <tr>
      <th>Parameter</th>
      <th>Description</th>
      <th>Reduce If OOM</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">--rollout-batch-size</code></td>
      <td>Total responses generated per step</td>
      <td>Yes</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">--train-batch-size</code></td>
      <td>Samples processed per optimizer step</td>
      <td>Yes</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">--gradient-accumulation-steps</code></td>
      <td>Micro-batch size = train_batch / grad_accum</td>
      <td>Increase (smaller micro-batches)</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">--max-seq-length-train</code></td>
      <td>Truncate long sequences</td>
      <td>Yes</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">--group-size</code></td>
      <td>Rollouts per question</td>
      <td>Yes</td>
    </tr>
  </tbody>
</table>

<h3 id="interpreting-training-plots">Interpreting Training Plots</h3>

<p>After training, run <code class="language-plaintext highlighter-rouge">plot_training.py</code> to visualize metrics:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>uv run python scripts/plot_training.py <span class="se">\</span>
    <span class="nt">--input</span> outputs/grpo_model/training_history.json <span class="se">\</span>
    <span class="nt">--output</span> training_plot.png
</code></pre></div></div>

<p>Hereâ€™s an example from our training run on Lambda Cloud:</p>

<p><img src="/assets/picture/2026-02-08-grpo-math-reasoning-lambda-cloud/training_plot.png" alt="GRPO Training Metrics" /></p>

<p>The plot has four panels. Hereâ€™s how to interpret each:</p>

<h4 id="panel-1-average-reward-per-step">Panel 1: Average Reward per Step</h4>

<p><strong>What it shows:</strong> Mean reward across all responses generated at each GRPO step.</p>

<p><strong>Healthy pattern:</strong></p>
<ul>
  <li>Gradual upward trend with noise</li>
  <li>Early steps: reward ~0.1-0.2 (model barely better than random)</li>
  <li>Later steps: reward ~0.3-0.5 (model learning)</li>
</ul>

<p><strong>Problematic patterns:</strong></p>
<ul>
  <li>Flat line: No learning (check rewards, advantages)</li>
  <li>Wild oscillations: Learning rate too high</li>
  <li>Sudden drops: Policy collapse (reduce learning rate or cliprange)</li>
</ul>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Healthy:                     Problematic (flat):
   â–²                            â–²
   â”‚     ....â—â—â—â—               â”‚ â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—
   â”‚  ...â—â—                     â”‚
   â”‚ â—â—â—                        â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º step      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º step
</code></pre></div></div>

<h4 id="panel-2-answer-reward-train-vs-val">Panel 2: Answer Reward (Train vs Val)</h4>

<p><strong>What it shows:</strong> Accuracy on training data (green) and validation data (red).</p>

<p><strong>Healthy pattern:</strong></p>
<ul>
  <li>Both curves trending upward</li>
  <li>Validation slightly below training (normal generalization gap)</li>
  <li>In our run: 6% â†’ 25% accuracy (4Ã— improvement!)</li>
</ul>

<p><strong>Problematic patterns:</strong></p>
<ul>
  <li>Train rising, val flat: Overfitting</li>
  <li>Both flat: Not learning</li>
  <li>Val higher than train: Data leakage or evaluation bug</li>
</ul>

<h4 id="panel-3-policy-gradient-loss">Panel 3: Policy Gradient Loss</h4>

<p><strong>What it shows:</strong> The loss value from the policy gradient objective.</p>

<p><strong>Healthy pattern:</strong></p>
<ul>
  <li>Generally decreasing trend with significant noise</li>
  <li>Fluctuations are normal (policy gradient has high variance)</li>
  <li>Should stabilize, not diverge</li>
</ul>

<p><strong>Problematic patterns:</strong></p>
<ul>
  <li>NaN values: Numerical instability (reduce learning rate)</li>
  <li>Steadily increasing: Wrong sign or bug</li>
  <li>Extremely low variance: Collapsed policy</li>
</ul>

<h4 id="panel-4-reward-range-minmaxmean">Panel 4: Reward Range (Min/Max/Mean)</h4>

<p><strong>What it shows:</strong> For each training step, this panel plots three values:</p>
<ul>
  <li><strong>Max reward</strong> (top of blue area): The best response in the batch (usually 1 = correct)</li>
  <li><strong>Min reward</strong> (bottom of blue area): The worst response in the batch (usually 0 = wrong)</li>
  <li><strong>Mean reward</strong> (green line): Average reward across all responses</li>
</ul>

<p><strong>Why this matters for GRPO:</strong></p>

<p>Remember, GRPO learns by <em>comparing</em> responses within a group. If the model generates 8 responses to a question:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Diverse (good for learning):        Uniform (no learning signal):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Response 1: âœ“ (r=1)     â”‚         â”‚ Response 1: âœ— (r=0)     â”‚
â”‚ Response 2: âœ— (r=0)     â”‚         â”‚ Response 2: âœ— (r=0)     â”‚
â”‚ Response 3: âœ“ (r=1)     â”‚         â”‚ Response 3: âœ— (r=0)     â”‚
â”‚ Response 4: âœ— (r=0)     â”‚         â”‚ Response 4: âœ— (r=0)     â”‚
â”‚ ...                     â”‚         â”‚ ...                     â”‚
â”‚ min=0, max=1, mean=0.5  â”‚         â”‚ min=0, max=0, mean=0    â”‚
â”‚                         â”‚         â”‚                         â”‚
â”‚ â†’ Advantages exist!     â”‚         â”‚ â†’ All advantages = 0    â”‚
â”‚ â†’ Model can learn       â”‚         â”‚ â†’ Nothing to learn from â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
</code></pre></div></div>

<p><strong>Healthy pattern:</strong></p>
<ul>
  <li>Blue shaded area spans from 0 to 1 â†’ Some responses correct, some wrong</li>
  <li>Mean line gradually rises â†’ Model getting better over time</li>
  <li>Gap between min and max persists â†’ Model is still exploring, still learning</li>
</ul>

<p><strong>Problematic patterns:</strong></p>

<table>
  <thead>
    <tr>
      <th>Pattern</th>
      <th>What You See</th>
      <th>What It Means</th>
      <th>Fix</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Range collapsed to 0</td>
      <td>Blue area stuck at bottom</td>
      <td>All responses wrong, no correct examples to reinforce</td>
      <td>Problems too hard, or temperature too low (model not exploring)</td>
    </tr>
    <tr>
      <td>Range collapsed to 1</td>
      <td>Blue area stuck at top</td>
      <td>All responses correct, nothing to discourage</td>
      <td>Problems too easy, no learning signal</td>
    </tr>
    <tr>
      <td>Mean not rising</td>
      <td>Green line flat</td>
      <td>Model not improving despite having diverse responses</td>
      <td>Check loss function, learning rate, or reward calculation</td>
    </tr>
  </tbody>
</table>

<h3 id="evaluation-results-base-model-vs-grpo-trained">Evaluation Results: Base Model vs GRPO-Trained</h3>

<p>After training, we evaluated both the base Qwen2.5-Math-1.5B model and our GRPO-trained model on 500 math problems from the MATH dataset. Hereâ€™s the comparison:</p>

<table>
  <thead>
    <tr>
      <th>Metric</th>
      <th>Base Model</th>
      <th>GRPO Model</th>
      <th>Change</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Correct answers</strong></td>
      <td>69 (13.8%)</td>
      <td>205 (41.0%)</td>
      <td><strong>+136 (+197%)</strong></td>
    </tr>
    <tr>
      <td><strong>Correct format, wrong answer</strong></td>
      <td>122 (24.4%)</td>
      <td>170 (34.0%)</td>
      <td>+48</td>
    </tr>
    <tr>
      <td><strong>Bad format (couldnâ€™t parse)</strong></td>
      <td>309 (61.8%)</td>
      <td>125 (25.0%)</td>
      <td>-184</td>
    </tr>
  </tbody>
</table>

<p><strong>Key improvements:</strong></p>

<ol>
  <li><strong>3Ã— accuracy improvement</strong> â€” From 13.8% to 41.0% correct answers</li>
  <li><strong>Format compliance</strong> â€” Bad format responses dropped from 61.8% to 25.0%</li>
  <li><strong>Learning to reason</strong> â€” The model learned to show work and box final answers</li>
</ol>

<h4 id="example-improvements">Example improvements</h4>

<p><strong>Problem 1: Polar coordinates</strong></p>
<blockquote>
  <p>Convert the point $(0, -3 \sqrt{3}, 3)$ from rectangular to spherical coordinates.</p>
</blockquote>

<ul>
  <li><strong>Base model:</strong> <code class="language-plaintext highlighter-rouge">$(6, \frac{2\pi}{3}, \pi)$</code> âŒ (wrong angles, no <code class="language-plaintext highlighter-rouge">\boxed{}</code>)</li>
  <li><strong>GRPO model:</strong> <code class="language-plaintext highlighter-rouge">$\boxed{(6, \frac{5\pi}{3}, \frac{2\pi}{3})}$</code> âœ“ (correct, properly boxed)</li>
</ul>

<p><strong>Problem 2: Double sum</strong></p>
<blockquote>
  <p>Compute $\sum_{j = 0}^\infty \sum_{k = 0}^\infty 2^{-3k - j - (k + j)^2}$.</p>
</blockquote>

<ul>
  <li><strong>Base model:</strong> <code class="language-plaintext highlighter-rouge">$\frac{4}{3}$</code> âŒ (no work shown, unboxed)</li>
  <li><strong>GRPO model:</strong> Step-by-step derivation â†’ <code class="language-plaintext highlighter-rouge">$\boxed{\frac{4}{3}}$</code> âœ“</li>
</ul>

<p><strong>Problem 3: Function evaluation</strong></p>
<blockquote>
  <p>Given $f(x) = \frac{x^5-1}{3}$, find $f^{-1}(-31/96)$.</p>
</blockquote>

<ul>
  <li><strong>Base model:</strong> <code class="language-plaintext highlighter-rouge">$-31/96$</code> âŒ (returned input, not inverse)</li>
  <li><strong>GRPO model:</strong> Derived inverse function â†’ <code class="language-plaintext highlighter-rouge">$\boxed{\frac{1}{2}}$</code> âœ“</li>
</ul>

<p>These examples show that GRPO training taught the model to:</p>
<ul>
  <li>Follow the expected format (<code class="language-plaintext highlighter-rouge">\boxed{}</code> for final answers)</li>
  <li>Show intermediate reasoning steps</li>
  <li>Actually compute answers rather than pattern-matching</li>
</ul>

<h3 id="summary-and-key-takeaways">Summary and Key Takeaways</h3>

<table>
  <thead>
    <tr>
      <th>Concept</th>
      <th>Implementation</th>
      <th>Why It Matters</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Group normalization</strong></td>
      <td><code class="language-plaintext highlighter-rouge">A = (r - Î¼) / Ïƒ</code> computed per question</td>
      <td>Natural baseline without value network</td>
    </tr>
    <tr>
      <td><strong>Response masking</strong></td>
      <td>Loss computed on response tokens only</td>
      <td>Donâ€™t reinforce the prompt</td>
    </tr>
    <tr>
      <td><strong>2-GPU architecture</strong></td>
      <td>vLLM on GPU 0, policy on GPU 1</td>
      <td>Avoid memory contention</td>
    </tr>
    <tr>
      <td><strong>Gradient checkpointing</strong></td>
      <td><code class="language-plaintext highlighter-rouge">policy.gradient_checkpointing_enable()</code></td>
      <td>Reduce memory 2-3Ã—</td>
    </tr>
    <tr>
      <td><strong>Off-policy training</strong></td>
      <td>Multiple gradient steps per rollout batch</td>
      <td>More efficient data usage</td>
    </tr>
  </tbody>
</table>

<p><strong>Quick reference - key hyperparameters:</strong></p>

<table>
  <thead>
    <tr>
      <th>Parameter</th>
      <th>Default</th>
      <th>Effect</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">group_size</code> (G)</td>
      <td>8</td>
      <td>More diversity â†’ better baseline estimates</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">learning_rate</code></td>
      <td>1e-5</td>
      <td>Higher â†’ faster but unstable</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">cliprange</code> (Îµ)</td>
      <td>0.2</td>
      <td>Higher â†’ more aggressive updates</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">gradient_accumulation_steps</code></td>
      <td>128</td>
      <td>Higher â†’ more stable gradients</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">epochs_per_rollout_batch</code></td>
      <td>1</td>
      <td>Higher â†’ more off-policy (needs clipping)</td>
    </tr>
  </tbody>
</table>

<p><strong>Next steps :</strong></p>

<ol>
  <li><strong>Experiment:</strong> Try different group sizes (4, 8, 16) and compare learning curves</li>
  <li><strong>Extend:</strong> Add your own reward functions for different tasks</li>
  <li><strong>Scale up:</strong> Try larger models (7B) with 4-GPU setups â€” larger models have more capacity to learn complex reasoning patterns and often start with stronger base capabilities. A 7B model needs ~14GB for weights alone, plus ~28GB for optimizer states, so youâ€™ll need 4 GPUs: 2 for vLLM inference (tensor parallelism) and 2 for policy training</li>
</ol>

<p>The math may seem daunting, but the core ideas are simple: sample multiple responses, compare them to each other, reinforce the good ones and avoid the bad ones. Thatâ€™s really all there is to GRPO!</p>

<hr />

<p><strong>Resources:</strong></p>
<ul>
  <li><a href="https://github.com/bearbearyu1223/qwen_math_grpo">GRPO Training Code (this noteâ€™s implementation)</a></li>
  <li><a href="https://huggingface.co/datasets/hendrycks/competition_math">MATH Dataset on HuggingFace</a> â€” 12,500 competition math problems</li>
  <li><a href="https://arxiv.org/abs/2402.03300">DeepSeekMath Paper</a> â€” Original GRPO formulation</li>
  <li><a href="https://arxiv.org/abs/2501.12948">DeepSeek-R1 Paper</a> â€” GRPO at scale</li>
  <li><a href="https://stanford-cs336.github.io/spring2025/">Stanford CS336: Language Modeling from Scratch</a></li>
  <li><a href="https://lambda.ai/service/gpu-cloud">Lambda Cloud</a> â€” GPU instances for training</li>
</ul>]]></content><author><name>[&quot;Han Yu&quot;]</name></author><category term="cs336" /><summary type="html"><![CDATA[Training Math Reasoning Models with GRPO on Lambda Cloud with 2xH100s]]></summary></entry><entry><title type="html">Study Notes: Stanford CS336 Language Modeling from Scratch [14]</title><link href="http://localhost:4000/cs336/2026/01/25/cs336-reinforcement-learning-for-language-model.html" rel="alternate" type="text/html" title="Study Notes: Stanford CS336 Language Modeling from Scratch [14]" /><published>2026-01-25T00:00:00-08:00</published><updated>2026-01-25T00:00:00-08:00</updated><id>http://localhost:4000/cs336/2026/01/25/cs336-reinforcement-learning-for-language-model</id><content type="html" xml:base="http://localhost:4000/cs336/2026/01/25/cs336-reinforcement-learning-for-language-model.html"><![CDATA[<h2 id="a-beginners-guide-to-reinforcement-learning-for-language-models">A Beginnerâ€™s Guide to Reinforcement Learning for Language Models</h2>

<p>Recent breakthroughs in AI reasoningâ€”like DeepSeek R1 and OpenAIâ€™s o1â€”have been powered by reinforcement learning (RL). But if youâ€™re new to RL, the math can feel intimidating. Terms like â€œpolicy gradients,â€ â€œbaselines,â€ and â€œimportance samplingâ€ get thrown around, and the equations look like alphabet soup.</p>

<p>In this note, I am trying to break down the core concepts of RL for language models in plain English, with simple examples and step-by-step explanations of a few key formulas.</p>

<p><em>This guide is based on my study notes from Stanford CS336 and resources like OpenAIâ€™s Spinning Up in Deep RL and Nathan Lambertâ€™s RLHF Book.</em></p>

<h3 id="table-of-contents">Table of Contents</h3>
<ul>
  <li><a href="#a-beginners-guide-to-reinforcement-learning-for-language-models">A Beginnerâ€™s Guide to Reinforcement Learning for Language Models</a>
    <ul>
      <li><a href="#table-of-contents">Table of Contents</a></li>
      <li><a href="#the-big-picture-training-dogs-and-language-models">The Big Picture: Training Dogs and Language Models</a></li>
      <li><a href="#part-1-language-models-as-policies">Part 1: Language Models as Policies</a>
        <ul>
          <li><a href="#what-is-a-policy">What is a Policy?</a></li>
          <li><a href="#the-two-operations-you-need">The Two Operations You Need</a></li>
        </ul>
      </li>
      <li><a href="#part-2-trajectories--recording-the-journey">Part 2: Trajectories â€” Recording the Journey</a>
        <ul>
          <li><a href="#what-is-a-trajectory">What is a Trajectory?</a></li>
          <li><a href="#a-concrete-example">A Concrete Example</a></li>
        </ul>
      </li>
      <li><a href="#part-3-rewards-and-returns--measuring-success">Part 3: Rewards and Returns â€” Measuring Success</a>
        <ul>
          <li><a href="#the-reward-function">The Reward Function</a></li>
          <li><a href="#the-return-adding-up-rewards">The Return: Adding Up Rewards</a></li>
          <li><a href="#the-objective-maximize-expected-return">The Objective: Maximize Expected Return</a></li>
        </ul>
      </li>
      <li><a href="#part-4-the-policy-gradient-vanilla-reinforce">Part 4: The Policy Gradient (Vanilla REINFORCE)</a>
        <ul>
          <li><a href="#the-key-equation">The Key Equation</a></li>
          <li><a href="#symbol-by-symbol-breakdown">Symbol-by-Symbol Breakdown</a></li>
          <li><a href="#the-log-derivative-trick-why-the-math-works">The Log-Derivative Trick: Why the Math Works</a></li>
          <li><a href="#deriving-the-policy-gradient-step-by-step">Deriving the Policy Gradient Step-by-Step</a></li>
          <li><a href="#intuitive-summary">Intuitive Summary</a></li>
        </ul>
      </li>
      <li><a href="#part-5-baselines--reducing-the-noise">Part 5: Baselines â€” Reducing the Noise</a>
        <ul>
          <li><a href="#the-problem-with-vanilla-reinforce">The Problem with Vanilla REINFORCE</a></li>
          <li><a href="#the-solution-subtract-a-baseline">The Solution: Subtract a Baseline</a></li>
          <li><a href="#a-concrete-example-1">A Concrete Example</a></li>
          <li><a href="#why-baselines-dont-add-bias">Why Baselines Donâ€™t Add Bias</a></li>
        </ul>
      </li>
      <li><a href="#part-6-off-policy-learning--reusing-old-data">Part 6: Off-Policy Learning â€” Reusing Old Data</a>
        <ul>
          <li><a href="#the-inefficiency-of-on-policy-learning">The Inefficiency of On-Policy Learning</a></li>
          <li><a href="#the-solution-importance-sampling">The Solution: Importance Sampling</a></li>
          <li><a href="#a-concrete-example-2">A Concrete Example</a></li>
          <li><a href="#the-catch-dont-stray-too-far">The Catch: Donâ€™t Stray Too Far</a></li>
        </ul>
      </li>
      <li><a href="#part-7-grpo--group-relative-policy-optimization">Part 7: GRPO â€” Group Relative Policy Optimization</a>
        <ul>
          <li><a href="#the-core-idea-compare-siblings">The Core Idea: Compare Siblings</a></li>
          <li><a href="#step-1-sample-multiple-outputs-per-question">Step 1: Sample Multiple Outputs Per Question</a></li>
          <li><a href="#step-2-compute-group-normalized-advantages">Step 2: Compute Group-Normalized Advantages</a></li>
          <li><a href="#step-3-the-grpo-clip-objective">Step 3: The GRPO-Clip Objective</a></li>
          <li><a href="#why-clipping-matters">Why Clipping Matters</a></li>
          <li><a href="#the-grpo-training-loop-visualized">The GRPO Training Loop Visualized</a></li>
          <li><a href="#the-complete-grpo-algorithm">The Complete GRPO Algorithm</a></li>
        </ul>
      </li>
      <li><a href="#part-8-putting-it-all-together">Part 8: Putting It All Together</a>
        <ul>
          <li><a href="#summary-table">Summary Table</a></li>
          <li><a href="#key-equations-at-a-glance">Key Equations at a Glance</a></li>
          <li><a href="#the-pytorch-connection">The PyTorch Connection</a></li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h3 id="the-big-picture-training-dogs-and-language-models">The Big Picture: Training Dogs and Language Models</h3>

<p>Before diving into equations, letâ€™s build intuition with an analogy.</p>

<p><strong>Imagine youâ€™re training a dog to do tricks.</strong> You canâ€™t tell the dog exactly which muscles to moveâ€”you can only reward it when it does something good. Over time, the dog learns to repeat actions that led to treats and avoid actions that didnâ€™t.</p>

<p>Reinforcement learning for language models works the same way:</p>

<table>
  <thead>
    <tr>
      <th>Dog Training</th>
      <th>LLM Training</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Dog decides what action to take</td>
      <td>LLM decides what token to generate</td>
    </tr>
    <tr>
      <td>You give a treat (or not)</td>
      <td>Reward function gives a score (0 or 1)</td>
    </tr>
    <tr>
      <td>Dog repeats actions that got treats</td>
      <td>LLM increases probability of tokens that led to rewards</td>
    </tr>
  </tbody>
</table>

<p>The key insight: <strong>we donâ€™t tell the model what to generate (e.g., what is the groundtruth)â€”we just tell it whether its answer was good or bad, and it figures out the rest.</strong></p>

<h3 id="part-1-language-models-as-policies">Part 1: Language Models as Policies</h3>

<h4 id="what-is-a-policy">What is a Policy?</h4>

<p>In RL terminology, a <strong>policy</strong> is just a decision-making strategy. For language models:</p>

<ul>
  <li><strong>State</strong> ($s_t$): The text generated so far (the context, or the prefix)</li>
  <li><strong>Action</strong> ($a_t$): The next token to generate</li>
  <li><strong>Policy</strong> ($\pi_\theta$): The probability distribution over possible next tokens</li>
</ul>

<p>Your LLM is a policy! Given a text prefix, it outputs probabilities for each possible next token:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>State:  "The capital of France is"
Policy: {"Paris": 0.85, "Lyon": 0.05, "the": 0.03, ...}
Action: Sample from this distribution â†’ "Paris"
</code></pre></div></div>

<p>Mathematically, we write this as:</p>

\[a_t \sim \pi_\theta(\cdot | s_t)\]

<p>This reads: â€œaction $a_t$ is sampled from the policy $\pi_\theta$ given state $s_t$.â€</p>

<h4 id="the-two-operations-you-need">The Two Operations You Need</h4>

<p>To train a policy with RL, you only need two operations:</p>

<table>
  <thead>
    <tr>
      <th>Operation</th>
      <th>What It Does</th>
      <th>Example</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Sampling</strong></td>
      <td>Draw a token from the probability distribution</td>
      <td>Pick â€œParisâ€ with 85% probability-the highest probability</td>
    </tr>
    <tr>
      <td><strong>Scoring</strong></td>
      <td>Compute the log-probability of a token</td>
      <td>$\log \pi_\theta(\text{â€œParisâ€} \mid s_t) = \log(0.85) \approx -0.16$</td>
    </tr>
  </tbody>
</table>

<p>Thatâ€™s it! You donâ€™t need to know anything else about the modelâ€™s internals.</p>

<h3 id="part-2-trajectories--recording-the-journey">Part 2: Trajectories â€” Recording the Journey</h3>

<h4 id="what-is-a-trajectory">What is a Trajectory?</h4>

<p>A <strong>trajectory</strong> (also called an episode or rollout) is the complete sequence of states and actions from start to finish:</p>

\[\tau = (s_0, a_0, s_1, a_1, \ldots, s_T, a_T)\]

<p>Think of it like recording a chess game move-by-moveâ€”you capture everything that happened.</p>

<h4 id="a-concrete-example">A Concrete Example</h4>

<p>Letâ€™s trace a trajectory for a math problem:</p>

<p><strong>Prompt:</strong> â€œWhat is 2+3? Think step by step.â€</p>

<table>
  <thead>
    <tr>
      <th>Timestep</th>
      <th>State ($s_t$)</th>
      <th>Action ($a_t$)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>â€œWhat is 2+3? Think step by step. <think>"</think></td>
      <td>â€œIâ€</td>
    </tr>
    <tr>
      <td>1</td>
      <td>â€œâ€¦ <think> I"</think></td>
      <td>â€œneedâ€</td>
    </tr>
    <tr>
      <td>2</td>
      <td>â€œâ€¦ <think> I need"</think></td>
      <td>â€œtoâ€</td>
    </tr>
    <tr>
      <td>3</td>
      <td>â€œâ€¦ <think> I need to"</think></td>
      <td>â€œaddâ€</td>
    </tr>
    <tr>
      <td>â€¦</td>
      <td>â€¦</td>
      <td>â€¦</td>
    </tr>
    <tr>
      <td>T</td>
      <td>â€œâ€¦ &lt;/think&gt; <answer>"</answer></td>
      <td>â€œ5â€</td>
    </tr>
    <tr>
      <td>T+1</td>
      <td>â€œâ€¦ <answer> 5"</answer></td>
      <td>â€&lt;/answer&gt;â€</td>
    </tr>
  </tbody>
</table>

<p>The trajectory ends when the model emits an end-of-text token (like <code class="language-plaintext highlighter-rouge">&lt;/answer&gt;</code>) or hits a maximum length.</p>

<p><strong>Key observation:</strong> In LLM-land, the â€œenvironmentâ€ is trivially deterministic. The next state is just the old state plus the new token:</p>

\[s_{t+1} = s_t | a_t\]

<p>(where $|$ means concatenation)</p>

<h3 id="part-3-rewards-and-returns--measuring-success">Part 3: Rewards and Returns â€” Measuring Success</h3>

<h4 id="the-reward-function">The Reward Function</h4>

<p>The <strong>reward</strong> $r_t = R(s_t, a_t)$ judges how good an action was. For RL on math problems, we typically use <strong>sparse rewards</strong>:</p>

<ul>
  <li><strong>Intermediate steps:</strong> $r_t = 0$ (no feedback until the end)</li>
  <li><strong>Final answer:</strong> $r_T = 1$ if correct, $0$ if wrong</li>
</ul>

<p><strong>Example:</strong></p>

<table>
  <thead>
    <tr>
      <th>Trajectory</th>
      <th>Final Answer</th>
      <th>Correct?</th>
      <th>Reward</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>â€œâ€¦ <answer>5</answer>â€</td>
      <td>5</td>
      <td>âœ“</td>
      <td>1</td>
    </tr>
    <tr>
      <td>â€œâ€¦ <answer>6</answer>â€</td>
      <td>6</td>
      <td>âœ—</td>
      <td>0</td>
    </tr>
  </tbody>
</table>

<h4 id="the-return-adding-up-rewards">The Return: Adding Up Rewards</h4>

<p>The <strong>return</strong> $R(\tau)$ is the total reward accumulated over a trajectory:</p>

\[R(\tau) = \sum_{t=0}^{T} r_t\]

<p>With sparse rewards, only the final step matters, so $R(\tau)$ equals the terminal reward (0 or 1).</p>

<h4 id="the-objective-maximize-expected-return">The Objective: Maximize Expected Return</h4>

<p>The goal of RL is to find policy parameters $\theta$ that maximize expected return:</p>

\[J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta}[R(\tau)]\]

<p>In plain English: <strong>â€œOn average, how much reward does my policy get?â€</strong></p>

<p>If $J(\theta) = 0.7$, that means your model solves 70% of problems correctly.</p>

<h3 id="part-4-the-policy-gradient-vanilla-reinforce">Part 4: The Policy Gradient (Vanilla REINFORCE)</h3>

<p>Now we get to the heart of RL: how do we actually improve the policy?</p>

<h4 id="the-key-equation">The Key Equation</h4>

<p>The <strong>Vanilla REINFORCE policy gradient</strong> tells us how to update parameters:</p>

\[\nabla_\theta J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta}\left[\sum_{t=0}^{T} \nabla_\theta \log \pi_\theta(a_t|s_t) \cdot R(\tau)\right]\]

<p>This looks complex, but the intuition is simple: <strong>increase the probability of actions that led to high rewards.</strong></p>

<h4 id="symbol-by-symbol-breakdown">Symbol-by-Symbol Breakdown</h4>

<p>Let me explain every symbol in this equation:</p>

<table>
  <thead>
    <tr>
      <th>Symbol</th>
      <th>Name</th>
      <th>Meaning</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>$J(\theta)$</td>
      <td>Objective function</td>
      <td>Expected total rewardâ€”the thing we want to maximize</td>
    </tr>
    <tr>
      <td>$\theta$</td>
      <td>Parameters</td>
      <td>All the weights in your language model (millions of numbers)</td>
    </tr>
    <tr>
      <td>$\nabla_\theta J$</td>
      <td>Gradient</td>
      <td>â€œWhich direction should I nudge each parameter to increase J - the expected total reward?â€</td>
    </tr>
    <tr>
      <td>$\mathbb{E}[\cdot]$</td>
      <td>Expectation</td>
      <td>Average over many samples</td>
    </tr>
    <tr>
      <td>$\tau$</td>
      <td>Trajectory</td>
      <td>One complete episode (e.g., prompt â†’ response â†’ end)</td>
    </tr>
    <tr>
      <td>$\tau \sim \pi_\theta$</td>
      <td>Sampling</td>
      <td>Generate trajectories by running the policy</td>
    </tr>
    <tr>
      <td>$\sum_t$</td>
      <td>Sum over timesteps</td>
      <td>Add up contributions from every token</td>
    </tr>
    <tr>
      <td>$s_t$</td>
      <td>State</td>
      <td>Text prefix at timestep $t$</td>
    </tr>
    <tr>
      <td>$a_t$</td>
      <td>Action</td>
      <td>Token generated at timestep $t$</td>
    </tr>
    <tr>
      <td>$\pi_\theta(a_t \mid s_t)$</td>
      <td>Probability</td>
      <td>How likely was this token given this context?</td>
    </tr>
    <tr>
      <td>$\log \pi_\theta(a_t \mid s_t)$</td>
      <td>Log-probability</td>
      <td>Same info, but in log space (more stable)</td>
    </tr>
    <tr>
      <td>$\nabla_\theta \log \pi_\theta(a_t \mid s_t)$</td>
      <td>Score function*</td>
      <td>Gradient of the log-probability; points in the direction that increases this tokenâ€™s probability</td>
    </tr>
    <tr>
      <td>$R(\tau)$</td>
      <td>Return</td>
      <td>Total reward for this trajectory (0 or 1)</td>
    </tr>
  </tbody>
</table>

<p><strong>Note on terminology</strong>: The name â€œscore function*â€ comes from statistics, despite â€œscoreâ€ sounding like a scalar, the score function is a vector pointing in the direction of steepest increase for the log-probability.</p>

<h4 id="the-log-derivative-trick-why-the-math-works">The Log-Derivative Trick: Why the Math Works</h4>

<p>The magic behind policy gradients is a simple calculus identity:</p>

\[\nabla_\theta P = P \cdot \nabla_\theta \log P\]

<p>This comes from the chain rule for logarithms:</p>

\[\frac{d}{d\theta} \log P = \frac{1}{P} \cdot \frac{d}{d\theta} P\]

<p>Rearranging:</p>

\[\frac{d}{d\theta} P = P \cdot \frac{d}{d\theta} \log P\]

<p><strong>Why is this useful?</strong> It lets us convert â€œgradient of an expectationâ€ into â€œexpectation of a gradientâ€â€”which we can estimate by sampling!</p>

<p><strong>Numerical example:</strong></p>

<p>Suppose $P(a) = 0.3$ is the probability of some action.</p>

<p>Direct gradient: $\nabla_\theta P = 1$ (some value)</p>

<p>Using the trick:</p>
<ul>
  <li>$\nabla_\theta \log P = \nabla_\theta \log(0.3) = \frac{1}{0.3} \cdot \nabla_\theta P = 3.33 \cdot 1$</li>
  <li>$P \cdot \nabla_\theta \log P = 0.3 \times 3.33 = 1$ âœ“</li>
</ul>

<p>Same answer! The trick is just a rearrangement that makes computation easier.</p>

<h4 id="deriving-the-policy-gradient-step-by-step">Deriving the Policy Gradient Step-by-Step</h4>

<p>Letâ€™s derive the REINFORCE equation from scratch.</p>

<p><strong>Step 1: Write out the expectation explicitly</strong></p>

\[J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta}[R(\tau)] = \sum_{\tau} P(\tau | \theta) \cdot R(\tau)\]

<p>This sums over all possible trajectories, weighted by their probability.</p>

<p><strong>Step 2: Take the gradient</strong></p>

\[\nabla_\theta J(\theta) = \sum_{\tau} \nabla_\theta P(\tau | \theta) \cdot R(\tau)\]

<p>Note: $R(\tau)$ doesnâ€™t depend on $\theta$ (itâ€™s just â€œwas the answer correct?â€)</p>

<p><strong>Step 3: Apply the log-derivative trick</strong></p>

\[\nabla_\theta J(\theta)= \sum_{\tau} P(\tau | \theta) \cdot \nabla_\theta \log P(\tau | \theta) \cdot R(\tau)\]

<p><strong>Step 4: Recognize this as an expectation</strong></p>

\[\nabla_\theta J(\theta)= \mathbb{E}_{\tau \sim \pi_\theta}\left[\nabla_\theta \log P(\tau | \theta) \cdot R(\tau)\right]\]

<p><strong>Step 5: Expand the trajectory probability</strong></p>

<p>A trajectoryâ€™s probability is:</p>

\[P(\tau | \theta) = \underbrace{\rho_0(s_0)}_{\text{initial prompt}} \cdot \prod_{t=0}^{T} \underbrace{P(s_{t+1}|s_t, a_t)}_{\text{environment}} \cdot \underbrace{\pi_\theta(a_t|s_t)}_{\text{policy}}\]

<p>Taking the log:</p>

\[\log P(\tau | \theta) = \log \rho_0(s_0) + \sum_t \log P(s_{t+1}|s_t, a_t) + \sum_t \log \pi_\theta(a_t|s_t)\]

<p>When we take $\nabla_\theta$, the first two terms vanish (they donâ€™t depend on $\theta$):</p>

\[\nabla_\theta \log P(\tau | \theta) = \sum_{t=0}^{T} \nabla_\theta \log \pi_\theta(a_t|s_t)\]

<p><strong>Final result:</strong></p>

\[\nabla_\theta J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta}\left[\sum_{t=0}^{T} \nabla_\theta \log \pi_\theta(a_t|s_t) \cdot R(\tau)\right]\]

<h4 id="intuitive-summary">Intuitive Summary</h4>

<p>The policy gradient says:</p>

<ol>
  <li><strong>Sample trajectories</strong> by running your policy</li>
  <li><strong>For each token</strong>, compute â€œhow to make it more likelyâ€ ($\nabla_\theta \log \pi_\theta$)</li>
  <li><strong>Scale by the reward</strong> â€” good outcomes get reinforced, bad ones donâ€™t</li>
  <li><strong>Average across trajectories</strong></li>
</ol>

<h3 id="part-5-baselines--reducing-the-noise">Part 5: Baselines â€” Reducing the Noise</h3>

<h4 id="the-problem-with-vanilla-reinforce">The Problem with Vanilla REINFORCE</h4>

<p>Vanilla REINFORCE has <strong>high variance</strong>. Hereâ€™s why:</p>

<p>Suppose your model already solves 90% of problems. Most trajectories get $R(\tau) = 1$, so the gradient says â€œincrease probability of these tokens!â€ even for trajectories that succeeded by luck.</p>

<p>The signal is noisyâ€”sometimes youâ€™re reinforcing good reasoning, sometimes just lucky guesses.</p>

<h4 id="the-solution-subtract-a-baseline">The Solution: Subtract a Baseline</h4>

<p>The fix is to subtract a <strong>baseline</strong> $b(s)$ that estimates â€œwhat return do we typically get?â€:</p>

\[\nabla_\theta J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta}\left[\sum_t \nabla_\theta \log \pi_\theta(a_t|s_t) \cdot \underbrace{(R(\tau) - b(s_t))}_{\text{advantage}}\right]\]

<p>The quantity $(R(\tau) - b(s_t))$ is called the <strong>advantage</strong>:</p>

<table>
  <thead>
    <tr>
      <th>Advantage</th>
      <th>Meaning</th>
      <th>Effect</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Positive</td>
      <td>Better than expected</td>
      <td>Reinforce these tokens</td>
    </tr>
    <tr>
      <td>Negative</td>
      <td>Worse than expected</td>
      <td>Discourage these tokens</td>
    </tr>
    <tr>
      <td>Zero</td>
      <td>Exactly as expected</td>
      <td>No change</td>
    </tr>
  </tbody>
</table>

<h4 id="a-concrete-example-1">A Concrete Example</h4>

<p><strong>Without baseline:</strong></p>

<table>
  <thead>
    <tr>
      <th>Trajectory</th>
      <th>$R(\tau)$</th>
      <th>Gradient Signal</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Correct</td>
      <td>1</td>
      <td>Make these tokens more likely!</td>
    </tr>
    <tr>
      <td>Wrong</td>
      <td>0</td>
      <td>Do nothing</td>
    </tr>
  </tbody>
</table>

<p>Every correct answer gets the same reinforcement, regardless of difficulty; every wrong answer gets no punishment. The model only learns from successes.
This is actually a key limitation of vanilla REINFORCE with 0/1 rewards! Youâ€™re not learning what to avoid, only what worked.</p>

<p><strong>With baseline</strong> (say, $b = 0.9$ because model gets 90% right):</p>

<table>
  <thead>
    <tr>
      <th>Trajectory</th>
      <th>$R(\tau)$</th>
      <th>Advantage = $R - 0.9$</th>
      <th>Gradient Signal</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Correct</td>
      <td>1</td>
      <td>+0.1</td>
      <td>â€œSlightly reinforceâ€</td>
    </tr>
    <tr>
      <td>Wrong</td>
      <td>0</td>
      <td>-0.9</td>
      <td>â€œStrongly discourage!â€</td>
    </tr>
  </tbody>
</table>

<p>Now the model can also learn avoiding failures rather than redundantly reinforcing successes!</p>

<h4 id="why-baselines-dont-add-bias">Why Baselines Donâ€™t Add Bias</h4>

<p>You might worry: â€œDoesnâ€™t subtracting something change the answer?â€</p>

<p>No! The baseline term vanishes in expectation. Letâ€™s prove it.</p>

<p><strong>The claim:</strong> For any baseline $b(s)$ that only depends on the state:</p>

\[\mathbb{E}_{a \sim \pi_\theta}[\nabla_\theta \log \pi_\theta(a|s) \cdot b(s)] = 0\]

<p><strong>The proof:</strong></p>

<p>Since $b(s)$ doesnâ€™t depend on the action $a$, we can pull it out:</p>

\[\mathbb{E}_{a \sim \pi_\theta}[\nabla_\theta \log \pi_\theta(a|s) \cdot b(s)]= b(s) \cdot \mathbb{E}_{a \sim \pi_\theta}[\nabla_\theta \log \pi_\theta(a|s)]\]

<p>Now we show the expectation of the score function is zero:</p>

\[\mathbb{E}_{a \sim \pi_\theta}[\nabla_\theta \log \pi_\theta(a|s)] = \sum_{a} \pi_\theta(a|s) \cdot \nabla_\theta \log \pi_\theta(a|s)\]

<p>Using the identity $\nabla_\theta \log P = \frac{\nabla_\theta P}{P}$:</p>

\[\mathbb{E}_{a \sim \pi_\theta}[\nabla_\theta \log \pi_\theta(a|s)]= \sum_{a} \pi_\theta(a|s) \cdot \frac{\nabla_\theta \pi_\theta(a|s)}{\pi_\theta(a|s)} = \sum_{a} \nabla_\theta \pi_\theta(a|s)\]

<p>Swapping sum and gradient:</p>

\[\mathbb{E}_{a \sim \pi_\theta}[\nabla_\theta \log \pi_\theta(a|s)]= \nabla_\theta \sum_{a} \pi_\theta(a|s) = \nabla_\theta 1 = 0\]

<p>The last step works because probabilities over all possible actions that can be taken sum to 1.</p>

<p><strong>Concrete example with softmax as the policy function:</strong></p>

<p>Letâ€™s work through a real example. Considering a language model, where token probabilities come from softmax (policy) over logits:</p>

\[\pi(a) = \frac{e^{z_a}}{\sum_k e^{z_k}}\]

<p>The log-probability simplifies nicely:</p>

\[\log \pi(a) = z_a - \log \sum_k e^{z_k}\]

<p>Taking gradients with respect to each logit $z_i$:</p>

<table>
  <thead>
    <tr>
      <th>Gradient</th>
      <th>Formula</th>
      <th>Intuition</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>$\frac{\partial \log \pi(a)}{\partial z_a}$</td>
      <td>$1 - \pi(a)$</td>
      <td>Increasing own logit (or probability) helps (less help if already confident with high probability)</td>
    </tr>
    <tr>
      <td>$\frac{\partial \log \pi(a)}{\partial z_b}$</td>
      <td>$-\pi(b)$</td>
      <td>Increasing competitorâ€™s logit (or probability) hurts</td>
    </tr>
  </tbody>
</table>

<p><em>Derivation:</em> For the chosen token, $\frac{\partial}{\partial z_a}[z_a - \log\sum_k e^{z_k}] = 1 - \frac{e^{z_a}}{\sum_k e^{z_k}} = 1 - \pi(a)$. For other tokens, $\frac{\partial}{\partial z_b}[z_a - \log\sum_k e^{z_k}] = 0 - \frac{e^{z_b}}{\sum_k e^{z_k}} = -\pi(b)$.</p>

<p><strong>Numerical example:</strong></p>

<p>Suppose we have 3 tokens with probabilities $[\pi(A), \pi(B), \pi(C)] = [0.5, 0.3, 0.2]$.</p>

<p>The score function for each token (as a vector over logits $[z_A, z_B, z_C]$):</p>

<table>
  <thead>
    <tr>
      <th>Token</th>
      <th>Score function $\nabla_z \log \pi$</th>
      <th>Weighted by $\pi$</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>A</td>
      <td>$[1-0.5, -0.3, -0.2] = [+0.5, -0.3, -0.2]$</td>
      <td>$0.5 \times [+0.5, -0.3, -0.2] = [+0.25, -0.15, -0.10]$</td>
    </tr>
    <tr>
      <td>B</td>
      <td>$[-0.5, 1-0.3, -0.2] = [-0.5, +0.7, -0.2]$</td>
      <td>$0.3 \times [-0.5, +0.7, -0.2] = [-0.15, +0.21, -0.06]$</td>
    </tr>
    <tr>
      <td>C</td>
      <td>$[-0.5, -0.3, 1-0.2] = [-0.5, -0.3, +0.8]$</td>
      <td>$0.2 \times [-0.5, -0.3, +0.8] = [-0.10, -0.06, +0.16]$</td>
    </tr>
    <tr>
      <td><strong>Sum</strong></td>
      <td>Â </td>
      <td>$[0, 0, 0]$ âœ“</td>
    </tr>
  </tbody>
</table>

<p>Each component sums to zero! For example, the first component: $0.25 - 0.15 - 0.10 = 0$.</p>

<p>The â€œincrease my probabilityâ€ directions (positive entries) are exactly canceled by the â€œdecrease othersâ€™ probabilityâ€ directions (negative entries) when weighted by the policy.</p>

<p><strong>Why this matters:</strong></p>

<p>We can subtract any function of the state from our rewards without changing the expected gradient:</p>

\[\mathbb{E}[\nabla_\theta \log \pi_\theta \cdot (R(\tau) - b(s))] = \mathbb{E}[\nabla_\theta \log \pi_\theta \cdot R(\tau)] - \underbrace{\mathbb{E}[\nabla_\theta \log \pi_\theta \cdot b(s)]}_{= 0}\]

<p>We get <strong>lower variance</strong> (because advantages are centered around zero) <strong>without introducing bias</strong>. Free lunch!</p>

<h3 id="part-6-off-policy-learning--reusing-old-data">Part 6: Off-Policy Learning â€” Reusing Old Data</h3>

<h4 id="the-inefficiency-of-on-policy-learning">The Inefficiency of On-Policy Learning</h4>

<p>Vanilla REINFORCE is <strong>on-policy</strong>: you generate rollouts, take one gradient step, then throw away the data and generate fresh rollouts.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Generate 1000 responses â†’ one gradient step â†’ discard â†’ Generate 1000 more â†’ ...

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ON-POLICY REINFORCE                                                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  Step 1: Sample 1000 questions from your question bank                      â”‚
â”‚          (e.g., "What is 2+3?", "Solve xÂ²=4", ...)                          â”‚
â”‚                         â”‚                                                   â”‚
â”‚                         â–¼                                                   â”‚
â”‚  Step 2: Current model Ï€_Î¸ GENERATES responses for each question            â”‚
â”‚          (This is expensive! Running inference 1000 times)                  â”‚
â”‚                         â”‚                                                   â”‚
â”‚                         â–¼                                                   â”‚
â”‚  Step 3: Check answers â†’ rewards [1, 0, 1, 1, 0, ...]                       â”‚
â”‚                         â”‚                                                   â”‚
â”‚                         â–¼                                                   â”‚
â”‚  Step 4: Compute gradient, update Î¸ â†’ Î¸'                                    â”‚
â”‚                         â”‚                                                   â”‚
â”‚                         â–¼                                                   â”‚
â”‚  Step 5: DISCARD all 1000 responses â† This is the wasteful part!            â”‚
â”‚                         â”‚                                                   â”‚
â”‚                         â–¼                                                   â”‚
â”‚  Step 6: Go back to Step 1 with the NEW model Ï€_Î¸'                          â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
</code></pre></div></div>
<p>This is wasteful! LLM inference is expensive, and weâ€™re only using each sample once.</p>

<h4 id="the-solution-importance-sampling">The Solution: Importance Sampling</h4>

<p>In <strong>off-policy</strong> learning, we reuse rollouts from a previous policy $\pi_{\theta_{\text{old}}}$ to train the current policy $\pi_\theta$.</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  OFF-POLICY                                                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  Step 1: Sample questions                                                   â”‚
â”‚                         â”‚                                                   â”‚
â”‚                         â–¼                                                   â”‚
â”‚  Step 2: Ï€_Î¸_old generates responses (expensive, but done ONCE)             â”‚
â”‚                         â”‚                                                   â”‚
â”‚                         â–¼                                                   â”‚
â”‚  Step 3: Check answers â†’ rewards                                            â”‚
â”‚                         â”‚                                                   â”‚
â”‚                         â–¼                                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                            â”‚
â”‚  â”‚  Step 4: Gradient step 1 (with importance   â”‚                            â”‚
â”‚  â”‚          weights to correct for Ï€_Î¸_old)    â”‚                            â”‚
â”‚  â”‚                     â”‚                       â”‚                            â”‚
â”‚  â”‚                     â–¼                       â”‚                            â”‚
â”‚  â”‚  Step 5: Gradient step 2 (same data!)       â”‚  â† Reuse the same          â”‚
â”‚  â”‚                     â”‚                       â”‚    responses multiple      â”‚
â”‚  â”‚                     â–¼                       â”‚    times!                  â”‚
â”‚  â”‚  Step 6: Gradient step 3 (same data!)       â”‚                            â”‚
â”‚  â”‚                     â”‚                       â”‚                            â”‚
â”‚  â”‚                     â–¼                       â”‚                            â”‚
â”‚  â”‚  ... (typically 4-8 steps per batch)        â”‚                            â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                            â”‚
â”‚                         â”‚                                                   â”‚
â”‚                         â–¼                                                   â”‚
â”‚  Step 7: NOW discard and generate fresh responses                           â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
</code></pre></div></div>

<p>The trick is <strong>importance sampling</strong>: we reweight samples to correct for the mismatch between old and new policies.</p>

\[g_{\text{off-policy}} = \frac{1}{N} \sum_{i=1}^{N} \sum_t \underbrace{\frac{\pi_\theta(a_t^{(i)}|s_t^{(i)})}{\pi_{\theta_{\text{old}}}(a_t^{(i)}|s_t^{(i)})}}_{\text{importance weight}} \nabla_\theta \log \pi_\theta(a_t^{(i)}|s_t^{(i)}) \cdot R(\tau^{(i)})\]

<p>\(N\) is number of trajectories in the batch (e.g., 1000 responses).</p>

<p>The importance weight $\rho_t = \frac{\pi_\theta}{\pi_{\theta_{\text{old}}}}$ corrects for the distribution shift.</p>

<h4 id="a-concrete-example-2">A Concrete Example</h4>

<p>Suppose the old policy generated token â€œParisâ€ with probability 0.5, but your current policy would generate it with probability 0.7.</p>

<p><strong>Without correction:</strong> Youâ€™d undercount â€œParisâ€ because it was sampled from a distribution that liked it less.</p>

<p><strong>With importance weight:</strong> $\rho = 0.7 / 0.5 = 1.4$</p>

<p>You upweight this sample by 40% to compensate.</p>

<table>
  <thead>
    <tr>
      <th>Old Policy</th>
      <th>Current Policy</th>
      <th>Importance Weight</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>P(â€œParisâ€) = 0.5</td>
      <td>P(â€œParisâ€) = 0.7</td>
      <td>0.7/0.5 = 1.4</td>
    </tr>
    <tr>
      <td>P(â€œLyonâ€) = 0.3</td>
      <td>P(â€œLyonâ€) = 0.1</td>
      <td>0.1/0.3 = 0.33</td>
    </tr>
  </tbody>
</table>

<h4 id="the-catch-dont-stray-too-far">The Catch: Donâ€™t Stray Too Far</h4>

<p>Importance sampling only works when $\pi_\theta$ and $\pi_{\theta_{\text{old}}}$ are similar. If they diverge:</p>

<ul>
  <li>Some importance weights explode (e.g., 100Ã—)</li>
  <li>Gradient estimates become unreliable</li>
  <li>Training becomes unstable</li>
</ul>

<p>This is why algorithms like PPO and GRPO <strong>clip</strong> the importance weightsâ€”more on this next!</p>

<p>Now letâ€™s put everything together with GRPO, the algorithm used to train DeepSeek R1.</p>

<h3 id="part-7-grpo--group-relative-policy-optimization">Part 7: GRPO â€” Group Relative Policy Optimization</h3>

<h4 id="the-core-idea-compare-siblings">The Core Idea: Compare Siblings</h4>

<p>Remember, we need a baseline to reduce variance. The standard approach is to train a separate model to predict expected returnsâ€”but this is extra work.</p>

<p><strong>GRPOâ€™s insight:</strong> Instead of learning a baseline, sample multiple answers for the same question and compare them to each other!</p>

<p>If you ask the model â€œWhat is 2+3?â€ five times and it gets three right and two wrong, the correct answers are â€œbetter than averageâ€ and the wrong ones are â€œworse than average.â€ No separate baseline network needed!</p>

<h4 id="step-1-sample-multiple-outputs-per-question">Step 1: Sample Multiple Outputs Per Question</h4>

<p>For each question $q$, sample $G$ outputs (the â€œgroupâ€):</p>

\[\{o^{(1)}, o^{(2)}, \ldots, o^{(G)}\} \sim \pi_\theta(\cdot | q)\]

<p><strong>Example with G=5:</strong></p>

<table>
  <thead>
    <tr>
      <th>Question</th>
      <th>Output $i$</th>
      <th>Answer</th>
      <th>Correct?</th>
      <th>Reward $r^{(i)}$</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>â€œWhat is 15Ã—7?â€</td>
      <td>1</td>
      <td>â€œ105â€</td>
      <td>âœ“</td>
      <td>1</td>
    </tr>
    <tr>
      <td>Â </td>
      <td>2</td>
      <td>â€œ105â€</td>
      <td>âœ“</td>
      <td>1</td>
    </tr>
    <tr>
      <td>Â </td>
      <td>3</td>
      <td>â€œ112â€</td>
      <td>âœ—</td>
      <td>0</td>
    </tr>
    <tr>
      <td>Â </td>
      <td>4</td>
      <td>â€œ105â€</td>
      <td>âœ“</td>
      <td>1</td>
    </tr>
    <tr>
      <td>Â </td>
      <td>5</td>
      <td>â€œ107â€</td>
      <td>âœ—</td>
      <td>0</td>
    </tr>
  </tbody>
</table>

<h4 id="step-2-compute-group-normalized-advantages">Step 2: Compute Group-Normalized Advantages</h4>

<p>The advantage for output $i$ is computed by normalizing within the group:</p>

\[A^{(i)} = \frac{r^{(i)} - \text{mean}(r^{(1)}, \ldots, r^{(G)})}{\text{std}(r^{(1)}, \ldots, r^{(G)}) + \epsilon}\]

<p><strong>Continuing the example:</strong></p>

<ul>
  <li>mean$(r) = (1+1+0+1+0)/5 = 0.6$</li>
  <li>std$(r) = 0.49$</li>
</ul>

<table>
  <thead>
    <tr>
      <th>Output $i$</th>
      <th>$r^{(i)}$</th>
      <th>$A^{(i)} = \frac{r^{(i)} - 0.6}{0.49}$</th>
      <th>Interpretation</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>1</td>
      <td>1</td>
      <td>+0.82</td>
      <td>Better than siblings â†’ reinforce</td>
    </tr>
    <tr>
      <td>2</td>
      <td>1</td>
      <td>+0.82</td>
      <td>Better than siblings â†’ reinforce</td>
    </tr>
    <tr>
      <td>3</td>
      <td>0</td>
      <td>-1.22</td>
      <td>Worse than siblings â†’ discourage</td>
    </tr>
    <tr>
      <td>4</td>
      <td>1</td>
      <td>+0.82</td>
      <td>Better than siblings â†’ reinforce</td>
    </tr>
    <tr>
      <td>5</td>
      <td>0</td>
      <td>-1.22</td>
      <td>Worse than siblings â†’ discourage</td>
    </tr>
  </tbody>
</table>

<p><strong>Key insight:</strong> The same advantage applies to <strong>every token</strong> in that output. If output 1 was correct, every token in its reasoning chain gets $A = +0.82$.</p>

<h4 id="step-3-the-grpo-clip-objective">Step 3: The GRPO-Clip Objective</h4>

<p>GRPO combines off-policy learning with <strong>clipping</strong> to stay stable:</p>

\[J_{\text{GRPO-Clip}}(\theta) = \mathbb{E}\left[\frac{1}{G}\sum_{i=1}^{G}\frac{1}{|o^{(i)}|}\sum_{t} \min\left(\rho_t \cdot A^{(i)}, \text{clip}(\rho_t, 1-\epsilon, 1+\epsilon) \cdot A^{(i)}\right)\right]\]

<p><strong>Symbol-by-symbol breakdown:</strong></p>

<table>
  <thead>
    <tr>
      <th>Symbol</th>
      <th>Name</th>
      <th>Meaning</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>$J_{\text{GRPO-Clip}}(\theta)$</td>
      <td>Objective function</td>
      <td>The thing we want to maximize â€” â€œhow good is our policy?â€</td>
    </tr>
    <tr>
      <td>$\theta$</td>
      <td>Parameters</td>
      <td>All the weights in our neural network</td>
    </tr>
    <tr>
      <td>$\mathbb{E}[\cdot]$</td>
      <td>Expectation</td>
      <td>Average over many sampled questions and responses</td>
    </tr>
    <tr>
      <td>$G$</td>
      <td>Group size</td>
      <td>Number of responses we generate per question (e.g., 8)</td>
    </tr>
    <tr>
      <td>$\frac{1}{G}\sum_{i=1}^{G}$</td>
      <td>Average over group</td>
      <td>Average the objective across all G responses for this question</td>
    </tr>
    <tr>
      <td>$i$</td>
      <td>Response index</td>
      <td>Which response in the group (1st, 2nd, â€¦, G-th)</td>
    </tr>
    <tr>
      <td>$o^{(i)}$</td>
      <td>Response i</td>
      <td>The i-th generated response (sequence of tokens)</td>
    </tr>
    <tr>
      <td>$|o^{(i)}|$</td>
      <td>Response length</td>
      <td>Number of tokens in response i</td>
    </tr>
    <tr>
      <td>$\frac{1}{|o^{(i)}|}\sum_{t}$</td>
      <td>Average over tokens</td>
      <td>Average the objective across all tokens in this response</td>
    </tr>
    <tr>
      <td>$t$</td>
      <td>Token index</td>
      <td>Which token position in the response</td>
    </tr>
    <tr>
      <td>$\rho_t$</td>
      <td>Probability ratio</td>
      <td>$\frac{\pi_\theta(o_t)}{\pi_{\theta_{old}}(o_t)}$ â€” how much more/less likely is this token under new vs old policy?</td>
    </tr>
    <tr>
      <td>$A^{(i)}$</td>
      <td>Advantage</td>
      <td>Was response i better or worse than average in its group?</td>
    </tr>
    <tr>
      <td>$\epsilon$</td>
      <td>Clip parameter</td>
      <td>How far we allow the policy to change (typically 0.1â€“0.2)</td>
    </tr>
    <tr>
      <td>$\text{clip}(\rho_t, 1-\epsilon, 1+\epsilon)$</td>
      <td>Clipped ratio</td>
      <td>Force $\rho_t$ to stay in range $[1-\epsilon, 1+\epsilon]$</td>
    </tr>
    <tr>
      <td>$\min(\cdot, \cdot)$</td>
      <td>Minimum</td>
      <td>Take the smaller of the two values (conservative update)</td>
    </tr>
  </tbody>
</table>

<p><strong>The probability ratio $\rho_t$ in detail:</strong></p>

\[\rho_t = \frac{\pi_\theta(o_t^{(i)} | q, o_{&lt;t}^{(i)})}{\pi_{\theta_{\text{old}}}(o_t^{(i)} | q, o_{&lt;t}^{(i)})}\]

<table>
  <thead>
    <tr>
      <th>$\rho_t$ value</th>
      <th>Meaning</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>1.0</td>
      <td>Token probability unchanged since we generated the same response</td>
    </tr>
    <tr>
      <td>1.5</td>
      <td>New policy is 50% more likely to generate this token</td>
    </tr>
    <tr>
      <td>0.7</td>
      <td>New policy is 30% less likely to generate this token</td>
    </tr>
  </tbody>
</table>

<p><strong>The clipping function:</strong></p>

<p>With $\epsilon = 0.2$, the clip function constrains $\rho_t$ to the range $[0.8, 1.2]$:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Input Ï_t:    0.5   0.8   1.0   1.2   1.5   2.0
              â†“     â†“     â†“     â†“     â†“     â†“
Output:       0.8   0.8   1.0   1.2   1.2   1.2
              â†‘           â†‘           â†‘
           clipped     unchanged   clipped
              up                     down
</code></pre></div></div>

<p><strong>The min operation â€” being conservative:</strong></p>

<p>We compute BOTH the clipped and unclipped objectives, then take the minimum:</p>

\[\min\left(\rho_t \cdot A^{(i)}, \text{clip}(\rho_t, 1-\epsilon, 1+\epsilon) \cdot A^{(i)}\right)\]

<p><strong>Case 1: Positive advantage (good response, $A &gt; 0$)</strong></p>

<table>
  <thead>
    <tr>
      <th>$\rho_t$</th>
      <th>Unclipped $\rho_t \cdot A$</th>
      <th>Clipped</th>
      <th>Min (used)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0.8</td>
      <td>0.8A</td>
      <td>0.8A</td>
      <td>0.8A</td>
    </tr>
    <tr>
      <td>1.0</td>
      <td>1.0A</td>
      <td>1.0A</td>
      <td>1.0A</td>
    </tr>
    <tr>
      <td>1.2</td>
      <td>1.2A</td>
      <td>1.2A</td>
      <td>1.2A</td>
    </tr>
    <tr>
      <td>1.5</td>
      <td>1.5A</td>
      <td>1.2A</td>
      <td><strong>1.2A</strong> â† capped!</td>
    </tr>
  </tbody>
</table>

<p>Once $\rho_t &gt; 1 + \epsilon$, the objective stops increasing. No more reward for pushing probability higher.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Objective
    â–²
    â”‚            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ capped at (1+Îµ)A
    â”‚           /
    â”‚          /
    â”‚         /
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º Ï_t
           1.0  1.2
</code></pre></div></div>

<p><strong>Case 2: Negative advantage (bad response, $A &lt; 0$)</strong></p>

<table>
  <thead>
    <tr>
      <th>$\rho_t$</th>
      <th>Unclipped $\rho_t \cdot A$</th>
      <th>Clipped</th>
      <th>Min (used)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>1.2</td>
      <td>-1.2A</td>
      <td>-1.2A</td>
      <td>-1.2A</td>
    </tr>
    <tr>
      <td>1.0</td>
      <td>-1.0A</td>
      <td>-1.0A</td>
      <td>-1.0A</td>
    </tr>
    <tr>
      <td>0.8</td>
      <td>-0.8A</td>
      <td>-0.8A</td>
      <td>-0.8A</td>
    </tr>
    <tr>
      <td>0.5</td>
      <td>-0.5A</td>
      <td>-0.8A</td>
      <td><strong>-0.8A</strong> â† capped!</td>
    </tr>
  </tbody>
</table>

<p>Once $\rho_t &lt; 1 - \epsilon$, the objective stops decreasing. No more penalty for pushing probability lower.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Objective
    â–²
    â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚             \
    â”‚              \
    â”‚               \
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â–º Ï_t
               0.8  1.0
</code></pre></div></div>

<p><strong>Complete worked example:</strong></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    GRPO-Clip Objective: Step by Step                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚   Question: "What is 2+3?"                                                  â”‚
â”‚                                                                             â”‚
â”‚   Step 1: Generate G=4 responses from Ï€_Î¸_old                               â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚  Response 1: "Let me think... 2+3 = 5" âœ“    reward = 1              â”‚   â”‚
â”‚   â”‚  Response 2: "2 plus 3 equals 6" âœ—          reward = 0              â”‚   â”‚
â”‚   â”‚  Response 3: "The answer is 5" âœ“            reward = 1              â”‚   â”‚
â”‚   â”‚  Response 4: "I believe it's 7" âœ—           reward = 0              â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                    â”‚                                        â”‚
â”‚                                    â–¼                                        â”‚
â”‚   Step 2: Compute group-normalized advantages                               â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚  mean(rewards) = 0.5,  std(rewards) = 0.5                           â”‚   â”‚
â”‚   â”‚                                                                     â”‚   â”‚
â”‚   â”‚  Aâ½Â¹â¾ = (1 - 0.5) / 0.5 = +1.0   (better than average)              â”‚   â”‚
â”‚   â”‚  Aâ½Â²â¾ = (0 - 0.5) / 0.5 = -1.0   (worse than average)               â”‚   â”‚
â”‚   â”‚  Aâ½Â³â¾ = (1 - 0.5) / 0.5 = +1.0   (better than average)              â”‚   â”‚
â”‚   â”‚  Aâ½â´â¾ = (0 - 0.5) / 0.5 = -1.0   (worse than average)               â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                    â”‚                                        â”‚
â”‚                                    â–¼                                        â”‚
â”‚   Step 3: For each token, compute clipped objective                         â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚  Example: Token "5" in Response 1 (A = +1.0)                        â”‚   â”‚
â”‚   â”‚                                                                     â”‚   â”‚
â”‚   â”‚  Ï€_Î¸_old("5" | context) = 0.4                                       â”‚   â”‚
â”‚   â”‚  Ï€_Î¸("5" | context) = 0.6        (after some gradient steps)        â”‚   â”‚
â”‚   â”‚                                                                     â”‚   â”‚
â”‚   â”‚  Ï_t = 0.6 / 0.4 = 1.5                                              â”‚   â”‚
â”‚   â”‚                                                                     â”‚   â”‚
â”‚   â”‚  Unclipped: Ï_t Ã— A = 1.5 Ã— 1.0 = 1.50                              â”‚   â”‚
â”‚   â”‚  Clipped:   clip(1.5, 0.8, 1.2) Ã— A = 1.2 Ã— 1.0 = 1.20              â”‚   â”‚
â”‚   â”‚                                                                     â”‚   â”‚
â”‚   â”‚  min(1.50, 1.20) = 1.20  â† Use the conservative value               â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                    â”‚                                        â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚  Example: Token "6" in Response 2 (A = -1.0)                        â”‚   â”‚
â”‚   â”‚                                                                     â”‚   â”‚
â”‚   â”‚  Ï€_Î¸_old("6" | context) = 0.3                                       â”‚   â”‚
â”‚   â”‚  Ï€_Î¸("6" | context) = 0.15       (model learned to avoid this)      â”‚   â”‚
â”‚   â”‚                                                                     â”‚   â”‚
â”‚   â”‚  Ï_t = 0.15 / 0.3 = 0.5                                             â”‚   â”‚
â”‚   â”‚                                                                     â”‚   â”‚
â”‚   â”‚  Unclipped: Ï_t Ã— A = 0.5 Ã— (-1.0) = -0.50                          â”‚   â”‚
â”‚   â”‚  Clipped:   clip(0.5, 0.8, 1.2) Ã— A = 0.8 Ã— (-1.0) = -0.80          â”‚   â”‚
â”‚   â”‚                                                                     â”‚   â”‚
â”‚   â”‚  min(-0.50, -0.80) = -0.80  â† Use the more negative value           â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                    â”‚                                        â”‚
â”‚                                    â–¼                                        â”‚
â”‚   Step 4: Average over all tokens and responses â†’ Final objective J(Î¸)      â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
</code></pre></div></div>

<p><strong>Plain English summary:</strong></p>

<p>The GRPO-Clip objective says:</p>

<blockquote>
  <ol>
    <li><strong>For each question:</strong> Generate G different responses, score them, compute advantages</li>
    <li><strong>For each token:</strong> Check how much the probability changed ($\rho_t$), multiply by advantage</li>
    <li><strong>But clip the change:</strong> Donâ€™t let the policy move more than $\epsilon$ away from the old policy</li>
    <li><strong>Average everything:</strong> Over all tokens and all responses</li>
  </ol>
</blockquote>

<h4 id="why-clipping-matters">Why Clipping Matters</h4>

<p>Without clipping, taking many gradient steps on the same batch leads to <strong>overfitting</strong>. Clipping ensures the policy can only move <strong>X% away from the old policy per batch</strong>. This keeps training stable.</p>

<h4 id="the-grpo-training-loop-visualized">The GRPO Training Loop Visualized</h4>

<p>Hereâ€™s the complete GRPO workflow showing how it reuses generated responses:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           GRPO TRAINING LOOP                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  Step 1: Sample a batch of questions from dataset                           â”‚
â”‚          ["What is 2+3?", "Solve xÂ²=4", "What is 7Ã—8?", ...]                â”‚
â”‚                         â”‚                                                   â”‚
â”‚                         â–¼                                                   â”‚
â”‚  Step 2: Snapshot current model as Ï€_Î¸_old                                  â”‚
â”‚                         â”‚                                                   â”‚
â”‚                         â–¼                                                   â”‚
â”‚  Step 3: Generate G responses per question using Ï€_Î¸_old                    â”‚
â”‚          (This is EXPENSIVE â€” full inference G times per question)          â”‚
â”‚                         â”‚                                                   â”‚
â”‚          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                    â”‚
â”‚          â”‚  Question: "What is 2+3?"   â”‚                                    â”‚
â”‚          â”‚  Response 1: "5" âœ“          â”‚                                    â”‚
â”‚          â”‚  Response 2: "6" âœ—          â”‚                                    â”‚
â”‚          â”‚  Response 3: "5" âœ“          â”‚                                    â”‚
â”‚          â”‚  Response 4: "7" âœ—          â”‚                                    â”‚
â”‚          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                    â”‚
â”‚                         â”‚                                                   â”‚
â”‚                         â–¼                                                   â”‚
â”‚  Step 4: Compute rewards and group-normalized advantages                    â”‚
â”‚          Aâ½Â¹â¾=+1.0, Aâ½Â²â¾=-1.0, Aâ½Â³â¾=+1.0, Aâ½â´â¾=-1.0                         â”‚
â”‚                         â”‚                                                   â”‚
â”‚                         â–¼                                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Step 5-8: MULTIPLE gradient steps on the SAME responses              â”‚  â”‚
â”‚  â”‚            (This is where we save compute!)                           â”‚  â”‚
â”‚  â”‚                                                                       â”‚  â”‚
â”‚  â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚  â”‚
â”‚  â”‚    â”‚  Gradient step 1:                                           â”‚    â”‚  â”‚
â”‚  â”‚    â”‚    - Compute Ï_t = Ï€_Î¸(token) / Ï€_Î¸_old(token) for all      â”‚    â”‚  â”‚
â”‚  â”‚    â”‚    - Apply clipping to keep Ï_t in [0.8, 1.2]               â”‚    â”‚  â”‚
â”‚  â”‚    â”‚    - Update Î¸                                               â”‚    â”‚  â”‚
â”‚  â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚  â”‚
â”‚  â”‚                           â”‚                                           â”‚  â”‚
â”‚  â”‚                           â–¼                                           â”‚  â”‚
â”‚  â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚  â”‚
â”‚  â”‚    â”‚  Gradient step 2: (same responses, updated Ï€_Î¸)             â”‚    â”‚  â”‚
â”‚  â”‚    â”‚    - Recompute Ï_t with new Ï€_Î¸                             â”‚    â”‚  â”‚
â”‚  â”‚    â”‚    - Clipping prevents Ï_t from going too far               â”‚    â”‚  â”‚
â”‚  â”‚    â”‚    - Update Î¸ again                                         â”‚    â”‚  â”‚
â”‚  â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚  â”‚
â”‚  â”‚                           â”‚                                           â”‚  â”‚
â”‚  â”‚                           â–¼                                           â”‚  â”‚
â”‚  â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚  â”‚
â”‚  â”‚    â”‚  Gradient steps 3, 4, ... (typically 4-8 total)             â”‚    â”‚  â”‚
â”‚  â”‚    â”‚    - Eventually Ï_t hits clip boundaries                    â”‚    â”‚  â”‚
â”‚  â”‚    â”‚    - Gradients become zero â†’ time for fresh data            â”‚    â”‚  â”‚
â”‚  â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                         â”‚                                                   â”‚
â”‚                         â–¼                                                   â”‚
â”‚  Step 9: Discard responses, go back to Step 1 with updated Ï€_Î¸              â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
</code></pre></div></div>

<p><strong>Why is this more efficient than vanilla REINFORCE?</strong></p>

<table>
  <thead>
    <tr>
      <th>Method</th>
      <th style="text-align: right">Responses generated</th>
      <th style="text-align: right">Gradient steps</th>
      <th style="text-align: right">Efficiency</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>REINFORCE</td>
      <td style="text-align: right">1000</td>
      <td style="text-align: right">1</td>
      <td style="text-align: right">1 step per 1000 inferences</td>
    </tr>
    <tr>
      <td>GRPO</td>
      <td style="text-align: right">1000</td>
      <td style="text-align: right">4-8</td>
      <td style="text-align: right">4-8 steps per 1000 inferences</td>
    </tr>
  </tbody>
</table>

<p>GRPO extracts 4-8Ã— more learning from each expensive batch of generated responses!</p>

<h4 id="the-complete-grpo-algorithm">The Complete GRPO Algorithm</h4>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Algorithm: GRPO

Input: initial model Ï€_Î¸, reward function R, questions D

1:  Ï€_Î¸ â† Ï€_Î¸_init                        # Start with base model

2:  for step = 1 to n_grpo_steps:          # Main training loop
3:      Sample batch of questions D_b
4:      Ï€_Î¸_old â† Ï€_Î¸                      # Snapshot current model
5:      
6:      # Sample G outputs per question
7:      for each question q in D_b:
8:          Sample {o^(1), ..., o^(G)} from Ï€_Î¸_old
9:          Compute rewards {r^(1), ..., r^(G)}
10:         Compute advantages A^(i) via group normalization
11:     
12:     # Take multiple gradient steps on same rollouts (off-policy)
13:     for train_step = 1 to n_train_steps_per_rollout_batch:
14:         Update Ï€_Î¸ by maximizing GRPO-Clip objective
15:     
16: Output: trained Ï€_Î¸
</code></pre></div></div>

<p><strong>Why this works:</strong></p>

<ol>
  <li><strong>Group normalization</strong> provides a baseline without training a separate network</li>
  <li><strong>Off-policy updates</strong> let us take multiple gradient steps per batch (efficient!)</li>
  <li><strong>Clipping</strong> prevents the policy from changing too much (stable!)</li>
</ol>

<h3 id="part-8-putting-it-all-together">Part 8: Putting It All Together</h3>

<h4 id="summary-table">Summary Table</h4>

<table>
  <thead>
    <tr>
      <th>Concept</th>
      <th>What It Is</th>
      <th>What It Does</th>
      <th>Example</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Policy</strong> $\pi_\theta$</td>
      <td>Your LLM parameterized by weights $\theta$</td>
      <td>Given context, outputs probability distribution over next tokens</td>
      <td>P(â€œParisâ€ \mid â€œThe capital of France isâ€) = 0.85</td>
    </tr>
    <tr>
      <td><strong>State</strong> $s_t$</td>
      <td>The text generated so far</td>
      <td>Provides context for the next decision</td>
      <td>â€œWhat is 2+3? <think> I need to"</think></td>
    </tr>
    <tr>
      <td><strong>Action</strong> $a_t$</td>
      <td>A single token</td>
      <td>The choice made at each step</td>
      <td>â€œaddâ€</td>
    </tr>
    <tr>
      <td><strong>Trajectory</strong> $\tau$</td>
      <td>Complete sequence $(s_0, a_0, s_1, a_1, â€¦, s_T, a_T)$</td>
      <td>One full episode from prompt to end-of-text</td>
      <td>Question â†’ reasoning â†’ answer â†’ <code class="language-plaintext highlighter-rouge">&lt;/answer&gt;</code></td>
    </tr>
    <tr>
      <td><strong>Reward</strong> $r_t$</td>
      <td>Scalar feedback signal</td>
      <td>Judges quality of action at state</td>
      <td>0 for intermediate steps, 1 if final answer correct</td>
    </tr>
    <tr>
      <td><strong>Return</strong> $R(\tau)$</td>
      <td>Sum of rewards over trajectory</td>
      <td>Single number measuring â€œhow good was this trajectory?â€</td>
      <td>R = 1 (correct) or R = 0 (wrong)</td>
    </tr>
    <tr>
      <td><strong>Score function</strong> $\nabla_\theta \log \pi_\theta(a \mid s)$</td>
      <td>Gradient of log-probability</td>
      <td>Direction in parameter space that increases P(action)</td>
      <td>A vector with one entry per model parameter</td>
    </tr>
    <tr>
      <td><strong>REINFORCE</strong></td>
      <td>Vanilla policy gradient algorithm</td>
      <td>Multiply score function by return, average over trajectories</td>
      <td>Good trajectories â†’ reinforce all their tokens</td>
    </tr>
    <tr>
      <td><strong>Baseline</strong> $b(s)$</td>
      <td>Estimate of expected return</td>
      <td>Subtract from reward to get advantage</td>
      <td>If model gets 70% right, b â‰ˆ 0.7</td>
    </tr>
    <tr>
      <td><strong>Advantage</strong> $A = R(\tau) - b$</td>
      <td>â€œBetter or worse than expected?â€</td>
      <td>Positive â†’ reinforce, Negative â†’ discourage, Zero â†’ no signal</td>
      <td>R=1, b=0.7 â†’ A=+0.3 (good!); R=0, b=0.7 â†’ A=-0.7 (bad!)</td>
    </tr>
    <tr>
      <td><strong>On-policy</strong></td>
      <td>Generate â†’ one gradient step â†’ discard</td>
      <td>Uses fresh data for each update</td>
      <td>Wasteful: 1000 inferences for 1 gradient step</td>
    </tr>
    <tr>
      <td><strong>Off-policy</strong></td>
      <td>Generate â†’ multiple gradient steps â†’ discard</td>
      <td>Reuses data with importance weights $\frac{\pi_\theta}{\pi_{\theta_{old}}}$</td>
      <td>Efficient: 1000 inferences for 4-8 gradient steps</td>
    </tr>
    <tr>
      <td><strong>Importance weight</strong> $\rho_t$</td>
      <td>Ratio $\frac{\pi_\theta(a)}{\pi_{\theta_{old}}(a)}$</td>
      <td>Corrects for distribution mismatch when reusing old data</td>
      <td>If old P=0.4, new P=0.6, then Ï=1.5</td>
    </tr>
    <tr>
      <td><strong>Clipping</strong></td>
      <td>Constrain $\rho_t$ to $[1-\epsilon, 1+\epsilon]$</td>
      <td>Prevents policy from changing too fast</td>
      <td>With Îµ=0.2, Ï stays in [0.8, 1.2]</td>
    </tr>
    <tr>
      <td><strong>GRPO</strong></td>
      <td>Group Relative Policy Optimization</td>
      <td>Sample G responses per question, use group statistics as baseline</td>
      <td>No need to train separate value network</td>
    </tr>
  </tbody>
</table>

<h4 id="key-equations-at-a-glance">Key Equations at a Glance</h4>

<table>
  <thead>
    <tr>
      <th>Equation</th>
      <th>Name</th>
      <th>Plain English</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>$\nabla_\theta J = \mathbb{E}<em>{\tau}[\sum_t \nabla</em>\theta \log \pi_\theta(a_t \mid s_t) \cdot R(\tau)]$</td>
      <td>Policy Gradient</td>
      <td>â€œIncrease probability of tokens that led to high rewardsâ€</td>
    </tr>
    <tr>
      <td>$\nabla_\theta J = \mathbb{E}<em>{\tau}[\sum_t \nabla</em>\theta \log \pi_\theta(a_t \mid s_t) \cdot (R(\tau) - b)]$</td>
      <td>Baselined Policy Gradient</td>
      <td>â€œReinforce better-than-expected, discourage worse-than-expectedâ€</td>
    </tr>
    <tr>
      <td>$A^{(i)} = \frac{r^{(i)} - \text{mean}(r)}{\text{std}(r)}$</td>
      <td>GRPO Advantage</td>
      <td>â€œCompare this response to its siblingsâ€</td>
    </tr>
    <tr>
      <td>$\min(\rho_t \cdot A, \text{clip}(\rho_t) \cdot A)$</td>
      <td>Clipped Objective</td>
      <td>â€œUpdate conservatively â€” donâ€™t change too much at onceâ€</td>
    </tr>
  </tbody>
</table>

<h4 id="the-pytorch-connection">The PyTorch Connection</h4>

<p>When you implement GRPO, the core looks like this:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">compute_grpo_loss</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">old_log_probs</span><span class="p">,</span> <span class="n">advantages</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">0.2</span><span class="p">):</span>
    <span class="s">"""
    Compute GRPO-Clip loss for a batch of trajectories.
    """</span>
    <span class="c1"># Get current log probabilities
</span>    <span class="n">logits</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">batch</span><span class="p">[</span><span class="s">"input_ids"</span><span class="p">])</span>
    <span class="n">log_probs</span> <span class="o">=</span> <span class="n">get_log_probs</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">batch</span><span class="p">[</span><span class="s">"input_ids"</span><span class="p">])</span>
    
    <span class="c1"># Compute probability ratios: Ï€_Î¸ / Ï€_Î¸_old
</span>    <span class="n">ratios</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="n">log_probs</span> <span class="o">-</span> <span class="n">old_log_probs</span><span class="p">)</span>
    
    <span class="c1"># Clipped objective
</span>    <span class="n">unclipped</span> <span class="o">=</span> <span class="n">ratios</span> <span class="o">*</span> <span class="n">advantages</span>
    <span class="n">clipped</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">ratios</span><span class="p">,</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">epsilon</span><span class="p">,</span> <span class="mi">1</span> <span class="o">+</span> <span class="n">epsilon</span><span class="p">)</span> <span class="o">*</span> <span class="n">advantages</span>
    
    <span class="c1"># Take minimum (conservative update)
</span>    <span class="n">loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">torch</span><span class="p">.</span><span class="nb">min</span><span class="p">(</span><span class="n">unclipped</span><span class="p">,</span> <span class="n">clipped</span><span class="p">).</span><span class="n">mean</span><span class="p">()</span>
    
    <span class="k">return</span> <span class="n">loss</span>
</code></pre></div></div>

<p>The key steps are:</p>
<ol>
  <li>Compute current vs. old log-probabilities</li>
  <li>Form the probability ratio</li>
  <li>Clip the ratio</li>
  <li>Take the minimum of clipped and unclipped objectives</li>
  <li>Maximize (so we minimize the negative)</li>
</ol>

<p>The math may look intimidating at first, but the core ideas are simple:</p>
<ul>
  <li><strong>Try things</strong> (sample trajectories)</li>
  <li><strong>See what works</strong> (check rewards)</li>
  <li><strong>Do more of what works</strong> (policy gradient)</li>
  <li><strong>Be smart about it</strong> (baselines, clipping, group normalization)</li>
</ul>

<p>Thatâ€™s really all there is to it!</p>

<hr />

<p><strong>Resources:</strong></p>
<ul>
  <li><a href="https://stanford-cs336.github.io/spring2025/">Stanford CS336: Language Modeling from Scratch</a></li>
  <li><a href="https://spinningup.openai.com/">OpenAI Spinning Up in Deep RL</a></li>
  <li><a href="https://rlhfbook.com/">Nathan Lambertâ€™s RLHF Book</a></li>
  <li><a href="https://arxiv.org/abs/2401.02954">DeepSeek R1 Paper</a></li>
  <li><a href="https://arxiv.org/abs/2402.03300">DeepSeekMath Paper</a> â€” Original GRPO formulation</li>
  <li><a href="https://arxiv.org/abs/1707.06347">PPO Paper</a> â€” Proximal Policy Optimization</li>
</ul>]]></content><author><name>[&quot;Han Yu&quot;]</name></author><category term="cs336" /><summary type="html"><![CDATA[A Beginnerâ€™s Guide to Reinforcement Learning for Language Models]]></summary></entry><entry><title type="html">Study Notes: Stanford CS336 Language Modeling from Scratch [13]</title><link href="http://localhost:4000/cs336/2026/01/19/cs336-sft-qwen3-for-math-reasoning.html" rel="alternate" type="text/html" title="Study Notes: Stanford CS336 Language Modeling from Scratch [13]" /><published>2026-01-19T00:00:00-08:00</published><updated>2026-01-19T00:00:00-08:00</updated><id>http://localhost:4000/cs336/2026/01/19/cs336-sft-qwen3-for-math-reasoning</id><content type="html" xml:base="http://localhost:4000/cs336/2026/01/19/cs336-sft-qwen3-for-math-reasoning.html"><![CDATA[<h2 id="fine-tuning-qwen3-17b-on-lambda-labs-for-math-reasoning">Fine-Tuning Qwen3-1.7B on Lambda Labs for Math Reasoning</h2>

<p>When developing machine learning training pipelines, thereâ€™s often a disconnect between local development environments and production-scale cloud infrastructure. You might prototype on your laptop (say, a MacBook with Apple Silicon), only to discover that your code breaks on CUDA GPUs, or that patterns that worked locally donâ€™t scale in the cloud.</p>

<p>In this post, Iâ€™ll share my workflow for developing Supervised Fine-Tuning (SFT) code on a MacBook with Apple Silicon, testing it locally, then seamlessly deploying to cloud instances like <img src="https://lambdalabs.com/favicon.ico" height="20" style="vertical-align: middle;" /> <a href="https://lambdalabs.com/">Lambda Labs</a>.</p>

<p><em>This workflow was developed while implementing SFT for Qwen3-1.7B on the MATH dataset, but the principles apply broadly to any PyTorch-based training pipeline development.</em></p>

<p><strong>All code is available on GitHub:</strong> <a href="https://github.com/bearbearyu1223/qwen3_supervised_fine_tuning">bearbearyu1223/qwen3_supervised_fine_tuning</a></p>

<h3 id="table-of-contents">Table of Contents</h3>
<ul>
  <li><a href="#fine-tuning-qwen3-17b-on-lambda-labs-for-math-reasoning">Fine-Tuning Qwen3-1.7B on Lambda Labs for Math Reasoning</a>
    <ul>
      <li><a href="#table-of-contents">Table of Contents</a></li>
      <li><a href="#the-challenge-bridging-local-and-cloud-development">The Challenge: Bridging Local and Cloud Development</a></li>
      <li><a href="#part-1-setting-up-local-development-environment">Part 1: Setting Up Local Development Environment</a>
        <ul>
          <li><a href="#why-apple-silicon-for-ml-development">Why Apple Silicon for ML Development?</a></li>
          <li><a href="#why-qwen3-17b-as-the-base-model">Why Qwen3-1.7B as the Base Model?</a></li>
          <li><a href="#project-structure-and-package-management">Project Structure and Package Management</a></li>
        </ul>
      </li>
      <li><a href="#part-2-writing-device-agnostic-training-code">Part 2: Writing Device-Agnostic Training Code</a>
        <ul>
          <li><a href="#automatic-hardware-detection">Automatic Hardware Detection</a></li>
          <li><a href="#numerical-precision-considerations">Numerical Precision Considerations</a></li>
          <li><a href="#gradient-accumulation-for-memory-efficiency">Gradient Accumulation for Memory Efficiency</a></li>
        </ul>
      </li>
      <li><a href="#part-3-the-training-pipeline">Part 3: The Training Pipeline</a>
        <ul>
          <li><a href="#data-preparation-the-math-dataset">Data Preparation: The MATH Dataset</a></li>
          <li><a href="#the-r1_zero-prompt-format">The r1_zero Prompt Format</a></li>
          <li><a href="#response-masking-for-sft">Response Masking for SFT</a></li>
        </ul>
      </li>
      <li><a href="#part-4-local-testing-and-validation">Part 4: Local Testing and Validation</a>
        <ul>
          <li><a href="#quick-sanity-checks">Quick Sanity Checks</a></li>
          <li><a href="#inference-backend-local-vs-cloud">Inference Backend: Local vs Cloud</a></li>
        </ul>
      </li>
      <li><a href="#part-5-scaling-with-huggingface-accelerate">Part 5: Scaling with HuggingFace Accelerate</a>
        <ul>
          <li><a href="#why-huggingface-accelerate">Why HuggingFace Accelerate</a></li>
          <li><a href="#code-changes-for-multi-gpu-support">Code Changes for Multi-GPU Support</a></li>
        </ul>
      </li>
      <li><a href="#part-6-deploying-to-lambda-cloud">Part 6: Deploying to Lambda Cloud</a>
        <ul>
          <li><a href="#step-by-step-deployment">Step-by-Step Deployment</a></li>
        </ul>
      </li>
      <li><a href="#part-7-evaluation-pipeline">Part 7: Evaluation Pipeline</a>
        <ul>
          <li><a href="#math-answer-grading">Math Answer Grading</a></li>
          <li><a href="#running-evaluation">Running Evaluation</a></li>
          <li><a href="#results-before-and-after-sft">Results: Before and After SFT</a></li>
        </ul>
      </li>
      <li><a href="#part-8-practical-recommendations">Part 8: Practical Recommendations</a>
        <ul>
          <li><a href="#development-workflow-summary">Development Workflow Summary</a></li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h3 id="the-challenge-bridging-local-and-cloud-development">The Challenge: Bridging Local and Cloud Development</h3>

<p>My typical ML development workflow faces a fundamental tensionâ€”I use a MacBook Pro with M-series chips for personal side projects, which creates some tradeoffs:</p>

<table>
  <thead>
    <tr>
      <th>Environment</th>
      <th>Pros</th>
      <th>Cons</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Local (MacBook)</strong></td>
      <td>Fast iteration, no cost, familiar tools</td>
      <td>Limited memory, slower training, no CUDA</td>
    </tr>
    <tr>
      <td><strong>Cloud (Lambda)</strong></td>
      <td>Powerful GPUs, scalable, CUDA support</td>
      <td>Setup overhead, costs money, less interactive</td>
    </tr>
  </tbody>
</table>

<p>The ideal workflow would let me:</p>
<ol>
  <li><strong>Develop locally</strong> with fast feedback loops</li>
  <li><strong>Test easily</strong> before committing cloud resources</li>
  <li><strong>Deploy seamlessly</strong> without rewriting code</li>
  <li><strong>Scale horizontally</strong> when more compute is available</li>
</ol>

<p>This post presents a battle-tested approach to achieving all four.</p>

<h3 id="part-1-setting-up-local-development-environment">Part 1: Setting Up Local Development Environment</h3>

<h4 id="why-apple-silicon-for-ml-development">Why Apple Silicon for ML Development?</h4>

<p>Beyond personal preference, Apple Silicon Macs offer a genuinely compelling development environment:</p>

<ul>
  <li><strong>Unified Memory Architecture</strong>: 16â€“64GB RAM shared between CPU and GPU</li>
  <li><strong>Metal Performance Shaders (MPS)</strong>: PyTorch backend for GPU acceleration</li>
  <li><strong>Power Efficiency</strong>: Extended battery life for portable development</li>
  <li><strong>Native ARM</strong>: Fast Python and native tool execution</li>
</ul>

<p>However, there are important limitations:</p>

<table>
  <thead>
    <tr>
      <th>Feature</th>
      <th>CUDA (NVIDIA)</th>
      <th>MPS (Apple Silicon)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Float16 Training</td>
      <td>Stable with gradient scaling</td>
      <td>Often causes NaN losses</td>
    </tr>
    <tr>
      <td>BFloat16</td>
      <td>Full support (Ampere+)</td>
      <td>Limited support</td>
    </tr>
    <tr>
      <td>Multi-GPU</td>
      <td>NCCL, NVLink</td>
      <td>Single GPU only</td>
    </tr>
    <tr>
      <td>Flash Attention</td>
      <td>Available</td>
      <td>Not available</td>
    </tr>
    <tr>
      <td>Memory</td>
      <td>Dedicated VRAM</td>
      <td>Shared system RAM</td>
    </tr>
  </tbody>
</table>

<p><strong>Key Insight</strong>: MPS (Metal Performance Shadersâ€”Appleâ€™s GPU-accelerated compute framework) is excellent for development and testing but usually requires float32 precision for numerical stability.</p>

<h4 id="why-qwen3-17b-as-the-base-model">Why Qwen3-1.7B as the Base Model?</h4>

<p>Choosing the right base model is critical for SFT projects. I selected <a href="https://huggingface.co/Qwen/Qwen3-1.7B">Qwen3-1.7B</a> for several reasons:</p>

<p><strong>1. Right-sized for the task</strong></p>

<table>
  <thead>
    <tr>
      <th>Model Size</th>
      <th>Local Dev (32GB Mac)</th>
      <th>Single A100 (40GB)</th>
      <th>Training Time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0.5Bâ€“1B</td>
      <td>Comfortable</td>
      <td>Very fast</td>
      <td>Minutes</td>
    </tr>
    <tr>
      <td><strong>1.7B</strong></td>
      <td><strong>Workable</strong></td>
      <td><strong>Fast</strong></td>
      <td><strong>~1 hour</strong></td>
    </tr>
    <tr>
      <td>4Bâ€“8B</td>
      <td>Challenging</td>
      <td>Comfortable</td>
      <td>Hours</td>
    </tr>
    <tr>
      <td>14B+</td>
      <td>Not feasible</td>
      <td>Tight fit</td>
      <td>Many hours</td>
    </tr>
  </tbody>
</table>

<p>At 1.7B parameters, Qwen3-1.7B hits a sweet spot: small enough to iterate quickly on a MacBook during development, yet large enough to demonstrate meaningful learning on complex math problems.</p>

<p><strong>2. Strong base capabilities</strong></p>

<p>Qwen3 models come with several architectural improvements:</p>

<ul>
  <li><strong>Improved tokenizer</strong>: Better handling of mathematical notation and LaTeX</li>
  <li><strong>Extended context</strong>: 32K context window for longer reasoning chains</li>
  <li><strong>Thinking mode support</strong>: Native support for <code class="language-plaintext highlighter-rouge">&lt;think&gt;</code> tags that align with our r1_zero format</li>
</ul>

<p><strong>3. Active ecosystem</strong></p>

<ul>
  <li>Regular updates from Alibabaâ€™s Qwen team</li>
  <li>Good HuggingFace Transformers integration</li>
  <li>Compatible with vLLM for fast inference</li>
  <li>Apache 2.0 license for commercial use</li>
</ul>

<p><strong>4. Qwen3 model family options</strong></p>

<p>The Qwen3 family provides a natural scaling path:</p>

<table>
  <thead>
    <tr>
      <th>Model</th>
      <th>Parameters</th>
      <th>Use Case</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Qwen3-0.6B</td>
      <td>0.6B</td>
      <td>Rapid prototyping, edge deployment</td>
    </tr>
    <tr>
      <td><strong>Qwen3-1.7B</strong></td>
      <td><strong>1.7B</strong></td>
      <td><strong>Development, single-GPU training</strong></td>
    </tr>
    <tr>
      <td>Qwen3-4B</td>
      <td>4B</td>
      <td>Production fine-tuning</td>
    </tr>
    <tr>
      <td>Qwen3-8B</td>
      <td>8B</td>
      <td>High-quality results, multi-GPU</td>
    </tr>
    <tr>
      <td>Qwen3-14B/32B</td>
      <td>14B/32B</td>
      <td>State-of-the-art, distributed training</td>
    </tr>
  </tbody>
</table>

<p>Starting with 1.7B allows rapid iteration. Once the pipeline is validated, scaling up to 4B or 8B for better results is straightforwardâ€”the same code works across all sizes.</p>

<h4 id="project-structure-and-package-management">Project Structure and Package Management</h4>

<p>I use <a href="https://github.com/astral-sh/uv"><code class="language-plaintext highlighter-rouge">uv</code></a> for fast, reproducible Python package management.</p>

<p><strong>Install uv:</strong></p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>curl <span class="nt">-LsSf</span> https://astral.sh/uv/install.sh | sh
</code></pre></div></div>

<p><strong>Project structure:</strong></p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>qwen3_supervised_fine_tuning/
â”œâ”€â”€ cs336_alignment/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ sft.py              # Main training code
â”‚   â”œâ”€â”€ evaluate_math.py    # Evaluation utilities
â”‚   â”œâ”€â”€ drgrpo_grader.py    # Math grading functions
â”‚   â””â”€â”€ prompts/
â”‚       â””â”€â”€ r1_zero.prompt  # Prompt template
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ run_sft.py          # Training entry point
â”‚   â”œâ”€â”€ run_math_eval.py    # Evaluation entry point
â”‚   â”œâ”€â”€ download_model.py   # Model downloader
â”‚   â””â”€â”€ download_math.py    # Data downloader
â”œâ”€â”€ data/math/              # MATH dataset
â”œâ”€â”€ pyproject.toml          # Dependencies
â””â”€â”€ uv.lock                 # Locked versions
</code></pre></div></div>

<p><strong>pyproject.toml</strong> with optional CUDA dependencies:</p>
<div class="language-toml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nn">[project]</span>
<span class="py">name</span> <span class="p">=</span> <span class="s">"qwen3-sft"</span>
<span class="py">requires-python</span> <span class="p">=</span> <span class="py">"&gt;</span><span class="p">=</span><span class="mf">3.11</span><span class="p">,</span><span class="err">&lt;</span><span class="mf">3.13</span><span class="s">"</span><span class="err">
</span><span class="py">dependencies</span> <span class="p">=</span> <span class="p">[</span>
    <span class="py">"accelerate&gt;</span><span class="p">=</span><span class="mf">1.5</span><span class="err">.</span><span class="mi">2</span><span class="s">",</span><span class="err">
</span>    <span class="s">"torch"</span><span class="p">,</span>
    <span class="py">"transformers&gt;</span><span class="p">=</span><span class="mf">4.50</span><span class="err">.</span><span class="mi">0</span><span class="s">",</span><span class="err">
</span>    <span class="py">"datasets&gt;</span><span class="p">=</span><span class="mf">3.0</span><span class="err">.</span><span class="mi">0</span><span class="s">",</span><span class="err">
</span>    <span class="py">"tqdm&gt;</span><span class="p">=</span><span class="mf">4.67</span><span class="err">.</span><span class="mi">1</span><span class="s">",</span><span class="err">
</span>    <span class="py">"matplotlib&gt;</span><span class="p">=</span><span class="mf">3.8</span><span class="err">.</span><span class="mi">0</span><span class="s">",</span><span class="err">
</span><span class="p">]</span>

<span class="nn">[project.optional-dependencies]</span>
<span class="py">cuda</span> <span class="p">=</span> <span class="p">[</span>
    <span class="py">"flash-attn=</span><span class="p">=</span><span class="mf">2.7</span><span class="err">.</span><span class="mi">4</span><span class="err">.post</span><span class="mi">1</span><span class="s">",</span><span class="err">
</span><span class="p">]</span>
</code></pre></div></div>

<p><strong>Local installation:</strong></p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>uv <span class="nb">sync</span>              <span class="c"># Basic install (Mac/CPU)</span>
uv <span class="nb">sync</span> <span class="nt">--extra</span> cuda <span class="c"># With CUDA extras (Linux with GPU)</span>
</code></pre></div></div>

<h3 id="part-2-writing-device-agnostic-training-code">Part 2: Writing Device-Agnostic Training Code</h3>

<p>The key to seamless local-to-cloud transitions is writing code that adapts to available hardware without manual changes.</p>

<h4 id="automatic-hardware-detection">Automatic Hardware Detection</h4>

<p>The training code implements sophisticated automatic device detection:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">detect_compute_environment</span><span class="p">(</span><span class="n">model_name_or_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ComputeEnvironment</span><span class="p">:</span>
    <span class="s">"""Detect hardware and recommend optimal training settings."""</span>

    <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="n">is_available</span><span class="p">():</span>
        <span class="n">num_gpus</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="n">device_count</span><span class="p">()</span>
        <span class="n">device_name</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="n">get_device_name</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">total_memory_gb</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="n">get_device_properties</span><span class="p">(</span><span class="mi">0</span><span class="p">).</span><span class="n">total_memory</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1024</span><span class="o">**</span><span class="mi">3</span><span class="p">)</span>
        <span class="n">supports_bf16</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="n">is_bf16_supported</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">ComputeEnvironment</span><span class="p">(</span>
            <span class="n">device</span><span class="o">=</span><span class="s">"cuda"</span><span class="p">,</span>
            <span class="n">num_gpus</span><span class="o">=</span><span class="n">num_gpus</span><span class="p">,</span>
            <span class="n">memory_gb</span><span class="o">=</span><span class="n">total_memory_gb</span><span class="p">,</span>
            <span class="n">supports_bf16</span><span class="o">=</span><span class="n">supports_bf16</span><span class="p">,</span>
            <span class="p">...</span>
        <span class="p">)</span>

    <span class="k">elif</span> <span class="n">torch</span><span class="p">.</span><span class="n">backends</span><span class="p">.</span><span class="n">mps</span><span class="p">.</span><span class="n">is_available</span><span class="p">():</span>
        <span class="c1"># Extract memory via sysctl on macOS
</span>        <span class="n">memory_gb</span> <span class="o">=</span> <span class="n">get_mac_memory</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">ComputeEnvironment</span><span class="p">(</span>
            <span class="n">device</span><span class="o">=</span><span class="s">"mps"</span><span class="p">,</span>
            <span class="n">num_gpus</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
            <span class="n">memory_gb</span><span class="o">=</span><span class="n">memory_gb</span><span class="p">,</span>
            <span class="n">supports_bf16</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>  <span class="c1"># MPS has limited BF16 support
</span>            <span class="p">...</span>
        <span class="p">)</span>

    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">ComputeEnvironment</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="s">"cpu"</span><span class="p">,</span> <span class="p">...)</span>
</code></pre></div></div>

<p>This allows running with <code class="language-plaintext highlighter-rouge">--auto</code> to automatically detect and configure optimal settings:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>uv run scripts/run_sft.py <span class="nt">--auto</span> <span class="se">\</span>
    <span class="nt">--model-name-or-path</span> models/qwen3-1.7b <span class="se">\</span>
    <span class="nt">--train-data-path</span> data/math/train.jsonl <span class="se">\</span>
    <span class="nt">--output-dir</span> outputs/sft_qwen3
</code></pre></div></div>

<h4 id="numerical-precision-considerations">Numerical Precision Considerations</h4>

<p>This is where many developers encounter their first â€œworks locally, fails on cloudâ€ (or vice versa) bug:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">get_dtype_and_precision</span><span class="p">(</span><span class="n">device</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">torch</span><span class="p">.</span><span class="n">dtype</span><span class="p">,</span> <span class="nb">str</span><span class="p">]:</span>
    <span class="s">"""
    Determine appropriate dtype and mixed precision setting.

    Critical insight: MPS does NOT support float16 training reliably.
    Using float16 on MPS often results in NaN losses.
    """</span>
    <span class="k">if</span> <span class="n">device</span> <span class="o">==</span> <span class="s">"cuda"</span><span class="p">:</span>
        <span class="c1"># CUDA: Use bfloat16 if available (Ampere+), else float16
</span>        <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="n">is_bf16_supported</span><span class="p">():</span>
            <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="n">bfloat16</span><span class="p">,</span> <span class="s">"bf16"</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="n">float16</span><span class="p">,</span> <span class="s">"fp16"</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># MPS and CPU: Use float32 for numerical stability
</span>        <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">,</span> <span class="s">"no"</span>
</code></pre></div></div>

<p><strong>Why This Matters</strong>:</p>

<table>
  <thead>
    <tr>
      <th>Device</th>
      <th>Recommended Dtype</th>
      <th>Reason</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>CUDA (Ampere+)</td>
      <td>bfloat16</td>
      <td>Best balance of speed and stability</td>
    </tr>
    <tr>
      <td>CUDA (older)</td>
      <td>float16</td>
      <td>With gradient scaling</td>
    </tr>
    <tr>
      <td>MPS</td>
      <td>float32</td>
      <td>float16 causes NaN losses</td>
    </tr>
    <tr>
      <td>CPU</td>
      <td>float32</td>
      <td>No mixed precision benefit</td>
    </tr>
  </tbody>
</table>

<h4 id="gradient-accumulation-for-memory-efficiency">Gradient Accumulation for Memory Efficiency</h4>

<p>With limited memory on laptops, gradient accumulation is essential. The code computes optimal settings based on available hardware:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Memory estimation (rough heuristics)
</span><span class="k">if</span> <span class="n">device</span> <span class="o">==</span> <span class="s">"cuda"</span><span class="p">:</span>
    <span class="n">memory_per_batch</span> <span class="o">=</span> <span class="n">model_size_billions</span> <span class="o">*</span> <span class="mi">6</span>  <span class="c1"># fp16 training
</span><span class="k">elif</span> <span class="n">device</span> <span class="o">==</span> <span class="s">"mps"</span><span class="p">:</span>
    <span class="n">memory_per_batch</span> <span class="o">=</span> <span class="n">model_size_billions</span> <span class="o">*</span> <span class="mi">12</span>  <span class="c1"># fp32 + shared memory
</span><span class="k">else</span><span class="p">:</span>
    <span class="n">memory_per_batch</span> <span class="o">=</span> <span class="n">model_size_billions</span> <span class="o">*</span> <span class="mi">16</span>

<span class="c1"># Compute max batch size with 70% safety margin
</span><span class="n">max_batch_size</span> <span class="o">=</span> <span class="nb">int</span><span class="p">((</span><span class="n">available_memory_gb</span> <span class="o">*</span> <span class="mf">0.7</span><span class="p">)</span> <span class="o">/</span> <span class="n">memory_per_batch</span><span class="p">)</span>
<span class="n">max_batch_size</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">min</span><span class="p">(</span><span class="n">max_batch_size</span><span class="p">,</span> <span class="n">device_caps</span><span class="p">[</span><span class="n">device</span><span class="p">]))</span>

<span class="c1"># Compute gradient accumulation for target effective batch
</span><span class="n">gradient_accumulation_steps</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">target_effective_batch</span> <span class="o">//</span> <span class="n">max_batch_size</span><span class="p">)</span>
</code></pre></div></div>

<p><strong>Memory Scaling Recommendations for Qwen3-1.7B</strong>:</p>

<table>
  <thead>
    <tr>
      <th>Device</th>
      <th>Memory</th>
      <th>batch_size</th>
      <th>gradient_accumulation_steps</th>
      <th>Effective Batch</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>MacBook M-series</td>
      <td>16â€“32GB</td>
      <td>1â€“2</td>
      <td>8â€“16</td>
      <td>16</td>
    </tr>
    <tr>
      <td>NVIDIA A100 (40GB)</td>
      <td>40GB</td>
      <td>8</td>
      <td>2</td>
      <td>16</td>
    </tr>
    <tr>
      <td>NVIDIA A100 (80GB)</td>
      <td>80GB</td>
      <td>16</td>
      <td>1</td>
      <td>16</td>
    </tr>
  </tbody>
</table>

<h3 id="part-3-the-training-pipeline">Part 3: The Training Pipeline</h3>

<h4 id="data-preparation-the-math-dataset">Data Preparation: The MATH Dataset</h4>

<p>The <a href="https://github.com/hendrycks/math">MATH dataset</a> is a collection of 12,500 challenging competition mathematics problems (12,000 train / 500 test). Each problem includes a detailed step-by-step solution, making it ideal for training models on mathematical reasoning.</p>

<p><strong>Dataset Structure:</strong></p>

<p>Each example contains:</p>
<ul>
  <li><code class="language-plaintext highlighter-rouge">problem</code>: The math question</li>
  <li><code class="language-plaintext highlighter-rouge">solution</code>: Step-by-step solution with reasoning</li>
  <li><code class="language-plaintext highlighter-rouge">answer</code>: Final answer (often in <code class="language-plaintext highlighter-rouge">\boxed{}</code> format)</li>
  <li><code class="language-plaintext highlighter-rouge">subject</code>: One of 7 mathematical topics</li>
  <li><code class="language-plaintext highlighter-rouge">level</code>: Difficulty from 1 (easiest) to 5 (hardest)</li>
</ul>

<p><strong>Subject Distribution:</strong></p>

<p>The dataset covers 7 mathematical topics:</p>

<table>
  <thead>
    <tr>
      <th>Subject</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Prealgebra</td>
      <td>Basic arithmetic, fractions, percentages</td>
    </tr>
    <tr>
      <td>Algebra</td>
      <td>Equations, polynomials, functions</td>
    </tr>
    <tr>
      <td>Number Theory</td>
      <td>Divisibility, primes, modular arithmetic</td>
    </tr>
    <tr>
      <td>Counting &amp; Probability</td>
      <td>Combinatorics, probability theory</td>
    </tr>
    <tr>
      <td>Geometry</td>
      <td>Triangles, circles, coordinate geometry</td>
    </tr>
    <tr>
      <td>Intermediate Algebra</td>
      <td>Complex equations, series, inequalities</td>
    </tr>
    <tr>
      <td>Precalculus</td>
      <td>Trigonometry, vectors, complex numbers</td>
    </tr>
  </tbody>
</table>

<p><strong>Example Problems by Difficulty:</strong></p>

<p>Here are examples showing the range of difficulty levels:</p>

<hr />

<p><strong>Level 2 (Prealgebra)</strong> â€” Straightforward algebraic manipulation:</p>

<blockquote>
  <p><strong>Problem:</strong> If $5x - 3 = 12$, what is the value of $5x + 3$?</p>

  <p><strong>Solution:</strong> Adding 6 to both sides of $5x - 3 = 12$ gives $5x - 3 + 6 = 12 + 6$. Simplifying both sides gives $5x + 3 = \boxed{18}$.</p>

  <p><strong>Answer:</strong> <code class="language-plaintext highlighter-rouge">18</code></p>
</blockquote>

<hr />

<p><strong>Level 3 (Algebra)</strong> â€” Requires understanding of functions:</p>

<blockquote>
  <p><strong>Problem:</strong> How many vertical asymptotes does the graph of $y=\frac{2}{x^2+x-6}$ have?</p>

  <p><strong>Solution:</strong> The denominator of the rational function factors into $x^2+x-6=(x-2)(x+3)$. Since the numerator is always nonzero, there is a vertical asymptote whenever the denominator is $0$, which occurs for $x = 2$ and $x = -3$. Therefore, the graph has $\boxed{2}$ vertical asymptotes.</p>

  <p><strong>Answer:</strong> <code class="language-plaintext highlighter-rouge">2</code></p>
</blockquote>

<hr />

<p><strong>Level 4 (Geometry)</strong> â€” Multi-step geometric reasoning:</p>

<blockquote>
  <p><strong>Problem:</strong> In triangle $\triangle ABC$, we have that $AB = AC = 14$ and $BC = 26$. What is the length of the shortest angle bisector in $ABC$? Express your answer in simplest radical form.</p>

  <p><strong>Solution:</strong> The shortest angle bisector will be from vertex $A$. Since $\triangle ABC$ is isosceles, the angle bisector from $A$ is also the perpendicular bisector of $BC$. Using the Pythagorean theorem with $AC = 14$ and $DC = \frac{1}{2} \cdot BC = 13$, we find $AD^2 = AC^2 - CD^2 = 14^2 - 13^2 = 27$. Therefore, $AD = \boxed{3\sqrt{3}}$.</p>

  <p><strong>Answer:</strong> <code class="language-plaintext highlighter-rouge">3\sqrt{3}</code></p>
</blockquote>

<hr />

<p><strong>Level 5 (Counting &amp; Probability)</strong> â€” Complex multi-case reasoning:</p>

<blockquote>
  <p><strong>Problem:</strong> Ryan has 3 red lava lamps and 3 blue lava lamps. He arranges them in a row on a shelf randomly, then turns 3 random lamps on. What is the probability that the leftmost lamp on the shelf is red, and the leftmost lamp which is turned on is also red?</p>

  <p><strong>Solution:</strong> There are $\binom{6}{3}=20$ ways to arrange the lamps, and $\binom{6}{3}=20$ ways to choose which are on, giving $20 \cdot 20=400$ total outcomes. Case 1: If the left lamp is on, there are $\binom{5}{2}=10$ ways to choose other on-lamps and $\binom{5}{2}=10$ ways to choose other red lamps, giving 100 possibilities. Case 2: If the left lamp isnâ€™t on, there are $\binom{5}{3}=10$ ways to choose on-lamps, and $\binom{4}{1}=4$ ways to choose the other red lamp, giving 40 possibilities. Total: $\frac{140}{400}=\boxed{\frac{7}{20}}$.</p>

  <p><strong>Answer:</strong> <code class="language-plaintext highlighter-rouge">\dfrac{7}{20}</code></p>
</blockquote>

<hr />

<p><strong>Download the dataset:</strong></p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>uv run python scripts/download_math.py
</code></pre></div></div>

<h4 id="the-r1_zero-prompt-format">The r1_zero Prompt Format</h4>

<p>The training uses the r1_zero prompt format which encourages chain-of-thought reasoning:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>A conversation between User and Assistant. The User asks a question,
and the Assistant solves it. The Assistant first thinks about the
reasoning process in the mind and then provides the User with the answer.
The reasoning process is enclosed within &lt;think&gt; &lt;/think&gt; and answer
is enclosed within &lt;answer&gt; &lt;/answer&gt; tags, respectively.

User: {question}
Assistant: &lt;think&gt;
{reasoning}
&lt;/think&gt; &lt;answer&gt; {answer} &lt;/answer&gt;
</code></pre></div></div>

<p>This format teaches the model to:</p>
<ol>
  <li>Think through the problem step-by-step in <code class="language-plaintext highlighter-rouge">&lt;think&gt;</code> tags</li>
  <li>Provide a clear final answer in <code class="language-plaintext highlighter-rouge">&lt;answer&gt;</code> tags</li>
</ol>

<h4 id="response-masking-for-sft">Response Masking for SFT</h4>

<p>A key implementation detail: we only compute loss on response tokens, not the prompt:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">MathSFTDataset</span><span class="p">(</span><span class="n">Dataset</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">):</span>
        <span class="c1"># Tokenize prompt and response separately
</span>        <span class="n">prompt_tokens</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">tokenizer</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="p">...)</span>
        <span class="n">response_tokens</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">tokenizer</span><span class="p">(</span><span class="n">response</span><span class="p">,</span> <span class="p">...)</span>

        <span class="c1"># Create response mask: 1 for response, 0 for prompt/padding
</span>        <span class="n">response_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">cat</span><span class="p">([</span>
            <span class="n">torch</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">prompt_tokens</span><span class="p">)),</span>
            <span class="n">torch</span><span class="p">.</span><span class="n">ones</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">response_tokens</span><span class="p">)),</span>
            <span class="n">torch</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">padding_length</span><span class="p">)</span>
        <span class="p">])</span>

        <span class="k">return</span> <span class="p">{</span>
            <span class="s">"input_ids"</span><span class="p">:</span> <span class="n">input_ids</span><span class="p">,</span>
            <span class="s">"attention_mask"</span><span class="p">:</span> <span class="n">attention_mask</span><span class="p">,</span>
            <span class="s">"response_mask"</span><span class="p">:</span> <span class="n">response_mask</span><span class="p">,</span>
        <span class="p">}</span>
</code></pre></div></div>

<p>The loss is then computed only on response tokens:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># NLL loss normalized by response tokens
</span><span class="n">nll_loss</span> <span class="o">=</span> <span class="n">masked_normalize</span><span class="p">(</span>
    <span class="n">tensor</span><span class="o">=-</span><span class="n">policy_log_probs</span><span class="p">,</span>
    <span class="n">mask</span><span class="o">=</span><span class="n">response_mask</span><span class="p">,</span>
    <span class="n">normalize_constant</span><span class="o">=</span><span class="n">num_response_tokens</span><span class="p">,</span>
<span class="p">)</span>
</code></pre></div></div>

<h3 id="part-4-local-testing-and-validation">Part 4: Local Testing and Validation</h3>

<p>Before deploying to cloud, thorough local testing saves time and money.</p>

<h4 id="quick-sanity-checks">Quick Sanity Checks</h4>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Test with minimal samples to verify pipeline works</span>
uv run python scripts/run_sft.py <span class="se">\</span>
    <span class="nt">--model-name-or-path</span> models/qwen3-1.7b <span class="se">\</span>
    <span class="nt">--train-data-path</span> data/math/train.jsonl <span class="se">\</span>
    <span class="nt">--output-dir</span> outputs/sft_test <span class="se">\</span>
    <span class="nt">--num-samples</span> 10 <span class="se">\</span>
    <span class="nt">--num-epochs</span> 1 <span class="se">\</span>
    <span class="nt">--batch-size</span> 1 <span class="se">\</span>
    <span class="nt">--gradient-accumulation-steps</span> 2
</code></pre></div></div>

<p><strong>What to verify:</strong></p>
<ol>
  <li>Model loads without errors</li>
  <li>Data pipeline produces valid batches</li>
  <li>Loss decreases (not NaN or constant)</li>
  <li>Checkpoints save correctly</li>
  <li>Model can be reloaded from checkpoint</li>
</ol>

<h4 id="inference-backend-local-vs-cloud">Inference Backend: Local vs Cloud</h4>

<p>A key challenge when developing on Apple Silicon is that <a href="https://github.com/vllm-project/vllm">vLLM</a>â€”the go-to inference engine for fast LLM servingâ€”requires CUDA and doesnâ€™t run on Macs. The evaluation code handles this with two backends:</p>

<table>
  <thead>
    <tr>
      <th>Environment</th>
      <th>Inference Backend</th>
      <th>Why</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Local (MPS)</td>
      <td>HuggingFace Transformers</td>
      <td>Pure PyTorch, runs anywhere</td>
    </tr>
    <tr>
      <td>Cloud (CUDA)</td>
      <td>vLLM</td>
      <td>Optimized kernels, 10â€“20Ã— faster</td>
    </tr>
  </tbody>
</table>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">get_inference_backend</span><span class="p">(</span><span class="n">model_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>
    <span class="s">"""Return appropriate inference backend for the current environment."""</span>
    <span class="k">if</span> <span class="n">device</span> <span class="o">==</span> <span class="s">"cuda"</span> <span class="ow">and</span> <span class="n">is_vllm_available</span><span class="p">():</span>
        <span class="kn">from</span> <span class="nn">vllm</span> <span class="kn">import</span> <span class="n">LLM</span>
        <span class="k">return</span> <span class="n">VLLMBackend</span><span class="p">(</span><span class="n">LLM</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model_path</span><span class="p">))</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># Fallback to HuggingFace for MPS/CPU
</span>        <span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">AutoTokenizer</span>
        <span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="p">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_path</span><span class="p">).</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="p">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_path</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">TransformersBackend</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="part-5-scaling-with-huggingface-accelerate">Part 5: Scaling with HuggingFace Accelerate</h3>

<h4 id="why-huggingface-accelerate">Why HuggingFace Accelerate</h4>

<table>
  <thead>
    <tr>
      <th>Feature</th>
      <th>Manual DDP</th>
      <th>Accelerate</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Code changes</td>
      <td>Significant</td>
      <td>Minimal</td>
    </tr>
    <tr>
      <td>Device placement</td>
      <td>Manual</td>
      <td>Automatic</td>
    </tr>
    <tr>
      <td>Gradient sync</td>
      <td>Manual</td>
      <td>Automatic</td>
    </tr>
    <tr>
      <td>Mixed precision</td>
      <td>Manual setup</td>
      <td>One flag</td>
    </tr>
    <tr>
      <td>Single/Multi GPU</td>
      <td>Different code paths</td>
      <td>Same code</td>
    </tr>
  </tbody>
</table>

<h4 id="code-changes-for-multi-gpu-support">Code Changes for Multi-GPU Support</h4>

<p>The key changes to support multi-GPU:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">accelerate</span> <span class="kn">import</span> <span class="n">Accelerator</span>

<span class="k">def</span> <span class="nf">train_sft</span><span class="p">(</span><span class="n">config</span><span class="p">):</span>
    <span class="c1"># Initialize Accelerator
</span>    <span class="n">accelerator</span> <span class="o">=</span> <span class="n">Accelerator</span><span class="p">(</span>
        <span class="n">gradient_accumulation_steps</span><span class="o">=</span><span class="n">config</span><span class="p">.</span><span class="n">gradient_accumulation_steps</span><span class="p">,</span>
        <span class="n">mixed_precision</span><span class="o">=</span><span class="s">"bf16"</span> <span class="k">if</span> <span class="n">device</span> <span class="o">==</span> <span class="s">"cuda"</span> <span class="k">else</span> <span class="s">"no"</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="c1"># Prepare model, optimizer, dataloader
</span>    <span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">dataloader</span><span class="p">,</span> <span class="n">scheduler</span> <span class="o">=</span> <span class="n">accelerator</span><span class="p">.</span><span class="n">prepare</span><span class="p">(</span>
        <span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">dataloader</span><span class="p">,</span> <span class="n">scheduler</span>
    <span class="p">)</span>

    <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">dataloader</span><span class="p">:</span>
        <span class="c1"># Use accelerator's gradient accumulation context
</span>        <span class="k">with</span> <span class="n">accelerator</span><span class="p">.</span><span class="n">accumulate</span><span class="p">(</span><span class="n">model</span><span class="p">):</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">compute_loss</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">batch</span><span class="p">)</span>
            <span class="n">accelerator</span><span class="p">.</span><span class="n">backward</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">accelerator</span><span class="p">.</span><span class="n">sync_gradients</span><span class="p">:</span>
                <span class="n">accelerator</span><span class="p">.</span><span class="n">clip_grad_norm_</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">max_norm</span><span class="p">)</span>

            <span class="n">optimizer</span><span class="p">.</span><span class="n">step</span><span class="p">()</span>
            <span class="n">scheduler</span><span class="p">.</span><span class="n">step</span><span class="p">()</span>
            <span class="n">optimizer</span><span class="p">.</span><span class="n">zero_grad</span><span class="p">()</span>

    <span class="c1"># Save only on main process
</span>    <span class="k">if</span> <span class="n">accelerator</span><span class="p">.</span><span class="n">is_main_process</span><span class="p">:</span>
        <span class="n">unwrapped_model</span> <span class="o">=</span> <span class="n">accelerator</span><span class="p">.</span><span class="n">unwrap_model</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
        <span class="n">unwrapped_model</span><span class="p">.</span><span class="n">save_pretrained</span><span class="p">(</span><span class="n">output_dir</span><span class="p">)</span>
</code></pre></div></div>

<p><strong>Key Accelerate Patterns</strong>:</p>

<ol>
  <li><strong><code class="language-plaintext highlighter-rouge">accelerator.prepare()</code></strong>: Wraps objects for distributed training</li>
  <li><strong><code class="language-plaintext highlighter-rouge">accelerator.accumulate()</code></strong>: Handles gradient accumulation correctly</li>
  <li><strong><code class="language-plaintext highlighter-rouge">accelerator.backward()</code></strong>: Syncs gradients across devices</li>
  <li><strong><code class="language-plaintext highlighter-rouge">accelerator.sync_gradients</code></strong>: True when accumulation cycle completes</li>
  <li><strong><code class="language-plaintext highlighter-rouge">accelerator.is_main_process</code></strong>: Only one process logs/saves</li>
</ol>

<h3 id="part-6-deploying-to-lambda-cloud">Part 6: Deploying to Lambda Cloud</h3>

<h4 id="step-by-step-deployment">Step-by-Step Deployment</h4>

<p>This guide uses a <strong>1x A100 40GB SXM4</strong> instance on <a href="https://lambdalabs.com/service/gpu-cloud">Lambda Cloud</a>.</p>

<p><strong>Step 1: Launch Instance and SSH</strong></p>

<p>Go to <a href="https://cloud.lambdalabs.com/">Lambda Cloud</a> and launch a <strong>1x A100 40GB SXM4</strong> instance</p>

<p><img src="/assets/picture/2026-01-19-cs336-sft-qwen3-for-math-reasoning/select_gou_instance.png" alt="Select GPU Instance on Lambda Cloud" /></p>

<p>SSH into your instance:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ssh ubuntu@&lt;your-instance-ip&gt;
</code></pre></div></div>

<p><img src="/assets/picture/2026-01-19-cs336-sft-qwen3-for-math-reasoning/ssh_into_lambda_compute_instance.png" alt="SSH into Lambda Compute Instance" /></p>

<p><strong>Step 2: Clone and Setup Environment</strong></p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Clone the repository</span>
git clone https://github.com/bearbearyu1223/qwen3_supervised_fine_tuning.git
<span class="nb">cd </span>qwen3_supervised_fine_tuning

<span class="c"># Install uv package manager</span>
curl <span class="nt">-LsSf</span> https://astral.sh/uv/install.sh | sh

<span class="c"># Install dependencies </span>
uv <span class="nb">sync</span> 

<span class="c"># Install dependecies with CUDA support for flash-attn and vLLM</span>
uv <span class="nb">sync</span> <span class="nt">--extra</span> cuda
</code></pre></div></div>

<p><strong>Step 3: Download Model and Data</strong></p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>uv run python scripts/download_model.py <span class="nt">--model-name</span> Qwen/Qwen3-1.7B
uv run python scripts/download_math.py
</code></pre></div></div>

<p><strong>Step 4: Run SFT Training</strong></p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Run with AUTO mode (auto-detects GPU and optimal settings)</span>
uv run accelerate launch scripts/run_sft.py <span class="nt">--auto</span> <span class="se">\</span>
    <span class="nt">--model-name-or-path</span> models/qwen3-1.7b <span class="se">\</span>
    <span class="nt">--train-data-path</span> data/math/train.jsonl <span class="se">\</span>
    <span class="nt">--output-dir</span> outputs/sft_qwen3
</code></pre></div></div>

<p><img src="/assets/picture/2026-01-19-cs336-sft-qwen3-for-math-reasoning/sft_qwen3_model.png" alt="SFT Training Progress" /></p>

<p>The <code class="language-plaintext highlighter-rouge">--auto</code> flag triggers automatic compute environment detection. On this Lambda instance, the script detected:</p>

<table>
  <thead>
    <tr>
      <th>Setting</th>
      <th>Value</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Platform</td>
      <td>NVIDIA A100-SXM4-40GB</td>
    </tr>
    <tr>
      <td>Device</td>
      <td>cuda</td>
    </tr>
    <tr>
      <td>Memory</td>
      <td>39.5 GB</td>
    </tr>
    <tr>
      <td>BF16 Support</td>
      <td>True</td>
    </tr>
    <tr>
      <td>FP16 Support</td>
      <td>True</td>
    </tr>
  </tbody>
</table>

<p>Based on these capabilities, the training configuration was automatically resolved to:</p>

<table>
  <thead>
    <tr>
      <th>Parameter</th>
      <th>Auto-Selected Value</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Batch size</td>
      <td>2</td>
    </tr>
    <tr>
      <td>Gradient accumulation</td>
      <td>4</td>
    </tr>
    <tr>
      <td>Effective batch size</td>
      <td>8</td>
    </tr>
    <tr>
      <td>Mixed precision</td>
      <td>bf16</td>
    </tr>
    <tr>
      <td>Model dtype</td>
      <td>torch.bfloat16</td>
    </tr>
    <tr>
      <td>Num workers</td>
      <td>4</td>
    </tr>
  </tbody>
</table>

<p>The training then proceeds through 12,000 examples (the full MATH training set) for 1,500 update steps. The training curves below show the loss and learning rate schedule:</p>

<p><img src="/assets/picture/2026-01-19-cs336-sft-qwen3-for-math-reasoning/training_curves.png" alt="Training Curves" /></p>

<p>The left plot shows the training loss dropping rapidly from ~2.3 to ~0.7 in the first 100 steps, then gradually decreasing to ~0.5 by the end of training. The right plot shows the learning rate schedule: a linear warmup to 2e-5 followed by linear decay to zero.</p>

<p><strong>Step 5: Evaluate the Trained Model</strong></p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>uv run python scripts/run_math_eval.py <span class="se">\</span>
    <span class="nt">--model-name-or-path</span> outputs/sft_qwen3/final <span class="se">\</span>
    <span class="nt">--output-path</span> outputs/sft_qwen3_eval.jsonl
</code></pre></div></div>

<h3 id="part-7-evaluation-pipeline">Part 7: Evaluation Pipeline</h3>

<h4 id="math-answer-grading">Math Answer Grading</h4>

<p>The evaluation uses a sophisticated grading pipeline that handles the complexity of mathematical answers:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">r1_zero_reward_fn</span><span class="p">(</span><span class="n">response</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">ground_truth</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">:</span>
    <span class="s">"""
    Grade a model response against the ground truth.

    Returns:
        format_reward: 1.0 if response has correct &lt;think&gt;/&lt;answer&gt; format
        answer_reward: 1.0 if answer is mathematically correct
        reward: Combined reward
    """</span>
    <span class="c1"># Check format: must have "&lt;/think&gt; &lt;answer&gt;" and "&lt;/answer&gt;" tags
</span>    <span class="k">if</span> <span class="s">"&lt;/think&gt; &lt;answer&gt;"</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">response</span> <span class="ow">or</span> <span class="s">"&lt;/answer&gt;"</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">response</span><span class="p">:</span>
        <span class="k">return</span> <span class="p">{</span><span class="s">"format_reward"</span><span class="p">:</span> <span class="mf">0.0</span><span class="p">,</span> <span class="s">"answer_reward"</span><span class="p">:</span> <span class="mf">0.0</span><span class="p">,</span> <span class="s">"reward"</span><span class="p">:</span> <span class="mf">0.0</span><span class="p">}</span>

    <span class="c1"># Extract answer from tags
</span>    <span class="n">model_answer</span> <span class="o">=</span> <span class="n">response</span><span class="p">.</span><span class="n">split</span><span class="p">(</span><span class="s">"&lt;answer&gt;"</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">].</span><span class="n">replace</span><span class="p">(</span><span class="s">"&lt;/answer&gt;"</span><span class="p">,</span> <span class="s">""</span><span class="p">)</span>

    <span class="c1"># Handle \boxed{} format
</span>    <span class="k">if</span> <span class="s">"</span><span class="se">\\</span><span class="s">boxed"</span> <span class="ow">in</span> <span class="n">model_answer</span><span class="p">:</span>
        <span class="n">model_answer</span> <span class="o">=</span> <span class="n">extract_boxed_answer</span><span class="p">(</span><span class="n">model_answer</span><span class="p">)</span>

    <span class="c1"># Grade using multiple strategies
</span>    <span class="n">is_correct</span> <span class="o">=</span> <span class="n">grade_answer</span><span class="p">(</span><span class="n">model_answer</span><span class="p">,</span> <span class="n">ground_truth</span><span class="p">)</span>

    <span class="k">return</span> <span class="p">{</span>
        <span class="s">"format_reward"</span><span class="p">:</span> <span class="mf">1.0</span><span class="p">,</span>
        <span class="s">"answer_reward"</span><span class="p">:</span> <span class="mf">1.0</span> <span class="k">if</span> <span class="n">is_correct</span> <span class="k">else</span> <span class="mf">0.0</span><span class="p">,</span>
        <span class="s">"reward"</span><span class="p">:</span> <span class="mf">1.0</span> <span class="k">if</span> <span class="n">is_correct</span> <span class="k">else</span> <span class="mf">0.5</span><span class="p">,</span>  <span class="c1"># Partial credit for format
</span>    <span class="p">}</span>
</code></pre></div></div>

<p>The grading uses multiple strategies with timeout protection:</p>
<ol>
  <li><strong>MATHD normalization</strong>: Dan Hendrycksâ€™ string normalization</li>
  <li><strong>Sympy symbolic equality</strong>: For algebraic equivalence</li>
  <li><strong>math_verify library</strong>: Advanced parsing for complex expressions</li>
</ol>

<h4 id="running-evaluation">Running Evaluation</h4>

<p><strong>Zero-shot evaluation (baseline):</strong></p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>uv run python scripts/run_math_eval.py <span class="se">\</span>
    <span class="nt">--model-name-or-path</span> models/qwen3-1.7b <span class="se">\</span>
    <span class="nt">--output-path</span> outputs/qwen3_base_eval.jsonl
</code></pre></div></div>

<p><strong>Fine-tuned model evaluation:</strong></p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>uv run python scripts/run_math_eval.py <span class="se">\</span>
    <span class="nt">--model-name-or-path</span> outputs/sft_qwen3/final <span class="se">\</span>
    <span class="nt">--output-path</span> outputs/sft_qwen3_eval.jsonl
</code></pre></div></div>

<p><img src="/assets/picture/2026-01-19-cs336-sft-qwen3-for-math-reasoning/eval_after_sft.png" alt="vLLM Evaluation Output" /></p>

<p>On CUDA, the evaluation script automatically uses <a href="https://github.com/vllm-project/vllm">vLLM</a> for high-throughput inference. The screenshot shows vLLMâ€™s initialization process:</p>

<ol>
  <li><strong>Model loading</strong>: The fine-tuned model loads into ~3.2 GB of GPU memory</li>
  <li><strong>CUDA graph compilation</strong>: vLLM compiles optimized CUDA graphs for the decode phase</li>
  <li><strong>KV cache allocation</strong>: With 30.87 GB available, vLLM allocates a KV cache supporting ~209K tokens</li>
</ol>

<p>Once initialized, vLLM generates responses for all 500 test problems at impressive speedsâ€”approximately 17,800 tokens/second throughput, with input processing at ~2,800 toks/s and output generation at ~6,300 toks/s. The entire evaluation completes in under 30 seconds, compared to several minutes with standard HuggingFace inference.</p>

<p>The aggregated metrics show the final results: 36% answer accuracy (<code class="language-plaintext highlighter-rouge">answer_reward: 0.36</code>) and 90% format compliance (<code class="language-plaintext highlighter-rouge">format_reward: 0.90</code>).</p>

<h4 id="results-before-and-after-sft">Results: Before and After SFT</h4>

<p>After training Qwen3-1.7B on ~12K examples from the MATH dataset with only one epoch, here are the evaluation results on 500 test problems:</p>

<table>
  <thead>
    <tr>
      <th>Metric</th>
      <th>Base Model (Zero-Shot)</th>
      <th>After SFT</th>
      <th>Improvement</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Correct Answers</strong></td>
      <td>133 (26.6%)</td>
      <td>180 (36.0%)</td>
      <td>+9.4pp (+35% relative)</td>
    </tr>
    <tr>
      <td><strong>Format Compliance</strong></td>
      <td>148 (29.6%)</td>
      <td>450 (90.0%)</td>
      <td>+60.4pp (+204% relative)</td>
    </tr>
    <tr>
      <td><strong>Format Only (wrong answer)</strong></td>
      <td>15 (3.0%)</td>
      <td>270 (54.0%)</td>
      <td>â€”</td>
    </tr>
    <tr>
      <td><strong>Neither (no format, wrong)</strong></td>
      <td>352 (70.4%)</td>
      <td>50 (10.0%)</td>
      <td>-60.4pp</td>
    </tr>
  </tbody>
</table>

<p><strong>Key Observations:</strong></p>

<ol>
  <li>
    <p><strong>Dramatic format improvement</strong>: The base Qwen3-1.7B model doesnâ€™t naturally output the <code class="language-plaintext highlighter-rouge">&lt;think&gt;...&lt;/think&gt; &lt;answer&gt;...&lt;/answer&gt;</code> format. After SFT, 90% of responses follow the expected formatâ€”a critical requirement for downstream applications that parse structured outputs.</p>
  </li>
  <li>
    <p><strong>Meaningful accuracy gains</strong>: Correct answers improved from 26.6% to 36.0%, a 35% relative improvement. While thereâ€™s still room for growth, this demonstrates the model is learning genuine mathematical reasoning, not just format mimicry.</p>
  </li>
  <li>
    <p><strong>The â€œformat onlyâ€ category</strong>: After SFT, 54% of responses have correct format but wrong answers. These are cases where the model follows the reasoning structure but makes mathematical errorsâ€”a good target for future improvements like RLHF or more training data.</p>
  </li>
</ol>

<p><strong>Example: Before SFT (Base Model)</strong></p>

<p>The base model often ignores the expected format entirely:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Problem: What is the smallest positive perfect cube that can be written
         as the sum of three consecutive integers?

Base Model Output:
&lt;/think&gt;
&lt;/think&gt;
To solve this problem, we need to find the smallest positive perfect cube
that can be expressed as the sum of three consecutive integers.
Let the three consecutive integers be $ n-1 $, $ n $, and $ n+1 $.
The sum of these three integers is: $(n-1) + n + (n+1) = 3n$
...
</code></pre></div></div>

<p>Notice the malformed <code class="language-plaintext highlighter-rouge">&lt;/think&gt;&lt;/think&gt;</code> without opening tags or <code class="language-plaintext highlighter-rouge">&lt;answer&gt;</code> tags.</p>

<p><strong>Example: After SFT</strong></p>

<p>The fine-tuned model produces well-structured reasoning:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Problem: What is the smallest positive perfect cube that can be written
         as the sum of three consecutive integers?

SFT Model Output:
Let the three consecutive integers be $n-1$, $n$, and $n+1$. Then
$(n-1)+n+(n+1)=3n$. We want to find the smallest positive perfect cube
that is divisible by 3. The smallest positive perfect cube divisible
by 3 is $3^3=27$. Therefore, the smallest positive perfect cube that
can be written as the sum of three consecutive integers is $\boxed{27}$.
&lt;/think&gt; &lt;answer&gt;\boxed{27}&lt;/answer&gt;
</code></pre></div></div>

<p>The SFT model shows clear reasoning followed by a properly formatted answer.</p>

<h3 id="part-8-practical-recommendations">Part 8: Practical Recommendations</h3>

<h4 id="development-workflow-summary">Development Workflow Summary</h4>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>+------------------------------------------------------------------+
|                    LOCAL DEVELOPMENT (Mac)                       |
+------------------------------------------------------------------+
|  1. Write code with device-agnostic patterns                     |
|  2. Test with small samples (--num-samples 10)                   |
|  3. Verify loss decreases, no NaN                                |
|  4. Commit and push to GitHub                                    |
+------------------------------------------------------------------+
                              |
                              v
+------------------------------------------------------------------+
|                PRODUCTION TRAINING (Lambda Cloud)                |
+------------------------------------------------------------------+
|  1. SSH into Lambda Cloud compute instance                       |
|  2. Clone repo, install with uv                                  |
|  3. Run training with --auto flag                                |
|  4. Evaluate and save results                                    |
+------------------------------------------------------------------+
</code></pre></div></div>

<hr />

<p><strong>Resources</strong>:</p>
<ul>
  <li><a href="https://github.com/bearbearyu1223/qwen3_supervised_fine_tuning">Project Repository</a> â€” Full source code for this blog post</li>
  <li><a href="https://huggingface.co/docs/accelerate">HuggingFace Accelerate Documentation</a></li>
  <li><a href="https://github.com/astral-sh/uv">uv Package Manager</a></li>
  <li><a href="https://pytorch.org/docs/stable/notes/mps.html">PyTorch MPS Backend</a></li>
  <li><a href="https://lambdalabs.com/">Lambda Labs GPU Cloud</a></li>
  <li><a href="https://huggingface.co/Qwen">Qwen3 Models</a></li>
  <li><a href="https://github.com/hendrycks/math">MATH Dataset</a></li>
</ul>]]></content><author><name>[&quot;Han Yu&quot;]</name></author><category term="cs336" /><summary type="html"><![CDATA[Fine-Tuning Qwen3-1.7B on Lambda Labs for Math Reasoning]]></summary></entry><entry><title type="html">Study Notes: Stanford CS336 Language Modeling from Scratch [12]</title><link href="http://localhost:4000/cs336/2026/01/11/cs336-local-to-cloud-training.html" rel="alternate" type="text/html" title="Study Notes: Stanford CS336 Language Modeling from Scratch [12]" /><published>2026-01-11T00:00:00-08:00</published><updated>2026-01-11T00:00:00-08:00</updated><id>http://localhost:4000/cs336/2026/01/11/cs336-local-to-cloud-training</id><content type="html" xml:base="http://localhost:4000/cs336/2026/01/11/cs336-local-to-cloud-training.html"><![CDATA[<h2 id="from-macbook-to-cloud-a-practical-guide-to-developing-and-scaling-llm-training-code">From MacBook to Cloud: A Practical Guide to Developing and Scaling LLM Training Code</h2>

<p>When developing machine learning training pipelines, thereâ€™s often a disconnect between local development environments and production-scale cloud infrastructure. You might prototype on your laptop (say, a MacBook with Apple Silicon), only to discover that your code breaks on CUDA GPUs, or that patterns that worked locally donâ€™t scale in the cloud.</p>

<p>In this note, Iâ€™ll share my workflow for developing Supervised Fine-Tuning (SFT) code on a MacBook with Apple Silicon, testing it locally, then seamlessly deploying to <img src="https://colab.research.google.com/img/colab_favicon_256px.png" height="20" style="vertical-align: middle;" /> <a href="https://colab.research.google.com/">Google Colab</a> or multi-GPU cloud instances like <img src="https://lambdalabs.com/favicon.ico" height="20" style="vertical-align: middle;" /> <a href="https://lambdalabs.com/">Lambda Labs</a>.</p>

<p><em>This workflow was developed while implementing SFT for Qwen2.5-Math-1.5B on the MATH dataset (for CS336 Assignment 5), but the principles apply broadly to any PyTorch-based training pipeline development.</em></p>

<h3 id="table-of-contents">Table of Contents</h3>
<ul>
  <li><a href="#from-macbook-to-cloud-a-practical-guide-to-developing-and-scaling-llm-training-code">From MacBook to Cloud: A Practical Guide to Developing and Scaling LLM Training Code</a>
    <ul>
      <li><a href="#table-of-contents">Table of Contents</a></li>
      <li><a href="#the-challenge-bridging-local-and-cloud-development">The Challenge: Bridging Local and Cloud Development</a></li>
      <li><a href="#part-1-setting-up-local-development-environment"><strong>Part 1: Setting Up Local Development Environment</strong></a>
        <ul>
          <li><a href="#why-apple-silicon-for-ml-development"><strong>Why Apple Silicon for ML Development?</strong></a></li>
          <li><a href="#project-structure-and-package-management"><strong>Project Structure and Package Management</strong></a></li>
        </ul>
      </li>
      <li><a href="#part-2-writing-device-agnostic-training-code"><strong>Part 2: Writing Device-Agnostic Training Code</strong></a>
        <ul>
          <li><a href="#handling-device-detection"><strong>Handling Device Detection</strong></a></li>
          <li><a href="#numerical-precision-considerations"><strong>Numerical Precision Considerations</strong></a></li>
          <li><a href="#gradient-accumulation-for-memory-efficiency"><strong>Gradient Accumulation for Memory Efficiency</strong></a></li>
        </ul>
      </li>
      <li><a href="#part-3-local-testing-and-validation">Part 3: Local Testing and Validation</a>
        <ul>
          <li><a href="#quick-sanity-checks">Quick Sanity Checks</a></li>
          <li><a href="#inference-engine-local-vs-cloud">Inference Engine: Local vs Cloud</a></li>
          <li><a href="#verifying-gradient-accumulation"><strong>Verifying Gradient Accumulation</strong></a></li>
        </ul>
      </li>
      <li><a href="#part-4-packaging-for-cloud-deployment"><strong>Part 4: Packaging for Cloud Deployment</strong></a>
        <ul>
          <li><a href="#repository-structure"><strong>Repository Structure</strong></a></li>
          <li><a href="#dependency-management-with-uv"><strong>Dependency Management with uv</strong></a></li>
        </ul>
      </li>
      <li><a href="#part-5-deploying-to-google-colab"><strong>Part 5: Deploying to Google Colab</strong></a>
        <ul>
          <li><a href="#single-gpu-training-on-google-colab"><strong>Single GPU Training on Google Colab</strong></a></li>
          <li><a href="#colab-specific-considerations">Colab-Specific Considerations</a></li>
        </ul>
      </li>
      <li><a href="#part-6-scaling-to-multi-gpu-with-accelerate"><strong>Part 6: Scaling to Multi-GPU with Accelerate</strong></a>
        <ul>
          <li><a href="#why-huggingface-accelerate"><strong>Why HuggingFace Accelerate</strong></a></li>
          <li><a href="#code-changes-for-multi-gpu-support"><strong>Code Changes for Multi-GPU Support</strong></a></li>
          <li><a href="#lambda-labs-deployment"><strong>Lambda Labs Deployment</strong></a></li>
        </ul>
      </li>
      <li><a href="#part-7-practical-recommendations-and-lessons-learned"><strong>Part 7: Practical Recommendations and Lessons Learned</strong></a>
        <ul>
          <li><a href="#development-workflow-summary"><strong>Development Workflow Summary</strong></a></li>
          <li><a href="#common-pitfalls-and-solutions"><strong>Common Pitfalls and Solutions</strong></a></li>
          <li><a href="#performance-comparison"><strong>Performance Comparison</strong></a></li>
        </ul>
      </li>
      <li><a href="#conclusion">Conclusion</a></li>
    </ul>
  </li>
</ul>

<h3 id="the-challenge-bridging-local-and-cloud-development">The Challenge: Bridging Local and Cloud Development</h3>

<p>My typical ML development workflow faces a fundamental tensionâ€”I use a MacBook Pro with M4 chips for personal side projects, which creates some tradeoffs:</p>

<table>
  <thead>
    <tr>
      <th>Environment</th>
      <th>Pros</th>
      <th>Cons</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Local (MacBook)</strong></td>
      <td>Fast iteration, no cost, familiar tools</td>
      <td>Limited memory, slower training, no CUDA (many GPU acceleration frameworks only support CUDA)</td>
    </tr>
    <tr>
      <td><strong>Cloud (Colab/Lambda)</strong></td>
      <td>Powerful GPUs, scalable, CUDA support</td>
      <td>Setup overhead, costs money, less interactive</td>
    </tr>
  </tbody>
</table>

<p>The ideal workflow would let me:</p>
<ol>
  <li><strong>Develop locally</strong> with fast feedback loops</li>
  <li><strong>Test easily</strong> before committing cloud resources</li>
  <li><strong>Deploy seamlessly</strong> without rewriting code</li>
  <li><strong>Scale horizontally</strong> when more compute is available</li>
</ol>

<p>This note presents a battle-tested approach to achieving all four.</p>

<h3 id="part-1-setting-up-local-development-environment"><strong>Part 1: Setting Up Local Development Environment</strong></h3>

<h4 id="why-apple-silicon-for-ml-development"><strong>Why Apple Silicon for ML Development?</strong></h4>

<p>Beyond personal preference (Iâ€™ve been an Apple product fan since grad school), Apple Silicon Macs offer a genuinely compelling development environment:</p>

<ul>
  <li><strong>Unified Memory Architecture</strong>: 16â€“64GB RAM shared between CPU and GPU</li>
  <li><strong>Metal Performance Shaders (MPS)</strong>: PyTorch backend for GPU acceleration</li>
  <li><strong>Power Efficiency</strong>: Extended battery life for portable development</li>
  <li><strong>Native ARM</strong>: Fast Python and native tool execution</li>
</ul>

<p>However, there are important limitations:</p>

<table>
  <thead>
    <tr>
      <th>Feature</th>
      <th>CUDA (NVIDIA)</th>
      <th>MPS (Apple Silicon)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Float16 Training</td>
      <td>Stable with gradient scaling</td>
      <td>Often causes NaN losses</td>
    </tr>
    <tr>
      <td>BFloat16</td>
      <td>Full support (Ampere+)</td>
      <td>Not supported</td>
    </tr>
    <tr>
      <td>Multi-GPU</td>
      <td>NCCL, NVLink</td>
      <td>Single GPU only</td>
    </tr>
    <tr>
      <td>Flash Attention</td>
      <td>Available</td>
      <td>Not available</td>
    </tr>
    <tr>
      <td>Memory</td>
      <td>Dedicated VRAM</td>
      <td>Shared system RAM</td>
    </tr>
  </tbody>
</table>

<p><strong>Key Insight</strong>: MPS ( Metal Performance Shadersâ€”Appleâ€™s GPU-accelerated compute framework for macOS and iOS) is excellent for development and testing but usually requires float32 precision for numerical stability. I need plan for this difference when writing device-agnostic code.</p>

<h4 id="project-structure-and-package-management"><strong>Project Structure and Package Management</strong></h4>

<p>I use <a href="https://github.com/astral-sh/uv"><code class="language-plaintext highlighter-rouge">uv</code></a> for fast, reproducible Python package management. Hereâ€™s how I set up my local dev environment for CS336 Assignment 5.</p>

<p><strong>Install uv:</strong></p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>curl <span class="nt">-LsSf</span> https://astral.sh/uv/install.sh | sh
</code></pre></div></div>

<p><strong>Project structure:</strong></p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>assignment5-alignment/
â”œâ”€â”€ cs336_alignment/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ sft.py              # Main training code
â”‚   â””â”€â”€ prompts/
â”‚       â””â”€â”€ r1_zero.prompt  # Prompt template
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ run_sft.py          # Training entry point
â”‚   â”œâ”€â”€ download_model.py   # Model downloader
â”‚   â””â”€â”€ download_math.py    # Data downloader
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ sft_training_colab.ipynb
â”œâ”€â”€ pyproject.toml          # Dependencies
â””â”€â”€ uv.lock                 # Locked versions
</code></pre></div></div>

<p><strong>pyproject.toml</strong> with optional CUDA dependencies:</p>
<div class="language-toml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nn">[project]</span>
<span class="py">name</span> <span class="p">=</span> <span class="s">"alignment"</span>
<span class="py">requires-python</span> <span class="p">=</span> <span class="py">"&gt;</span><span class="p">=</span><span class="mf">3.11</span><span class="p">,</span><span class="err">&lt;</span><span class="mf">3.13</span><span class="s">"</span><span class="err">
</span><span class="py">dependencies</span> <span class="p">=</span> <span class="p">[</span>
    <span class="py">"accelerate&gt;</span><span class="p">=</span><span class="mf">1.5</span><span class="err">.</span><span class="mi">2</span><span class="s">",</span><span class="err">
</span>    <span class="s">"torch"</span><span class="p">,</span>
    <span class="py">"transformers&gt;</span><span class="p">=</span><span class="mf">4.50</span><span class="err">.</span><span class="mi">0</span><span class="s">",</span><span class="err">
</span>    <span class="py">"tqdm&gt;</span><span class="p">=</span><span class="mf">4.67</span><span class="err">.</span><span class="mi">1</span><span class="s">",</span><span class="err">
</span>    <span class="py">"matplotlib&gt;</span><span class="p">=</span><span class="mf">3.8</span><span class="err">.</span><span class="mi">0</span><span class="s">",</span><span class="err">
</span><span class="p">]</span>

<span class="nn">[project.optional-dependencies]</span>
<span class="py">cuda</span> <span class="p">=</span> <span class="p">[</span>
    <span class="py">"flash-attn=</span><span class="p">=</span><span class="mf">2.7</span><span class="err">.</span><span class="mi">4</span><span class="err">.post</span><span class="mi">1</span><span class="s">",</span><span class="err">
</span><span class="p">]</span>
</code></pre></div></div>

<p><strong>Local installation:</strong></p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>uv <span class="nb">sync</span>              <span class="c"># Basic install (Mac/CPU)</span>
uv <span class="nb">sync</span> <span class="nt">--extra</span> cuda <span class="c"># With CUDA extras (Linux with GPU)</span>
</code></pre></div></div>

<h3 id="part-2-writing-device-agnostic-training-code"><strong>Part 2: Writing Device-Agnostic Training Code</strong></h3>

<p>The key to seamless local-to-cloud transitions is writing code that adapts to available hardware without manual changes.</p>

<h4 id="handling-device-detection"><strong>Handling Device Detection</strong></h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">get_device</span><span class="p">(</span><span class="n">device_str</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s">"auto"</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
    <span class="s">"""Get the best available device for training."""</span>
    <span class="k">if</span> <span class="n">device_str</span> <span class="o">!=</span> <span class="s">"auto"</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">device_str</span>
    <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="n">is_available</span><span class="p">():</span>
        <span class="k">return</span> <span class="s">"cuda"</span>
    <span class="k">elif</span> <span class="n">torch</span><span class="p">.</span><span class="n">backends</span><span class="p">.</span><span class="n">mps</span><span class="p">.</span><span class="n">is_available</span><span class="p">():</span>
        <span class="k">return</span> <span class="s">"mps"</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="s">"cpu"</span>
</code></pre></div></div>

<h4 id="numerical-precision-considerations"><strong>Numerical Precision Considerations</strong></h4>

<p>This is where many developers encounter their first â€œworks locally, fails on cloudâ€ (or vice versa) bug:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">get_dtype_and_precision</span><span class="p">(</span><span class="n">device</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">torch</span><span class="p">.</span><span class="n">dtype</span><span class="p">,</span> <span class="nb">str</span><span class="p">]:</span>
    <span class="s">"""
    Determine appropriate dtype and mixed precision setting.

    Critical insight: MPS does NOT support float16 training reliably.
    Using float16 on MPS often results in NaN losses due to lack of
    proper mixed-precision support and gradient scaling.
    """</span>
    <span class="k">if</span> <span class="n">device</span> <span class="o">==</span> <span class="s">"cuda"</span><span class="p">:</span>
        <span class="c1"># CUDA: Use bfloat16 if available (Ampere+), else float16
</span>        <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="n">is_bf16_supported</span><span class="p">():</span>
            <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="n">bfloat16</span><span class="p">,</span> <span class="s">"bf16"</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="n">float16</span><span class="p">,</span> <span class="s">"fp16"</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># MPS and CPU: Use float32 for numerical stability
</span>        <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">,</span> <span class="s">"no"</span>
</code></pre></div></div>

<p><strong>Why This Matters</strong>:</p>

<table>
  <thead>
    <tr>
      <th>Device</th>
      <th>Recommended Dtype</th>
      <th>Reason</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>CUDA (Ampere+)</td>
      <td>bfloat16</td>
      <td>Best balance of speed and stability</td>
    </tr>
    <tr>
      <td>CUDA (older)</td>
      <td>float16</td>
      <td>With gradient scaling</td>
    </tr>
    <tr>
      <td>MPS</td>
      <td>float32</td>
      <td>float16 may cause NaN losses</td>
    </tr>
    <tr>
      <td>CPU</td>
      <td>float32</td>
      <td>No mixed precision benefit</td>
    </tr>
  </tbody>
</table>

<p>I learned this the hard way when my training showed <code class="language-plaintext highlighter-rouge">loss: nan</code> on MPS after working fine conceptually. The fix was simple once identified:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Before (broken on MPS)
</span><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="p">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_path</span><span class="p">,</span> <span class="n">torch_dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float16</span><span class="p">)</span>

<span class="c1"># After (works everywhere)
</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">float32</span> <span class="k">if</span> <span class="n">device</span> <span class="ow">in</span> <span class="p">[</span><span class="s">"mps"</span><span class="p">,</span> <span class="s">"cpu"</span><span class="p">]</span> <span class="k">else</span> <span class="n">torch</span><span class="p">.</span><span class="n">bfloat16</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="p">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_path</span><span class="p">,</span> <span class="n">torch_dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
</code></pre></div></div>

<h4 id="gradient-accumulation-for-memory-efficiency"><strong>Gradient Accumulation for Memory Efficiency</strong></h4>

<p>With limited memory on laptops (even 32GB unified memory), gradient accumulation is essential:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Effective batch size = batch_size * gradient_accumulation_steps
# Example: batch_size=1, grad_accum=8 -&gt; effective batch of 8
</span>
<span class="k">for</span> <span class="n">step</span><span class="p">,</span> <span class="n">batch</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">dataloader</span><span class="p">):</span>
    <span class="c1"># Forward pass
</span>    <span class="n">loss</span> <span class="o">=</span> <span class="n">compute_loss</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">batch</span><span class="p">)</span>

    <span class="c1"># Scale loss for gradient accumulation
</span>    <span class="n">scaled_loss</span> <span class="o">=</span> <span class="n">loss</span> <span class="o">/</span> <span class="n">gradient_accumulation_steps</span>
    <span class="n">scaled_loss</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span>

    <span class="c1"># Only update weights every N steps
</span>    <span class="k">if</span> <span class="p">(</span><span class="n">step</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="n">gradient_accumulation_steps</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="n">clip_grad_norm_</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">max_grad_norm</span><span class="p">)</span>
        <span class="n">optimizer</span><span class="p">.</span><span class="n">step</span><span class="p">()</span>
        <span class="n">scheduler</span><span class="p">.</span><span class="n">step</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="p">.</span><span class="n">zero_grad</span><span class="p">()</span>
</code></pre></div></div>
<p><strong>Memory Scaling Recommendations For CS336 Assignment 5 SFT on Qwen2.5-Math-1.5B with the MATH dataset</strong></p>

<table>
  <thead>
    <tr>
      <th>Device</th>
      <th>Chip Generations</th>
      <th>Typical Memory</th>
      <th>Found In</th>
      <th>batch_size</th>
      <th>gradient_accumulation_steps</th>
      <th>Effective Batch</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Apple M-series (base)</td>
      <td>M1, M2, M3, M4</td>
      <td>8â€“16GB</td>
      <td>MacBook Air, 13â€ MacBook Pro</td>
      <td>1</td>
      <td>16</td>
      <td>16</td>
    </tr>
    <tr>
      <td>Apple M-series Pro</td>
      <td>M1, M2, M3, M4</td>
      <td>18â€“48GB</td>
      <td>14â€/16â€ MacBook Pro</td>
      <td>2â€“4</td>
      <td>4â€“8</td>
      <td>16</td>
    </tr>
    <tr>
      <td>Apple M-series Max</td>
      <td>M1, M2, M3, M4</td>
      <td>36â€“128GB</td>
      <td>14â€/16â€ MacBook Pro (high-end)</td>
      <td>4â€“8</td>
      <td>2â€“4</td>
      <td>16</td>
    </tr>
    <tr>
      <td>Apple M-series Ultra</td>
      <td>M1, M2</td>
      <td>64â€“192GB</td>
      <td>Mac Studio, Mac Pro</td>
      <td>8â€“16</td>
      <td>1â€“2</td>
      <td>16</td>
    </tr>
    <tr>
      <td>NVIDIA A100 (40GB)</td>
      <td>â€”</td>
      <td>40GB</td>
      <td>Cloud (Lambda, GCP, AWS)</td>
      <td>8</td>
      <td>2</td>
      <td>16</td>
    </tr>
    <tr>
      <td>NVIDIA A100 (80GB)</td>
      <td>â€”</td>
      <td>80GB</td>
      <td>Cloud (Lambda, GCP, AWS)</td>
      <td>16</td>
      <td>1</td>
      <td>16</td>
    </tr>
  </tbody>
</table>

<p><em>Effective batch = batch_size Ã— gradient_accumulation_steps. Larger batch sizes reduce training time but require more memory.</em></p>

<p><strong>Key insights:</strong></p>

<ul>
  <li>
    <p><strong>Memory constrains batch size, not effective batch size.</strong> When GPU memory is limited, reduce <code class="language-plaintext highlighter-rouge">batch_size</code> and increase <code class="language-plaintext highlighter-rouge">gradient_accumulation_steps</code> to maintain the same effective batch size. The model sees identical gradients either wayâ€”accumulation just trades memory for time.</p>
  </li>
  <li>
    <p><strong>Gradient accumulation is a memory-saving trick.</strong> Instead of computing gradients on 16 samples at once (which requires storing all intermediate activations), you process 1 sample 16 times, accumulating gradients before each optimizer step. This uses ~1/16th the memory at the cost of ~16Ã— more forward/backward passes.</p>
  </li>
  <li>
    <p><strong>Effective batch size should stay constant across devices.</strong> Notice that all rows target an effective batch of 16. This ensures consistent training dynamics regardless of hardwareâ€”important for reproducibility when moving between local development and cloud training.</p>
  </li>
  <li>
    <p><strong>Diminishing returns on large batch sizes.</strong> Beyond a certain point, larger batch sizes donâ€™t proportionally speed up training due to memory bandwidth limits (GPUs can only move data so fast, once your batch is large enough to fully utilize the GPU, making it bigger just creates a queueâ€”the GPU canâ€™t process it any faster) and reduced gradient noise (which can actually help optimization).</p>
  </li>
</ul>

<h3 id="part-3-local-testing-and-validation">Part 3: Local Testing and Validation</h3>

<p>Before deploying to cloud, thorough local testing saves time and money.</p>

<h4 id="quick-sanity-checks">Quick Sanity Checks</h4>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Test with minimal samples to verify pipeline works</span>
uv run python scripts/run_sft.py <span class="se">\</span>
    <span class="nt">--model-name-or-path</span> models/qwen2.5-math-1.5b <span class="se">\</span>
    <span class="nt">--train-data-path</span> data/math/train.jsonl <span class="se">\</span>
    <span class="nt">--output-dir</span> outputs/sft_test <span class="se">\</span>
    <span class="nt">--num-samples</span> 10 <span class="se">\</span>
    <span class="nt">--num-epochs</span> 1 <span class="se">\</span>
    <span class="nt">--batch-size</span> 1 <span class="se">\</span>
    <span class="nt">--gradient-accumulation-steps</span> 2
</code></pre></div></div>

<p><strong>What to verify:</strong></p>
<ol>
  <li>Model loads without errors</li>
  <li>Data pipeline produces valid batches</li>
  <li>Loss decreases (not NaN or constant)</li>
  <li>Checkpoints save correctly</li>
  <li>Model can be reloaded from checkpoint</li>
</ol>

<h4 id="inference-engine-local-vs-cloud">Inference Engine: Local vs Cloud</h4>

<p>A key challenge when developing on Apple Silicon is that <a href="https://github.com/vllm-project/vllm">vLLM</a>â€”the go-to inference engine for fast LLM servingâ€”requires CUDA and doesnâ€™t run on Macs. This means I need two inference backends during the initial development phase:</p>

<table>
  <thead>
    <tr>
      <th>Environment</th>
      <th>Inference Backend</th>
      <th>Why</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Local (MPS)</td>
      <td>HuggingFace Transformers</td>
      <td>Pure PyTorch, runs anywhere</td>
    </tr>
    <tr>
      <td>Cloud (CUDA)</td>
      <td>vLLM</td>
      <td>Optimized kernels, PagedAttention, 10â€“20Ã— faster</td>
    </tr>
  </tbody>
</table>

<p><strong>My approach</strong>: Write a simple abstraction layer that switches backends based on the available hardware:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">get_inference_backend</span><span class="p">(</span><span class="n">model_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>
    <span class="s">"""Return appropriate inference backend for the current environment."""</span>
    <span class="k">if</span> <span class="n">device</span> <span class="o">==</span> <span class="s">"cuda"</span> <span class="ow">and</span> <span class="n">is_vllm_available</span><span class="p">():</span>
        <span class="kn">from</span> <span class="nn">vllm</span> <span class="kn">import</span> <span class="n">LLM</span>
        <span class="k">return</span> <span class="n">VLLMBackend</span><span class="p">(</span><span class="n">LLM</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model_path</span><span class="p">))</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># Fallback to HuggingFace for MPS/CPU
</span>        <span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">AutoTokenizer</span>
        <span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="p">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_path</span><span class="p">).</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="p">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_path</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">TransformersBackend</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">)</span>
</code></pre></div></div>

<p><strong>What this enables:</strong></p>
<ul>
  <li><strong>Local development</strong>: Test generation logic, prompt templates, and output parsing using the Transformers backend on my Mac</li>
  <li><strong>Cloud deployment</strong>: Automatically switch to vLLM for fast, batched inference without changing my evaluation code</li>
</ul>

<p><strong>Trade-off to keep in mind</strong>: my local inference is much slower than cloud. For local testing, I need to use small sample sizes (10â€“50 examples) to validate correctness, before move to run full evaluations on cloud.</p>

<h4 id="verifying-gradient-accumulation"><strong>Verifying Gradient Accumulation</strong></h4>

<p>A common bug is incorrect gradient accumulation scaling. Hereâ€™s a verification approach:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">verify_gradient_accumulation</span><span class="p">():</span>
    <span class="s">"""
    Verify that accumulated gradients match single large batch.

    The gradients should be identical (within floating point tolerance)
    whether we:
    1. Process 8 samples in one batch, or
    2. Process 1 sample 8 times with gradient accumulation
    """</span>
    <span class="n">model_single</span> <span class="o">=</span> <span class="n">create_model</span><span class="p">()</span>
    <span class="n">model_accum</span> <span class="o">=</span> <span class="n">create_model</span><span class="p">()</span>

    <span class="c1"># Copy weights
</span>    <span class="n">model_accum</span><span class="p">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">model_single</span><span class="p">.</span><span class="n">state_dict</span><span class="p">())</span>

    <span class="c1"># Method 1: Single large batch
</span>    <span class="n">large_batch</span> <span class="o">=</span> <span class="n">get_batch</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">compute_loss</span><span class="p">(</span><span class="n">model_single</span><span class="p">,</span> <span class="n">large_batch</span><span class="p">)</span>
    <span class="n">loss</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">grad_single</span> <span class="o">=</span> <span class="n">get_gradients</span><span class="p">(</span><span class="n">model_single</span><span class="p">)</span>

    <span class="c1"># Method 2: Accumulated small batches
</span>    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">8</span><span class="p">):</span>
        <span class="n">small_batch</span> <span class="o">=</span> <span class="n">get_batch</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">compute_loss</span><span class="p">(</span><span class="n">model_accum</span><span class="p">,</span> <span class="n">small_batch</span><span class="p">)</span> <span class="o">/</span> <span class="mi">8</span>  <span class="c1"># Scale!
</span>        <span class="n">loss</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">grad_accum</span> <span class="o">=</span> <span class="n">get_gradients</span><span class="p">(</span><span class="n">model_accum</span><span class="p">)</span>

    <span class="c1"># Verify they match
</span>    <span class="k">assert</span> <span class="n">torch</span><span class="p">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">grad_single</span><span class="p">,</span> <span class="n">grad_accum</span><span class="p">,</span> <span class="n">rtol</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="part-4-packaging-for-cloud-deployment"><strong>Part 4: Packaging for Cloud Deployment</strong></h3>

<h4 id="repository-structure"><strong>Repository Structure</strong></h4>

<p>Push my code to GitHub for easy cloud access, for example</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>git add cs336_alignment/ scripts/ notebooks/ pyproject.toml uv.lock
git commit <span class="nt">-m</span> <span class="s2">"Add SFT training pipeline"</span>
git push origin main
</code></pre></div></div>

<h4 id="dependency-management-with-uv"><strong>Dependency Management with uv</strong></h4>

<p>The <code class="language-plaintext highlighter-rouge">uv.lock</code> file ensures reproducible environments:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Generate lock file locally</span>
uv lock

<span class="c"># On cloud, install exact versions</span>
uv <span class="nb">sync</span>  <span class="c"># Reads uv.lock automatically</span>
</code></pre></div></div>

<p><strong>Why uv over pip/conda/poetry?</strong></p>

<table>
  <thead>
    <tr>
      <th>Aspect</th>
      <th>pip</th>
      <th>conda</th>
      <th>Poetry</th>
      <th>uv</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Speed</td>
      <td>Moderate</td>
      <td>Slow</td>
      <td>Slow</td>
      <td>Very fast (Rust-based)</td>
    </tr>
    <tr>
      <td>Lock file</td>
      <td>âŒ (requires pip-tools)</td>
      <td>âŒ (manual export)</td>
      <td>âœ…</td>
      <td>âœ…</td>
    </tr>
    <tr>
      <td>PyTorch/CUDA handling</td>
      <td>Manual</td>
      <td>Good</td>
      <td>Finicky</td>
      <td>Smooth</td>
    </tr>
    <tr>
      <td>Mac â†’ Linux portability</td>
      <td>Poor</td>
      <td>Poor</td>
      <td>Good</td>
      <td>Excellent</td>
    </tr>
    <tr>
      <td>Dependency resolution</td>
      <td>Basic</td>
      <td>Solver can be slow</td>
      <td>Good but slow</td>
      <td>Fast and reliable</td>
    </tr>
  </tbody>
</table>

<p><strong>Why this matters for ML workflows:</strong></p>

<ul>
  <li>
    <p><strong>Speed</strong>: ML projects have heavy dependencies (PyTorch, Transformers, flash-attn). Poetry can take 30â€“60s to resolve; uv takes 1â€“5s.</p>
  </li>
  <li>
    <p><strong>PyTorch complexity</strong>: PyTorch has separate wheels for CPU, CUDA 11.8, CUDA 12.1, etc. Poetry often requires manual configuration with custom sources. uv handles this automatically.</p>
  </li>
  <li>
    <p><strong>Cross-platform</strong>: I am developing on Mac (ARM) and deploying to Linux (x86 + CUDA). uvâ€™s lock file captures platform-specific metadata, so <code class="language-plaintext highlighter-rouge">uv sync</code> installs the correct versions on each platform without separate environment files.</p>
  </li>
</ul>

<p><strong>When you might still choose Poetry:</strong></p>
<ul>
  <li>Publishing packages to PyPI (Poetry has built-in support)</li>
  <li>Your team already uses it and has established workflows</li>
  <li>You need Poetryâ€™s plugin ecosystem</li>
</ul>

<p>For ML development workflows like this one, uvâ€™s speed and PyTorch handling are significant wins.</p>

<h3 id="part-5-deploying-to-google-colab"><strong>Part 5: Deploying to Google Colab</strong></h3>

<h4 id="single-gpu-training-on-google-colab"><strong>Single GPU Training on Google Colab</strong></h4>
<p><a href="https://colab.research.google.com/">Google Colab</a> provides easy access to cloud GPUs without any setup. With your packaged repo, you can create a notebook with the following cells to run training on Colab:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Cell 1: Clone and setup
</span><span class="err">!</span><span class="n">git</span> <span class="n">clone</span> <span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">github</span><span class="p">.</span><span class="n">com</span><span class="o">/</span><span class="n">YOUR_USERNAME</span><span class="o">/</span><span class="n">assignment5</span><span class="o">-</span><span class="n">alignment</span><span class="p">.</span><span class="n">git</span>
<span class="o">%</span><span class="n">cd</span> <span class="n">assignment5</span><span class="o">-</span><span class="n">alignment</span>
<span class="err">!</span><span class="n">git</span> <span class="n">checkout</span> <span class="n">main</span>

<span class="c1"># Cell 2: Install uv and dependencies
</span><span class="err">!</span><span class="n">curl</span> <span class="o">-</span><span class="n">LsSf</span> <span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">astral</span><span class="p">.</span><span class="n">sh</span><span class="o">/</span><span class="n">uv</span><span class="o">/</span><span class="n">install</span><span class="p">.</span><span class="n">sh</span> <span class="o">|</span> <span class="n">sh</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">[</span><span class="s">'PATH'</span><span class="p">]</span> <span class="o">=</span> <span class="sa">f</span><span class="s">"</span><span class="si">{</span><span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="n">expanduser</span><span class="p">(</span><span class="s">'~'</span><span class="p">)</span><span class="si">}</span><span class="s">/.local/bin:</span><span class="si">{</span><span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">[</span><span class="s">'PATH'</span><span class="p">]</span><span class="si">}</span><span class="s">"</span>
<span class="err">!</span><span class="n">uv</span> <span class="n">sync</span> <span class="o">--</span><span class="n">extra</span> <span class="n">cuda</span>

<span class="c1"># Cell 3: Download model and data
</span><span class="err">!</span><span class="n">uv</span> <span class="n">run</span> <span class="n">python</span> <span class="n">scripts</span><span class="o">/</span><span class="n">download_model</span><span class="p">.</span><span class="n">py</span> <span class="o">--</span><span class="n">model</span><span class="o">-</span><span class="n">name</span> <span class="n">Qwen</span><span class="o">/</span><span class="n">Qwen2</span><span class="p">.</span><span class="mi">5</span><span class="o">-</span><span class="n">Math</span><span class="o">-</span><span class="mf">1.5</span><span class="n">B</span>
<span class="err">!</span><span class="n">uv</span> <span class="n">run</span> <span class="n">python</span> <span class="n">scripts</span><span class="o">/</span><span class="n">download_math</span><span class="p">.</span><span class="n">py</span>

<span class="c1"># Cell 4: Run training
</span><span class="err">!</span><span class="n">uv</span> <span class="n">run</span> <span class="n">python</span> <span class="n">scripts</span><span class="o">/</span><span class="n">run_sft</span><span class="p">.</span><span class="n">py</span> \
    <span class="o">--</span><span class="n">model</span><span class="o">-</span><span class="n">name</span><span class="o">-</span><span class="ow">or</span><span class="o">-</span><span class="n">path</span> <span class="n">models</span><span class="o">/</span><span class="n">qwen2</span><span class="p">.</span><span class="mi">5</span><span class="o">-</span><span class="n">math</span><span class="o">-</span><span class="mf">1.5</span><span class="n">b</span> \
    <span class="o">--</span><span class="n">train</span><span class="o">-</span><span class="n">data</span><span class="o">-</span><span class="n">path</span> <span class="n">data</span><span class="o">/</span><span class="n">math</span><span class="o">/</span><span class="n">train</span><span class="p">.</span><span class="n">jsonl</span> \
    <span class="o">--</span><span class="n">output</span><span class="o">-</span><span class="nb">dir</span> <span class="n">outputs</span><span class="o">/</span><span class="n">sft_model</span> \
    <span class="o">--</span><span class="n">batch</span><span class="o">-</span><span class="n">size</span> <span class="mi">2</span> \
    <span class="o">--</span><span class="n">gradient</span><span class="o">-</span><span class="n">accumulation</span><span class="o">-</span><span class="n">steps</span> <span class="mi">8</span> \
    <span class="o">--</span><span class="n">device</span> <span class="n">cuda</span>
</code></pre></div></div>

<h4 id="colab-specific-considerations">Colab-Specific Considerations</h4>

<table>
  <thead>
    <tr>
      <th>Aspect</th>
      <th>Recommendation</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Runtime selection</strong></td>
      <td>Runtime â†’ Change runtime type â†’ Select GPU (T4 for free tier, A100 for Pro+)</td>
    </tr>
    <tr>
      <td><strong>Session timeout</strong></td>
      <td>Save checkpoints every 1â€“2 epochs; free tier can preempt without warning</td>
    </tr>
    <tr>
      <td><strong>Persistence</strong></td>
      <td>Mount Google Drive for outputs to survive session resets</td>
    </tr>
    <tr>
      <td><strong>Memory limits</strong></td>
      <td>T4 has 16GB VRAMâ€”use <code class="language-plaintext highlighter-rouge">batch_size=2</code> with gradient accumulation</td>
    </tr>
    <tr>
      <td><strong>Background execution</strong></td>
      <td>Pro+ onlyâ€”training continues after closing browser</td>
    </tr>
  </tbody>
</table>

<p><strong>Google Drive mounting:</strong></p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">google.colab</span> <span class="kn">import</span> <span class="n">drive</span>
<span class="n">drive</span><span class="p">.</span><span class="n">mount</span><span class="p">(</span><span class="s">'/content/drive'</span><span class="p">)</span>

<span class="c1"># Save outputs to Drive
</span><span class="n">output_dir</span> <span class="o">=</span> <span class="s">'/content/drive/MyDrive/sft_outputs'</span>
</code></pre></div></div>

<p><strong>Saving to Google Drive</strong>:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">google.colab</span> <span class="kn">import</span> <span class="n">drive</span>
<span class="n">drive</span><span class="p">.</span><span class="n">mount</span><span class="p">(</span><span class="s">'/content/drive'</span><span class="p">)</span>
<span class="err">!</span><span class="n">cp</span> <span class="o">-</span><span class="n">r</span> <span class="n">outputs</span><span class="o">/</span><span class="n">sft_model</span><span class="o">/</span><span class="n">final</span> <span class="o">/</span><span class="n">content</span><span class="o">/</span><span class="n">drive</span><span class="o">/</span><span class="n">MyDrive</span><span class="o">/</span><span class="n">sft_model</span>
</code></pre></div></div>

<h3 id="part-6-scaling-to-multi-gpu-with-accelerate"><strong>Part 6: Scaling to Multi-GPU with Accelerate</strong></h3>

<h4 id="why-huggingface-accelerate"><strong>Why HuggingFace Accelerate</strong></h4>

<p>Google Colab typically provides only 1 GPU. For multi-GPU training (Lambda Labs, AWS, etc.), we can use HuggingFace Accelerate:</p>

<table>
  <thead>
    <tr>
      <th>Feature</th>
      <th>Manual DDP</th>
      <th>Accelerate</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Code changes</td>
      <td>Significant</td>
      <td>Minimal</td>
    </tr>
    <tr>
      <td>Device placement</td>
      <td>Manual</td>
      <td>Automatic</td>
    </tr>
    <tr>
      <td>Gradient sync</td>
      <td>Manual</td>
      <td>Automatic</td>
    </tr>
    <tr>
      <td>Mixed precision</td>
      <td>Manual setup</td>
      <td>One flag</td>
    </tr>
    <tr>
      <td>Single/Multi GPU</td>
      <td>Different code paths</td>
      <td>Same code</td>
    </tr>
  </tbody>
</table>

<h4 id="code-changes-for-multi-gpu-support"><strong>Code Changes for Multi-GPU Support</strong></h4>

<p>The key changes to support multi-GPU:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">accelerate</span> <span class="kn">import</span> <span class="n">Accelerator</span>

<span class="k">def</span> <span class="nf">train_sft</span><span class="p">(</span><span class="n">config</span><span class="p">):</span>
    <span class="c1"># Initialize Accelerator
</span>    <span class="n">accelerator</span> <span class="o">=</span> <span class="n">Accelerator</span><span class="p">(</span>
        <span class="n">gradient_accumulation_steps</span><span class="o">=</span><span class="n">config</span><span class="p">.</span><span class="n">gradient_accumulation_steps</span><span class="p">,</span>
        <span class="n">mixed_precision</span><span class="o">=</span><span class="s">"bf16"</span> <span class="k">if</span> <span class="n">device</span> <span class="o">==</span> <span class="s">"cuda"</span> <span class="k">else</span> <span class="s">"no"</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="c1"># Prepare model, optimizer, dataloader
</span>    <span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">dataloader</span><span class="p">,</span> <span class="n">scheduler</span> <span class="o">=</span> <span class="n">accelerator</span><span class="p">.</span><span class="n">prepare</span><span class="p">(</span>
        <span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">dataloader</span><span class="p">,</span> <span class="n">scheduler</span>
    <span class="p">)</span>

    <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">dataloader</span><span class="p">:</span>
        <span class="c1"># Use accelerator's gradient accumulation context
</span>        <span class="k">with</span> <span class="n">accelerator</span><span class="p">.</span><span class="n">accumulate</span><span class="p">(</span><span class="n">model</span><span class="p">):</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">compute_loss</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">batch</span><span class="p">)</span>
            <span class="n">accelerator</span><span class="p">.</span><span class="n">backward</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>  <span class="c1"># Instead of loss.backward()
</span>
            <span class="k">if</span> <span class="n">accelerator</span><span class="p">.</span><span class="n">sync_gradients</span><span class="p">:</span>
                <span class="n">accelerator</span><span class="p">.</span><span class="n">clip_grad_norm_</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">max_norm</span><span class="p">)</span>

            <span class="n">optimizer</span><span class="p">.</span><span class="n">step</span><span class="p">()</span>
            <span class="n">scheduler</span><span class="p">.</span><span class="n">step</span><span class="p">()</span>
            <span class="n">optimizer</span><span class="p">.</span><span class="n">zero_grad</span><span class="p">()</span>

    <span class="c1"># Save only on main process
</span>    <span class="k">if</span> <span class="n">accelerator</span><span class="p">.</span><span class="n">is_main_process</span><span class="p">:</span>
        <span class="n">unwrapped_model</span> <span class="o">=</span> <span class="n">accelerator</span><span class="p">.</span><span class="n">unwrap_model</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
        <span class="n">save_model</span><span class="p">(</span><span class="n">unwrapped_model</span><span class="p">,</span> <span class="n">output_dir</span><span class="p">)</span>
</code></pre></div></div>

<p><strong>Key Accelerate Patterns</strong>:</p>

<ol>
  <li><strong><code class="language-plaintext highlighter-rouge">accelerator.prepare()</code></strong>: Wraps objects for distributed training</li>
  <li><strong><code class="language-plaintext highlighter-rouge">accelerator.accumulate()</code></strong>: Handles gradient accumulation correctly</li>
  <li><strong><code class="language-plaintext highlighter-rouge">accelerator.backward()</code></strong>: Syncs gradients across devices</li>
  <li><strong><code class="language-plaintext highlighter-rouge">accelerator.sync_gradients</code></strong>: True when accumulation cycle completes</li>
  <li><strong><code class="language-plaintext highlighter-rouge">accelerator.is_main_process</code></strong>: Only one process logs/saves</li>
</ol>

<h4 id="lambda-labs-deployment"><strong>Lambda Labs Deployment</strong></h4>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># SSH into Lambda instance</span>
ssh ubuntu@your-instance-ip

<span class="c"># Setup</span>
git clone https://github.com/YOUR_USERNAME/assignment5-alignment.git
<span class="nb">cd </span>assignment5-alignment

curl <span class="nt">-LsSf</span> https://astral.sh/uv/install.sh | sh
<span class="nb">source</span> ~/.local/bin/env
uv <span class="nb">sync</span> <span class="nt">--extra</span> cuda

<span class="c"># Download model and data</span>
uv run python scripts/download_model.py
uv run python scripts/download_math.py

<span class="c"># Multi-GPU training (auto-detects available GPUs)</span>
uv run accelerate launch <span class="nt">--multi_gpu</span> scripts/run_sft.py <span class="se">\</span>
    <span class="nt">--model-name-or-path</span> models/qwen2.5-math-1.5b <span class="se">\</span>
    <span class="nt">--batch-size</span> 4 <span class="se">\</span>
    <span class="nt">--gradient-accumulation-steps</span> 2
</code></pre></div></div>

<p><strong>Scaling Guide</strong>:</p>

<table>
  <thead>
    <tr>
      <th>GPUs</th>
      <th>batch_size</th>
      <th>grad_accum</th>
      <th>Effective Batch</th>
      <th>Command</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>1</td>
      <td>4</td>
      <td>4</td>
      <td>16</td>
      <td><code class="language-plaintext highlighter-rouge">uv run python scripts/run_sft.py</code></td>
    </tr>
    <tr>
      <td>2</td>
      <td>4</td>
      <td>2</td>
      <td>16</td>
      <td><code class="language-plaintext highlighter-rouge">accelerate launch --num_processes 2</code></td>
    </tr>
    <tr>
      <td>4</td>
      <td>4</td>
      <td>1</td>
      <td>16</td>
      <td><code class="language-plaintext highlighter-rouge">accelerate launch --num_processes 4</code></td>
    </tr>
    <tr>
      <td>8</td>
      <td>4</td>
      <td>1</td>
      <td>32</td>
      <td><code class="language-plaintext highlighter-rouge">accelerate launch --num_processes 8</code></td>
    </tr>
  </tbody>
</table>

<h3 id="part-7-practical-recommendations-and-lessons-learned"><strong>Part 7: Practical Recommendations and Lessons Learned</strong></h3>

<h4 id="development-workflow-summary"><strong>Development Workflow Summary</strong></h4>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>+------------------------------------------------------------------+
|                    LOCAL DEVELOPMENT (Mac)                        |
+------------------------------------------------------------------+
|  1. Write code with device-agnostic patterns                     |
|  2. Test with small samples (--num-samples 10)                   |
|  3. Verify loss decreases, no NaN                                |
|  4. Run unit tests (pytest)                                      |
|  5. Commit and push to GitHub                                    |
+------------------------------------------------------------------+
                              |
                              v
+------------------------------------------------------------------+
|                    CLOUD VALIDATION (Colab)                       |
+------------------------------------------------------------------+
|  1. Clone repo, install dependencies                             |
|  2. Quick test with 100 samples                                  |
|  3. Verify CUDA path works correctly                             |
|  4. Check memory usage fits GPU                                  |
|  5. Save checkpoint to Google Drive                              |
+------------------------------------------------------------------+
                              |
                              v
+------------------------------------------------------------------+
|                PRODUCTION TRAINING (Lambda/Cloud)                 |
+------------------------------------------------------------------+
|  1. Use accelerate launch for multi-GPU                          |
|  2. Full dataset training                                        |
|  3. Monitor with logging/wandb                                   |
|  4. Save final model and metrics                                 |
+------------------------------------------------------------------+
</code></pre></div></div>

<h4 id="common-pitfalls-and-solutions"><strong>Common Pitfalls and Solutions</strong></h4>

<table>
  <thead>
    <tr>
      <th>Pitfall</th>
      <th>Symptom</th>
      <th>Solution</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Float16 on MPS</td>
      <td><code class="language-plaintext highlighter-rouge">loss: nan</code></td>
      <td>Use float32 on MPS</td>
    </tr>
    <tr>
      <td>Wrong grad accumulation</td>
      <td>Gradients donâ€™t match</td>
      <td>Divide loss by accumulation steps</td>
    </tr>
    <tr>
      <td>Missing <code class="language-plaintext highlighter-rouge">is_main_process</code> check</td>
      <td>Duplicate logs/saves</td>
      <td>Guard with <code class="language-plaintext highlighter-rouge">accelerator.is_main_process</code></td>
    </tr>
    <tr>
      <td>Hardcoded device</td>
      <td>Crashes on different hardware</td>
      <td>Use <code class="language-plaintext highlighter-rouge">get_device("auto")</code></td>
    </tr>
    <tr>
      <td>No checkpoint saving</td>
      <td>Lost progress on timeout</td>
      <td>Save every N steps</td>
    </tr>
  </tbody>
</table>

<h4 id="performance-comparison"><strong>Performance Comparison</strong></h4>

<p>From my experiments with Qwen2.5-Math-1.5B on MATH dataset:</p>

<table>
  <thead>
    <tr>
      <th>Environment</th>
      <th>Device</th>
      <th>batch_size x grad_accum</th>
      <th>Time per 100 steps</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>MacBook M2 Pro</td>
      <td>MPS</td>
      <td>1 x 8</td>
      <td>~45 min</td>
    </tr>
    <tr>
      <td>Colab Free</td>
      <td>T4</td>
      <td>2 x 8</td>
      <td>~12 min</td>
    </tr>
    <tr>
      <td>Colab Pro</td>
      <td>A100</td>
      <td>8 x 2</td>
      <td>~3 min</td>
    </tr>
    <tr>
      <td>Lambda (4x A100)</td>
      <td>4x A100</td>
      <td>4 x 1 (per GPU)</td>
      <td>~1 min</td>
    </tr>
  </tbody>
</table>

<h3 id="conclusion">Conclusion</h3>

<p>Developing ML training code that works seamlessly from a MacBook to multi-GPU cloud instances requires intentional design:</p>

<ol>
  <li><strong>Device-agnostic code</strong>: Abstract device selection and dtype handling</li>
  <li><strong>Numerical stability</strong>: Use float32 on MPS, mixed precision on CUDA</li>
  <li><strong>Memory efficiency</strong>: Implement gradient accumulation from the start</li>
  <li><strong>Reproducible environments</strong>: Use <code class="language-plaintext highlighter-rouge">uv</code> with lock files</li>
  <li><strong>Distributed-ready</strong>: Integrate Accelerate for painless multi-GPU scaling</li>
</ol>

<p>The workflow Iâ€™ve sharedâ€”develop locally on MacBook, validate on Colab, scale on cloud with distributed trainingâ€”provides fast iteration during development while enabling production-scale training when needed. The key insight is that <strong>the code should adapt to the hardware, not the other way around</strong>.</p>

<p>I hope this empowers you to develop confidently on your laptop, knowing that deploying to powerful cloud GPUs is a matter of changing a single commandâ€”not rewriting your training pipeline.</p>

<hr />

<p><strong>Resources</strong>:</p>
<ul>
  <li><a href="https://huggingface.co/docs/accelerate">HuggingFace Accelerate Documentation</a></li>
  <li><a href="https://github.com/astral-sh/uv">uv Package Manager</a></li>
  <li><a href="https://pytorch.org/docs/stable/notes/mps.html">PyTorch MPS Backend</a></li>
  <li><a href="https://lambdalabs.com/">Lambda Labs GPU Cloud</a></li>
</ul>]]></content><author><name>[&quot;Han Yu&quot;]</name></author><category term="cs336" /><summary type="html"><![CDATA[From MacBook to Cloud: A Practical Guide to Developing and Scaling LLM Training Code]]></summary></entry><entry><title type="html">Study Notes: Stanford CS336 Language Modeling from Scratch [11]</title><link href="http://localhost:4000/cs336/2025/11/16/cs336-the-complete-experiment-for-tinystories-transformer.html" rel="alternate" type="text/html" title="Study Notes: Stanford CS336 Language Modeling from Scratch [11]" /><published>2025-11-16T00:00:00-08:00</published><updated>2025-11-16T00:00:00-08:00</updated><id>http://localhost:4000/cs336/2025/11/16/cs336-the-complete-experiment-for-tinystories-transformer</id><content type="html" xml:base="http://localhost:4000/cs336/2025/11/16/cs336-the-complete-experiment-for-tinystories-transformer.html"><![CDATA[<h2 id="end-to-end-transformer-training-on-tinystories">End-to-End Transformer Training on TinyStories</h2>
<p>This note walks through the complete, end-to-end process of building a Transformer language model from scratch and training it on the TinyStories dataset. It covers every major componentâ€”from byte-pair encoding tokenization and multi-head attention with rotary embeddings to training-loop design and advanced text-generation strategies.</p>

<p>The goal is to provide a clear, practical reference for completing Assignment 1 of CS336, which is often the most time-consuming and technically challenging assignment in the course. It also serves as a summary and recap of Module 1, based on my previous ten CS336 notes:</p>

<table>
  <thead>
    <tr>
      <th>#</th>
      <th>Title</th>
      <th>Date Created</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>1</td>
      <td><a href="https://bearbearyu1223.github.io/cs336/2025/07/20/cs336-note-get-started.html">Getting Started with CS336</a></td>
      <td>July 20, 2025</td>
    </tr>
    <tr>
      <td>2</td>
      <td><a href="https://bearbearyu1223.github.io/cs336/2025/07/22/cs336-note-simple-bpe.html">A Simple Byte-Pair Encoding Implementation</a></td>
      <td>July 22, 2025</td>
    </tr>
    <tr>
      <td>3</td>
      <td><a href="https://bearbearyu1223.github.io/cs336/2025/07/26/cs336-note-train-bpe-tinystories.html">Training BPE on TinyStories</a></td>
      <td>July 26, 2025</td>
    </tr>
    <tr>
      <td>4</td>
      <td><a href="https://bearbearyu1223.github.io/cs336/2025/08/10/cs336-gpt2-regex-for-pretokenization-explaind.html">Understanding GPT-2â€™s Regex Pretokenizer</a></td>
      <td>Aug 10, 2025</td>
    </tr>
    <tr>
      <td>5</td>
      <td><a href="https://bearbearyu1223.github.io/cs336/2025/09/13/cs336-build-a-transformer-language-model.html">Building a Transformer Language Model</a></td>
      <td>Sep 13, 2025</td>
    </tr>
    <tr>
      <td>6</td>
      <td><a href="https://bearbearyu1223.github.io/cs336/2025/09/17/cs336-transformer-architecture-overview.html">Transformer Architecture Overview</a></td>
      <td>Sep 17, 2025</td>
    </tr>
    <tr>
      <td>7</td>
      <td><a href="https://bearbearyu1223.github.io/cs336/2025/09/28/cs336-understand-computation-cost-of-transformer-model.html">Understanding the Computational Cost of Transformers</a></td>
      <td>Sep 28, 2025</td>
    </tr>
    <tr>
      <td>8</td>
      <td><a href="https://bearbearyu1223.github.io/cs336/2025/10/05/cs336-training-a-transformer-lm-part-1.html">Training a Transformer LM â€” Part 1</a></td>
      <td>Oct 5, 2025</td>
    </tr>
    <tr>
      <td>9</td>
      <td><a href="https://bearbearyu1223.github.io/cs336/2025/10/19/cs336-implement-softmax-log_softmax-cross_entropy.html">Implementing Softmax, Log-Softmax, and Cross-Entropy</a></td>
      <td>Oct 19, 2025</td>
    </tr>
    <tr>
      <td>10</td>
      <td><a href="https://bearbearyu1223.github.io/cs336/2025/11/02/cs336-building-a-complete-training-loop.html">Building a Complete Training Loop</a></td>
      <td>Nov 2, 2025</td>
    </tr>
  </tbody>
</table>

<p>The full implementation is shared on GitHub:</p>

<p><a href="https://github.com/bearbearyu1223/tinystories-transformer"><svg height="16" width="16" viewBox="0 0 16 16" style="display: inline-block; vertical-align: text-bottom;"><path fill="currentColor" d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z"></path></svg> github.com/bearbearyu1223/tinystories-transformer</a></p>

<p>All training experiments in this note were run on a Apple MacBook Pro (<strong>M4 Chip</strong>) with <strong>24 GB</strong> of unified memory, a <strong>10-core GPU</strong>, and <strong>Metal 3</strong> support. The full training run required for the assignment took approximately <strong>7 hours</strong> to complete.</p>

<h3 id="table-of-contents">Table of Contents</h3>
<ol>
  <li><a href="#introduction">Introduction: Building the System to enable Model Training Experiments</a></li>
  <li><a href="#bpe-tokenization">BPE Tokenization: Efficient Subword Encoding</a></li>
  <li><a href="#transformer-architecture">Transformer Architecture: RoPE, RMSNorm, and SwiGLU</a></li>
  <li><a href="#training-configurations">Three-Tiered Training Configuration</a></li>
  <li><a href="#training-pipeline">The Training Pipeline: Memory-Efficient and Robust</a></li>
  <li><a href="#text-generation">Text Generation: Temperature, Top-k, and Top-p Sampling</a></li>
  <li><a href="#training-analysis">Training Analysis: Scaling Laws in Action</a></li>
  <li><a href="#production-considerations">Production Considerations</a></li>
  <li><a href="#takeaways">Key Takeaways</a></li>
</ol>

<hr />

<h3 id="introduction">Introduction: Building the System to enable Model Training Experiments</h3>

<p>Training a language model involves much more than implementing a Transformer and calling <code class="language-plaintext highlighter-rouge">loss.backward()</code>. A production system requires careful orchestration of tokenization, architecture design, training dynamics, checkpoint management, and generation strategiesâ€”each with its own subtleties and potential pitfalls.</p>

<p><strong>What we built:</strong></p>
<ul>
  <li>A complete BPE tokenizer with parallel training on multi-core systems</li>
  <li>A Transformer LM with modern architectural choices (RoPE, RMSNorm, SwiGLU)</li>
  <li>Three training configurations: quicktest (&lt; 1 min), development/test (~20 min), production (~7 hours)</li>
  <li>Multiple text generation strategies with temperature and nucleus sampling</li>
  <li>Comprehensive training analysis with visualization tools</li>
  <li>Memory-mapped data loading for datasets larger than RAM</li>
</ul>

<p><strong>The dataset:</strong> TinyStories (Eldan &amp; Li, 2023) contains short stories written by GPT-3.5 and GPT-4, designed to be simple enough for small models to learn coherent language generation while maintaining grammatical correctness and narrative structure.</p>

<p><strong>Model scale:</strong></p>
<ul>
  <li>17M parameters (excluding the embedding layers)</li>
  <li>10,000 BPE vocabulary</li>
  <li>256-token context length</li>
  <li>4 transformer layers with 16 attention heads</li>
</ul>

<p>This note will dive deep into each component, explaining not just the â€œwhatâ€ but the â€œwhyâ€ behind every design decision.</p>

<hr />

<h3 id="bpe-tokenization">BPE Tokenization: Efficient Subword Encoding</h3>

<p>Before training a language model, we need to convert text into tokens. The choice of tokenization algorithm significantly impacts model performance, training efficiency, and out-of-vocabulary handling. See my previous notes in <a href="https://bearbearyu1223.github.io/cs336/2025/07/22/cs336-note-simple-bpe.html">A Simple Byte-Pair Encoding Implementation</a>, <a href="https://bearbearyu1223.github.io/cs336/2025/07/26/cs336-note-train-bpe-tinystories.html">Training BPE on TinyStories</a>, and <a href="https://bearbearyu1223.github.io/cs336/2025/08/10/cs336-gpt2-regex-for-pretokenization-explaind.html">Understanding GPT-2â€™s Regex Pretokenizer</a> as references.</p>

<h4 id="why-bpe-over-character-or-word-level-tokenization">Why BPE Over Character or Word-Level Tokenization?</h4>

<p><strong>Character-level tokenization:</strong></p>
<ul>
  <li>âœ“ Never encounters unknown tokens</li>
  <li>âœ— Very long sequences â†’ expensive attention computation</li>
  <li>âœ— Makes it harder for the model to learn meaningful word-level structure</li>
</ul>

<p><strong>Word-level tokenization:</strong></p>
<ul>
  <li>âœ“ Tokens correspond to natural semantic units</li>
  <li>âœ— Vocabulary becomes extremely large (hundreds of thousands to millions of words)</li>
  <li>âœ— Performs poorly on rare words, typos, and morphological variations</li>
</ul>

<p><strong>Byte Pair Encoding (BPE):</strong></p>
<ul>
  <li>âœ“ Compact, manageable vocabulary (typically 10Kâ€“50K)</li>
  <li>âœ“ Robust to rare words, misspellings, and out-of-vocabulary terms via subword fallback</li>
  <li>âœ“ Produces reasonable sequence lengths</li>
  <li>âœ“ Language-agnostic â€” works across diverse writing systems</li>
</ul>

<h4 id="the-bpe-algorithm">The BPE Algorithm</h4>

<p>BPE iteratively merges the most frequent pair of tokens, starting from individual bytes.</p>

<p><strong>Algorithm:</strong></p>

<ol>
  <li><strong>Initialize vocabulary</strong> with all bytes (256 base tokens)</li>
  <li><strong>For</strong> each iteration $i = 1, 2, \ldots, N$:
    <ul>
      <li>Count all adjacent token pairs in the corpus</li>
      <li>Find most frequent pair $(a, b)$</li>
      <li>Create new token $c = ab$</li>
      <li>Replace all occurrences of $(a, b)$ with $c$</li>
      <li>Add $c$ to vocabulary</li>
    </ul>
  </li>
</ol>

<h4 id="parallel-bpe-training-scaling-to-large-corpora">Parallel BPE Training: Scaling to Large Corpora</h4>

<p>Training BPE on multi-gigabyte datasets on a single core is prohibitively slow. Our implementation uses parallel processing with careful handling of chunk boundaries.</p>

<p><strong>Key challenge:</strong> When splitting files across cores, we canâ€™t split in the middle of a special token boundary (e.g., <code class="language-plaintext highlighter-rouge">&lt;|endoftext|&gt;</code>), or weâ€™ll corrupt the data.</p>

<p><strong>Solution:</strong> Find chunk boundaries aligned with special tokens:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">find_chunk_boundaries</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">num_processes</span><span class="p">,</span> <span class="n">special_token_bytes</span><span class="p">):</span>
    <span class="s">"""
    Find chunk boundaries in a file aligned with special tokens.

    This ensures we never split a file in the middle of a special token,
    which would corrupt the tokenization.
    """</span>
    <span class="n">f</span><span class="p">.</span><span class="n">seek</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">os</span><span class="p">.</span><span class="n">SEEK_END</span><span class="p">)</span>
    <span class="n">file_size</span> <span class="o">=</span> <span class="n">f</span><span class="p">.</span><span class="n">tell</span><span class="p">()</span>
    <span class="n">f</span><span class="p">.</span><span class="n">seek</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

    <span class="n">chunk_size</span> <span class="o">=</span> <span class="n">file_size</span> <span class="o">//</span> <span class="n">num_processes</span>
    <span class="n">boundaries</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_processes</span><span class="p">):</span>
        <span class="n">target_pos</span> <span class="o">=</span> <span class="n">i</span> <span class="o">*</span> <span class="n">chunk_size</span>
        <span class="n">f</span><span class="p">.</span><span class="n">seek</span><span class="p">(</span><span class="n">target_pos</span><span class="p">)</span>

        <span class="c1"># Read ahead to find next special token
</span>        <span class="n">search_window</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">chunk_size</span><span class="p">,</span> <span class="n">file_size</span> <span class="o">-</span> <span class="n">target_pos</span><span class="p">)</span>
        <span class="n">data</span> <span class="o">=</span> <span class="n">f</span><span class="p">.</span><span class="n">read</span><span class="p">(</span><span class="n">search_window</span><span class="p">)</span>
        <span class="n">idx</span> <span class="o">=</span> <span class="n">data</span><span class="p">.</span><span class="n">find</span><span class="p">(</span><span class="n">special_token_bytes</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">idx</span> <span class="o">!=</span> <span class="o">-</span><span class="mi">1</span><span class="p">:</span>
            <span class="n">boundary_pos</span> <span class="o">=</span> <span class="n">target_pos</span> <span class="o">+</span> <span class="n">idx</span> <span class="o">+</span> <span class="nb">len</span><span class="p">(</span><span class="n">special_token_bytes</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">boundary_pos</span> <span class="o">=</span> <span class="n">target_pos</span>

        <span class="n">boundaries</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">boundary_pos</span><span class="p">)</span>

    <span class="n">boundaries</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">file_size</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">boundaries</span>
</code></pre></div></div>

<p><strong>Parallel training workflow:</strong></p>

<ol>
  <li><strong>Chunk the corpus</strong> at special token boundaries</li>
  <li><strong>Process chunks in parallel</strong> using multiprocessing</li>
  <li><strong>Aggregate pair counts</strong> from all workers</li>
  <li><strong>Merge globally most frequent pair</strong></li>
  <li><strong>Repeat</strong> until vocabulary size reached</li>
</ol>

<p><strong>Performance impact:</strong></p>
<ul>
  <li>Single-core: ~45 minutes for 2GB corpus</li>
  <li>8-core parallelization: ~6 minutes for same corpus</li>
  <li><strong>7.5Ã— speedup</strong> with careful boundary alignment</li>
</ul>

<h4 id="practical-bpe-training">Practical BPE Training</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">cs336_basics.bpe</span> <span class="kn">import</span> <span class="n">train_bpe</span>

<span class="c1"># Train tokenizer on TinyStories
</span><span class="n">train_bpe</span><span class="p">(</span>
    <span class="n">input_path</span><span class="o">=</span><span class="s">"data/TinyStoriesV2-GPT4-train.txt"</span><span class="p">,</span>
    <span class="n">vocab_size</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span>
    <span class="n">special_tokens</span><span class="o">=</span><span class="p">[</span><span class="s">"&lt;|endoftext|&gt;"</span><span class="p">],</span>
    <span class="n">output_dir</span><span class="o">=</span><span class="s">"tokenizer"</span><span class="p">,</span>
    <span class="n">num_processes</span><span class="o">=</span><span class="mi">8</span>  <span class="c1"># Can use all cores as needed
</span><span class="p">)</span>
</code></pre></div></div>

<p>This creates:</p>
<ul>
  <li><code class="language-plaintext highlighter-rouge">tokenizer/</code>: <code class="language-plaintext highlighter-rouge">vocab.pkl</code> for Vocabulary mapping</li>
  <li><code class="language-plaintext highlighter-rouge">tokenizer/</code>: <code class="language-plaintext highlighter-rouge">merges.pkl</code> for Merge rules</li>
  <li>Cached tokenized arrays from input dataset for instant loading (e.g., <code class="language-plaintext highlighter-rouge">data_test/train_tokens.npy</code> and <code class="language-plaintext highlighter-rouge">data_test/val_tokens.npy</code>)</li>
</ul>

<hr />

<h3 id="transformer-architecture">Transformer Architecture: RoPE, RMSNorm, and SwiGLU</h3>

<p>Modern Transformers have evolved beyond the original â€œAttention is All You Needâ€ architecture. Our implementation incorporates three key innovations from recent research: Rotary Position Embeddings (RoPE), RMS Normalization, and SwiGLU activation.</p>

<h4 id="rotary-position-embeddings-rope">Rotary Position Embeddings (RoPE)</h4>

<p><strong>The problem with absolute position embeddings:</strong></p>
<ul>
  <li>Standard learned embeddings donâ€™t generalize to longer sequences than seen during training</li>
  <li>No notion of relative distance between tokens</li>
</ul>

<p><strong>RoPE solution:</strong> Encode positional information by rotating query and key vectors in the complex plane.</p>

<p><strong>Mathematical formulation:</strong></p>

<p>For position $m$ and dimension pair $(2i, 2i+1)$, apply rotation matrix:</p>

\[\begin{pmatrix} q_{2i}^{(m)} \\ q_{2i+1}^{(m)} \end{pmatrix} = \begin{pmatrix} \cos(m\theta_i) &amp; -\sin(m\theta_i) \\ \sin(m\theta_i) &amp; \cos(m\theta_i) \end{pmatrix} \begin{pmatrix} q_{2i} \\ q_{2i+1} \end{pmatrix}\]

<p>Where $\theta_i = 10000^{-2i/d}$ (frequency decreases with dimension)</p>

<p><strong>Key property:</strong> The dot product $q^{(m)} \cdot k^{(n)}$ depends only on relative position $m - n$:</p>

\[\text{RoPE}(q_m, k_n, m, n) = \text{RoPE}(q_m, k_n, 0, n-m)\]

<p><strong>Implementation:</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">rotate_half</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="s">"batch seq n_heads d_head"</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Float</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="s">"batch seq n_heads d_head"</span><span class="p">]:</span>
    <span class="s">"""
    Rotate the second half of the last dimension to the first half.
    This implements the rotation: [x1, x2, x3, x4] â†’ [-x3, -x4, x1, x2]
    """</span>
    <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">chunk</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="n">cat</span><span class="p">((</span><span class="o">-</span><span class="n">x2</span><span class="p">,</span> <span class="n">x1</span><span class="p">),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">apply_rotary_pos_emb</span><span class="p">(</span>
    <span class="n">x</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="s">"batch seq n_heads d_head"</span><span class="p">],</span>
    <span class="n">freqs_cos</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="s">"seq d_head"</span><span class="p">],</span>
    <span class="n">freqs_sin</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="s">"seq d_head"</span><span class="p">],</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Float</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="s">"batch seq n_heads d_head"</span><span class="p">]:</span>
    <span class="s">"""
    Apply rotary position embeddings to input tensor.

    This implements: x_rotated = x * cos(mÎ¸) + rotate_half(x) * sin(mÎ¸)
    """</span>
    <span class="c1"># Expand frequency tensors to match input dimensions
</span>    <span class="n">freqs_cos</span> <span class="o">=</span> <span class="n">freqs_cos</span><span class="p">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">).</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>  <span class="c1"># [1, seq, 1, d_head]
</span>    <span class="n">freqs_sin</span> <span class="o">=</span> <span class="n">freqs_sin</span><span class="p">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">).</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>

    <span class="c1"># Apply rotation
</span>    <span class="k">return</span> <span class="n">x</span> <span class="o">*</span> <span class="n">freqs_cos</span> <span class="o">+</span> <span class="n">rotate_half</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">*</span> <span class="n">freqs_sin</span>
</code></pre></div></div>

<p><strong>Why RoPE matters:</strong></p>
<ul>
  <li><strong>Better length generalization:</strong> Models trained on 512 tokens can inference on 2048+ tokens</li>
  <li><strong>Relative position encoding:</strong> Attention naturally focuses on nearby tokens</li>
  <li><strong>No learned parameters:</strong> Purely geometric transformation</li>
</ul>

<h4 id="rms-normalization-simpler-and-faster">RMS Normalization: Simpler and Faster</h4>

<p><strong>LayerNorm (traditional):</strong></p>

<p>$\text{LayerNorm}(x) = \gamma \cdot \frac{x - \mu}{\sigma} + \beta$</p>

<p>Where $\mu$ and $\sigma$ are mean and standard deviation.</p>

<p><strong>RMSNorm (modern):</strong></p>

<p>$\text{RMSNorm}(x) = \gamma \cdot \frac{x}{\text{RMS}(x)} \quad \text{where} \quad \text{RMS}(x) = \sqrt{\frac{1}{d}\sum_{i=1}^d x_i^2}$</p>

<p><strong>Key differences:</strong></p>
<ul>
  <li>âœ— No mean centering (no $-\mu$ term)</li>
  <li>âœ— No bias term ($\beta$)</li>
  <li>âœ“ 10-30% faster computation</li>
  <li>âœ“ Equivalent performance in practice</li>
</ul>

<p><strong>Implementation:</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">RMSNorm</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="s">"""
    RMS Normalization layer.

    Normalizes by root mean square rather than standard deviation,
    removing the mean centering step for efficiency.
    """</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">eps</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-5</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">eps</span> <span class="o">=</span> <span class="n">eps</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">ones</span><span class="p">(</span><span class="n">d_model</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="s">"batch seq d_model"</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Float</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="s">"batch seq d_model"</span><span class="p">]:</span>
        <span class="c1"># Compute RMS: sqrt(mean(x^2))
</span>        <span class="n">rms</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">x</span> <span class="o">**</span> <span class="mi">2</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">eps</span><span class="p">)</span>

        <span class="c1"># Normalize and scale
</span>        <span class="n">x_normed</span> <span class="o">=</span> <span class="n">x</span> <span class="o">/</span> <span class="n">rms</span>
        <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">weight</span> <span class="o">*</span> <span class="n">x_normed</span>
</code></pre></div></div>

<p><strong>Why RMSNorm matters:</strong></p>
<ul>
  <li>Adopted by LLaMA, GPT-NeoX, and other modern LLMs</li>
  <li>Simpler backward pass (fewer terms to compute)</li>
  <li>Lower memory bandwidth requirements</li>
</ul>

<h4 id="swiglu-gated-linear-units-with-swish">SwiGLU: Gated Linear Units with Swish</h4>

<p><strong>Standard FFN (original Transformer):</strong></p>

<p>$\text{FFN}(x) = W_2 \cdot \text{ReLU}(W_1 x)$</p>

<p><strong>SwiGLU (modern):</strong></p>

<p>$\text{SwiGLU}(x) = (W_1 x \otimes \text{Swish}(W_3 x)) W_2$</p>

<p>Where:</p>
<ul>
  <li>$\text{Swish}(x) = x \cdot \sigma(x)$ (smooth, non-monotonic activation)</li>
  <li>$\otimes$ is element-wise multiplication (gating mechanism)</li>
</ul>

<p><strong>Why gating works:</strong>
The gating mechanism allows the network to control information flow:</p>
<ul>
  <li>$W_1 x$: Transformed features</li>
  <li>$\text{Swish}(W_3 x)$: Gates that decide what to pass through</li>
  <li>Element-wise product: Selective information routing</li>
</ul>

<p><strong>Implementation:</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">SwiGLU</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="s">"""
    SwiGLU activation function: Swish-Gated Linear Unit.

    Combines Swish activation with a gating mechanism for better
    representational capacity than standard ReLU.
    """</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">w1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">w2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_ff</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">w3</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="s">"batch seq d_model"</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Float</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="s">"batch seq d_model"</span><span class="p">]:</span>
        <span class="c1"># Swish activation: x * sigmoid(x)
</span>        <span class="n">swish</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">w3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">*</span> <span class="n">torch</span><span class="p">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">w3</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>

        <span class="c1"># Gated linear unit
</span>        <span class="n">gated</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">w1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">*</span> <span class="n">swish</span>

        <span class="c1"># Project back to d_model
</span>        <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">w2</span><span class="p">(</span><span class="n">gated</span><span class="p">)</span>
</code></pre></div></div>

<p><strong>Why SwiGLU matters:</strong></p>
<ul>
  <li><strong>Better performance:</strong> PaLM paper shows 1-2% improvement over standard FFN</li>
  <li><strong>Smooth gradients:</strong> Swish has non-zero gradients for negative inputs (unlike ReLU)</li>
  <li><strong>Gating flexibility:</strong> Network learns what information to propagate</li>
</ul>

<h4 id="complete-transformer-block">Complete Transformer Block</h4>

<p>Putting it all together:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">TransformerBlock</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="s">"""
    Single Transformer block with modern architectural choices:
    - RoPE for positional encoding
    - RMSNorm for normalization
    - SwiGLU for feed-forward network
    """</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">context_length</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>

        <span class="c1"># Pre-normalization (RMSNorm before attention)
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">norm1</span> <span class="o">=</span> <span class="n">RMSNorm</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>

        <span class="c1"># Multi-head attention with RoPE
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">attn</span> <span class="o">=</span> <span class="n">MultiHeadAttention</span><span class="p">(</span>
            <span class="n">d_model</span><span class="o">=</span><span class="n">d_model</span><span class="p">,</span>
            <span class="n">num_heads</span><span class="o">=</span><span class="n">num_heads</span><span class="p">,</span>
            <span class="n">context_length</span><span class="o">=</span><span class="n">context_length</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="c1"># Pre-normalization (RMSNorm before FFN)
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">norm2</span> <span class="o">=</span> <span class="n">RMSNorm</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>

        <span class="c1"># SwiGLU feed-forward network
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">ffn</span> <span class="o">=</span> <span class="n">SwiGLU</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="s">"batch seq d_model"</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Float</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="s">"batch seq d_model"</span><span class="p">]:</span>
        <span class="c1"># Attention block with residual connection
</span>        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">attn</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">norm1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>

        <span class="c1"># FFN block with residual connection
</span>        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">ffn</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">norm2</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>

        <span class="k">return</span> <span class="n">x</span>
</code></pre></div></div>

<p><strong>Architectural choices summary:</strong></p>

<table>
  <thead>
    <tr>
      <th>Component</th>
      <th>Traditional</th>
      <th>Modern (Our Choice)</th>
      <th>Benefit</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Position Encoding</td>
      <td>Learned/Sinusoidal</td>
      <td>RoPE</td>
      <td>Length generalization</td>
    </tr>
    <tr>
      <td>Normalization</td>
      <td>LayerNorm</td>
      <td>RMSNorm</td>
      <td>10-30% faster</td>
    </tr>
    <tr>
      <td>Activation</td>
      <td>ReLU/GeLU</td>
      <td>SwiGLU</td>
      <td>1-2% better performance</td>
    </tr>
    <tr>
      <td>Norm Placement</td>
      <td>Post-norm</td>
      <td>Pre-norm</td>
      <td>Training stability</td>
    </tr>
  </tbody>
</table>

<hr />

<h3 id="training-configurations">Three-Tiered Training Configuration</h3>

<p>One of the most practical aspects of this implementation is the <strong>three-tiered training configuration</strong>, designed to balance <strong>rapid iteration</strong> with <strong>final model quality</strong>. Instead of forcing every experiment to run a full multi-hour training job, the system provides lightweight modes for debugging, development, and production training.</p>

<h4 id="the-problem-long-feedback-loops">The Problem: Long Feedback Loops</h4>

<p>Training a realistic language model can take <strong>hours or even days</strong>, creating extremely slow feedback cycles:</p>

<ul>
  <li><strong>Full TinyStories training:</strong> ~7 hours on an M4 MacBook Pro</li>
  <li>Make a small code change? â†’ another 7 hours to validate</li>
  <li>Debug a tensor shape issue? â†’ another long wait</li>
  <li>Experiment with a hyperparameter? â†’ you see the patternâ€¦</li>
</ul>

<p>This makes rapid model development <strong>painful and impractical</strong>. No one wants to wait half a day just to check whether a single attention-head change broke the model.</p>

<h4 id="the-solution-graduated-configurations">The Solution: Graduated Configurations</h4>

<p>The shared implementation in  <a href="https://github.com/bearbearyu1223/tinystories-transformer"><svg height="16" width="16" viewBox="0 0 16 16" style="display: inline-block; vertical-align: text-bottom;"><path fill="currentColor" d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z"></path></svg> github.com/bearbearyu1223/tinystories-transformer</a> provides three configurations with increasing complexity:</p>

<table>
  <thead>
    <tr>
      <th>Configuration</th>
      <th>Iterations</th>
      <th>Vocab Size</th>
      <th>Dataset Size</th>
      <th>Model Size</th>
      <th>Time</th>
      <th>Use Case</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong><a href="https://github.com/bearbearyu1223/tinystories-transformer/blob/main/config_quicktest.json">config_quicktest.json</a></strong></td>
      <td>10</td>
      <td>2,000</td>
      <td>10K lines</td>
      <td>0.94M</td>
      <td>&lt; 1 min</td>
      <td>Code validation, CI/CD</td>
    </tr>
    <tr>
      <td><strong><a href="https://github.com/bearbearyu1223/tinystories-transformer/blob/main/config_test.json">config_test.json</a></strong></td>
      <td>1,000</td>
      <td>5,000</td>
      <td>50K lines</td>
      <td>4.1M</td>
      <td>~20 min</td>
      <td>Active fast development</td>
    </tr>
    <tr>
      <td><strong><a href="https://github.com/bearbearyu1223/tinystories-transformer/blob/main/config_tinystories.json">config_tinystories.json</a></strong></td>
      <td>20,000</td>
      <td>10,000</td>
      <td>15.6M lines</td>
      <td>17M</td>
      <td>~7 hours</td>
      <td>Production training experiment</td>
    </tr>
  </tbody>
</table>

<h4 id="configuration-1-quicktest-sanity-check">Configuration 1: Quicktest (Sanity Check)</h4>

<p><strong>Purpose:</strong> Ultra-fast validation that your code works at all.</p>

<p><strong>config_quicktest.json:</strong></p>
<div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span><span class="w">
  </span><span class="nl">"data_dir"</span><span class="p">:</span><span class="w"> </span><span class="s2">"data_quicktest"</span><span class="p">,</span><span class="w">
  </span><span class="nl">"train_file"</span><span class="p">:</span><span class="w"> </span><span class="s2">"TinyStoriesV2-GPT4-train-quicktest.txt"</span><span class="p">,</span><span class="w">
  </span><span class="nl">"val_file"</span><span class="p">:</span><span class="w"> </span><span class="s2">"TinyStoriesV2-GPT4-valid-quicktest.txt"</span><span class="p">,</span><span class="w">
  </span><span class="nl">"vocab_size"</span><span class="p">:</span><span class="w"> </span><span class="mi">2000</span><span class="p">,</span><span class="w">
  </span><span class="nl">"context_length"</span><span class="p">:</span><span class="w"> </span><span class="mi">64</span><span class="p">,</span><span class="w">
  </span><span class="nl">"d_model"</span><span class="p">:</span><span class="w"> </span><span class="mi">256</span><span class="p">,</span><span class="w">
  </span><span class="nl">"num_layers"</span><span class="p">:</span><span class="w"> </span><span class="mi">4</span><span class="p">,</span><span class="w">
  </span><span class="nl">"num_heads"</span><span class="p">:</span><span class="w"> </span><span class="mi">8</span><span class="p">,</span><span class="w">
  </span><span class="nl">"d_ff"</span><span class="p">:</span><span class="w"> </span><span class="mi">672</span><span class="p">,</span><span class="w">
  </span><span class="nl">"batch_size"</span><span class="p">:</span><span class="w"> </span><span class="mi">32</span><span class="p">,</span><span class="w">
  </span><span class="nl">"max_iters"</span><span class="p">:</span><span class="w"> </span><span class="mi">10</span><span class="p">,</span><span class="w">
  </span><span class="nl">"log_file"</span><span class="p">:</span><span class="w"> </span><span class="s2">"logs/quicktest_training.log"</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre></div></div>

<p><strong>What you will be able to get:</strong></p>
<ul>
  <li>Training runs in &lt; 1 minute</li>
  <li>Verifies code correctness (no shape mismatches, no NaN losses)</li>
  <li>Useful for CI/CD pipelines</li>
  <li>Not useful for actual model quality</li>
</ul>

<p><strong>When to use:</strong></p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># After changing tensor operations</span>
uv run train-transformer config_quicktest.json

<span class="c"># In CI/CD pipeline</span>
pytest <span class="o">&amp;&amp;</span> uv run train-transformer config_quicktest.json
</code></pre></div></div>

<h4 id="configuration-2-test-active-development">Configuration 2: Test (Active Development)</h4>

<p><strong>Purpose:</strong> Production-like quality in development timeframes.</p>

<p><strong>config_test.json:</strong></p>
<div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span><span class="w">
  </span><span class="nl">"data_dir"</span><span class="p">:</span><span class="w"> </span><span class="s2">"data_test"</span><span class="p">,</span><span class="w">
  </span><span class="nl">"train_file"</span><span class="p">:</span><span class="w"> </span><span class="s2">"TinyStoriesV2-GPT4-train-test.txt"</span><span class="p">,</span><span class="w">
  </span><span class="nl">"val_file"</span><span class="p">:</span><span class="w"> </span><span class="s2">"TinyStoriesV2-GPT4-valid-test.txt"</span><span class="p">,</span><span class="w">
  </span><span class="nl">"vocab_size"</span><span class="p">:</span><span class="w"> </span><span class="mi">5000</span><span class="p">,</span><span class="w">
  </span><span class="nl">"context_length"</span><span class="p">:</span><span class="w"> </span><span class="mi">128</span><span class="p">,</span><span class="w">
  </span><span class="nl">"d_model"</span><span class="p">:</span><span class="w"> </span><span class="mi">512</span><span class="p">,</span><span class="w">
  </span><span class="nl">"num_layers"</span><span class="p">:</span><span class="w"> </span><span class="mi">4</span><span class="p">,</span><span class="w">
  </span><span class="nl">"num_heads"</span><span class="p">:</span><span class="w"> </span><span class="mi">16</span><span class="p">,</span><span class="w">
  </span><span class="nl">"d_ff"</span><span class="p">:</span><span class="w"> </span><span class="mi">1344</span><span class="p">,</span><span class="w">
  </span><span class="nl">"batch_size"</span><span class="p">:</span><span class="w"> </span><span class="mi">64</span><span class="p">,</span><span class="w">
  </span><span class="nl">"max_iters"</span><span class="p">:</span><span class="w"> </span><span class="mi">1000</span><span class="p">,</span><span class="w">
  </span><span class="nl">"log_file"</span><span class="p">:</span><span class="w"> </span><span class="s2">"logs/test_training.log"</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre></div></div>

<p><strong>What you will be able to get:</strong></p>
<ul>
  <li>Training loss: 8.58 â†’ 3.11 (63.8% reduction)</li>
  <li>Perplexity: 5,309 â†’ 23.4 (99.6% reduction)</li>
  <li>Model generates coherent (if simple) sentences</li>
  <li><strong>Fast enough for hyperparameter tuning</strong></li>
</ul>

<p><strong>Training dynamics (test configuration):</strong></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Phase 1 (0-300 iters): Rapid initial learning
  Loss: 8.6 â†’ 3.7 (massive initial drop)

Phase 2 (300-700 iters): Steady optimization
  Loss: 3.7 â†’ 3.2 (perplexity stabilizes)

Phase 3 (700-1000 iters): Fine-tuning
  Loss: 3.2 â†’ 3.1 (diminishing returns)
</code></pre></div></div>

<p><strong>When to use:</strong></p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Testing new architecture changes</span>
uv run train-transformer config_test.json

<span class="c"># Hyperparameter sweep (different learning rates, etc.)</span>
<span class="k">for </span>lr <span class="k">in </span>1e-4 3e-4 1e-3<span class="p">;</span> <span class="k">do
  </span>uv run train-transformer config_test.json <span class="nt">--lr</span> <span class="nv">$lr</span>
<span class="k">done</span>
</code></pre></div></div>

<h4 id="configuration-3-tinystories-production-training-experiments">Configuration 3: TinyStories (Production Training Experiments)</h4>

<p><strong>Purpose:</strong> Best possible model quality, no compromises.</p>

<p><strong>config_tinystories.json:</strong></p>
<div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span><span class="w">
  </span><span class="nl">"data_dir"</span><span class="p">:</span><span class="w"> </span><span class="s2">"data"</span><span class="p">,</span><span class="w">
  </span><span class="nl">"train_file"</span><span class="p">:</span><span class="w"> </span><span class="s2">"TinyStoriesV2-GPT4-train.txt"</span><span class="p">,</span><span class="w">
  </span><span class="nl">"val_file"</span><span class="p">:</span><span class="w"> </span><span class="s2">"TinyStoriesV2-GPT4-valid.txt"</span><span class="p">,</span><span class="w">
  </span><span class="nl">"vocab_size"</span><span class="p">:</span><span class="w"> </span><span class="mi">10000</span><span class="p">,</span><span class="w">
  </span><span class="nl">"context_length"</span><span class="p">:</span><span class="w"> </span><span class="mi">256</span><span class="p">,</span><span class="w">
  </span><span class="nl">"d_model"</span><span class="p">:</span><span class="w"> </span><span class="mi">512</span><span class="p">,</span><span class="w">
  </span><span class="nl">"num_layers"</span><span class="p">:</span><span class="w"> </span><span class="mi">4</span><span class="p">,</span><span class="w">
  </span><span class="nl">"num_heads"</span><span class="p">:</span><span class="w"> </span><span class="mi">16</span><span class="p">,</span><span class="w">
  </span><span class="nl">"d_ff"</span><span class="p">:</span><span class="w"> </span><span class="mi">1344</span><span class="p">,</span><span class="w">
  </span><span class="nl">"batch_size"</span><span class="p">:</span><span class="w"> </span><span class="mi">128</span><span class="p">,</span><span class="w">
  </span><span class="nl">"max_iters"</span><span class="p">:</span><span class="w"> </span><span class="mi">20000</span><span class="p">,</span><span class="w">
  </span><span class="nl">"log_file"</span><span class="p">:</span><span class="w"> </span><span class="s2">"logs/tinystories_training.log"</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre></div></div>

<p><strong>What you will be able to get:</strong></p>
<ul>
  <li>Training loss: 9.25 â†’ 1.61 (82.6% reduction)</li>
  <li>Perplexity: ~10,500 â†’ ~5.0 (99.95% reduction)</li>
  <li>Model generates coherent multi-sentence stories</li>
  <li>Production-quality checkpoint for deployment</li>
</ul>

<p><strong>Training dynamics (full configuration):</strong></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Warmup (0-1000): Learning rate warmup, rapid gains
  Loss: 9.25 â†’ 2.50

Main training (1000-6000): Steady improvement
  Loss: 2.50 â†’ 1.61

Long-term (6000-20000): Continued refinement
  Perplexity continues improving, no signs of plateauing
</code></pre></div></div>

<p><strong>When to use:</strong></p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Final production model</span>
uv run train-transformer config_tinystories.json

<span class="c"># Overnight training run for best results, runs your training job in the background, keeps it alive after closing the terminal, and writes all output (stdout + stderr) to training.log</span>
<span class="nb">nohup </span>uv run train-transformer config_tinystories.json <span class="o">&gt;</span> training.log 2&gt;&amp;1 &amp;
</code></pre></div></div>

<h4 id="the-power-of-graduated-configurations">The Power of Graduated Configurations</h4>

<p>This three-tiered approach provides:</p>

<ol>
  <li><strong>Rapid iteration:</strong> Fix bugs in minutes, not hours</li>
  <li><strong>Confident scaling:</strong> Test config validates production config will work</li>
  <li><strong>Clear development workflow:</strong>
    <ul>
      <li>Write code â†’ Test with quicktest</li>
      <li>Validate quality â†’ Run test config</li>
      <li>Deploy â†’ Use tinystories checkpoint</li>
    </ul>
  </li>
</ol>

<hr />

<h3 id="training-pipeline">The Training Pipeline: Memory-Efficient and Robust</h3>

<p>A production training pipeline must handle datasets larger than RAM, resume from crashes, and provide clear visibility into training progress.</p>

<h4 id="memory-mapped-data-loading">Memory-Mapped Data Loading</h4>

<p><strong>The challenge:</strong> TinyStories full dataset is 2.1GB tokenized. Loading into RAM:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">dataset</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="s">'train_tokens.npy'</span><span class="p">)</span>  <span class="c1"># Loads entire 2.1GB into memory!
</span></code></pre></div></div>

<p>This works for small datasets but fails at scale.</p>

<p><strong>The solution:</strong> Memory-mapped arrays using Unix <code class="language-plaintext highlighter-rouge">mmap</code> system call:
A reference implementation to illustrate the idea</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">load_dataset</span><span class="p">(</span><span class="n">data_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">:</span>
    <span class="s">"""
    Load dataset using memory-mapped mode for memory efficiency.

    Memory mapping allows treating files as arrays without loading
    into RAM. The OS loads only accessed pages on-demand.
    """</span>
    <span class="n">dataset</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="n">data_path</span><span class="p">,</span> <span class="n">mmap_mode</span><span class="o">=</span><span class="s">"r"</span><span class="p">)</span>

    <span class="c1"># Verify data integrity
</span>    <span class="n">max_token</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">.</span><span class="nb">max</span><span class="p">()</span>
    <span class="n">min_token</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">.</span><span class="nb">min</span><span class="p">()</span>

    <span class="k">if</span> <span class="n">max_token</span> <span class="o">&gt;=</span> <span class="n">vocab_size</span><span class="p">:</span>
        <span class="k">raise</span> <span class="nb">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s">"Invalid token </span><span class="si">{</span><span class="n">max_token</span><span class="si">}</span><span class="s"> &gt;= vocab_size </span><span class="si">{</span><span class="n">vocab_size</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">min_token</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">raise</span> <span class="nb">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s">"Negative token </span><span class="si">{</span><span class="n">min_token</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">dataset</span>
</code></pre></div></div>

<p><strong>How memory mapping works:</strong></p>

<ol>
  <li><strong>Create virtual memory mapping:</strong> File appears as if loaded into RAM</li>
  <li><strong>Page fault on access:</strong> When you read <code class="language-plaintext highlighter-rouge">dataset[1000000]</code>, OS loads just that 4KB page</li>
  <li><strong>LRU caching:</strong> OS automatically keeps recently-accessed pages in RAM</li>
  <li><strong>Eviction:</strong> When RAM is full, OS evicts least-recently-used pages</li>
</ol>

<p><strong>Performance:</strong></p>
<ul>
  <li>Memory usage: Constant (few MB) regardless of dataset size</li>
  <li>Speed: Near-RAM speed for sequential access (OS prefetching)</li>
  <li>Scales: Can handle TB-scale datasets on machines with GBs of RAM</li>
</ul>

<h4 id="timestamp-based-logging">Timestamp-Based Logging</h4>

<p><strong>The problem:</strong> Running multiple experiments overwrites log files, losing history.</p>

<p><strong>The solution:</strong> Timestamp-based log files:
A reference implementation to illustrate the idea</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">_setup_logging</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">log_file</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">log_level</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>
    <span class="s">"""Setup logging with timestamps to avoid overwriting previous runs."""</span>
    <span class="k">if</span> <span class="n">log_file</span><span class="p">:</span>
        <span class="n">log_path</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="n">log_file</span><span class="p">)</span>
        <span class="n">log_path</span><span class="p">.</span><span class="n">parent</span><span class="p">.</span><span class="n">mkdir</span><span class="p">(</span><span class="n">parents</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

        <span class="c1"># Add timestamp: e.g., logs/test_training_20251116_122750.log
</span>        <span class="n">timestamp</span> <span class="o">=</span> <span class="n">datetime</span><span class="p">.</span><span class="n">now</span><span class="p">().</span><span class="n">strftime</span><span class="p">(</span><span class="s">'%Y%m%d_%H%M%S'</span><span class="p">)</span>
        <span class="n">stem</span> <span class="o">=</span> <span class="n">log_path</span><span class="p">.</span><span class="n">stem</span>
        <span class="n">suffix</span> <span class="o">=</span> <span class="n">log_path</span><span class="p">.</span><span class="n">suffix</span>
        <span class="n">timestamped_filename</span> <span class="o">=</span> <span class="sa">f</span><span class="s">"</span><span class="si">{</span><span class="n">stem</span><span class="si">}</span><span class="s">_</span><span class="si">{</span><span class="n">timestamp</span><span class="si">}{</span><span class="n">suffix</span><span class="si">}</span><span class="s">"</span>
        <span class="n">timestamped_log_path</span> <span class="o">=</span> <span class="n">log_path</span><span class="p">.</span><span class="n">parent</span> <span class="o">/</span> <span class="n">timestamped_filename</span>

        <span class="n">file_handler</span> <span class="o">=</span> <span class="n">logging</span><span class="p">.</span><span class="n">FileHandler</span><span class="p">(</span><span class="n">timestamped_log_path</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s">'w'</span><span class="p">)</span>
        <span class="n">logger</span><span class="p">.</span><span class="n">addHandler</span><span class="p">(</span><span class="n">file_handler</span><span class="p">)</span>

        <span class="bp">self</span><span class="p">.</span><span class="n">actual_log_file</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">timestamped_log_path</span><span class="p">)</span>
</code></pre></div></div>

<p><strong>Benefits:</strong></p>
<ul>
  <li>Never lose experimental results</li>
  <li>Easy to compare multiple runs</li>
  <li>Git-friendly (no log file conflicts)</li>
</ul>

<h4 id="robust-checkpoint-management">Robust Checkpoint Management</h4>

<p><strong>What to save in checkpoints:</strong>
A reference implementation to illustrate the idea</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">save_checkpoint</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">iteration</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">checkpoint_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>
    <span class="s">"""Save complete training state for resumption."""</span>
    <span class="n">checkpoint</span> <span class="o">=</span> <span class="p">{</span>
        <span class="c1"># Model state
</span>        <span class="s">'model_state_dict'</span><span class="p">:</span> <span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">state_dict</span><span class="p">(),</span>

        <span class="c1"># Optimizer state (critical for AdamW momentum!)
</span>        <span class="s">'optimizer_state_dict'</span><span class="p">:</span> <span class="bp">self</span><span class="p">.</span><span class="n">optimizer</span><span class="p">.</span><span class="n">state_dict</span><span class="p">(),</span>

        <span class="c1"># Training progress
</span>        <span class="s">'iteration'</span><span class="p">:</span> <span class="n">iteration</span><span class="p">,</span>

        <span class="c1"># Model architecture (for loading during inference)
</span>        <span class="s">'config'</span><span class="p">:</span> <span class="p">{</span>
            <span class="s">'vocab_size'</span><span class="p">:</span> <span class="bp">self</span><span class="p">.</span><span class="n">config</span><span class="p">[</span><span class="s">'vocab_size'</span><span class="p">],</span>
            <span class="s">'d_model'</span><span class="p">:</span> <span class="bp">self</span><span class="p">.</span><span class="n">config</span><span class="p">[</span><span class="s">'d_model'</span><span class="p">],</span>
            <span class="s">'num_layers'</span><span class="p">:</span> <span class="bp">self</span><span class="p">.</span><span class="n">config</span><span class="p">[</span><span class="s">'num_layers'</span><span class="p">],</span>
            <span class="s">'num_heads'</span><span class="p">:</span> <span class="bp">self</span><span class="p">.</span><span class="n">config</span><span class="p">[</span><span class="s">'num_heads'</span><span class="p">],</span>
            <span class="s">'d_ff'</span><span class="p">:</span> <span class="bp">self</span><span class="p">.</span><span class="n">config</span><span class="p">[</span><span class="s">'d_ff'</span><span class="p">],</span>
            <span class="s">'context_length'</span><span class="p">:</span> <span class="bp">self</span><span class="p">.</span><span class="n">config</span><span class="p">[</span><span class="s">'context_length'</span><span class="p">],</span>
        <span class="p">}</span>
    <span class="p">}</span>

    <span class="n">torch</span><span class="p">.</span><span class="n">save</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">,</span> <span class="n">checkpoint_path</span><span class="p">)</span>
</code></pre></div></div>

<p><strong>Why optimizer state matters:</strong></p>

<p>AdamW maintains two momentum buffers (first and second moments) for each parameter. Without these:</p>
<ul>
  <li>Learning restarts from scratch</li>
  <li>Previous gradient history lost</li>
  <li>Convergence slows dramatically</li>
</ul>

<p><strong>Loading for inference:</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">checkpoint</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="s">"checkpoint_final.pt"</span><span class="p">)</span>
<span class="n">config</span> <span class="o">=</span> <span class="n">checkpoint</span><span class="p">[</span><span class="s">'config'</span><span class="p">]</span>

<span class="c1"># Rebuild model from saved architecture
</span><span class="n">model</span> <span class="o">=</span> <span class="n">TransformerLM</span><span class="p">(</span><span class="o">**</span><span class="n">config</span><span class="p">)</span>
<span class="n">model</span><span class="p">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">[</span><span class="s">'model_state_dict'</span><span class="p">])</span>
<span class="n">model</span><span class="p">.</span><span class="nb">eval</span><span class="p">()</span>
</code></pre></div></div>

<h4 id="training-loop-structure">Training Loop Structure</h4>
<p>A reference implementation to illustrate the idea</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="s">"""Main training loop with evaluation, checkpointing, and logging."""</span>
    <span class="k">for</span> <span class="n">iteration</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">start_iter</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">max_iters</span><span class="p">):</span>
        <span class="c1"># Dynamic learning rate scheduling
</span>        <span class="n">lr</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">_get_lr</span><span class="p">(</span><span class="n">iteration</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">param_group</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">optimizer</span><span class="p">.</span><span class="n">param_groups</span><span class="p">:</span>
            <span class="n">param_group</span><span class="p">[</span><span class="s">'lr'</span><span class="p">]</span> <span class="o">=</span> <span class="n">lr</span>

        <span class="c1"># Training step
</span>        <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">_get_batch</span><span class="p">(</span><span class="s">'train'</span><span class="p">)</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">apply_softmax</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">cross_entropy</span><span class="p">(</span><span class="n">logits</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">logits</span><span class="p">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)),</span> <span class="n">y</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>
        <span class="n">loss</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span>

        <span class="c1"># Gradient clipping for stability
</span>        <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="n">clip_grad_norm_</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="mf">1.0</span><span class="p">)</span>

        <span class="bp">self</span><span class="p">.</span><span class="n">optimizer</span><span class="p">.</span><span class="n">step</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">optimizer</span><span class="p">.</span><span class="n">zero_grad</span><span class="p">()</span>

        <span class="c1"># Periodic evaluation
</span>        <span class="k">if</span> <span class="n">iteration</span> <span class="o">%</span> <span class="bp">self</span><span class="p">.</span><span class="n">eval_interval</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">train_loss</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">_estimate_loss</span><span class="p">(</span><span class="s">'train'</span><span class="p">)</span>
            <span class="n">val_loss</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">_estimate_loss</span><span class="p">(</span><span class="s">'val'</span><span class="p">)</span>
            <span class="n">perplexity</span> <span class="o">=</span> <span class="n">math</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="n">val_loss</span><span class="p">)</span>

            <span class="n">logger</span><span class="p">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s">"[Iteration </span><span class="si">{</span><span class="n">iteration</span><span class="si">}</span><span class="s">] Evaluating model..."</span><span class="p">)</span>
            <span class="n">logger</span><span class="p">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s">"  Train loss: </span><span class="si">{</span><span class="n">train_loss</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
            <span class="n">logger</span><span class="p">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s">"  Val loss:   </span><span class="si">{</span><span class="n">val_loss</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
            <span class="n">logger</span><span class="p">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s">"  Perplexity: </span><span class="si">{</span><span class="n">perplexity</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>

        <span class="c1"># Periodic checkpointing
</span>        <span class="k">if</span> <span class="n">iteration</span> <span class="o">%</span> <span class="bp">self</span><span class="p">.</span><span class="n">checkpoint_interval</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">iteration</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">checkpoint_path</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">checkpoint_dir</span> <span class="o">/</span> <span class="sa">f</span><span class="s">"checkpoint_iter_</span><span class="si">{</span><span class="n">iteration</span><span class="si">}</span><span class="s">.pt"</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">save_checkpoint</span><span class="p">(</span><span class="n">iteration</span><span class="p">,</span> <span class="n">checkpoint_path</span><span class="p">)</span>

    <span class="c1"># Final checkpoint
</span>    <span class="bp">self</span><span class="p">.</span><span class="n">save_checkpoint</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">max_iters</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">checkpoint_dir</span> <span class="o">/</span> <span class="s">"checkpoint_final.pt"</span><span class="p">)</span>
</code></pre></div></div>

<hr />

<h3 id="text-generation">Text Generation: Temperature, Top-k, and Top-p Sampling</h3>

<p>After training, we need sophisticated decoding strategies to turn model predictions into coherent text. The generation strategy dramatically impacts output qualityâ€”itâ€™s the difference between repetitive nonsense and creative storytelling.</p>

<h4 id="the-generation-problem">The Generation Problem</h4>

<p>At each step, the model outputs a probability distribution over 10,000 tokens. We need to:</p>
<ol>
  <li>Sample the next token from this distribution</li>
  <li>Balance coherence (following likely continuations) with diversity (avoiding repetition)</li>
  <li>Avoid both â€œtoo deterministicâ€ (boring) and â€œtoo randomâ€ (nonsensical)</li>
</ol>

<h4 id="temperature-scaling-controlling-randomness">Temperature Scaling: Controlling Randomness</h4>

<p><strong>The idea:</strong> Adjust the â€œsharpnessâ€ of the probability distribution before sampling.</p>

<p><strong>Formula:</strong></p>

<p>$P(x_{t+1} = i) = \frac{\exp(v_i / \tau)}{\sum_j \exp(v_j / \tau)}$</p>

<p>Where:</p>
<ul>
  <li>$v_i$ = modelâ€™s logit for token $i$</li>
  <li>$\tau$ = temperature parameter</li>
</ul>

<p><strong>Effects:</strong></p>
<ul>
  <li>$\tau \to 0$: Distribution becomes peaked (nearly greedy/deterministic)</li>
  <li>$\tau = 1.0$: Standard softmax (modelâ€™s original distribution)</li>
  <li>$\tau &gt; 1$: Distribution becomes flatter (more random/creative)</li>
</ul>

<p><strong>Concrete example:</strong></p>

<p>Original logits: <code class="language-plaintext highlighter-rouge">[2.5, 1.0, 0.2, -1.5]</code> for tokens <code class="language-plaintext highlighter-rouge">["cat", "dog", "banana", "spaceship"]</code></p>

<table>
  <thead>
    <tr>
      <th>Temperature</th>
      <th>P(cat)</th>
      <th>P(dog)</th>
      <th>P(banana)</th>
      <th>P(spaceship)</th>
      <th>Character</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Ï„ = 0.1</td>
      <td>0.996</td>
      <td>0.004</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>Deterministic</td>
    </tr>
    <tr>
      <td>Ï„ = 0.5</td>
      <td>0.938</td>
      <td>0.054</td>
      <td>0.008</td>
      <td>0.000</td>
      <td>Confident</td>
    </tr>
    <tr>
      <td>Ï„ = 1.0</td>
      <td>0.600</td>
      <td>0.246</td>
      <td>0.099</td>
      <td>0.055</td>
      <td>Balanced</td>
    </tr>
    <tr>
      <td>Ï„ = 1.5</td>
      <td>0.473</td>
      <td>0.264</td>
      <td>0.157</td>
      <td>0.106</td>
      <td>Creative</td>
    </tr>
    <tr>
      <td>Ï„ = 2.0</td>
      <td>0.398</td>
      <td>0.274</td>
      <td>0.190</td>
      <td>0.138</td>
      <td>Random</td>
    </tr>
  </tbody>
</table>

<h4 id="top-k-sampling-limiting-vocabulary">Top-k Sampling: Limiting Vocabulary</h4>

<p><strong>The idea:</strong> Sample from only the k most likely tokens, ignoring the long tail.</p>

<p><strong>Algorithm:</strong></p>
<ol>
  <li>Sort tokens by probability (descending)</li>
  <li>Keep only top k tokens</li>
  <li>Set all other probabilities to zero</li>
  <li>Renormalize and sample</li>
</ol>

<p><strong>Example (k=3):</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Original: P = [0.60, 0.25, 0.10, 0.05]
# Top-3:    P = [0.60, 0.25, 0.10, 0.00]
# Renorm:   P = [0.632, 0.263, 0.105, 0.0]
</span></code></pre></div></div>

<p><strong>Problem with top-k:</strong> Fixed k doesnâ€™t adapt to distribution shape. Sometimes top-3 captures 99% probability; sometimes itâ€™s only 50%.</p>

<h4 id="top-p-nucleus-sampling-adaptive-vocabulary">Top-p (Nucleus) Sampling: Adaptive Vocabulary</h4>

<p><strong>The idea:</strong> Keep the smallest set of tokens whose cumulative probability exceeds p.</p>

<p><strong>Algorithm:</strong></p>
<ol>
  <li>Sort tokens by probability (descending)</li>
  <li>Compute cumulative probabilities</li>
  <li>Find the <em>first</em> token where cumulative probability â‰¥ p</li>
  <li>Keep all tokens up to and including that token</li>
  <li>Renormalize and sample</li>
</ol>

<p><strong>Example (p=0.9):</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Probs:      [0.60, 0.25, 0.10, 0.05]
# Cumulative: [0.60, 0.85, 0.95, 1.00]
# Cutoff:                     ^
# Keep first 3 tokens (0.95 â‰¥ 0.9)
# Renormalized: [0.632, 0.263, 0.105, 0.0]
</span></code></pre></div></div>
<p><strong>Adaptive behavior:</strong></p>
<ul>
  <li>Peaked distribution (confident model): Few tokens kept</li>
  <li>Flat distribution (uncertain model): Many tokens kept</li>
</ul>

<h4 id="complete-generation-pipeline">Complete Generation Pipeline</h4>
<p>A reference implementation to illustrate the idea</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">TextGenerator</span><span class="p">:</span>
    <span class="s">"""Text generator using a trained Transformer language model.

    Attributes:
        model: Trained TransformerLM
        tokenizer: BPE tokenizer
        device: torch device (cuda/mps/cpu)
        config: Model configuration from checkpoint
    """</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">checkpoint_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">tokenizer_dir</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s">"tokenizer"</span><span class="p">,</span>
        <span class="n">device</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span>
    <span class="p">):</span>
        <span class="s">"""Initialize the text generator.

        Args:
            checkpoint_path: Path to model checkpoint (.pt file)
            tokenizer_dir: Directory containing vocab.pkl and merges.pkl
            device: Device to use ('cuda', 'mps', 'cpu', or None for auto-detect)
        """</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">device</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">_get_device</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Using device: </span><span class="si">{</span><span class="bp">self</span><span class="p">.</span><span class="n">device</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>

        <span class="c1"># Load checkpoint
</span>        <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Loading checkpoint from </span><span class="si">{</span><span class="n">checkpoint_path</span><span class="si">}</span><span class="s">..."</span><span class="p">)</span>
        <span class="n">checkpoint</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="n">checkpoint_path</span><span class="p">,</span> <span class="n">map_location</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">config</span> <span class="o">=</span> <span class="n">checkpoint</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="s">'config'</span><span class="p">,</span> <span class="p">{})</span>

        <span class="c1"># Load tokenizer
</span>        <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Loading tokenizer from </span><span class="si">{</span><span class="n">tokenizer_dir</span><span class="si">}</span><span class="s">..."</span><span class="p">)</span>
        <span class="n">vocab_path</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="n">tokenizer_dir</span><span class="p">)</span> <span class="o">/</span> <span class="s">"vocab.pkl"</span>
        <span class="n">merges_path</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="n">tokenizer_dir</span><span class="p">)</span> <span class="o">/</span> <span class="s">"merges.pkl"</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">vocab_path</span><span class="p">.</span><span class="n">exists</span><span class="p">()</span> <span class="ow">or</span> <span class="ow">not</span> <span class="n">merges_path</span><span class="p">.</span><span class="n">exists</span><span class="p">():</span>
            <span class="k">raise</span> <span class="nb">FileNotFoundError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s">"Tokenizer files not found in </span><span class="si">{</span><span class="n">tokenizer_dir</span><span class="si">}</span><span class="s">. "</span>
                <span class="sa">f</span><span class="s">"Expected vocab.pkl and merges.pkl"</span>
            <span class="p">)</span>

        <span class="n">vocab</span><span class="p">,</span> <span class="n">merges</span> <span class="o">=</span> <span class="n">load_tokenizer</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">vocab_path</span><span class="p">),</span> <span class="nb">str</span><span class="p">(</span><span class="n">merges_path</span><span class="p">))</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">Tokenizer</span><span class="p">(</span><span class="n">vocab</span><span class="p">,</span> <span class="n">merges</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Loaded tokenizer with vocab size: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">vocab</span><span class="p">)</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>

        <span class="c1"># Initialize model
</span>        <span class="k">print</span><span class="p">(</span><span class="s">"Initializing model..."</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">TransformerLM</span><span class="p">(</span>
            <span class="n">vocab_size</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">config</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="s">'vocab_size'</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">vocab</span><span class="p">)),</span>
            <span class="n">context_length</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">config</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="s">'context_length'</span><span class="p">,</span> <span class="mi">256</span><span class="p">),</span>
            <span class="n">d_model</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">config</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="s">'d_model'</span><span class="p">,</span> <span class="mi">512</span><span class="p">),</span>
            <span class="n">num_layers</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">config</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="s">'num_layers'</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span>
            <span class="n">num_heads</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">config</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="s">'num_heads'</span><span class="p">,</span> <span class="mi">16</span><span class="p">),</span>
            <span class="n">d_ff</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">config</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="s">'d_ff'</span><span class="p">,</span> <span class="mi">1344</span><span class="p">),</span>
            <span class="n">rope_theta</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">config</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="s">'rope_theta'</span><span class="p">,</span> <span class="mf">10000.0</span><span class="p">),</span>
            <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">device</span><span class="p">,</span>
        <span class="p">).</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>

        <span class="c1"># Load model weights
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">[</span><span class="s">'model_state_dict'</span><span class="p">])</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="nb">eval</span><span class="p">()</span>

        <span class="c1"># Print model info
</span>        <span class="n">total_params</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">p</span><span class="p">.</span><span class="n">numel</span><span class="p">()</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">parameters</span><span class="p">())</span>
        <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Model loaded successfully!"</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"  Total parameters: </span><span class="si">{</span><span class="n">total_params</span><span class="si">:</span><span class="p">,</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"  Context length: </span><span class="si">{</span><span class="bp">self</span><span class="p">.</span><span class="n">config</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="s">'context_length'</span><span class="p">,</span> <span class="mi">256</span><span class="p">)</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"  Model dimension: </span><span class="si">{</span><span class="bp">self</span><span class="p">.</span><span class="n">config</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="s">'d_model'</span><span class="p">,</span> <span class="mi">512</span><span class="p">)</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"  Layers: </span><span class="si">{</span><span class="bp">self</span><span class="p">.</span><span class="n">config</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="s">'num_layers'</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
        <span class="k">print</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">_get_device</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">device</span><span class="p">:</span>
        <span class="s">"""Get the device for inference."""</span>
        <span class="k">if</span> <span class="n">device</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="n">device</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

        <span class="c1"># Auto-detect
</span>        <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="n">is_available</span><span class="p">():</span>
            <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="n">device</span><span class="p">(</span><span class="s">'cuda'</span><span class="p">)</span>
        <span class="k">elif</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">backends</span><span class="p">,</span> <span class="s">'mps'</span><span class="p">)</span> <span class="ow">and</span> <span class="n">torch</span><span class="p">.</span><span class="n">backends</span><span class="p">.</span><span class="n">mps</span><span class="p">.</span><span class="n">is_available</span><span class="p">():</span>
            <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="n">device</span><span class="p">(</span><span class="s">'mps'</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="n">device</span><span class="p">(</span><span class="s">'cpu'</span><span class="p">)</span>

    <span class="o">@</span><span class="n">torch</span><span class="p">.</span><span class="n">no_grad</span><span class="p">()</span>
    <span class="k">def</span> <span class="nf">generate</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">prompt</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">max_tokens</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span>
        <span class="n">temperature</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span>
        <span class="n">top_k</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
        <span class="n">top_p</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
        <span class="n">stop_token</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
        <span class="s">"""Generate text from a prompt.

        Args:
            prompt: Input text to continue
            max_tokens: Maximum number of tokens to generate
            temperature: Sampling temperature (higher = more random)
                        Use 1.0 for standard sampling, 0.0 for greedy
            top_k: Keep only top k tokens with highest probability (None = no filtering)
            top_p: Keep tokens with cumulative probability &gt;= top_p (None = no filtering)
            stop_token: Stop generation if this token is generated

        Returns:
            Generated text (prompt + generated continuation)
        """</span>
        <span class="c1"># Encode prompt
</span>        <span class="n">input_ids</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">tokenizer</span><span class="p">.</span><span class="n">encode</span><span class="p">(</span><span class="n">prompt</span><span class="p">)</span>
        <span class="n">input_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">input_ids</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">long</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>

        <span class="c1"># Generate tokens
</span>        <span class="n">generated_ids</span> <span class="o">=</span> <span class="n">input_ids</span><span class="p">.</span><span class="n">copy</span><span class="p">()</span>

        <span class="k">for</span> <span class="n">step</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_tokens</span><span class="p">):</span>
            <span class="c1"># Get context window (last context_length tokens)
</span>            <span class="n">context_length</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">config</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="s">'context_length'</span><span class="p">,</span> <span class="mi">256</span><span class="p">)</span>
            <span class="n">context</span> <span class="o">=</span> <span class="n">input_tensor</span><span class="p">[:,</span> <span class="o">-</span><span class="n">context_length</span><span class="p">:]</span>

            <span class="c1"># Forward pass
</span>            <span class="k">try</span><span class="p">:</span>
                <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="n">no_grad</span><span class="p">():</span>
                    <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">(</span><span class="n">context</span><span class="p">,</span> <span class="n">apply_softmax</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
            <span class="k">except</span> <span class="nb">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
                <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"</span><span class="se">\n</span><span class="s">Error in forward pass at step </span><span class="si">{</span><span class="n">step</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
                <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Context shape: </span><span class="si">{</span><span class="n">context</span><span class="p">.</span><span class="n">shape</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
                <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Context device: </span><span class="si">{</span><span class="n">context</span><span class="p">.</span><span class="n">device</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
                <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Error: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
                <span class="k">raise</span>

            <span class="c1"># Handle different output shapes
</span>            <span class="k">if</span> <span class="n">logits</span><span class="p">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
                <span class="c1"># Model returned [batch_size, vocab_size] - already at last position
</span>                <span class="n">next_token_logits</span> <span class="o">=</span> <span class="n">logits</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="k">elif</span> <span class="n">logits</span><span class="p">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">3</span><span class="p">:</span>
                <span class="c1"># Model returned [batch_size, seq_len, vocab_size]
</span>                <span class="n">next_token_logits</span> <span class="o">=</span> <span class="n">logits</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="nb">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s">"Unexpected logits shape: </span><span class="si">{</span><span class="n">logits</span><span class="p">.</span><span class="n">shape</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>

            <span class="c1"># Apply temperature
</span>            <span class="k">if</span> <span class="n">temperature</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">next_token_logits</span> <span class="o">=</span> <span class="n">next_token_logits</span> <span class="o">/</span> <span class="n">temperature</span>

            <span class="c1"># Apply top-k filtering
</span>            <span class="k">if</span> <span class="n">top_k</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
                <span class="c1"># Get the k-th largest value
</span>                <span class="n">top_k_values</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">topk</span><span class="p">(</span><span class="n">next_token_logits</span><span class="p">,</span> <span class="nb">min</span><span class="p">(</span><span class="n">top_k</span><span class="p">,</span> <span class="n">next_token_logits</span><span class="p">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)))</span>
                <span class="n">kth_value</span> <span class="o">=</span> <span class="n">top_k_values</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
                <span class="c1"># Set all values below the k-th largest to -inf
</span>                <span class="n">indices_to_remove</span> <span class="o">=</span> <span class="n">next_token_logits</span> <span class="o">&lt;</span> <span class="n">kth_value</span>
                <span class="n">next_token_logits</span><span class="p">[</span><span class="n">indices_to_remove</span><span class="p">]</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="s">'-inf'</span><span class="p">)</span>

            <span class="c1"># Apply top-p (nucleus) filtering
</span>            <span class="k">if</span> <span class="n">top_p</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
                <span class="n">sorted_logits</span><span class="p">,</span> <span class="n">sorted_indices</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">sort</span><span class="p">(</span><span class="n">next_token_logits</span><span class="p">,</span> <span class="n">descending</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
                <span class="n">cumulative_probs</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">F</span><span class="p">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">sorted_logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

                <span class="c1"># Remove tokens with cumulative probability above the threshold
</span>                <span class="n">sorted_indices_to_remove</span> <span class="o">=</span> <span class="n">cumulative_probs</span> <span class="o">&gt;</span> <span class="n">top_p</span>
                <span class="c1"># Keep at least one token
</span>                <span class="n">sorted_indices_to_remove</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="bp">False</span>

                <span class="c1"># Map back to original indices
</span>                <span class="n">indices_to_remove</span> <span class="o">=</span> <span class="n">sorted_indices</span><span class="p">[</span><span class="n">sorted_indices_to_remove</span><span class="p">]</span>
                <span class="n">next_token_logits</span><span class="p">[</span><span class="n">indices_to_remove</span><span class="p">]</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="s">'-inf'</span><span class="p">)</span>

            <span class="c1"># Sample next token
</span>            <span class="k">if</span> <span class="n">temperature</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="c1"># Greedy decoding
</span>                <span class="n">next_token</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">next_token_logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
                <span class="n">next_token_id</span> <span class="o">=</span> <span class="n">next_token</span><span class="p">.</span><span class="n">item</span><span class="p">()</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># Sample from distribution
</span>                <span class="n">probs</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">next_token_logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
                <span class="n">next_token</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">multinomial</span><span class="p">(</span><span class="n">probs</span><span class="p">,</span> <span class="n">num_samples</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
                <span class="n">next_token_id</span> <span class="o">=</span> <span class="n">next_token</span><span class="p">.</span><span class="n">item</span><span class="p">()</span>

            <span class="c1"># Append to generated sequence
</span>            <span class="n">generated_ids</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">next_token_id</span><span class="p">)</span>

            <span class="c1"># Add new token to input tensor
</span>            <span class="c1"># Create a 2D tensor of shape [1, 1] to concatenate
</span>            <span class="n">new_token_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([[</span><span class="n">next_token_id</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">long</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>
            <span class="n">input_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">cat</span><span class="p">([</span><span class="n">input_tensor</span><span class="p">,</span> <span class="n">new_token_tensor</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

            <span class="c1"># Check for stop token
</span>            <span class="k">if</span> <span class="n">stop_token</span><span class="p">:</span>
                <span class="n">next_token_text</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">tokenizer</span><span class="p">.</span><span class="n">decode</span><span class="p">([</span><span class="n">next_token_id</span><span class="p">])</span>
                <span class="k">if</span> <span class="n">stop_token</span> <span class="ow">in</span> <span class="n">next_token_text</span><span class="p">:</span>
                    <span class="k">break</span>

        <span class="c1"># Decode generated text
</span>        <span class="n">generated_text</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">tokenizer</span><span class="p">.</span><span class="n">decode</span><span class="p">(</span><span class="n">generated_ids</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">generated_text</span>

    <span class="k">def</span> <span class="nf">generate_multiple</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">prompt</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">num_samples</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span>
        <span class="n">max_tokens</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span>
        <span class="n">temperature</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span>
        <span class="n">top_k</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
        <span class="n">top_p</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
        <span class="s">"""Generate multiple samples from a prompt.

        Args:
            prompt: Input text to continue
            num_samples: Number of samples to generate
            max_tokens: Maximum tokens per sample
            temperature: Sampling temperature
            top_k: Top-k filtering
            top_p: Top-p filtering

        Returns:
            List of generated texts
        """</span>
        <span class="n">samples</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_samples</span><span class="p">):</span>
            <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Generating sample </span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s">/</span><span class="si">{</span><span class="n">num_samples</span><span class="si">}</span><span class="s">..."</span><span class="p">)</span>
            <span class="n">sample</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">generate</span><span class="p">(</span>
                <span class="n">prompt</span><span class="o">=</span><span class="n">prompt</span><span class="p">,</span>
                <span class="n">max_tokens</span><span class="o">=</span><span class="n">max_tokens</span><span class="p">,</span>
                <span class="n">temperature</span><span class="o">=</span><span class="n">temperature</span><span class="p">,</span>
                <span class="n">top_k</span><span class="o">=</span><span class="n">top_k</span><span class="p">,</span>
                <span class="n">top_p</span><span class="o">=</span><span class="n">top_p</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">samples</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">sample</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">samples</span>
</code></pre></div></div>

<h4 id="recommended-settings-by-use-case">Recommended Settings by Use Case</h4>

<table>
  <thead>
    <tr>
      <th>Task</th>
      <th>Temperature</th>
      <th>Top-k</th>
      <th>Top-p</th>
      <th>Rationale</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Factual QA</strong></td>
      <td>0.1</td>
      <td>10</td>
      <td>None</td>
      <td>Deterministic, high confidence</td>
    </tr>
    <tr>
      <td><strong>Code completion</strong></td>
      <td>0.2</td>
      <td>20</td>
      <td>0.9</td>
      <td>Mostly deterministic, some creativity</td>
    </tr>
    <tr>
      <td><strong>Story writing</strong></td>
      <td>0.8</td>
      <td>None</td>
      <td>0.9</td>
      <td>Balanced creativity and coherence</td>
    </tr>
    <tr>
      <td><strong>Poetry</strong></td>
      <td>1.2</td>
      <td>None</td>
      <td>0.95</td>
      <td>High creativity, surprising word choices</td>
    </tr>
    <tr>
      <td><strong>Brainstorming</strong></td>
      <td>1.5</td>
      <td>None</td>
      <td>0.98</td>
      <td>Maximum diversity</td>
    </tr>
  </tbody>
</table>

<p><strong>Example usage:</strong></p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Deterministic completion</span>
uv run generate-text <span class="se">\</span>
  <span class="nt">--checkpoint</span> checkpoints/checkpoint_final.pt <span class="se">\</span>
  <span class="nt">--prompt</span> <span class="s2">"Once upon a time"</span> <span class="se">\</span>
  <span class="nt">--temperature</span> 0.1

<span class="c"># Creative story generation</span>
uv run generate-text <span class="se">\</span>
  <span class="nt">--checkpoint</span> checkpoints/checkpoint_final.pt <span class="se">\</span>
  <span class="nt">--prompt</span> <span class="s2">"Once upon a time"</span> <span class="se">\</span>
  <span class="nt">--temperature</span> 0.8 <span class="se">\</span>
  <span class="nt">--top-p</span> 0.9 <span class="se">\</span>
  <span class="nt">--max-tokens</span> 200
</code></pre></div></div>

<h4 id="example-generated-story-from-trained-model">Example: Generated Story from Trained Model</h4>

<p>Hereâ€™s an example of text generation using the fully trained model (20,000 iterations on TinyStories):</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>uv run generate-text <span class="se">\</span>
    <span class="nt">--checkpoint</span> checkpoints/checkpoint_final.pt <span class="se">\</span>
    <span class="nt">--prompt</span> <span class="s2">"The little girl found a magic"</span> <span class="se">\</span>
    <span class="nt">--stop-token</span> <span class="s2">"."</span> <span class="se">\</span>
    <span class="nt">--max-tokens</span> 200
</code></pre></div></div>

<p><strong>Output:</strong></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Using device: mps
Loading checkpoint from checkpoints/checkpoint_final.pt...
Loading tokenizer from tokenizer...
Loaded tokenizer with vocab size: 10000
Initializing model...
Model loaded successfully!
  Total parameters: 22,696,448
  Context length: 256
  Model dimension: 512
  Layers: 4

================================================================================
GENERATING TEXT
================================================================================
Prompt: The little girl found a magic
Max tokens: 200
Temperature: 1.0
================================================================================

Generated text:
--------------------------------------------------------------------------------
The little girl found a magical stone, she had to pay the frog laying for the
rabbit's young wisdom, so the frog was never seen again.
--------------------------------------------------------------------------------
</code></pre></div></div>

<p><strong>Analysis:</strong></p>
<ul>
  <li>âœ“ <strong>Grammatically coherent</strong>: Subject-verb agreement, proper sentence structure</li>
  <li>âœ“ <strong>Narrative elements</strong>: Characters (girl, frog, rabbit), magical object (stone), consequence (frog disappears)</li>
  <li>âœ“ <strong>Logical flow</strong>: The story has a clear cause-and-effect structure</li>
  <li>âš  <strong>Semantic quirks</strong>: â€œfrog laying for the rabbitâ€™s young wisdomâ€ shows the model is creative but occasionally produces unexpected phrases</li>
</ul>

<p>This demonstrates that the 17M parameter model successfully learned story generation patterns from TinyStories, producing coherent short narratives despite its relatively small size.</p>

<hr />

<h3 id="training-analysis">Training Analysis: Scaling Laws in Action</h3>

<p>One of the most valuable aspects of this implementation is the comprehensive training analysis, which reveals how model scale, dataset size, and training time affect final performance.</p>

<h4 id="overview-three-configurations-tested">Overview: Three Configurations Tested</h4>

<table>
  <thead>
    <tr>
      <th>Configuration</th>
      <th>Iterations</th>
      <th>Vocab Size</th>
      <th>Dataset Size</th>
      <th>Context Length</th>
      <th>Model Size</th>
      <th>Training Time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong><a href="https://github.com/bearbearyu1223/tinystories-transformer/blob/main/config_quicktest.json">config_quicktest.json</a></strong></td>
      <td>10</td>
      <td>2,000</td>
      <td>400K tokens</td>
      <td>64</td>
      <td>~0.94M params</td>
      <td>~0.6s</td>
    </tr>
    <tr>
      <td><strong><a href="https://github.com/bearbearyu1223/tinystories-transformer/blob/main/config_test.json">config_test.json</a></strong></td>
      <td>1,000</td>
      <td>5,000</td>
      <td>1.8M tokens</td>
      <td>128</td>
      <td>~4.1M params</td>
      <td>~2.5min</td>
    </tr>
    <tr>
      <td><strong><a href="https://github.com/bearbearyu1223/tinystories-transformer/blob/main/config_tinystories.json">config_tinystories.json</a></strong></td>
      <td>20,000</td>
      <td>10,000</td>
      <td>Full dataset</td>
      <td>256</td>
      <td>~17M params</td>
      <td>~7 hours</td>
    </tr>
  </tbody>
</table>

<h4 id="training-progress-comparison">Training Progress Comparison</h4>

<p>The training comparison chart reveals three distinct learning curves with fundamentally different characteristics:</p>

<p><img src="/assets/picture/2025-11-16-cs336-the-complete-experiment-for-tinystories-transformer/training_comparison.png" alt="training comparison" width="80%" /></p>

<p><strong>Chart Explain:</strong></p>
<ul>
  <li><strong>Top Left</strong>: Training loss across all configurations (each config has its own color)</li>
  <li><strong>Top Right</strong>: Validation loss across all configurations (each config has its own color)</li>
  <li><strong>Bottom Left</strong>: Perplexity over time (log y-scale) with final values annotated</li>
  <li><strong>Bottom Right</strong>: Final loss comparison bar chart showing train/val side-by-side</li>
</ul>

<h4 id="configuration-1-quicktest-sanity-check-1">Configuration 1: Quicktest (Sanity Check)</h4>

<p><strong>Purpose:</strong> Ultra-fast sanity check for code validation</p>

<p><img src="/assets/picture/2025-11-16-cs336-the-complete-experiment-for-tinystories-transformer/quicktest_training_progress.png" alt="quick test progress" width="80%" /></p>

<p><strong>Configuration Details:</strong></p>
<ul>
  <li>Iterations: 10 (&lt; 1 minute on M4 MacBook Pro)</li>
  <li>Dataset: 400,242 training tokens, 39,316 validation tokens</li>
  <li>Model: 938,624 parameters (~3.58 MB)</li>
  <li>Vocabulary: 2,000 BPE tokens</li>
</ul>

<p><strong>Training Metrics:</strong></p>

<table>
  <thead>
    <tr>
      <th>Metric</th>
      <th>Initial</th>
      <th>Final</th>
      <th>Change</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Training Loss</td>
      <td>7.66</td>
      <td>7.55</td>
      <td>-0.11 (-1.4%)</td>
    </tr>
    <tr>
      <td>Validation Loss</td>
      <td>7.65</td>
      <td>7.55</td>
      <td>-0.10 (-1.3%)</td>
    </tr>
    <tr>
      <td>Perplexity</td>
      <td>2108.18</td>
      <td>1891.42</td>
      <td>-216.76 (-10.3%)</td>
    </tr>
  </tbody>
</table>

<p><strong>Strengths:</strong></p>
<ul>
  <li>âœ“ Extremely fast turnaround for development iteration</li>
  <li>âœ“ Stable training with no divergence</li>
  <li>âœ“ Minimal overfitting (train/val losses nearly identical)</li>
</ul>

<p><strong>Limitations:</strong></p>
<ul>
  <li>Limited learning in just 10 iterations</li>
  <li>Small vocabulary restricts expressiveness</li>
  <li>Useful primarily for code validation, not actual model quality</li>
</ul>

<p><strong>Use Cases:</strong></p>
<ul>
  <li>Code debugging and testing</li>
  <li>CI/CD pipeline validation</li>
  <li>Quick sanity checks before longer training runs</li>
</ul>

<h4 id="configuration-2-test-active-development-1">Configuration 2: Test (Active Development)</h4>

<p><strong>Purpose:</strong> Development and feature validation</p>

<p><img src="/assets/picture/2025-11-16-cs336-the-complete-experiment-for-tinystories-transformer/test_training_progress.png" alt="test progress" width="80%" /></p>

<p><strong>Configuration Details:</strong></p>
<ul>
  <li>Iterations: 1,000 (~20 minutes on M4 MacBook Pro)</li>
  <li>Dataset: 1,812,095 training tokens, 179,622 validation tokens</li>
  <li>Model: 4,117,760 parameters (~15.71 MB)</li>
  <li>Vocabulary: 5,000 BPE tokens</li>
</ul>

<p><strong>Training Metrics:</strong></p>

<table>
  <thead>
    <tr>
      <th>Metric</th>
      <th>Initial</th>
      <th>Iter 500</th>
      <th>Final</th>
      <th>Total Change</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Training Loss</td>
      <td>8.58</td>
      <td>3.33</td>
      <td>3.11</td>
      <td>-5.47 (-63.8%)</td>
    </tr>
    <tr>
      <td>Validation Loss</td>
      <td>8.58</td>
      <td>3.36</td>
      <td>3.15</td>
      <td>-5.43 (-63.3%)</td>
    </tr>
    <tr>
      <td>Perplexity</td>
      <td>5308.63</td>
      <td>28.88</td>
      <td>23.41</td>
      <td>-5285.22 (-99.6%)</td>
    </tr>
  </tbody>
</table>

<p><strong>Strengths:</strong></p>
<ul>
  <li>âœ“ Significant loss reduction (&gt;60%) in reasonable time</li>
  <li>âœ“ Excellent train/val agreement (minimal overfitting)</li>
  <li>âœ“ Perplexity drops to practical levels</li>
  <li>âœ“ Fast enough for iterative development</li>
</ul>

<p><strong>Training Dynamics:</strong></p>
<ul>
  <li><strong>Phase 1 (0-300)</strong>: Rapid initial learning, loss drops from 8.6 to ~3.7</li>
  <li><strong>Phase 2 (300-700)</strong>: Steady optimization, perplexity stabilizes</li>
  <li><strong>Phase 3 (700-1000)</strong>: Fine-tuning, diminishing returns</li>
</ul>

<p><strong>Performance:</strong></p>
<ul>
  <li>Throughput: ~22,000-39,000 tokens/second</li>
  <li>Memory efficient: 15.71 MB model size</li>
  <li>No gradient explosion or training instability</li>
</ul>

<p><strong>Use Cases:</strong></p>
<ul>
  <li>Feature development and testing</li>
  <li>Hyperparameter tuning experiments</li>
  <li>Ablation studies</li>
  <li>Pre-production validation</li>
</ul>

<h4 id="configuration-3-train-on-full-tinystories-dataset-production-training-experiments">Configuration 3: Train on Full TinyStories Dataset (Production Training Experiments)</h4>

<p><strong>Purpose:</strong> Full production training for best model quality</p>

<p><img src="/assets/picture/2025-11-16-cs336-the-complete-experiment-for-tinystories-transformer/tinystories_training_progress.png" alt="full training progress" width="80%" /></p>

<p><strong>Configuration Details:</strong></p>
<ul>
  <li>Iterations: 20,000 (~7 hours on M4 MacBook Pro)</li>
  <li>Dataset: Full TinyStories corpus (2.1GB training, 21MB validation)</li>
  <li>Model: ~17M parameters</li>
  <li>Vocabulary: 10,000 BPE tokens</li>
</ul>

<p><strong>Training Metrics (First 6000 iterations shown):</strong></p>

<table>
  <thead>
    <tr>
      <th>Metric</th>
      <th>Initial</th>
      <th>Iter 1000</th>
      <th>Iter 3000</th>
      <th>Iter 6000</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Training Loss</td>
      <td>9.25</td>
      <td>2.50</td>
      <td>1.82</td>
      <td>1.61</td>
    </tr>
    <tr>
      <td>Validation Loss</td>
      <td>9.25</td>
      <td>2.50</td>
      <td>1.81</td>
      <td>1.61</td>
    </tr>
    <tr>
      <td>Perplexity</td>
      <td>~10,500</td>
      <td>~12.2</td>
      <td>~6.2</td>
      <td>~5.0</td>
    </tr>
  </tbody>
</table>

<p><strong>Strengths:</strong></p>
<ul>
  <li>âœ“ Massive loss reduction (&gt;85% by iteration 6000)</li>
  <li>âœ“ Perfect train/val alignment (no overfitting)</li>
  <li>âœ“ Continued improvement through 20K iterations</li>
  <li>âœ“ Production-quality perplexity values</li>
</ul>

<p><strong>Training Dynamics:</strong></p>
<ul>
  <li><strong>Warmup (0-1000)</strong>: Learning rate warmup, rapid initial gains</li>
  <li><strong>Main Training (1000-6000+)</strong>: Steady, consistent improvement</li>
  <li><strong>Learning Rate Schedule</strong>: Cosine decay maintains stability</li>
</ul>

<p><strong>Long-Term Learning:</strong>
The model shows no signs of plateauing even at 6000 iterations, suggesting:</p>
<ul>
  <li>More capacity to learn from the dataset</li>
  <li>Effective regularization preventing overfitting</li>
  <li>Well-tuned learning rate schedule</li>
</ul>

<p><strong>Performance:</strong></p>
<ul>
  <li>Throughput: ~3,400-8,500 tokens/second (varies with evaluation)</li>
  <li>Stable memory usage throughout training</li>
  <li>Checkpoints saved every 2000 iterations for resumability</li>
</ul>

<p><strong>Use Cases:</strong></p>
<ul>
  <li>Production deployment</li>
  <li>Final model evaluation</li>
  <li>Publishing and sharing</li>
  <li>Research baselines</li>
</ul>

<h4 id="cross-configuration-insights">Cross-Configuration Insights</h4>

<p>The three configurations demonstrate clear scaling relationships:</p>

<table>
  <thead>
    <tr>
      <th>Config</th>
      <th>Params</th>
      <th>Dataset</th>
      <th>Final Loss</th>
      <th>Perplexity</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Quicktest</td>
      <td>0.94M</td>
      <td>400K</td>
      <td>7.55</td>
      <td>1891</td>
    </tr>
    <tr>
      <td>Test</td>
      <td>4.1M</td>
      <td>1.8M</td>
      <td>3.15</td>
      <td>23.4</td>
    </tr>
    <tr>
      <td>TinyStories</td>
      <td>17M</td>
      <td>627M</td>
      <td>1.61</td>
      <td>5.0</td>
    </tr>
  </tbody>
</table>

<p><strong>Key Finding</strong>: Each 4Ã— increase in model size together with larger dataset yields ~50% loss reduction and ~80% perplexity improvement. This is <strong>consistent with neural scaling laws</strong> from Kaplan et al. (2020), providing empirical validation on the TinyStories dataset.</p>

<hr />

<h3 id="production-considerations">Production Considerations</h3>

<p>The complete implementation, including all design considerations and production-ready features described in this blog post, is available as an open-source project on GitHub:</p>

<p><strong>Repository:</strong> <a href="https://github.com/bearbearyu1223/tinystories-transformer">github.com/bearbearyu1223/tinystories-transformer</a></p>

<p><strong>License:</strong> MIT (free for commercial and research use)</p>

<p>This repository provides a comprehensive, production-ready Transformer implementation with the following characteristics:</p>

<h4 id="repository-structure-and-design-philosophy">Repository Structure and Design Philosophy</h4>

<p>The codebase is organized to separate concerns and enable rapid iteration:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tinystories-transformer/
â”œâ”€â”€ cs336_basics/              # Core implementation modules
â”‚   â”œâ”€â”€ transformer_lm.py      # Main Transformer language model
â”‚   â”œâ”€â”€ multihead_attention.py # Attention with RoPE
â”‚   â”œâ”€â”€ bpe.py                 # Parallel BPE tokenizer training
â”‚   â”œâ”€â”€ rmsnorm.py             # RMS normalization
â”‚   â””â”€â”€ swiglu.py              # SwiGLU activation
â”œâ”€â”€ train_transformer.py       # Training script with full pipeline
â”œâ”€â”€ generate_text.py           # Text generation with sampling strategies
â”œâ”€â”€ setup_data.py              # Automated dataset download/setup
â”œâ”€â”€ visualize_training.py      # Training visualization generator
â”œâ”€â”€ config_quicktest.json      # Ultra-fast validation config
â”œâ”€â”€ config_test.json           # Development config
â””â”€â”€ config_tinystories.json    # Production training config
</code></pre></div></div>

<p><strong>Design principles:</strong></p>
<ul>
  <li><strong>Modularity:</strong> Each component (attention, normalization, activation) is a separate, testable module</li>
  <li><strong>Configurability:</strong> All hyperparameters exposed via JSON configs</li>
  <li><strong>Automation:</strong> One-command setup for datasets, training, generation, visualization</li>
  <li><strong>Documentation:</strong> Comprehensive guides (README, TRAINING_ANALYSIS, GENERATION_GUIDE)</li>
</ul>

<h4 id="key-implementation-features">Key Implementation Features</h4>

<p><strong>1. Automated Dataset Setup</strong></p>

<p>The repository includes <code class="language-plaintext highlighter-rouge">setup_data.py</code> to eliminate manual data preparation:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Single command downloads and prepares all datasets</span>
uv run setup-data
</code></pre></div></div>

<p>This automatically:</p>
<ul>
  <li>Downloads 2.1GB TinyStories dataset from HuggingFace</li>
  <li>Creates three data directories (full, quicktest, test)</li>
  <li>Validates data integrity</li>
  <li>Provides progress reporting</li>
</ul>

<p><strong>2. Three-Tiered Configuration System</strong></p>

<p>Production-tested configurations for different use cases:</p>
<ul>
  <li><code class="language-plaintext highlighter-rouge">config_quicktest.json</code>: &lt; 1 minute validation</li>
  <li><code class="language-plaintext highlighter-rouge">config_test.json</code>: ~20 minute development</li>
  <li><code class="language-plaintext highlighter-rouge">config_tinystories.json</code>: ~7 hour production training</li>
</ul>

<p>Each configuration is battle-tested with documented training curves and performance characteristics (see TRAINING_ANALYSIS.md in the repository).</p>

<p><strong>3. Modern Dependency Management</strong></p>

<p>Uses <code class="language-plaintext highlighter-rouge">uv</code> for fast, reproducible Python environments:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Install dependencies and CLI commands</span>
uv <span class="nb">sync</span>

<span class="c"># All commands immediately available</span>
uv run train-transformer config_test.json
uv run generate-text <span class="nt">--checkpoint</span> checkpoints/checkpoint_final.pt
uv run visualize-training
</code></pre></div></div>

<h4 id="getting-started-with-the-repository">Getting Started with the Repository</h4>

<p><strong>Quick Start (5 minutes):</strong></p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 1. Clone repository</span>
git clone https://github.com/bearbearyu1223/tinystories-transformer.git
<span class="nb">cd </span>tinystories-transformer

<span class="c"># 2. Set up environment</span>
uv venv <span class="o">&amp;&amp;</span> uv <span class="nb">sync</span>

<span class="c"># 3. Download datasets</span>
uv run setup-data

<span class="c"># 4. Run quick validation (&lt; 1 min)</span>
uv run train-transformer config_quicktest.json

<span class="c"># 5. Generate text</span>
uv run generate-text <span class="se">\</span>
  <span class="nt">--checkpoint</span> checkpoints_quicktest/checkpoint_final.pt <span class="se">\</span>
  <span class="nt">--prompt</span> <span class="s2">"Once upon a time"</span>
</code></pre></div></div>

<p><strong>Development Workflow:</strong></p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 1. Develop features with test config (20 min)</span>
uv run train-transformer config_test.json

<span class="c"># 2. Visualize training progress</span>
uv run visualize-training

<span class="c"># 3. Test generation quality</span>
uv run generate-text <span class="se">\</span>
  <span class="nt">--checkpoint</span> checkpoints_test/checkpoint_final.pt <span class="se">\</span>
  <span class="nt">--prompt</span> <span class="s2">"Once upon a time"</span> <span class="se">\</span>
  <span class="nt">--temperature</span> 0.8 <span class="se">\</span>
  <span class="nt">--top-p</span> 0.9 <span class="se">\</span>
  <span class="nt">--max-tokens</span> 200

<span class="c"># 4. When satisfied, run production training (7 hours)</span>
uv run train-transformer config_tinystories.json
</code></pre></div></div>
<p><strong>Explore the full implementation:</strong> <a href="https://github.com/bearbearyu1223/tinystories-transformer">github.com/bearbearyu1223/tinystories-transformer</a></p>

<hr />

<h3 id="takeaways">Key Takeaways</h3>

<p>Building a production language model from scratch reveals lessons that go beyond any single paper or tutorial. Here are the essential insights:</p>

<h4 id="1-modern-architecture-matters">1. <strong>Modern Architecture Matters</strong></h4>

<p>The original Transformer (2017) has evolved significantly:</p>
<ul>
  <li><strong>RoPE</strong> replaces learned position embeddings â†’ Better length generalization</li>
  <li><strong>RMSNorm</strong> replaces LayerNorm â†’ 10-30% faster, same performance</li>
  <li><strong>SwiGLU</strong> replaces ReLU â†’ 1-2% better results</li>
</ul>

<p>These arenâ€™t just incremental improvementsâ€”theyâ€™re now standard in production systems (LLaMA, GPT-NeoX, PaLM).</p>

<h4 id="2-tokenization-is-critical">2. <strong>Tokenization Is Critical</strong></h4>

<p>BPE with 10K vocabulary is a sweet spot:</p>
<ul>
  <li>Large enough to capture common words as single tokens</li>
  <li>Small enough for fast softmax and embedding lookup</li>
  <li>Good out-of-vocabulary handling via subwords</li>
</ul>

<p><strong>Anti-pattern:</strong> Using character-level (too long sequences) or word-level (too many OOV).</p>

<h4 id="3-graduated-configurations-enable-rapid-iteration">3. <strong>Graduated Configurations Enable Rapid Iteration</strong></h4>

<p>The three-tiered config system saves weeks of development time:</p>
<ul>
  <li><strong>Quicktest:</strong> Validate correctness in seconds</li>
  <li><strong>Test:</strong> Tune hyperparameters in minutes</li>
  <li><strong>Production:</strong> Train final model overnight</li>
</ul>

<h4 id="4-memory-mapping-scales-to-arbitrary-dataset-sizes">4. <strong>Memory Mapping Scales to Arbitrary Dataset Sizes</strong></h4>

<p>Memory-mapped arrays let you train on TB-scale datasets with GB-scale RAM:</p>
<ul>
  <li>Constant memory usage regardless of dataset size</li>
  <li>OS handles caching automatically</li>
  <li>Near-RAM performance for sequential access</li>
</ul>

<p><strong>Critical for:</strong> Training on Common Crawl, Books, Wikipedia combined (600GB+).</p>

<h4 id="5-generation-strategy-matters-as-much-as-architecture">5. <strong>Generation Strategy Matters As Much As Architecture</strong></h4>

<p>Even a perfectly trained model produces garbage with bad decoding:</p>
<ul>
  <li><strong>Temperature</strong> controls creativity vs. coherence</li>
  <li><strong>Top-p</strong> prevents sampling from the long tail</li>
  <li><strong>Different tasks need different settings</strong></li>
</ul>

<p><strong>Recommended baseline:</strong> <code class="language-plaintext highlighter-rouge">temperature=0.8, top_p=0.9</code></p>

<h4 id="6-comprehensive-logging-reveals-training-dynamics">6. <strong>Comprehensive Logging Reveals Training Dynamics</strong></h4>

<p>Timestamp-based logging with periodic evaluation shows:</p>
<ul>
  <li>When learning plateaus (time to stop)</li>
  <li>When overfitting starts (train/val divergence)</li>
  <li>Whether learning rate schedule is appropriate</li>
</ul>

<p><strong>Anti-pattern:</strong> Training blindly to max_iters without monitoring metrics.</p>

<h4 id="7-checkpoints-must-include-everything">7. <strong>Checkpoints Must Include Everything</strong></h4>

<p>A complete checkpoint includes:</p>
<ul>
  <li>Model parameters (obviously)</li>
  <li><strong>Optimizer state</strong> (momentum buffersâ€”critical for resumption)</li>
  <li><strong>Iteration count</strong> (for exact resumption)</li>
  <li><strong>Model config</strong> (for loading during inference)</li>
</ul>

<p><strong>Learned the hard way:</strong> Losing optimizer state means restarting training from scratch.</p>

<h4 id="8-validation-before-long-runs">8. <strong>Validation Before Long Runs</strong></h4>

<p>Always run a 100-iteration validation before launching multi-day training:</p>
<ul>
  <li>Verify loss decreases</li>
  <li>Check GPU memory usage</li>
  <li>Validate data loading</li>
  <li>Test checkpoint save/load</li>
</ul>

<p><strong>10 minutes of validation can save days of wasted compute.</strong></p>

<h4 id="9-scaling-laws-are-predictive">9. <strong>Scaling Laws Are Predictive</strong></h4>

<p>Our results confirm neural scaling laws:</p>
<ul>
  <li>4Ã— model size together with larger dataset â†’ ~50% loss reduction</li>
</ul>

<h4 id="10-production-code-needs-different-discipline">10. <strong>Production Code Needs Different Discipline</strong></h4>

<p>Research code gets away with:</p>
<ul>
  <li>Hardcoded hyperparameters</li>
  <li>No error handling</li>
  <li>Single-file scripts</li>
</ul>

<p>Production code requires:</p>
<ul>
  <li>Configuration management (JSON configs)</li>
  <li>Robust error handling (data validation)</li>
  <li>Automated setup (setup_data.py)</li>
  <li>Comprehensive documentation</li>
  <li>Reproducibility (locked dependencies)</li>
</ul>

<hr />

<h2 id="conclusion">Conclusion</h2>

<p>This note demonstrates that building production language models from scratch is achievable with the right architecture, training infrastructure, and engineering discipline. The complete systemâ€”from BPE tokenization to text generationâ€”shows how modern research ideas (RoPE, RMSNorm, SwiGLU) translate into working code, and how practical engineering (graduated configs, memory mapping, robust checkpointing) makes the difference between a research prototype and a production system.</p>

<p><strong>Next steps to Explore</strong></p>

<ol>
  <li><strong>Experiment with architecture:</strong> Try different layer counts, head counts, d_ff ratios</li>
  <li><strong>Tune generation:</strong> Find optimal temperature/top-p for different use case</li>
  <li><strong>Scale up:</strong> Apply these patterns to larger models (100M+ parameters)</li>
  <li><strong>Add features:</strong> Implement gradient checkpointing, distributed training, flash attention</li>
</ol>

<hr />

<p><em>Implementation details, training logs, and visualizations available in the repository. Questions and contributions welcome!</em></p>]]></content><author><name>[&quot;Han Yu&quot;]</name></author><category term="cs336" /><summary type="html"><![CDATA[End-to-End Transformer Training on TinyStories This note walks through the complete, end-to-end process of building a Transformer language model from scratch and training it on the TinyStories dataset. It covers every major componentâ€”from byte-pair encoding tokenization and multi-head attention with rotary embeddings to training-loop design and advanced text-generation strategies.]]></summary></entry><entry><title type="html">Study Notes: Stanford CS336 Language Modeling from Scratch [10]</title><link href="http://localhost:4000/cs336/2025/11/02/cs336-building-a-complete-training-loop.html" rel="alternate" type="text/html" title="Study Notes: Stanford CS336 Language Modeling from Scratch [10]" /><published>2025-11-02T00:00:00-07:00</published><updated>2025-11-02T00:00:00-07:00</updated><id>http://localhost:4000/cs336/2025/11/02/cs336-building-a-complete-training-loop</id><content type="html" xml:base="http://localhost:4000/cs336/2025/11/02/cs336-building-a-complete-training-loop.html"><![CDATA[<h2 id="building-a-complete-training-loop">Building a Complete Training Loop</h2>

<p>This note documents the journey of assembling all the core components such as optimizer, learning rate scheduling, data loading, checkpointing, and decoding - into a complete training pipeline for Transformer language models. Weâ€™ll explore how each piece fits together, the design decisions behind them, and the practical considerations that make the difference between research code and production systems.</p>

<h3 id="table-of-contents">Table of Contents</h3>
<ol>
  <li><a href="#introduction">Introduction: The Big Picture</a></li>
  <li><a href="#adamw-optimizer">The AdamW Optimizer: Decoupled Weight Decay Regularization</a></li>
  <li><a href="#lr-scheduling">Learning Rate Scheduling: The LLaMA Approach</a></li>
  <li><a href="#data-loading">Memory-Efficient Data Loading</a></li>
  <li><a href="#checkpointing">Checkpoint Management</a></li>
  <li><a href="#decoding">Decoding Strategies: From Model to Text</a></li>
  <li><a href="#training-script">Putting It All Together: The Training Script</a></li>
  <li><a href="#testing">Testing and Validation</a></li>
  <li><a href="#takeaways">Key Takeaways</a></li>
</ol>

<hr />

<h3 id="introduction">Introduction: The Big Picture</h3>

<p>Training a large language model isnâ€™t just about implementing a forward pass and calling <code class="language-plaintext highlighter-rouge">loss.backward()</code>. A production training pipeline requires careful orchestration of multiple components, each with its own subtleties and potential pitfalls. In this note, weâ€™ll go through how to build a complete training pipeline from scratch, learning why each component matters and how they interact.</p>

<p><strong>What weâ€™ll build:</strong></p>
<ul>
  <li>An implementation of the AdamW optimizer</li>
  <li>A cosine learning rate schedule with warmup, as used in LLaMA</li>
  <li>Memory-mapped data loading to manage loading datasets larger than RAM</li>
  <li>Robust checkpoint saving/loading for long training runs</li>
  <li>Multiple decoding strategies (temperature scaling, top-p sampling)</li>
  <li>A complete sample training script that ties everything together</li>
</ul>

<hr />

<h3 id="adamw-optimizer">The AdamW Optimizer: Decoupled Weight Decay Regularization</h3>

<p>The first step in building our training loop is implementing the optimizer. While PyTorch provides <code class="language-plaintext highlighter-rouge">torch.optim.AdamW</code>, understanding the exact algorithm is crucial for debugging training issues and understanding why certain hyperparameters matter.</p>

<h4 id="the-algorithm">The Algorithm</h4>

<p>The AdamW algorithm (from â€œDecoupled Weight Decay Regularizationâ€ by Loshchilov &amp; Hutter, 2019) differs from standard Adam in how it applies weight decay. Hereâ€™s Algorithm 2 from the paper:</p>

<p><strong>Initialize:</strong></p>
<ul>
  <li>Learnable parameters: $\theta$</li>
  <li>First moment vector: $m \leftarrow 0$ (same shape as $\theta$)</li>
  <li>Second moment vector: $v \leftarrow 0$ (same shape as $\theta$)</li>
</ul>

<p><strong>For</strong> $t = 1, 2, \ldots, T$:</p>

<ol>
  <li>
    <p>Sample batch of data $B_t$</p>
  </li>
  <li>
    <p>Compute gradient:</p>

\[g \leftarrow \nabla_\theta \ell(\theta; B_t)\]
  </li>
  <li>
    <p>Update biased first moment estimate:</p>

\[m \leftarrow \beta_1 m + (1 - \beta_1) g\]
  </li>
  <li>
    <p>Update biased second raw moment estimate:</p>

\[v \leftarrow \beta_2 v + (1 - \beta_2) g^2\]
  </li>
  <li>
    <p>Compute bias-corrected learning rate:</p>

\[\alpha_t \leftarrow \alpha \cdot \frac{\sqrt{1 - \beta_2^t}}{1 - \beta_1^t}\]
  </li>
  <li>
    <p>Update parameters with adaptive learning rate:</p>

\[\theta \leftarrow \theta - \alpha_t \frac{m}{\sqrt{v + \varepsilon}}\]
  </li>
  <li>
    <p>Apply decoupled weight decay:</p>

\[\theta \leftarrow \theta - \alpha \lambda \theta\]
  </li>
</ol>

<h4 id="why-decoupled-weight-decay-matters">Why Decoupled Weight Decay Matters</h4>

<p>The key innovation in AdamW is <strong>decoupling weight decay from the gradient-based update</strong>. To understand why this matters, letâ€™s compare the two approaches:</p>

<p><strong>Standard Adam with L2 Regularization:</strong></p>

<p>In traditional Adam with L2 regularization, we add the weight decay term to the gradient before computing adaptive moments:</p>

\[g \leftarrow \nabla_\theta \ell(\theta; B_t) + \lambda \theta\]

<p>Then we proceed with the normal Adam update using this modified gradient. This means:</p>
<ul>
  <li>Weight decay affects the adaptive moment estimates ($m$ and $v$)</li>
  <li>The effective weight decay depends on the adaptive learning rate</li>
  <li>Parameters with large gradients get less regularization (due to adaptive scaling)</li>
</ul>

<p><strong>AdamW with Decoupled Weight Decay:</strong></p>

<p>In AdamW, we apply weight decay <strong>after</strong> the adaptive update as a separate step:</p>

\[\theta \leftarrow \theta - \alpha_t \frac{m}{\sqrt{v + \varepsilon}} - \alpha \lambda \theta\]

<p>This decoupling means:</p>
<ul>
  <li>Weight decay is independent of gradient statistics</li>
  <li>All parameters receive consistent regularization proportional to their magnitude</li>
  <li>Weight decay directly shrinks parameters toward zero, regardless of gradient history</li>
</ul>

<p><strong>Why This Improves Performance:</strong></p>

<ol>
  <li>
    <p><strong>Better generalization</strong>: Decoupled weight decay provides more consistent regularization across all parameters, leading to better generalization on downstream tasks.</p>
  </li>
  <li>
    <p><strong>Works with large learning rates</strong>: In standard Adam + L2, increasing the learning rate also increases the effective regularization, creating unwanted coupling. AdamW removes this coupling.</p>
  </li>
  <li>
    <p><strong>More interpretable</strong>: The weight decay hyperparameter $\lambda$ directly controls regularization strength, making it easier to tune.</p>
  </li>
</ol>

<p><strong>Practical Impact:</strong></p>

<p>For large language models, this difference is crucial. The original BERT used Adam with L2 regularization and achieved 84.4% on MNLI. Simply switching to AdamW with the same hyperparameters improved accuracy to 84.8% - a significant gain from this single algorithmic change. Similar improvements have been observed across many other deep learning tasks.</p>

<h4 id="complete-implementation">Complete Implementation</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">AdamW</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="n">Optimizer</span><span class="p">):</span>
    <span class="s">"""
    Implements AdamW optimizer following Algorithm 1 from
    "Decoupled Weight Decay Regularization" (Loshchilov &amp; Hutter, 2019).
    """</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span> <span class="n">betas</span><span class="o">=</span><span class="p">(</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.999</span><span class="p">),</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-8</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="mf">0.0</span><span class="p">):</span>
        <span class="n">defaults</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">,</span> <span class="n">betas</span><span class="o">=</span><span class="n">betas</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="n">eps</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="n">weight_decay</span><span class="p">)</span>
        <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">defaults</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">group</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">param_groups</span><span class="p">:</span>
            <span class="n">beta1</span><span class="p">,</span> <span class="n">beta2</span> <span class="o">=</span> <span class="n">group</span><span class="p">[</span><span class="s">'betas'</span><span class="p">]</span>
            <span class="n">lr</span> <span class="o">=</span> <span class="n">group</span><span class="p">[</span><span class="s">'lr'</span><span class="p">]</span>
            <span class="n">weight_decay</span> <span class="o">=</span> <span class="n">group</span><span class="p">[</span><span class="s">'weight_decay'</span><span class="p">]</span>
            <span class="n">eps</span> <span class="o">=</span> <span class="n">group</span><span class="p">[</span><span class="s">'eps'</span><span class="p">]</span>

            <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">group</span><span class="p">[</span><span class="s">'params'</span><span class="p">]:</span>
                <span class="k">if</span> <span class="n">p</span><span class="p">.</span><span class="n">grad</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
                    <span class="k">continue</span>

                <span class="n">grad</span> <span class="o">=</span> <span class="n">p</span><span class="p">.</span><span class="n">grad</span><span class="p">.</span><span class="n">data</span>

                <span class="n">state</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">state</span><span class="p">[</span><span class="n">p</span><span class="p">]</span>

                <span class="c1"># Initialize state on first step
</span>                <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">state</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="n">state</span><span class="p">[</span><span class="s">'step'</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
                    <span class="n">state</span><span class="p">[</span><span class="s">'exp_avg'</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">p</span><span class="p">.</span><span class="n">data</span><span class="p">)</span>
                    <span class="n">state</span><span class="p">[</span><span class="s">'exp_avg_sq'</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">p</span><span class="p">.</span><span class="n">data</span><span class="p">)</span>

                <span class="n">exp_avg</span><span class="p">,</span> <span class="n">exp_avg_sq</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s">'exp_avg'</span><span class="p">],</span> <span class="n">state</span><span class="p">[</span><span class="s">'exp_avg_sq'</span><span class="p">]</span>
                <span class="n">state</span><span class="p">[</span><span class="s">'step'</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>

                <span class="c1"># Update biased first moment: m â† Î²â‚m + (1 - Î²â‚)g
</span>                <span class="n">exp_avg</span><span class="p">.</span><span class="n">mul_</span><span class="p">(</span><span class="n">beta1</span><span class="p">).</span><span class="n">add_</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta1</span><span class="p">)</span>

                <span class="c1"># Update biased second moment: v â† Î²â‚‚v + (1 - Î²â‚‚)gÂ²
</span>                <span class="n">exp_avg_sq</span><span class="p">.</span><span class="n">mul_</span><span class="p">(</span><span class="n">beta2</span><span class="p">).</span><span class="n">addcmul_</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="n">grad</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta2</span><span class="p">)</span>

                <span class="c1"># Compute bias correction terms
</span>                <span class="n">bias_correction1</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">beta1</span> <span class="o">**</span> <span class="n">state</span><span class="p">[</span><span class="s">'step'</span><span class="p">]</span>
                <span class="n">bias_correction2</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">beta2</span> <span class="o">**</span> <span class="n">state</span><span class="p">[</span><span class="s">'step'</span><span class="p">]</span>

                <span class="c1"># Compute adjusted learning rate: Î±_t â† Î± âˆš(1-(Î²â‚‚)^t) / (1-(Î²â‚)^t)
</span>                <span class="n">alpha_t</span> <span class="o">=</span> <span class="n">lr</span> <span class="o">*</span> <span class="n">math</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">bias_correction2</span><span class="p">)</span> <span class="o">/</span> <span class="n">bias_correction1</span>

                <span class="c1"># Update parameters: Î¸ â† Î¸ - Î±_t m / âˆš(v+Îµ)
</span>                <span class="n">denom</span> <span class="o">=</span> <span class="n">exp_avg_sq</span><span class="p">.</span><span class="n">sqrt</span><span class="p">().</span><span class="n">add_</span><span class="p">(</span><span class="n">eps</span><span class="p">)</span>
                <span class="n">p</span><span class="p">.</span><span class="n">addcdiv_</span><span class="p">(</span><span class="n">exp_avg</span><span class="p">,</span> <span class="n">denom</span><span class="p">,</span> <span class="n">value</span><span class="o">=-</span><span class="n">alpha_t</span><span class="p">)</span>

                <span class="c1"># Apply decoupled weight decay: Î¸ â† Î¸ - Î±Î»Î¸
</span>                <span class="k">if</span> <span class="n">weight_decay</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="n">p</span><span class="p">.</span><span class="n">add_</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=-</span><span class="n">lr</span> <span class="o">*</span> <span class="n">weight_decay</span><span class="p">)</span>
</code></pre></div></div>

<p><strong>Usage example:</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span> <span class="o">=</span> <span class="n">TransformerLM</span><span class="p">(</span><span class="n">vocab_size</span><span class="o">=</span><span class="mi">50257</span><span class="p">,</span> <span class="n">d_model</span><span class="o">=</span><span class="mi">768</span><span class="p">,</span> <span class="p">...)</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">AdamW</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>

<span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">dataloader</span><span class="p">:</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">compute_loss</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">batch</span><span class="p">)</span>
    <span class="n">loss</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="p">.</span><span class="n">step</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="p">.</span><span class="n">zero_grad</span><span class="p">()</span>
</code></pre></div></div>

<hr />

<h3 id="lr-scheduling">Learning Rate Scheduling: The LLaMA Approach</h3>

<p>Modern large language models donâ€™t use a fixed learning rate. Instead, they employ sophisticated schedules that warm up the learning rate at the start and gradually decay it during training. The LLaMA paper (Touvron et al., 2023) uses a three-phase cosine schedule that has become standard.</p>

<h4 id="the-three-phase-schedule">The Three-Phase Schedule</h4>

<p><strong>Phase 1 - Warmup</strong> ($t &lt; T_w$):</p>

\[\alpha_t = \frac{t}{T_w} \cdot \alpha_{\text{max}}\]

<p>Linear increase from 0 to $\alpha_{\text{max}}$ over $T_w$ steps.</p>

<p><strong>Phase 2 - Cosine Annealing</strong> ($T_w \leq t \leq T_c$):</p>

\[\alpha_t = \alpha_{\text{min}} + \frac{1}{2} \left(1 + \cos\left(\frac{t - T_w}{T_c - T_w} \cdot \pi\right)\right) \left(\alpha_{\text{max}} - \alpha_{\text{min}}\right)\]

<p>Smooth cosine decay from $\alpha_{\text{max}}$ to $\alpha_{\text{min}}$.</p>

<p><strong>Understanding the Smooth Cosine Decay:</strong></p>

<p>The beauty of cosine annealing lies in its smoothness. The diagram below shows how the learning rate evolves during the cosine annealing phase:</p>

<p><img src="/assets/picture/2025-11-02-cs336-building-a-complete-training-loop/cosine_decay.png" alt="Smooth Cosine Decay" width="50%" /></p>

<p><em>Figure: Cosine annealing schedule showing the smooth decay of learning rate from Î±<sub>max</sub> to Î±<sub>min</sub></em></p>

<p><strong>Breaking down the cosine formula:</strong></p>

<p>Letâ€™s denote $p = \frac{t - T_w}{T_c - T_w}$ as the progress through the cosine phase (where $p \in [0, 1]$).</p>

<p>The formula becomes:</p>

\[\alpha_t = \alpha_{\text{min}} + \frac{1}{2}(1 + \cos(p \cdot \pi)) \cdot (\alpha_{\text{max}} - \alpha_{\text{min}})\]

<p><strong>Why cosine creates a smooth curve:</strong></p>

<ol>
  <li><strong>At start</strong> ($p = 0$):
    <ul>
      <li>$\cos(0) = 1$</li>
      <li>$\alpha_t = \alpha_{\text{min}} + 1 \cdot (\alpha_{\text{max}} - \alpha_{\text{min}}) = \alpha_{\text{max}}$</li>
    </ul>
  </li>
  <li><strong>At middle</strong> ($p = 0.5$):
    <ul>
      <li>$\cos(\pi/2) = 0$</li>
      <li>$\alpha_t = \alpha_{\text{min}} + 0.5 \cdot (\alpha_{\text{max}} - \alpha_{\text{min}})$ (halfway point)</li>
    </ul>
  </li>
  <li><strong>At end</strong> ($p = 1$):
    <ul>
      <li>$\cos(\pi) = -1$</li>
      <li>$\alpha_t = \alpha_{\text{min}} + 0 \cdot (\alpha_{\text{max}} - \alpha_{\text{min}}) = \alpha_{\text{min}}$</li>
    </ul>
  </li>
</ol>

<p><strong>Key properties of the smooth cosine decay:</strong></p>

<ul>
  <li><strong>Gentle start</strong>: Derivative is near zero at $t = T_w$, creating a smooth transition from warmup</li>
  <li><strong>Steepest descent</strong>: Maximum decay rate occurs at the midpoint ($p = 0.5$)</li>
  <li><strong>Gentle landing</strong>: Derivative approaches zero as $t \to T_c$, allowing fine-tuning</li>
  <li><strong>No discontinuities</strong>: The function and its derivative are continuous everywhere</li>
</ul>

<p><strong>Phase 3 - Constant</strong> ($t &gt; T_c$):</p>

\[\alpha_t = \alpha_{\text{min}}\]

<p>Maintain minimum learning rate.</p>

<h4 id="why-this-schedule-works">Why This Schedule Works</h4>

<p><strong>Warmup phase:</strong> Starting with a small learning rate prevents the model from making destructive updates when parameters are still randomly initialized. Gradients can be large and unstable early in training, and a small learning rate provides stability.</p>

<p><strong>Cosine decay:</strong> The smooth decay helps the model settle into a good minimum. The cosine schedule provides:</p>
<ul>
  <li>Fast initial decay (when model is still far from optimum)</li>
  <li>Slower decay later (allowing fine-tuning)</li>
  <li>No sharp transitions (unlike step decay schedules)</li>
</ul>

<p><strong>Constant minimum:</strong> Maintaining Î±_min instead of decaying to zero allows continued (albeit slow) learning, which can be useful for very long training runs.</p>

<h4 id="implementation">Implementation</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">get_lr_cosine_schedule</span><span class="p">(</span>
    <span class="n">it</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">max_learning_rate</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
    <span class="n">min_learning_rate</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
    <span class="n">warmup_iters</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">cosine_cycle_iters</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
    <span class="s">"""
    Get learning rate at iteration `it` using cosine schedule with warmup.

    Three phases:
    1. Warmup: Linear increase from 0 to max_learning_rate
    2. Cosine annealing: Smooth decay from max to min learning rate
    3. Constant: Maintain min_learning_rate

    Args:
        it: Current iteration (0-indexed)
        max_learning_rate: Maximum learning rate (Î±_max)
        min_learning_rate: Minimum learning rate (Î±_min)
        warmup_iters: Number of warmup iterations (T_w)
        cosine_cycle_iters: Total iterations for cosine cycle (T_c)

    Returns:
        Learning rate for current iteration
    """</span>
    <span class="c1"># Phase 1: Warmup (t &lt; T_w)
</span>    <span class="k">if</span> <span class="n">it</span> <span class="o">&lt;</span> <span class="n">warmup_iters</span><span class="p">:</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">it</span> <span class="o">/</span> <span class="n">warmup_iters</span><span class="p">)</span> <span class="o">*</span> <span class="n">max_learning_rate</span>

    <span class="c1"># Phase 2: Cosine annealing (T_w â‰¤ t â‰¤ T_c)
</span>    <span class="k">if</span> <span class="n">it</span> <span class="o">&lt;=</span> <span class="n">cosine_cycle_iters</span><span class="p">:</span>
        <span class="n">progress</span> <span class="o">=</span> <span class="p">(</span><span class="n">it</span> <span class="o">-</span> <span class="n">warmup_iters</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">cosine_cycle_iters</span> <span class="o">-</span> <span class="n">warmup_iters</span><span class="p">)</span>
        <span class="n">cosine_decay</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">math</span><span class="p">.</span><span class="n">cos</span><span class="p">(</span><span class="n">progress</span> <span class="o">*</span> <span class="n">math</span><span class="p">.</span><span class="n">pi</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">min_learning_rate</span> <span class="o">+</span> <span class="n">cosine_decay</span> <span class="o">*</span> <span class="p">(</span><span class="n">max_learning_rate</span> <span class="o">-</span> <span class="n">min_learning_rate</span><span class="p">)</span>

    <span class="c1"># Phase 3: Constant (t &gt; T_c)
</span>    <span class="k">return</span> <span class="n">min_learning_rate</span>
</code></pre></div></div>

<p><strong>Critical detail:</strong> The warmup condition is <code class="language-plaintext highlighter-rouge">it &lt; warmup_iters</code> (strict inequality), not <code class="language-plaintext highlighter-rouge">it &lt;= warmup_iters</code>. This ensures iteration <code class="language-plaintext highlighter-rouge">warmup_iters</code> is the first iteration at <code class="language-plaintext highlighter-rouge">max_learning_rate</code>, not the last warmup iteration.</p>

<h4 id="integration-with-training-loop">Integration with Training Loop</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span> <span class="n">iter_num</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_iters</span><span class="p">):</span>
    <span class="c1"># Get learning rate for this iteration
</span>    <span class="n">lr</span> <span class="o">=</span> <span class="n">get_lr_cosine_schedule</span><span class="p">(</span>
        <span class="n">it</span><span class="o">=</span><span class="n">iter_num</span><span class="p">,</span>
        <span class="n">max_learning_rate</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span>
        <span class="n">min_learning_rate</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">,</span>
        <span class="n">warmup_iters</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span>
        <span class="n">cosine_cycle_iters</span><span class="o">=</span><span class="mi">100000</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="c1"># Update optimizer learning rate
</span>    <span class="k">for</span> <span class="n">param_group</span> <span class="ow">in</span> <span class="n">optimizer</span><span class="p">.</span><span class="n">param_groups</span><span class="p">:</span>
        <span class="n">param_group</span><span class="p">[</span><span class="s">'lr'</span><span class="p">]</span> <span class="o">=</span> <span class="n">lr</span>

    <span class="c1"># Training step
</span>    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">get_batch</span><span class="p">(...)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">loss</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="p">.</span><span class="n">step</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="p">.</span><span class="n">zero_grad</span><span class="p">()</span>
</code></pre></div></div>

<p><strong>Typical hyperparameters for large models:</strong></p>
<ul>
  <li>Warmup: 2,000-10,000 iterations (1-5% of total training)</li>
  <li>Max LR: 1e-4 to 1e-3 (depends on model size; larger models use smaller LR)</li>
  <li>Min LR: 10% of max LR</li>
  <li>Cosine cycle: Total training iterations</li>
</ul>

<hr />

<h3 id="data-loading">Memory-Efficient Data Loading</h3>

<p>When training on large text datasets (hundreds of GBs to TBs), loading the entire dataset into RAM is impossible. The solution is <strong>memory-mapped arrays</strong> using the Unix <code class="language-plaintext highlighter-rouge">mmap</code> system call.</p>

<h4 id="the-problem">The Problem</h4>

<p>Consider training GPT-3-scale models:</p>
<ul>
  <li>Common Crawl: ~570GB tokenized</li>
  <li>Books: ~150GB tokenized</li>
  <li>Total: ~800GB of tokens</li>
</ul>

<p>Your machine might have 64-128GB of RAM. Loading this data is impossible.</p>

<h4 id="the-solution-memory-mapping">The Solution: Memory Mapping</h4>

<p>Memory mapping lets you â€œpretendâ€ the entire dataset is in memory, but the OS only loads the portion you actually access.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Memory-mapped loading
</span><span class="n">dataset</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="s">'train_tokens.npy'</span><span class="p">,</span> <span class="n">mmap_mode</span><span class="o">=</span><span class="s">'r'</span><span class="p">)</span>
<span class="c1"># This doesn't load the file into RAM!
# It creates a memory map to the file on disk
</span>
<span class="c1"># When you access dataset[1000000:1000512],
# the OS loads just that small portion into RAM
</span></code></pre></div></div>

<p><strong>Understanding Virtual Memory</strong></p>

<p>Before diving into how memory mapping works, itâ€™s important to understand the concept of virtual memoryâ€”the foundation that makes memory mapping possible.</p>

<p>Virtual memory is a way for your computer to make it look like you have more memory (RAM) than you actually do. It does this by using part of your disk (storage) to act as an extension of RAM. Every program â€œthinksâ€ it has access to a large, continuous block of memoryâ€”but behind the scenes, the operating system (OS) is moving chunks of data between RAM and disk as needed.</p>

<p><strong>How Memory Mapping Works: Step by Step</strong></p>

<ol>
  <li>
    <p><strong>Mapping file to memory</strong>: The system call <code class="language-plaintext highlighter-rouge">mmap()</code> creates a link between a file on disk and an area in virtual memory. You can then access it like a normal array, even if the file is huge (e.g., 800GB).</p>
  </li>
  <li>
    <p><strong>Page fault (on first access)</strong>: When your code accesses something like <code class="language-plaintext highlighter-rouge">dataset[i]</code>, the OS sees that the data isnâ€™t in RAM yet. It triggers a <strong>page fault</strong>â€”a signal that tells the OS to fetch that data page from disk.</p>
  </li>
  <li>
    <p><strong>Loading data into RAM</strong>: The OS loads the specific page (a small chunk, usually 4KB) from disk into physical RAM. Now <code class="language-plaintext highlighter-rouge">dataset[i]</code> can be read directly from fast memory.</p>
  </li>
  <li>
    <p><strong>Caching nearby elements</strong>: The OS often loads neighboring pages too (since theyâ€™ll likely be accessed soon). So if you later access <code class="language-plaintext highlighter-rouge">dataset[i+1]</code>, itâ€™s already in RAMâ€”fast!</p>
  </li>
  <li>
    <p><strong>Eviction when RAM is full</strong>: When RAM gets full, the OS automatically evicts less-used pages (writes them back to disk if modified). This keeps the system running smoothly without running out of memory.</p>
  </li>
</ol>

<p><strong>Key insight</strong>: Memory mapping leverages the OSâ€™s virtual memory system to handle datasets much larger than available RAM, loading only the data you need on-demand and caching intelligently based on access patterns.</p>

<h4 id="implementation-1">Implementation</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">get_batch</span><span class="p">(</span>
    <span class="n">dataset</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">,</span>  <span class="c1"># Can be memory-mapped!
</span>    <span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">context_length</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">device</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s">"cpu"</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">]:</span>
    <span class="s">"""
    Sample a batch of sequences from dataset.

    Supports both regular arrays and memory-mapped arrays transparently.
    Memory-mapped arrays use the Unix mmap system call to map files to virtual
    memory, allowing you to "pretend" you have the entire dataset in memory
    while only loading accessed portions on-demand.

    Args:
        dataset: Token array (regular or memory-mapped)
        batch_size: Number of sequences to sample
        context_length: Length of each sequence
        device: Device to place tensors on

    Returns:
        x: Input sequences [batch_size, context_length]
        y: Target sequences [batch_size, context_length] (shifted by 1)
    """</span>
    <span class="c1"># Sample random start positions
</span>    <span class="n">max_start</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span> <span class="o">-</span> <span class="n">context_length</span> <span class="o">-</span> <span class="mi">1</span>
    <span class="n">start_indices</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">max_start</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">)</span>

    <span class="c1"># Extract sequences (this triggers page faults for memory-mapped arrays)
</span>    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">stack</span><span class="p">([</span><span class="n">dataset</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span> <span class="o">+</span> <span class="n">context_length</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">start_indices</span><span class="p">])</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">stack</span><span class="p">([</span><span class="n">dataset</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">:</span><span class="n">i</span> <span class="o">+</span> <span class="n">context_length</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">start_indices</span><span class="p">])</span>

    <span class="c1"># Convert to PyTorch tensors
</span>    <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">x</span><span class="p">).</span><span class="nb">long</span><span class="p">().</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">y</span><span class="p">).</span><span class="nb">long</span><span class="p">().</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span>


<span class="k">def</span> <span class="nf">load_dataset</span><span class="p">(</span><span class="n">data_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">:</span>
    <span class="s">"""
    Load dataset using memory-mapped mode for memory efficiency.

    Args:
        data_path: Path to .npy file containing tokenized data
        vocab_size: Expected vocabulary size for validation

    Returns:
        Memory-mapped numpy array
    """</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Loading dataset from </span><span class="si">{</span><span class="n">data_path</span><span class="si">}</span><span class="s">..."</span><span class="p">)</span>

    <span class="c1"># Load with memory mapping for large datasets
</span>    <span class="n">dataset</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="n">data_path</span><span class="p">,</span> <span class="n">mmap_mode</span><span class="o">=</span><span class="s">"r"</span><span class="p">)</span>

    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"  Loaded </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span><span class="si">:</span><span class="p">,</span><span class="si">}</span><span class="s"> tokens"</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"  Data type: </span><span class="si">{</span><span class="n">dataset</span><span class="p">.</span><span class="n">dtype</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"  Memory-mapped: </span><span class="si">{</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="n">memmap</span><span class="p">)</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>

    <span class="c1"># Verify data integrity
</span>    <span class="n">max_token</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">.</span><span class="nb">max</span><span class="p">()</span>
    <span class="n">min_token</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">.</span><span class="nb">min</span><span class="p">()</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"  Token range: [</span><span class="si">{</span><span class="n">min_token</span><span class="si">}</span><span class="s">, </span><span class="si">{</span><span class="n">max_token</span><span class="si">}</span><span class="s">]"</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">max_token</span> <span class="o">&gt;=</span> <span class="n">vocab_size</span><span class="p">:</span>
        <span class="k">raise</span> <span class="nb">ValueError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s">"Data contains token </span><span class="si">{</span><span class="n">max_token</span><span class="si">}</span><span class="s"> &gt;= vocab_size </span><span class="si">{</span><span class="n">vocab_size</span><span class="si">}</span><span class="s">. "</span>
            <span class="sa">f</span><span class="s">"Data may be corrupted or vocab_size is incorrect."</span>
        <span class="p">)</span>

    <span class="k">if</span> <span class="n">min_token</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">raise</span> <span class="nb">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s">"Data contains negative token </span><span class="si">{</span><span class="n">min_token</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>

    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"  âœ“ Data integrity verified"</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">dataset</span>
</code></pre></div></div>

<h4 id="important-considerations">Important Considerations</h4>

<p><strong>1. Data type matching:</strong></p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Ensure dtype matches your vocabulary size
</span><span class="n">dataset</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">memmap</span><span class="p">(</span><span class="s">'tokens.dat'</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="s">'int32'</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s">'r'</span><span class="p">)</span>  <span class="c1"># For vocab &lt; 2^31
# or
</span><span class="n">dataset</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">memmap</span><span class="p">(</span><span class="s">'tokens.dat'</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="s">'int64'</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s">'r'</span><span class="p">)</span>  <span class="c1"># For safety
</span></code></pre></div></div>

<p><strong>2. Data integrity:</strong>
Always verify that token values are within valid range:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">assert</span> <span class="n">dataset</span><span class="p">.</span><span class="nb">max</span><span class="p">()</span> <span class="o">&lt;</span> <span class="n">vocab_size</span><span class="p">,</span> <span class="s">"Invalid token values!"</span>
<span class="k">assert</span> <span class="n">dataset</span><span class="p">.</span><span class="nb">min</span><span class="p">()</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">,</span> <span class="s">"Negative token values!"</span>
</code></pre></div></div>

<p><strong>3. Performance tips:</strong></p>
<ul>
  <li>Access data sequentially when possible (better cache locality)</li>
  <li>Use larger batch sizes to amortize page fault overhead</li>
  <li>Store data on fast SSD rather than HDD</li>
</ul>

<hr />

<h3 id="checkpointing">Checkpoint Management</h3>

<p>Training large models can take days or weeks. Checkpoint management is crucial for:</p>
<ul>
  <li>Resuming after crashes or preemption</li>
  <li>Evaluating models at different training stages</li>
  <li>Storing model configurations for reproducibility</li>
</ul>

<h4 id="what-to-save">What to Save</h4>

<p>A complete checkpoint includes:</p>
<ol>
  <li><strong>Model state</strong>: All parameter values</li>
  <li><strong>Optimizer state</strong>: Momentum buffers, learning rate, etc.</li>
  <li><strong>Iteration count</strong>: For resuming at exact position</li>
  <li><strong>Model configuration</strong>: For reconstructing architecture</li>
</ol>

<p>Many implementations forget #4, making it hard to load models for inference later.</p>

<p><strong>Why Model Configuration Matters</strong></p>

<p>Think of it this way:</p>
<ul>
  <li><strong>Model configuration</strong> = The modelâ€™s recipe (layer sizes, dropout rates, architecture choices)</li>
  <li><strong>Model state</strong> = The modelâ€™s learned ingredients (weights and biases)</li>
</ul>

<p>Without the configuration, you wouldnâ€™t know how to rebuild the same model structure later.</p>

<p><strong>Example: A Simple Neural Network</strong></p>

<p>Letâ€™s say you built this model in PyTorch:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>

<span class="k">class</span> <span class="nc">MyModel</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_size</span><span class="o">=</span><span class="mi">784</span><span class="p">,</span> <span class="n">hidden_size</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span> <span class="n">output_size</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.2</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">ReLU</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">)</span>
</code></pre></div></div>

<p>When you train it, youâ€™ll want to save not only the weights, but also the model configuration:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">config</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s">"input_size"</span><span class="p">:</span> <span class="mi">784</span><span class="p">,</span>
    <span class="s">"hidden_size"</span><span class="p">:</span> <span class="mi">256</span><span class="p">,</span>
    <span class="s">"output_size"</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span>
    <span class="s">"dropout"</span><span class="p">:</span> <span class="mf">0.2</span>
<span class="p">}</span>

<span class="n">checkpoint</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s">"model_state"</span><span class="p">:</span> <span class="n">model</span><span class="p">.</span><span class="n">state_dict</span><span class="p">(),</span>
    <span class="s">"optimizer_state"</span><span class="p">:</span> <span class="n">optimizer</span><span class="p">.</span><span class="n">state_dict</span><span class="p">(),</span>
    <span class="s">"iteration"</span><span class="p">:</span> <span class="n">step</span><span class="p">,</span>
    <span class="s">"config"</span><span class="p">:</span> <span class="n">config</span>
<span class="p">}</span>
<span class="n">torch</span><span class="p">.</span><span class="n">save</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">,</span> <span class="s">"checkpoint.pth"</span><span class="p">)</span>
</code></pre></div></div>

<p><strong>Later (for inference or resume training):</strong></p>

<p>You can rebuild the model exactly the same way:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">checkpoint</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="s">"checkpoint.pth"</span><span class="p">)</span>
<span class="n">config</span> <span class="o">=</span> <span class="n">checkpoint</span><span class="p">[</span><span class="s">"config"</span><span class="p">]</span>

<span class="c1"># Rebuild model using saved configuration
</span><span class="n">model</span> <span class="o">=</span> <span class="n">MyModel</span><span class="p">(</span><span class="o">**</span><span class="n">config</span><span class="p">)</span>
<span class="n">model</span><span class="p">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">[</span><span class="s">"model_state"</span><span class="p">])</span>
</code></pre></div></div>

<p>This same principle applies to Transformer language models, where the configuration includes <code class="language-plaintext highlighter-rouge">vocab_size</code>, <code class="language-plaintext highlighter-rouge">d_model</code>, <code class="language-plaintext highlighter-rouge">num_layers</code>, <code class="language-plaintext highlighter-rouge">num_heads</code>, <code class="language-plaintext highlighter-rouge">d_ff</code>, <code class="language-plaintext highlighter-rouge">context_length</code>, etc.</p>

<h4 id="implementation-2">Implementation</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">save_checkpoint</span><span class="p">(</span>
    <span class="n">model</span><span class="p">:</span> <span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">,</span>
    <span class="n">optimizer</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="n">Optimizer</span><span class="p">,</span>
    <span class="n">iteration</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">out</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">model_config</span><span class="p">:</span> <span class="nb">dict</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
    <span class="s">"""
    Save complete training state to checkpoint file.

    Args:
        model: Model to save
        optimizer: Optimizer to save
        iteration: Current training iteration
        out: Output path for checkpoint
        model_config: Optional model architecture configuration
    """</span>
    <span class="n">checkpoint</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s">'model_state_dict'</span><span class="p">:</span> <span class="n">model</span><span class="p">.</span><span class="n">state_dict</span><span class="p">(),</span>
        <span class="s">'optimizer_state_dict'</span><span class="p">:</span> <span class="n">optimizer</span><span class="p">.</span><span class="n">state_dict</span><span class="p">(),</span>
        <span class="s">'iteration'</span><span class="p">:</span> <span class="n">iteration</span><span class="p">,</span>
    <span class="p">}</span>

    <span class="c1"># Save model config for easy loading during inference
</span>    <span class="k">if</span> <span class="n">model_config</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">checkpoint</span><span class="p">[</span><span class="s">'model_config'</span><span class="p">]</span> <span class="o">=</span> <span class="n">model_config</span>

    <span class="n">torch</span><span class="p">.</span><span class="n">save</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">,</span> <span class="n">out</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">load_checkpoint</span><span class="p">(</span>
    <span class="n">src</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">model</span><span class="p">:</span> <span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">,</span>
    <span class="n">optimizer</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="n">Optimizer</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
    <span class="s">"""
    Load training state from checkpoint file.

    Args:
        src: Path to checkpoint file
        model: Model to load state into
        optimizer: Optimizer to load state into

    Returns:
        Iteration number from checkpoint
    """</span>
    <span class="n">checkpoint</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="n">src</span><span class="p">,</span> <span class="n">map_location</span><span class="o">=</span><span class="s">'cpu'</span><span class="p">)</span>

    <span class="n">model</span><span class="p">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">[</span><span class="s">'model_state_dict'</span><span class="p">])</span>
    <span class="n">optimizer</span><span class="p">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">[</span><span class="s">'optimizer_state_dict'</span><span class="p">])</span>

    <span class="k">return</span> <span class="n">checkpoint</span><span class="p">[</span><span class="s">'iteration'</span><span class="p">]</span>
</code></pre></div></div>

<h4 id="checkpoint-strategy">Checkpoint Strategy</h4>

<p><strong>During training:</strong></p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Save periodically during training
</span><span class="k">if</span> <span class="n">iter_num</span> <span class="o">%</span> <span class="n">checkpoint_interval</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">iter_num</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
    <span class="n">checkpoint_path</span> <span class="o">=</span> <span class="sa">f</span><span class="s">"checkpoints/checkpoint_iter_</span><span class="si">{</span><span class="n">iter_num</span><span class="si">}</span><span class="s">.pt"</span>
    <span class="n">save_checkpoint</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">iter_num</span><span class="p">,</span> <span class="n">checkpoint_path</span><span class="p">,</span> <span class="n">model_config</span><span class="p">)</span>

<span class="c1"># Save final checkpoint with both iteration number and "final" name
</span><span class="n">final_checkpoint_iter</span> <span class="o">=</span> <span class="sa">f</span><span class="s">"checkpoints/checkpoint_iter_</span><span class="si">{</span><span class="n">max_iters</span><span class="si">}</span><span class="s">.pt"</span>
<span class="n">final_checkpoint</span> <span class="o">=</span> <span class="s">"checkpoints/checkpoint_final.pt"</span>
<span class="n">save_checkpoint</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">max_iters</span><span class="p">,</span> <span class="n">final_checkpoint_iter</span><span class="p">,</span> <span class="n">model_config</span><span class="p">)</span>
<span class="n">save_checkpoint</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">max_iters</span><span class="p">,</span> <span class="n">final_checkpoint</span><span class="p">,</span> <span class="n">model_config</span><span class="p">)</span>
</code></pre></div></div>

<p><strong>Resuming from checkpoint:</strong></p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">if</span> <span class="n">resume_from</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
    <span class="n">start_iter</span> <span class="o">=</span> <span class="n">load_checkpoint</span><span class="p">(</span><span class="n">resume_from</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Resumed from iteration </span><span class="si">{</span><span class="n">start_iter</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="n">start_iter</span> <span class="o">=</span> <span class="mi">0</span>

<span class="k">for</span> <span class="n">iter_num</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">start_iter</span><span class="p">,</span> <span class="n">max_iters</span><span class="p">):</span>
    <span class="c1"># Training continues from where it left off
</span>    <span class="p">...</span>
</code></pre></div></div>

<p><strong>For inference (loading model configuration):</strong></p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">checkpoint</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="s">"checkpoint.pt"</span><span class="p">)</span>
<span class="n">config</span> <span class="o">=</span> <span class="n">checkpoint</span><span class="p">[</span><span class="s">'model_config'</span><span class="p">]</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">TransformerLM</span><span class="p">(</span>
    <span class="n">vocab_size</span><span class="o">=</span><span class="n">config</span><span class="p">[</span><span class="s">'vocab_size'</span><span class="p">],</span>
    <span class="n">d_model</span><span class="o">=</span><span class="n">config</span><span class="p">[</span><span class="s">'d_model'</span><span class="p">],</span>
    <span class="n">num_layers</span><span class="o">=</span><span class="n">config</span><span class="p">[</span><span class="s">'num_layers'</span><span class="p">],</span>
    <span class="n">num_heads</span><span class="o">=</span><span class="n">config</span><span class="p">[</span><span class="s">'num_heads'</span><span class="p">],</span>
    <span class="n">d_ff</span><span class="o">=</span><span class="n">config</span><span class="p">[</span><span class="s">'d_ff'</span><span class="p">],</span>
    <span class="n">context_length</span><span class="o">=</span><span class="n">config</span><span class="p">[</span><span class="s">'context_length'</span><span class="p">],</span>
<span class="p">)</span>
<span class="n">model</span><span class="p">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">[</span><span class="s">'model_state_dict'</span><span class="p">])</span>
</code></pre></div></div>

<hr />

<h3 id="decoding">Decoding Strategies: From Model to Text</h3>

<p>After training, your model can predict the next word given the previous ones. But you need a method to:</p>
<ol>
  <li>Turn those predictions into probabilities</li>
  <li>Pick the next word/token from that probability distribution</li>
</ol>

<p>That process is called <strong>decoding</strong>. The decoding strategy significantly impacts generation qualityâ€”itâ€™s the difference between coherent text and random gibberish.</p>

<h4 id="step-1-softmax--turning-logits-into-probabilities">Step 1: Softmax â€” Turning Logits into Probabilities</h4>

<p>The model outputs a vector of <strong>logits</strong>â€”raw scores for every possible token in the vocabulary. We turn these into probabilities using the <strong>softmax</strong> formula:</p>

\[P(x_{t+1} = i \mid x_{1..t}) = \frac{e^{v_i}}{\sum_{j} e^{v_j}}\]

<p>Where:</p>
<ul>
  <li>$v_i$ is the modelâ€™s score (logit) for token $i$</li>
  <li>The numerator $e^{v_i}$ makes higher scores more likely</li>
  <li>The denominator $\sum_{j} e^{v_j}$ normalizes everything so probabilities sum to 1</li>
</ul>

<p>This gives us a probability distribution over all words in the vocabulary.</p>

<h4 id="step-2-decoding--picking-the-next-token">Step 2: Decoding â€” Picking the Next Token</h4>

<p>Now that we have probabilities, we need to choose one token to continue the text. We can:</p>

<ul>
  <li><strong>Pick the highest-probability token</strong> (greedy decoding) â†’ Safe but repetitive</li>
  <li><strong>Randomly sample from the probabilities</strong> â†’ Makes text more creative</li>
  <li><strong>Use other tricks to balance randomness and coherence</strong> â†’ The strategies below</li>
</ul>

<p>Letâ€™s explore two powerful techniques for controlling this balance.</p>

<h4 id="temperature-scaling">Temperature Scaling</h4>

<p><strong>Problem:</strong> Raw softmax outputs can be too peaked (always choosing the most likely token) or too flat (generating random nonsense).</p>

<p><strong>Solution:</strong> Temperature scaling modifies the softmax distribution:</p>

\[\text{softmax}(v, \tau)_i = \frac{\exp(v_i/\tau)}{\sum_{j} \exp(v_j/\tau)}\]

<p><strong>Effects:</strong></p>
<ul>
  <li>$\tau &lt; 1$: makes the distribution sharper (model becomes more confident, deterministic, greedy)</li>
  <li>$\tau = 1$: Standard softmax (modelâ€™s original distribution)</li>
  <li>$\tau &gt; 1$: makes the distribution flatter (model becomes more random, creative, diverse)</li>
</ul>

<p><strong>Implementation:</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">softmax_with_temperature</span><span class="p">(</span>
    <span class="n">logits</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">temperature</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span>
    <span class="n">dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="s">"""
    Apply softmax with temperature scaling.

    Args:
        logits: Model output logits
        temperature: Temperature parameter Ï„
        dim: Dimension to apply softmax

    Returns:
        Temperature-scaled probability distribution
    """</span>
    <span class="k">if</span> <span class="n">temperature</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">raise</span> <span class="nb">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s">"Temperature must be positive, got </span><span class="si">{</span><span class="n">temperature</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>

    <span class="c1"># Scale logits by temperature
</span>    <span class="n">scaled_logits</span> <span class="o">=</span> <span class="n">logits</span> <span class="o">/</span> <span class="n">temperature</span>

    <span class="c1"># Apply softmax (numerically stable)
</span>    <span class="n">probs</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">functional</span><span class="p">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">scaled_logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">dim</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">probs</span>
</code></pre></div></div>

<p><strong>Usage:</strong></p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">logits</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">apply_softmax</span><span class="o">=</span><span class="bp">False</span><span class="p">)[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:]</span>  <span class="c1"># Get next-token logits
</span>
<span class="c1"># Deterministic (greedy)
</span><span class="n">probs</span> <span class="o">=</span> <span class="n">softmax_with_temperature</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>

<span class="c1"># Balanced
</span><span class="n">probs</span> <span class="o">=</span> <span class="n">softmax_with_temperature</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>

<span class="c1"># Creative
</span><span class="n">probs</span> <span class="o">=</span> <span class="n">softmax_with_temperature</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="mf">1.5</span><span class="p">)</span>
</code></pre></div></div>

<p><strong>Concrete Example:</strong></p>

<p>Letâ€™s say the model predicts the next word with these raw logits and probabilities:</p>

<table>
  <thead>
    <tr>
      <th>Token</th>
      <th>Raw Logit</th>
      <th>$\tau=1.0$ (standard)</th>
      <th>$\tau=0.5$ (sharper)</th>
      <th>$\tau=2.0$ (flatter)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>â€œcatâ€</td>
      <td>2.5</td>
      <td>0.60</td>
      <td>0.94</td>
      <td>0.52</td>
    </tr>
    <tr>
      <td>â€œdogâ€</td>
      <td>1.0</td>
      <td>0.25</td>
      <td>0.05</td>
      <td>0.25</td>
    </tr>
    <tr>
      <td>â€œbananaâ€</td>
      <td>0.2</td>
      <td>0.10</td>
      <td>0.01</td>
      <td>0.16</td>
    </tr>
    <tr>
      <td>â€œspaceshipâ€</td>
      <td>-1.5</td>
      <td>0.05</td>
      <td>0.00</td>
      <td>0.07</td>
    </tr>
  </tbody>
</table>

<p><strong>Observations:</strong></p>

<ul>
  <li>
    <p><strong>With $\tau = 0.5$</strong> (sharper): â€œcatâ€ becomes dominant (0.94), nearly eliminating other options. The model is very confident and predictable.</p>
  </li>
  <li>
    <p><strong>With $\tau = 1.0$</strong> (standard): Uses the modelâ€™s original learned distribution. Balanced between confidence and diversity.</p>
  </li>
  <li>
    <p><strong>With $\tau = 2.0$</strong> (flatter): Probabilities become more uniform. â€œdogâ€ maintains its probability, â€œbananaâ€ nearly doubles (0.10 â†’ 0.16), and even â€œspaceshipâ€ becomes viable (0.05 â†’ 0.07). The model is more creative and exploratory.</p>
  </li>
</ul>

<h4 id="top-p-nucleus-sampling">Top-p (Nucleus) Sampling</h4>

<p><strong>Problem:</strong> Even with temperature scaling, the model might assign non-zero probability to thousands of tokens, many of which are nonsensical in context.</p>

<p><strong>Solution:</strong> Top-p sampling (Holtzman et al., 2020) truncates the distribution to the smallest set of tokens whose cumulative probability exceeds threshold p.</p>

<p><strong>Algorithm:</strong></p>

<p>Define the nucleus $V(p)$ as the smallest set such that:</p>

\[\sum_{i \in V(p)} P(i) \geq p\]

<p>Then the filtered probability distribution is:</p>

\[P_{\text{filtered}}(i) = \begin{cases}
\frac{P(i)}{\sum_{j \in V(p)} P(j)} &amp; \text{if } i \in V(p) \\
0 &amp; \text{otherwise}
\end{cases}\]

<p><strong>Implementation:</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">top_p_sampling</span><span class="p">(</span><span class="n">probs</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">p</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.9</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="s">"""
    Apply top-p (nucleus) sampling to probability distribution.

    Args:
        probs: Probability distribution [batch_size, vocab_size]
        p: Cumulative probability threshold (typical: 0.9, 0.95)

    Returns:
        Filtered and renormalized probability distribution
    """</span>
    <span class="c1"># Sort probabilities in descending order
</span>    <span class="n">sorted_probs</span><span class="p">,</span> <span class="n">sorted_indices</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">sort</span><span class="p">(</span><span class="n">probs</span><span class="p">,</span> <span class="n">descending</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

    <span class="c1"># Compute cumulative probabilities
</span>    <span class="n">cumulative_probs</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">sorted_probs</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

    <span class="c1"># Find cutoff: keep tokens until cumulative prob &gt;= p
</span>    <span class="n">mask</span> <span class="o">=</span> <span class="n">cumulative_probs</span> <span class="o">&lt;=</span> <span class="n">p</span>

    <span class="c1"># Always keep at least the top token
</span>    <span class="n">mask</span><span class="p">[...,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="bp">True</span>

    <span class="c1"># Zero out probabilities not in nucleus
</span>    <span class="n">filtered_sorted_probs</span> <span class="o">=</span> <span class="n">sorted_probs</span> <span class="o">*</span> <span class="n">mask</span><span class="p">.</span><span class="nb">float</span><span class="p">()</span>

    <span class="c1"># Scatter back to original positions
</span>    <span class="n">filtered_probs</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">probs</span><span class="p">)</span>
    <span class="n">filtered_probs</span><span class="p">.</span><span class="n">scatter_</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">sorted_indices</span><span class="p">,</span> <span class="n">src</span><span class="o">=</span><span class="n">filtered_sorted_probs</span><span class="p">)</span>

    <span class="c1"># Renormalize
</span>    <span class="n">filtered_probs</span> <span class="o">=</span> <span class="n">filtered_probs</span> <span class="o">/</span> <span class="n">filtered_probs</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">filtered_probs</span>
</code></pre></div></div>

<p><strong>Example:</strong></p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Original distribution
</span><span class="n">probs</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.05</span><span class="p">,</span> <span class="mf">0.05</span><span class="p">])</span>

<span class="c1"># p=0.8: Keep top 2 tokens (0.5 + 0.3 = 0.8)
</span><span class="n">filtered</span> <span class="o">=</span> <span class="n">top_p_sampling</span><span class="p">(</span><span class="n">probs</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>
<span class="c1"># Result: [0.625, 0.375, 0, 0, 0]
</span></code></pre></div></div>

<p><strong>Applying to our earlier example:</strong></p>

<p>With our â€œcatâ€, â€œdogâ€, â€œbananaâ€, â€œspaceshipâ€ example, if we use <strong>$p = 0.9$</strong>:</p>

<ol>
  <li>Sort by probability: [â€œcatâ€ (0.60), â€œdogâ€ (0.25), â€œbananaâ€ (0.10), â€œspaceshipâ€ (0.05)]</li>
  <li>Cumulative sum: 0.60, 0.85, 0.95, 1.00</li>
  <li>Keep tokens until cumulative â‰¥ 0.9: Keep {â€œcatâ€, â€œdogâ€, â€œbananaâ€}</li>
  <li>Remove â€œspaceshipâ€ (too low probability)</li>
  <li>Renormalize and sample from the remaining three tokens</li>
</ol>

<p><strong>Result:</strong> The model only samples from {â€œcatâ€, â€œdogâ€, â€œbananaâ€}, avoiding the extremely unlikely â€œspaceshipâ€.</p>

<h4 id="summary-putting-it-all-together">Summary: Putting It All Together</h4>

<table>
  <thead>
    <tr>
      <th>Step</th>
      <th>Purpose</th>
      <th>Key Parameter</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Softmax</strong></td>
      <td>Turns model logits into probabilities</td>
      <td>None</td>
    </tr>
    <tr>
      <td><strong>Temperature</strong></td>
      <td>Controls confidence vs. creativity</td>
      <td>$\tau$ (typical: 0.7-1.5)</td>
    </tr>
    <tr>
      <td><strong>Top-p Sampling</strong></td>
      <td>Limits randomness to most probable tokens</td>
      <td>$p$ (typical: 0.9-0.95)</td>
    </tr>
  </tbody>
</table>

<p><strong>Recommended combinations:</strong></p>
<ul>
  <li><strong>Factual tasks</strong>: $\tau = 0.1$ (nearly greedy)</li>
  <li><strong>Balanced generation</strong>: $\tau = 0.8$, $p = 0.9$</li>
  <li><strong>Creative writing</strong>: $\tau = 1.2$, $p = 0.95$</li>
</ul>

<h4 id="autoregressive-decoding">Autoregressive Decoding</h4>

<p>Putting it together for text generation:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">decode</span><span class="p">(</span>
    <span class="n">model</span><span class="p">:</span> <span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">,</span>
    <span class="n">prompt_tokens</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">max_new_tokens</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">50</span><span class="p">,</span>
    <span class="n">temperature</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span>
    <span class="n">top_p</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
    <span class="n">eos_token_id</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
    <span class="n">device</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s">"cpu"</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="s">"""
    Generate text autoregressively from a prompt.

    Args:
        model: Trained TransformerLM
        prompt_tokens: Initial prompt [batch_size, seq_len]
        max_new_tokens: Maximum tokens to generate
        temperature: Sampling temperature
        top_p: Nucleus sampling threshold (None to disable)
        eos_token_id: End-of-sequence token for early stopping
        device: Device to run on

    Returns:
        Generated sequence [batch_size, seq_len + num_generated]
    """</span>
    <span class="n">model</span><span class="p">.</span><span class="nb">eval</span><span class="p">()</span>

    <span class="k">if</span> <span class="n">prompt_tokens</span><span class="p">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">prompt_tokens</span> <span class="o">=</span> <span class="n">prompt_tokens</span><span class="p">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

    <span class="n">generated</span> <span class="o">=</span> <span class="n">prompt_tokens</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

    <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_new_tokens</span><span class="p">):</span>
            <span class="c1"># Get logits for next token
</span>            <span class="n">logits</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">generated</span><span class="p">,</span> <span class="n">apply_softmax</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
            <span class="n">next_token_logits</span> <span class="o">=</span> <span class="n">logits</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:]</span>

            <span class="c1"># Apply temperature scaling
</span>            <span class="n">next_token_probs</span> <span class="o">=</span> <span class="n">softmax_with_temperature</span><span class="p">(</span>
                <span class="n">next_token_logits</span><span class="p">,</span>
                <span class="n">temperature</span><span class="o">=</span><span class="n">temperature</span>
            <span class="p">)</span>

            <span class="c1"># Apply top-p filtering if requested
</span>            <span class="k">if</span> <span class="n">top_p</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
                <span class="n">next_token_probs</span> <span class="o">=</span> <span class="n">top_p_sampling</span><span class="p">(</span><span class="n">next_token_probs</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">top_p</span><span class="p">)</span>

            <span class="c1"># Sample next token
</span>            <span class="n">next_token</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">multinomial</span><span class="p">(</span><span class="n">next_token_probs</span><span class="p">,</span> <span class="n">num_samples</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

            <span class="c1"># Append to sequence
</span>            <span class="n">generated</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">cat</span><span class="p">([</span><span class="n">generated</span><span class="p">,</span> <span class="n">next_token</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

            <span class="c1"># Check for EOS
</span>            <span class="k">if</span> <span class="n">eos_token_id</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span> <span class="ow">and</span> <span class="p">(</span><span class="n">next_token</span> <span class="o">==</span> <span class="n">eos_token_id</span><span class="p">).</span><span class="nb">all</span><span class="p">():</span>
                <span class="k">break</span>

    <span class="k">return</span> <span class="n">generated</span>
</code></pre></div></div>

<hr />

<h3 id="training-script">Putting It All Together: The Training Script</h3>

<p>Now we assemble all components into a production training script. The key is making everything configurable via command-line arguments.</p>

<h4 id="command-line-interface">Command-Line Interface</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">parse_args</span><span class="p">():</span>
    <span class="n">parser</span> <span class="o">=</span> <span class="n">argparse</span><span class="p">.</span><span class="n">ArgumentParser</span><span class="p">(</span><span class="n">description</span><span class="o">=</span><span class="s">"Train a Transformer language model"</span><span class="p">)</span>

    <span class="c1"># Data
</span>    <span class="n">parser</span><span class="p">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s">"--train_data"</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span> <span class="n">required</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="n">parser</span><span class="p">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s">"--val_data"</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span> <span class="n">required</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="n">parser</span><span class="p">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s">"--vocab_size"</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">required</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

    <span class="c1"># Model architecture
</span>    <span class="n">parser</span><span class="p">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s">"--d_model"</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">768</span><span class="p">)</span>
    <span class="n">parser</span><span class="p">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s">"--num_layers"</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
    <span class="n">parser</span><span class="p">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s">"--num_heads"</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
    <span class="n">parser</span><span class="p">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s">"--d_ff"</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">3072</span><span class="p">)</span>
    <span class="n">parser</span><span class="p">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s">"--context_length"</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">512</span><span class="p">)</span>

    <span class="c1"># Training
</span>    <span class="n">parser</span><span class="p">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s">"--batch_size"</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">32</span><span class="p">)</span>
    <span class="n">parser</span><span class="p">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s">"--max_iters"</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">100000</span><span class="p">)</span>
    <span class="n">parser</span><span class="p">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s">"--gradient_accumulation_steps"</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="c1"># Optimizer
</span>    <span class="n">parser</span><span class="p">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s">"--lr"</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">float</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">)</span>
    <span class="n">parser</span><span class="p">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s">"--min_lr"</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">float</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">)</span>
    <span class="n">parser</span><span class="p">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s">"--weight_decay"</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">float</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
    <span class="n">parser</span><span class="p">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s">"--grad_clip"</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">float</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>

    <span class="c1"># Learning rate schedule
</span>    <span class="n">parser</span><span class="p">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s">"--warmup_iters"</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">2000</span><span class="p">)</span>
    <span class="n">parser</span><span class="p">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s">"--lr_decay_iters"</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">100000</span><span class="p">)</span>

    <span class="c1"># Logging and checkpointing
</span>    <span class="n">parser</span><span class="p">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s">"--eval_interval"</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">500</span><span class="p">)</span>
    <span class="n">parser</span><span class="p">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s">"--log_interval"</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">parser</span><span class="p">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s">"--checkpoint_interval"</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">5000</span><span class="p">)</span>
    <span class="n">parser</span><span class="p">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s">"--checkpoint_dir"</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="s">"checkpoints"</span><span class="p">)</span>

    <span class="c1"># Resume
</span>    <span class="n">parser</span><span class="p">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s">"--resume_from"</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">parser</span><span class="p">.</span><span class="n">parse_args</span><span class="p">()</span>
</code></pre></div></div>

<h4 id="understanding-key-training-parameters">Understanding Key Training Parameters</h4>

<p>Before diving into the training loop, letâ€™s clarify two important hyperparameters that control how training progresses:</p>

<p><strong>max_iters (Maximum Iterations)</strong></p>

<p>The total number of training steps (iterations) to run.</p>

<p><strong>One iteration</strong> = one forward pass + one backward pass + one optimizer step</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span> <span class="n">iter_num</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_iters</span><span class="p">):</span>  <span class="c1"># e.g., 100,000 steps
</span>    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">get_batch</span><span class="p">(...)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">loss</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="p">.</span><span class="n">step</span><span class="p">()</span>
</code></pre></div></div>

<p><strong>Example:</strong></p>
<ul>
  <li>If <code class="language-plaintext highlighter-rouge">max_iters = 100,000</code> and <code class="language-plaintext highlighter-rouge">batch_size = 32</code>:</li>
  <li>Model will train for 100,000 steps</li>
  <li>Each step processes 32 examples</li>
  <li>Total examples seen = 100,000 Ã— 32 = 3,200,000 (with repetition if dataset is smaller)</li>
</ul>

<p><strong>gradient_accumulation_steps (Gradient Accumulation)</strong></p>

<p>The number of mini-batches to accumulate gradients over before updating weights.</p>

<p><strong>Why use it?</strong> To simulate larger batch sizes when GPU memory is limited.</p>

<p><strong>Without gradient accumulation</strong> (standard training):</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Effective batch size = 32
</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">get_batch</span><span class="p">(</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">)</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">loss</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span>      <span class="c1"># Compute gradients
</span><span class="n">optimizer</span><span class="p">.</span><span class="n">step</span><span class="p">()</span>     <span class="c1"># Update weights immediately
</span></code></pre></div></div>

<p><strong>With gradient accumulation</strong> (e.g., <code class="language-plaintext highlighter-rouge">gradient_accumulation_steps = 4</code>):</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Effective batch size = 32 Ã— 4 = 128
</span><span class="n">total_loss</span> <span class="o">=</span> <span class="mf">0.0</span>
<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">4</span><span class="p">):</span>  <span class="c1"># Accumulate over 4 mini-batches
</span>    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">get_batch</span><span class="p">(</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span> <span class="o">/</span> <span class="mi">4</span>  <span class="c1"># Scale loss to average over accumulation
</span>    <span class="n">loss</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span>  <span class="c1"># Accumulate gradients (don't update yet!)
</span>    <span class="n">total_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="p">.</span><span class="n">item</span><span class="p">()</span>

<span class="n">optimizer</span><span class="p">.</span><span class="n">step</span><span class="p">()</span>     <span class="c1"># Now update with accumulated gradients
</span><span class="n">optimizer</span><span class="p">.</span><span class="n">zero_grad</span><span class="p">()</span>
</code></pre></div></div>

<p><strong>Key benefits:</strong></p>
<ol>
  <li><strong>Simulate larger batches</strong>: Want batch_size=128 but only have memory for 32? Use <code class="language-plaintext highlighter-rouge">gradient_accumulation_steps=4</code></li>
  <li><strong>Effective batch size</strong> = <code class="language-plaintext highlighter-rouge">batch_size Ã— gradient_accumulation_steps</code></li>
  <li><strong>Smoother gradients</strong>: Larger effective batches lead to more stable training</li>
</ol>

<h4 id="main-training-loop">Main Training Loop</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">main</span><span class="p">():</span>
    <span class="n">args</span> <span class="o">=</span> <span class="n">parse_args</span><span class="p">()</span>

    <span class="c1"># Create checkpoint directory
</span>    <span class="n">os</span><span class="p">.</span><span class="n">makedirs</span><span class="p">(</span><span class="n">args</span><span class="p">.</span><span class="n">checkpoint_dir</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

    <span class="c1"># Load datasets with memory mapping
</span>    <span class="n">train_data</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span><span class="n">args</span><span class="p">.</span><span class="n">train_data</span><span class="p">,</span> <span class="n">args</span><span class="p">.</span><span class="n">vocab_size</span><span class="p">)</span>
    <span class="n">val_data</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span><span class="n">args</span><span class="p">.</span><span class="n">val_data</span><span class="p">,</span> <span class="n">args</span><span class="p">.</span><span class="n">vocab_size</span><span class="p">)</span>

    <span class="c1"># Initialize model
</span>    <span class="n">model</span> <span class="o">=</span> <span class="n">TransformerLM</span><span class="p">(</span>
        <span class="n">vocab_size</span><span class="o">=</span><span class="n">args</span><span class="p">.</span><span class="n">vocab_size</span><span class="p">,</span>
        <span class="n">context_length</span><span class="o">=</span><span class="n">args</span><span class="p">.</span><span class="n">context_length</span><span class="p">,</span>
        <span class="n">d_model</span><span class="o">=</span><span class="n">args</span><span class="p">.</span><span class="n">d_model</span><span class="p">,</span>
        <span class="n">num_layers</span><span class="o">=</span><span class="n">args</span><span class="p">.</span><span class="n">num_layers</span><span class="p">,</span>
        <span class="n">num_heads</span><span class="o">=</span><span class="n">args</span><span class="p">.</span><span class="n">num_heads</span><span class="p">,</span>
        <span class="n">d_ff</span><span class="o">=</span><span class="n">args</span><span class="p">.</span><span class="n">d_ff</span><span class="p">,</span>
    <span class="p">).</span><span class="n">to</span><span class="p">(</span><span class="n">args</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>

    <span class="c1"># Store model configuration for checkpoints
</span>    <span class="n">model_config</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s">'vocab_size'</span><span class="p">:</span> <span class="n">args</span><span class="p">.</span><span class="n">vocab_size</span><span class="p">,</span>
        <span class="s">'d_model'</span><span class="p">:</span> <span class="n">args</span><span class="p">.</span><span class="n">d_model</span><span class="p">,</span>
        <span class="s">'num_layers'</span><span class="p">:</span> <span class="n">args</span><span class="p">.</span><span class="n">num_layers</span><span class="p">,</span>
        <span class="s">'num_heads'</span><span class="p">:</span> <span class="n">args</span><span class="p">.</span><span class="n">num_heads</span><span class="p">,</span>
        <span class="s">'d_ff'</span><span class="p">:</span> <span class="n">args</span><span class="p">.</span><span class="n">d_ff</span><span class="p">,</span>
        <span class="s">'context_length'</span><span class="p">:</span> <span class="n">args</span><span class="p">.</span><span class="n">context_length</span><span class="p">,</span>
    <span class="p">}</span>

    <span class="c1"># Initialize optimizer
</span>    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">AdamW</span><span class="p">(</span>
        <span class="n">model</span><span class="p">.</span><span class="n">parameters</span><span class="p">(),</span>
        <span class="n">lr</span><span class="o">=</span><span class="n">args</span><span class="p">.</span><span class="n">lr</span><span class="p">,</span>
        <span class="n">betas</span><span class="o">=</span><span class="p">(</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.95</span><span class="p">),</span>
        <span class="n">weight_decay</span><span class="o">=</span><span class="n">args</span><span class="p">.</span><span class="n">weight_decay</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="c1"># Resume from checkpoint if specified
</span>    <span class="n">start_iter</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">if</span> <span class="n">args</span><span class="p">.</span><span class="n">resume_from</span><span class="p">:</span>
        <span class="n">start_iter</span> <span class="o">=</span> <span class="n">load_checkpoint</span><span class="p">(</span><span class="n">args</span><span class="p">.</span><span class="n">resume_from</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Resumed from iteration </span><span class="si">{</span><span class="n">start_iter</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>

    <span class="c1"># Training loop
</span>    <span class="n">model</span><span class="p">.</span><span class="n">train</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">iter_num</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">start_iter</span><span class="p">,</span> <span class="n">args</span><span class="p">.</span><span class="n">max_iters</span><span class="p">):</span>
        <span class="c1"># Get learning rate for this iteration
</span>        <span class="n">lr</span> <span class="o">=</span> <span class="n">get_lr_cosine_schedule</span><span class="p">(</span>
            <span class="n">iter_num</span><span class="p">,</span>
            <span class="n">max_learning_rate</span><span class="o">=</span><span class="n">args</span><span class="p">.</span><span class="n">lr</span><span class="p">,</span>
            <span class="n">min_learning_rate</span><span class="o">=</span><span class="n">args</span><span class="p">.</span><span class="n">min_lr</span><span class="p">,</span>
            <span class="n">warmup_iters</span><span class="o">=</span><span class="n">args</span><span class="p">.</span><span class="n">warmup_iters</span><span class="p">,</span>
            <span class="n">cosine_cycle_iters</span><span class="o">=</span><span class="n">args</span><span class="p">.</span><span class="n">lr_decay_iters</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="c1"># Update learning rate in optimizer
</span>        <span class="k">for</span> <span class="n">param_group</span> <span class="ow">in</span> <span class="n">optimizer</span><span class="p">.</span><span class="n">param_groups</span><span class="p">:</span>
            <span class="n">param_group</span><span class="p">[</span><span class="s">'lr'</span><span class="p">]</span> <span class="o">=</span> <span class="n">lr</span>

        <span class="c1"># Training step with gradient accumulation
</span>        <span class="n">total_loss</span> <span class="o">=</span> <span class="mf">0.0</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">args</span><span class="p">.</span><span class="n">gradient_accumulation_steps</span><span class="p">):</span>
            <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">get_batch</span><span class="p">(</span><span class="n">train_data</span><span class="p">,</span> <span class="n">args</span><span class="p">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">args</span><span class="p">.</span><span class="n">context_length</span><span class="p">,</span> <span class="n">args</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>
            <span class="n">logits</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">apply_softmax</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">cross_entropy</span><span class="p">(</span><span class="n">logits</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">logits</span><span class="p">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)),</span> <span class="n">y</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span> <span class="o">/</span> <span class="n">args</span><span class="p">.</span><span class="n">gradient_accumulation_steps</span>
            <span class="n">loss</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span>
            <span class="n">total_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="p">.</span><span class="n">item</span><span class="p">()</span>

        <span class="c1"># Gradient clipping
</span>        <span class="k">if</span> <span class="n">args</span><span class="p">.</span><span class="n">grad_clip</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="n">clip_grad_norm_</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">args</span><span class="p">.</span><span class="n">grad_clip</span><span class="p">)</span>

        <span class="c1"># Optimizer step
</span>        <span class="n">optimizer</span><span class="p">.</span><span class="n">step</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="p">.</span><span class="n">zero_grad</span><span class="p">()</span>

        <span class="c1"># Logging
</span>        <span class="k">if</span> <span class="n">iter_num</span> <span class="o">%</span> <span class="n">args</span><span class="p">.</span><span class="n">log_interval</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"[</span><span class="si">{</span><span class="n">iter_num</span><span class="si">}</span><span class="s">/</span><span class="si">{</span><span class="n">args</span><span class="p">.</span><span class="n">max_iters</span><span class="si">}</span><span class="s">] loss: </span><span class="si">{</span><span class="n">total_loss</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="s"> | lr: </span><span class="si">{</span><span class="n">lr</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">e</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>

        <span class="c1"># Evaluation
</span>        <span class="k">if</span> <span class="n">iter_num</span> <span class="o">%</span> <span class="n">args</span><span class="p">.</span><span class="n">eval_interval</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">val_loss</span> <span class="o">=</span> <span class="n">evaluate</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">val_data</span><span class="p">,</span> <span class="n">args</span><span class="p">)</span>
            <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"[</span><span class="si">{</span><span class="n">iter_num</span><span class="si">}</span><span class="s">] val_loss: </span><span class="si">{</span><span class="n">val_loss</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>

        <span class="c1"># Save checkpoint
</span>        <span class="k">if</span> <span class="n">iter_num</span> <span class="o">%</span> <span class="n">args</span><span class="p">.</span><span class="n">checkpoint_interval</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">iter_num</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">checkpoint_path</span> <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">args</span><span class="p">.</span><span class="n">checkpoint_dir</span><span class="p">,</span> <span class="sa">f</span><span class="s">"checkpoint_iter_</span><span class="si">{</span><span class="n">iter_num</span><span class="si">}</span><span class="s">.pt"</span><span class="p">)</span>
            <span class="n">save_checkpoint</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">iter_num</span><span class="p">,</span> <span class="n">checkpoint_path</span><span class="p">,</span> <span class="n">model_config</span><span class="p">)</span>

    <span class="c1"># Save final checkpoint
</span>    <span class="n">final_checkpoint</span> <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">args</span><span class="p">.</span><span class="n">checkpoint_dir</span><span class="p">,</span> <span class="s">"checkpoint_final.pt"</span><span class="p">)</span>
    <span class="n">save_checkpoint</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">args</span><span class="p">.</span><span class="n">max_iters</span><span class="p">,</span> <span class="n">final_checkpoint</span><span class="p">,</span> <span class="n">model_config</span><span class="p">)</span>
</code></pre></div></div>

<h4 id="usage">Usage</h4>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Train from scratch</span>
python <span class="nt">-m</span> cs336_basics.train <span class="se">\</span>
    <span class="nt">--train_data</span> data/train.npy <span class="se">\</span>
    <span class="nt">--val_data</span> data/val.npy <span class="se">\</span>
    <span class="nt">--vocab_size</span> 50257 <span class="se">\</span>
    <span class="nt">--d_model</span> 768 <span class="se">\</span>
    <span class="nt">--num_layers</span> 12 <span class="se">\</span>
    <span class="nt">--num_heads</span> 12 <span class="se">\</span>
    <span class="nt">--d_ff</span> 3072 <span class="se">\</span>
    <span class="nt">--batch_size</span> 32 <span class="se">\</span>
    <span class="nt">--max_iters</span> 100000 <span class="se">\</span>
    <span class="nt">--lr</span> 1e-3 <span class="se">\</span>
    <span class="nt">--warmup_iters</span> 2000

<span class="c"># Resume from checkpoint</span>
python <span class="nt">-m</span> cs336_basics.train <span class="se">\</span>
    <span class="nt">--train_data</span> data/train.npy <span class="se">\</span>
    <span class="nt">--val_data</span> data/val.npy <span class="se">\</span>
    <span class="nt">--vocab_size</span> 50257 <span class="se">\</span>
    <span class="nt">--resume_from</span> checkpoints/checkpoint_iter_50000.pt
</code></pre></div></div>

<hr />

<h3 id="testing">Testing and Validation</h3>

<p>Production systems require comprehensive testing. Hereâ€™s how we can validate our training pipeline to ensure correctness before launching expensive, multi-day training runs.</p>

<h4 id="unit-tests-for-components">Unit Tests for Components</h4>

<p>Each component should have its own unit tests to verify correctness in isolation.</p>

<p><strong>Test AdamW Optimizer:</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">test_adamw</span><span class="p">():</span>
    <span class="s">"""Test AdamW matches reference implementation."""</span>
    <span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>

    <span class="c1"># Create simple model
</span>    <span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">AdamW</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>

    <span class="c1"># Create dummy data
</span>    <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>

    <span class="c1"># Training step
</span>    <span class="n">loss</span> <span class="o">=</span> <span class="p">((</span><span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">).</span><span class="n">mean</span><span class="p">()</span>
    <span class="n">loss</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="p">.</span><span class="n">step</span><span class="p">()</span>

    <span class="c1"># Verify weights were updated
</span>    <span class="k">assert</span> <span class="n">loss</span><span class="p">.</span><span class="n">item</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">0</span>  <span class="c1"># Loss should be non-zero
</span>    <span class="c1"># Compare against PyTorch's implementation for exact match
</span></code></pre></div></div>

<p><strong>Test Learning Rate Schedule:</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">test_learning_rate_schedule</span><span class="p">():</span>
    <span class="s">"""Test learning rate schedule matches specification."""</span>
    <span class="n">max_lr</span> <span class="o">=</span> <span class="mf">1.0</span>
    <span class="n">min_lr</span> <span class="o">=</span> <span class="mf">0.1</span>
    <span class="n">warmup_iters</span> <span class="o">=</span> <span class="mi">100</span>
    <span class="n">cosine_cycle_iters</span> <span class="o">=</span> <span class="mi">1000</span>

    <span class="c1"># Test warmup phase
</span>    <span class="n">lr_start</span> <span class="o">=</span> <span class="n">get_lr_cosine_schedule</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">max_lr</span><span class="p">,</span> <span class="n">min_lr</span><span class="p">,</span> <span class="n">warmup_iters</span><span class="p">,</span> <span class="n">cosine_cycle_iters</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">lr_start</span> <span class="o">==</span> <span class="mf">0.0</span><span class="p">,</span> <span class="s">"LR should start at 0"</span>

    <span class="n">lr_mid_warmup</span> <span class="o">=</span> <span class="n">get_lr_cosine_schedule</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="n">max_lr</span><span class="p">,</span> <span class="n">min_lr</span><span class="p">,</span> <span class="n">warmup_iters</span><span class="p">,</span> <span class="n">cosine_cycle_iters</span><span class="p">)</span>
    <span class="k">assert</span> <span class="nb">abs</span><span class="p">(</span><span class="n">lr_mid_warmup</span> <span class="o">-</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">max_lr</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mf">1e-6</span><span class="p">,</span> <span class="s">"LR should be halfway at warmup midpoint"</span>

    <span class="n">lr_end_warmup</span> <span class="o">=</span> <span class="n">get_lr_cosine_schedule</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">max_lr</span><span class="p">,</span> <span class="n">min_lr</span><span class="p">,</span> <span class="n">warmup_iters</span><span class="p">,</span> <span class="n">cosine_cycle_iters</span><span class="p">)</span>
    <span class="k">assert</span> <span class="nb">abs</span><span class="p">(</span><span class="n">lr_end_warmup</span> <span class="o">-</span> <span class="n">max_lr</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mf">1e-6</span><span class="p">,</span> <span class="s">"LR should be max at end of warmup"</span>

    <span class="c1"># Test cosine phase
</span>    <span class="n">lr_mid_cosine</span> <span class="o">=</span> <span class="n">get_lr_cosine_schedule</span><span class="p">(</span><span class="mi">550</span><span class="p">,</span> <span class="n">max_lr</span><span class="p">,</span> <span class="n">min_lr</span><span class="p">,</span> <span class="n">warmup_iters</span><span class="p">,</span> <span class="n">cosine_cycle_iters</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">min_lr</span> <span class="o">&lt;</span> <span class="n">lr_mid_cosine</span> <span class="o">&lt;</span> <span class="n">max_lr</span><span class="p">,</span> <span class="s">"LR should be decaying in cosine phase"</span>

    <span class="n">lr_end_cosine</span> <span class="o">=</span> <span class="n">get_lr_cosine_schedule</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="n">max_lr</span><span class="p">,</span> <span class="n">min_lr</span><span class="p">,</span> <span class="n">warmup_iters</span><span class="p">,</span> <span class="n">cosine_cycle_iters</span><span class="p">)</span>
    <span class="k">assert</span> <span class="nb">abs</span><span class="p">(</span><span class="n">lr_end_cosine</span> <span class="o">-</span> <span class="n">min_lr</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mf">1e-6</span><span class="p">,</span> <span class="s">"LR should be min at end of cosine"</span>

    <span class="c1"># Test constant phase
</span>    <span class="n">lr_after</span> <span class="o">=</span> <span class="n">get_lr_cosine_schedule</span><span class="p">(</span><span class="mi">1500</span><span class="p">,</span> <span class="n">max_lr</span><span class="p">,</span> <span class="n">min_lr</span><span class="p">,</span> <span class="n">warmup_iters</span><span class="p">,</span> <span class="n">cosine_cycle_iters</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">lr_after</span> <span class="o">==</span> <span class="n">min_lr</span><span class="p">,</span> <span class="s">"LR should remain at min after cosine phase"</span>
</code></pre></div></div>

<p><strong>Test Top-p Sampling:</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">test_top_p_sampling</span><span class="p">():</span>
    <span class="s">"""Test top-p sampling filters correctly."""</span>
    <span class="kn">import</span> <span class="nn">torch</span>

    <span class="n">probs</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.05</span><span class="p">,</span> <span class="mf">0.05</span><span class="p">]])</span>
    <span class="n">filtered</span> <span class="o">=</span> <span class="n">top_p_sampling</span><span class="p">(</span><span class="n">probs</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>

    <span class="c1"># Should keep only top 2 tokens (0.5 + 0.3 = 0.8)
</span>    <span class="k">assert</span> <span class="p">(</span><span class="n">filtered</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:</span><span class="mi">2</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">).</span><span class="nb">all</span><span class="p">(),</span> <span class="s">"Top 2 tokens should have non-zero probability"</span>
    <span class="k">assert</span> <span class="p">(</span><span class="n">filtered</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">:]</span> <span class="o">==</span> <span class="mi">0</span><span class="p">).</span><span class="nb">all</span><span class="p">(),</span> <span class="s">"Remaining tokens should be filtered out"</span>

    <span class="c1"># Should be renormalized
</span>    <span class="k">assert</span> <span class="n">torch</span><span class="p">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">filtered</span><span class="p">.</span><span class="nb">sum</span><span class="p">(),</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1.0</span><span class="p">)),</span> <span class="s">"Probabilities should sum to 1"</span>

    <span class="c1"># Check renormalization is correct
</span>    <span class="n">expected</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">0.625</span><span class="p">,</span> <span class="mf">0.375</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">]])</span>
    <span class="k">assert</span> <span class="n">torch</span><span class="p">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">filtered</span><span class="p">,</span> <span class="n">expected</span><span class="p">,</span> <span class="n">atol</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">),</span> <span class="s">"Renormalization should be correct"</span>
</code></pre></div></div>

<p><strong>Test Temperature Scaling:</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">test_temperature_scaling</span><span class="p">():</span>
    <span class="s">"""Test temperature scaling affects distribution correctly."""</span>
    <span class="kn">import</span> <span class="nn">torch</span>

    <span class="n">logits</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">]])</span>

    <span class="c1"># Standard softmax
</span>    <span class="n">probs_normal</span> <span class="o">=</span> <span class="n">softmax_with_temperature</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>

    <span class="c1"># Low temperature (sharper)
</span>    <span class="n">probs_sharp</span> <span class="o">=</span> <span class="n">softmax_with_temperature</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">probs_sharp</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">probs_normal</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="s">"Low temp should increase max probability"</span>

    <span class="c1"># High temperature (flatter)
</span>    <span class="n">probs_flat</span> <span class="o">=</span> <span class="n">softmax_with_temperature</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="mf">2.0</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">probs_flat</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">&lt;</span> <span class="n">probs_normal</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="s">"High temp should decrease max probability"</span>
</code></pre></div></div>

<h4 id="integration-test">Integration Test</h4>

<p>Test the entire training pipeline end-to-end with a small synthetic dataset.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">test_training_integration</span><span class="p">():</span>
    <span class="s">"""End-to-end test of training pipeline."""</span>
    <span class="kn">import</span> <span class="nn">tempfile</span>
    <span class="kn">import</span> <span class="nn">subprocess</span>
    <span class="kn">import</span> <span class="nn">os</span>
    <span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>

    <span class="c1"># Create small synthetic dataset
</span>    <span class="n">vocab_size</span> <span class="o">=</span> <span class="mi">1000</span>
    <span class="n">train_data</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">int64</span><span class="p">)</span>
    <span class="n">val_data</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">int64</span><span class="p">)</span>

    <span class="c1"># Save to temporary files
</span>    <span class="k">with</span> <span class="n">tempfile</span><span class="p">.</span><span class="n">TemporaryDirectory</span><span class="p">()</span> <span class="k">as</span> <span class="n">tmpdir</span><span class="p">:</span>
        <span class="n">train_path</span> <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">tmpdir</span><span class="p">,</span> <span class="s">"train.npy"</span><span class="p">)</span>
        <span class="n">val_path</span> <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">tmpdir</span><span class="p">,</span> <span class="s">"val.npy"</span><span class="p">)</span>
        <span class="n">checkpoint_dir</span> <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">tmpdir</span><span class="p">,</span> <span class="s">"checkpoints"</span><span class="p">)</span>

        <span class="n">np</span><span class="p">.</span><span class="n">save</span><span class="p">(</span><span class="n">train_path</span><span class="p">,</span> <span class="n">train_data</span><span class="p">)</span>
        <span class="n">np</span><span class="p">.</span><span class="n">save</span><span class="p">(</span><span class="n">val_path</span><span class="p">,</span> <span class="n">val_data</span><span class="p">)</span>
        <span class="n">os</span><span class="p">.</span><span class="n">makedirs</span><span class="p">(</span><span class="n">checkpoint_dir</span><span class="p">)</span>

        <span class="c1"># Run training for 10 iterations
</span>        <span class="n">result</span> <span class="o">=</span> <span class="n">subprocess</span><span class="p">.</span><span class="n">run</span><span class="p">([</span>
            <span class="s">"python"</span><span class="p">,</span> <span class="s">"-m"</span><span class="p">,</span> <span class="s">"cs336_basics.train"</span><span class="p">,</span>
            <span class="s">"--train_data"</span><span class="p">,</span> <span class="n">train_path</span><span class="p">,</span>
            <span class="s">"--val_data"</span><span class="p">,</span> <span class="n">val_path</span><span class="p">,</span>
            <span class="s">"--vocab_size"</span><span class="p">,</span> <span class="nb">str</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">),</span>
            <span class="s">"--d_model"</span><span class="p">,</span> <span class="s">"128"</span><span class="p">,</span>
            <span class="s">"--num_layers"</span><span class="p">,</span> <span class="s">"2"</span><span class="p">,</span>
            <span class="s">"--num_heads"</span><span class="p">,</span> <span class="s">"4"</span><span class="p">,</span>
            <span class="s">"--d_ff"</span><span class="p">,</span> <span class="s">"512"</span><span class="p">,</span>
            <span class="s">"--max_iters"</span><span class="p">,</span> <span class="s">"10"</span><span class="p">,</span>
            <span class="s">"--checkpoint_interval"</span><span class="p">,</span> <span class="s">"10"</span><span class="p">,</span>
            <span class="s">"--checkpoint_dir"</span><span class="p">,</span> <span class="n">checkpoint_dir</span><span class="p">,</span>
        <span class="p">],</span> <span class="n">check</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">capture_output</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">text</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

        <span class="c1"># Verify checkpoint was created
</span>        <span class="n">checkpoint_path</span> <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">checkpoint_dir</span><span class="p">,</span> <span class="s">"checkpoint_final.pt"</span><span class="p">)</span>
        <span class="k">assert</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="n">exists</span><span class="p">(</span><span class="n">checkpoint_path</span><span class="p">),</span> <span class="s">"Final checkpoint should be created"</span>

        <span class="c1"># Test checkpoint loading
</span>        <span class="n">checkpoint</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="n">checkpoint_path</span><span class="p">)</span>
        <span class="k">assert</span> <span class="s">"model_state_dict"</span> <span class="ow">in</span> <span class="n">checkpoint</span>
        <span class="k">assert</span> <span class="s">"optimizer_state_dict"</span> <span class="ow">in</span> <span class="n">checkpoint</span>
        <span class="k">assert</span> <span class="s">"iteration"</span> <span class="ow">in</span> <span class="n">checkpoint</span>
        <span class="k">assert</span> <span class="n">checkpoint</span><span class="p">[</span><span class="s">"iteration"</span><span class="p">]</span> <span class="o">==</span> <span class="mi">10</span>

        <span class="k">print</span><span class="p">(</span><span class="s">"âœ“ Training ran successfully and created checkpoint"</span><span class="p">)</span>

        <span class="c1"># Test resumption from checkpoint
</span>        <span class="n">result</span> <span class="o">=</span> <span class="n">subprocess</span><span class="p">.</span><span class="n">run</span><span class="p">([</span>
            <span class="s">"python"</span><span class="p">,</span> <span class="s">"-m"</span><span class="p">,</span> <span class="s">"cs336_basics.train"</span><span class="p">,</span>
            <span class="s">"--train_data"</span><span class="p">,</span> <span class="n">train_path</span><span class="p">,</span>
            <span class="s">"--val_data"</span><span class="p">,</span> <span class="n">val_path</span><span class="p">,</span>
            <span class="s">"--vocab_size"</span><span class="p">,</span> <span class="nb">str</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">),</span>
            <span class="s">"--d_model"</span><span class="p">,</span> <span class="s">"128"</span><span class="p">,</span>
            <span class="s">"--num_layers"</span><span class="p">,</span> <span class="s">"2"</span><span class="p">,</span>
            <span class="s">"--max_iters"</span><span class="p">,</span> <span class="s">"15"</span><span class="p">,</span>
            <span class="s">"--checkpoint_dir"</span><span class="p">,</span> <span class="n">checkpoint_dir</span><span class="p">,</span>
            <span class="s">"--resume_from"</span><span class="p">,</span> <span class="n">checkpoint_path</span><span class="p">,</span>
        <span class="p">],</span> <span class="n">check</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">capture_output</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">text</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

        <span class="c1"># Verify training continued from iteration 10
</span>        <span class="k">assert</span> <span class="s">"Resumed from iteration 10"</span> <span class="ow">in</span> <span class="n">result</span><span class="p">.</span><span class="n">stdout</span>

        <span class="k">print</span><span class="p">(</span><span class="s">"âœ“ Training resumed successfully from checkpoint"</span><span class="p">)</span>
</code></pre></div></div>

<h4 id="pre-training-validation-checklist">Pre-Training Validation Checklist</h4>

<p>Before launching a long training run, verify:</p>

<ol>
  <li><strong>Loss decreases on small data</strong>: Train for 100 iterations on a tiny dataset and verify loss goes down</li>
  <li><strong>Checkpoints save/load correctly</strong>: Save and load a checkpoint, verify iteration count and loss match</li>
  <li><strong>Learning rate schedule looks correct</strong>: Plot the LR over iterations and verify the curve matches expectations</li>
  <li><strong>Memory usage is reasonable</strong>: Monitor GPU memory and ensure it doesnâ€™t exceed available capacity</li>
  <li><strong>Data loading works</strong>: Verify data batches have correct shape and token values are in valid range</li>
  <li><strong>Gradient norms are stable</strong>: Log gradient norms during warmup, verify they decrease and donâ€™t explode</li>
</ol>

<p><strong>Quick validation script:</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Quick 100-iteration validation run
</span><span class="n">python</span> <span class="o">-</span><span class="n">m</span> <span class="n">cs336_basics</span><span class="p">.</span><span class="n">train</span> \
    <span class="o">--</span><span class="n">train_data</span> <span class="n">data</span><span class="o">/</span><span class="n">train</span><span class="p">.</span><span class="n">npy</span> \
    <span class="o">--</span><span class="n">val_data</span> <span class="n">data</span><span class="o">/</span><span class="n">val</span><span class="p">.</span><span class="n">npy</span> \
    <span class="o">--</span><span class="n">vocab_size</span> <span class="mi">50257</span> \
    <span class="o">--</span><span class="n">max_iters</span> <span class="mi">100</span> \
    <span class="o">--</span><span class="n">log_interval</span> <span class="mi">10</span> \
    <span class="o">--</span><span class="n">checkpoint_interval</span> <span class="mi">50</span>

<span class="c1"># Expected output:
# [0/100] loss: 10.8234 | lr: 0.00e+00    (high initial loss)
# [10/100] loss: 9.2156 | lr: 5.00e-05   (loss decreasing)
# [50/100] loss: 7.8901 | lr: 2.50e-04   (loss continuing to decrease)
# [100/100] loss: 6.5432 | lr: 5.00e-04  (loss still decreasing)
</span></code></pre></div></div>

<p>If loss doesnâ€™t decrease in 100 iterations, something is wrongâ€”debug before launching a long run!</p>

<hr />

<h3 id="takeaways">Key Takeaways</h3>

<p>Building a production training pipeline requires attention to many details beyond the core model architecture. Here are the essential lessons:</p>

<h4 id="1-correctness-over-convenience">1. <strong>Correctness Over Convenience</strong></h4>
<p>Follow paper specifications exactly, especially for:</p>
<ul>
  <li>Optimizer algorithms (AdamWâ€™s decoupled weight decay)</li>
  <li>Learning rate schedules (strict inequalities matter)</li>
  <li>Bias correction formulas</li>
</ul>

<p>Small deviations can cause subtle training instabilities that only appear after days of training.</p>

<h4 id="2-memory-efficiency-is-critical">2. <strong>Memory Efficiency Is Critical</strong></h4>
<p>For large-scale training:</p>
<ul>
  <li>Use memory-mapped arrays for datasets larger than RAM</li>
  <li>Monitor peak memory usage during training</li>
  <li>Consider gradient checkpointing for very large models</li>
</ul>

<h4 id="3-checkpoint-everything">3. <strong>Checkpoint Everything</strong></h4>
<p>A complete checkpoint includes:</p>
<ul>
  <li>Model parameters</li>
  <li>Optimizer state (momentum buffers!)</li>
  <li>Iteration count</li>
  <li>Model configuration</li>
  <li>Random seeds (for reproducibility)</li>
</ul>

<p>Donâ€™t learn this lesson the hard way after losing a week of training.</p>

<h4 id="4-make-everything-configurable">4. <strong>Make Everything Configurable</strong></h4>
<p>Use command-line arguments for all hyperparameters:</p>
<ul>
  <li>Enables systematic hyperparameter sweeps</li>
  <li>Makes it easy to resume with different settings</li>
  <li>Documents what settings were used</li>
</ul>

<h4 id="5-test-before-long-training-runs">5. <strong>Test Before Long Training Runs</strong></h4>
<ul>
  <li>Run integration tests on small synthetic data</li>
  <li>Train for 100 iterations and verify:
    <ul>
      <li>Loss decreases</li>
      <li>Checkpoints save/load correctly</li>
      <li>Learning rate schedule looks correct</li>
      <li>Memory usage is reasonable</li>
    </ul>
  </li>
</ul>

<p>A 10-minute test can save days of wasted compute.</p>

<h4 id="6-generation-quality-depends-on-decoding">6. <strong>Generation Quality Depends on Decoding</strong></h4>
<p>Even a well-trained model can produce poor text with bad decoding settings:</p>
<ul>
  <li>Start with <code class="language-plaintext highlighter-rouge">temperature=0.8, top_p=0.9</code></li>
  <li>Adjust based on task (lower temperature for factual, higher for creative)</li>
  <li>Always use some form of sampling (greedy decoding produces repetitive text)</li>
</ul>

<h4 id="7-monitor-training-actively">7. <strong>Monitor Training Actively</strong></h4>
<p>Log frequently and watch for:</p>
<ul>
  <li>Loss spikes (may indicate learning rate too high)</li>
  <li>Loss plateaus (may need more data or capacity)</li>
  <li>Gradient norms (should decrease during warmup)</li>
  <li>Generation samples (qualitative assessment)</li>
</ul>

<h4 id="8-production-code-is-different">8. <strong>Production Code Is Different</strong></h4>
<p>Research code can get away with:</p>
<ul>
  <li>Hardcoded hyperparameters</li>
  <li>No checkpointing</li>
  <li>Single-file scripts</li>
</ul>

<p>Production code needs:</p>
<ul>
  <li>Configuration management</li>
  <li>Robust error handling</li>
  <li>Comprehensive logging</li>
  <li>Restart/resume capability</li>
</ul>

<p>This note covered the engineering necessary to turn research ideas into a working system. The components we builtâ€”AdamW optimizer, cosine schedule, memory-mapped data loading, checkpointing, and decoding strategiesâ€”form the foundation of modern LLM training pipelines. These same patterns appear in systems like GPT-3, LLaMA, and other large language models.</p>

<p>The next step is scaling: distributed training across multiple GPUs, larger datasets, and bigger models. But the fundamentals remain the same: correct implementations of proven algorithms, careful attention to numerical stability, and robust engineering practices.</p>]]></content><author><name>[&quot;Han Yu&quot;]</name></author><category term="cs336" /><summary type="html"><![CDATA[Building a Complete Training Loop]]></summary></entry><entry><title type="html">Study Notes: Stanford CS336 Language Modeling from Scratch [9]</title><link href="http://localhost:4000/cs336/2025/10/19/cs336-implement-softmax-log_softmax-cross_entropy.html" rel="alternate" type="text/html" title="Study Notes: Stanford CS336 Language Modeling from Scratch [9]" /><published>2025-10-19T00:00:00-07:00</published><updated>2025-10-19T00:00:00-07:00</updated><id>http://localhost:4000/cs336/2025/10/19/cs336-implement-softmax-log_softmax-cross_entropy</id><content type="html" xml:base="http://localhost:4000/cs336/2025/10/19/cs336-implement-softmax-log_softmax-cross_entropy.html"><![CDATA[<h2 id="understanding-softmax-log-softmax-and-cross-entropy-a-complete-implementation-guide">Understanding Softmax, Log-Softmax, and Cross-Entropy: A Complete Implementation Guide</h2>
<p>This note explains how to implement <code class="language-plaintext highlighter-rouge">Softmax</code>, <code class="language-plaintext highlighter-rouge">Log-Softmax</code>, and <code class="language-plaintext highlighter-rouge">Cross-Entropy</code> from scratch in PyTorch, highlighting key mathematical tricks to ensure numerical stability. It shows why subtracting the maximum logit before exponentiation prevents <strong>overflow</strong> and <strong>underflow</strong>, and walks through essential PyTorch tensor operationsâ€”<code class="language-plaintext highlighter-rouge">dim</code>, <code class="language-plaintext highlighter-rouge">keepdim</code>, <code class="language-plaintext highlighter-rouge">view</code>, and <code class="language-plaintext highlighter-rouge">reshape</code>â€”that are critical for implementing machine learning algorithms efficiently.</p>

<h3 id="table-of-contents">Table of Contents</h3>
<ol>
  <li><a href="#numerical-stability-deep-dive">Numerical Stability Deep Dive</a></li>
  <li><a href="#pytorch-fundamentals">PyTorch Fundamentals: <code class="language-plaintext highlighter-rouge">dim</code>, <code class="language-plaintext highlighter-rouge">keepdim</code>, and <code class="language-plaintext highlighter-rouge">view</code></a></li>
  <li><a href="#the-implementation">The Implementation</a></li>
  <li><a href="#detailed-explanation">Detailed Explanation of the Implementation</a></li>
</ol>

<hr />

<h3 id="numerical-stability-deep-dive">Numerical Stability Deep Dive</h3>

<p>Before diving into the implementation, itâ€™s crucial to understand why numerical stability matters. When implementing operations like softmax and cross-entropy, we must carefully handle potential <strong>overflow</strong> and <strong>underflow</strong> issues. Letâ€™s explore these challenges and their solutions.</p>

<h4 id="why-we-subtract-the-maximum-preventing-overflow-and-underflow">Why We Subtract the Maximum: Preventing Overflow and Underflow</h4>

<p>When implementing softmax from scratch, youâ€™ll encounter a critical numerical stability trick: subtracting the maximum value before computing exponentials. Letâ€™s explore why this matters with concrete examples.</p>

<h4 id="the-problem-exponential-overflow">The Problem: Exponential Overflow</h4>

<p>The softmax formula is:</p>

<p>$\text{softmax}(x_i) = \frac{e^{x_i}}{\sum_{j} e^{x_j}}$</p>

<h5 id="example-1-large-numbers-cause-overflow">Example 1: Large Numbers Cause Overflow</h5>

<p>Suppose you have logits: <code class="language-plaintext highlighter-rouge">[1000, 1001, 1002]</code></p>

<p><strong>Naive approach (without subtracting max):</strong></p>

<p>$e^{1000} \approx 2 \times 10^{434}$,
$e^{1001} \approx 5 \times 10^{434}$,
$e^{1002} \approx 1.4 \times 10^{435}$</p>

<p>These numbers are astronomically large and exceed what a computer can represent (approximately $10^{308}$ for 64-bit floats). Python/PyTorch returns <code class="language-plaintext highlighter-rouge">inf</code> (infinity), leading to:</p>

<p><strong>Result:</strong> <code class="language-plaintext highlighter-rouge">[inf, inf, inf]</code> â†’ Division gives <code class="language-plaintext highlighter-rouge">[nan, nan, nan]</code> âŒ</p>

<p><strong>Demonstration in PyTorch:</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch</span>

<span class="n">logits</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1000.0</span><span class="p">,</span> <span class="mf">1001.0</span><span class="p">,</span> <span class="mf">1002.0</span><span class="p">])</span>

<span class="c1"># Naive implementation (broken!)
</span><span class="n">exp_vals</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Exponentials:"</span><span class="p">,</span> <span class="n">exp_vals</span><span class="p">)</span>
<span class="c1"># Output: tensor([inf, inf, inf])
</span>
<span class="n">sum_exp</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">exp_vals</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Sum:"</span><span class="p">,</span> <span class="n">sum_exp</span><span class="p">)</span>
<span class="c1"># Output: tensor(inf)
</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">exp_vals</span> <span class="o">/</span> <span class="n">sum_exp</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Result:"</span><span class="p">,</span> <span class="n">result</span><span class="p">)</span>
<span class="c1"># Output: tensor([nan, nan, nan])  â† BROKEN!
</span></code></pre></div></div>

<h5 id="example-2-with-numerical-stability-trick">Example 2: With Numerical Stability Trick</h5>

<p>Same logits: <code class="language-plaintext highlighter-rouge">[1000, 1001, 1002]</code></p>

<p><strong>Step 1:</strong> Subtract the maximum value (1002):</p>

<p>$[1000, 1001, 1002] - 1002 = [-2, -1, 0]$</p>

<p><strong>Step 2:</strong> Compute exponentials on the stable values:</p>

<p>$e^{-2} \approx 0.135$,
$e^{-1} \approx 0.368$,
$e^{0} = 1.0$</p>

<p>These are manageable numbers with no overflow!</p>

<p><strong>Step 3:</strong> Normalize to get probabilities:</p>

<p>$\text{sum} = 0.135 + 0.368 + 1.0 = 1.503$</p>

<p>$\text{softmax} = \left[\frac{0.135}{1.503}, \frac{0.368}{1.503}, \frac{1.0}{1.503}\right] = [0.090, 0.245, 0.665]$</p>

<p><strong>Demonstration in PyTorch:</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch</span>

<span class="n">logits</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1000.0</span><span class="p">,</span> <span class="mf">1001.0</span><span class="p">,</span> <span class="mf">1002.0</span><span class="p">])</span>

<span class="c1"># Stable implementation
</span><span class="n">max_val</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nb">max</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Max value:"</span><span class="p">,</span> <span class="n">max_val</span><span class="p">)</span>
<span class="c1"># Output: tensor(1002.)
</span>
<span class="n">logits_stable</span> <span class="o">=</span> <span class="n">logits</span> <span class="o">-</span> <span class="n">max_val</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Stabilized logits:"</span><span class="p">,</span> <span class="n">logits_stable</span><span class="p">)</span>
<span class="c1"># Output: tensor([-2., -1.,  0.])
</span>
<span class="n">exp_vals</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="n">logits_stable</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Exponentials:"</span><span class="p">,</span> <span class="n">exp_vals</span><span class="p">)</span>
<span class="c1"># Output: tensor([0.1353, 0.3679, 1.0000])
</span>
<span class="n">sum_exp</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">exp_vals</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Sum:"</span><span class="p">,</span> <span class="n">sum_exp</span><span class="p">)</span>
<span class="c1"># Output: tensor(1.5032)
</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">exp_vals</span> <span class="o">/</span> <span class="n">sum_exp</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Result:"</span><span class="p">,</span> <span class="n">result</span><span class="p">)</span>
<span class="c1"># Output: tensor([0.0900, 0.2447, 0.6652])  â† WORKS!
</span>
<span class="c1"># Verify probabilities sum to 1
</span><span class="k">print</span><span class="p">(</span><span class="s">"Sum of probabilities:"</span><span class="p">,</span> <span class="n">torch</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">result</span><span class="p">))</span>
<span class="c1"># Output: tensor(1.0000)
</span></code></pre></div></div>
<h4 id="mathematical-proof-why-this-works">Mathematical Proof: Why This Works</h4>

<p>The stability trick is mathematically sound because:</p>

<p>$\text{softmax}(x) = \text{softmax}(x - c)$</p>

<p>for any constant $c$!</p>

<p><strong>Proof:</strong></p>

<p>$\frac{e^{x_i - c}}{\sum_{j} e^{x_j - c}} = \frac{e^{x_i} \cdot e^{-c}}{\sum_{j} e^{x_j} \cdot e^{-c}} = \frac{e^{x_i} \cdot e^{-c}}{e^{-c} \cdot \sum_{j} e^{x_j}} = \frac{e^{x_i}}{\sum_{j} e^{x_j}}$</p>

<p>The $e^{-c}$ terms cancel out! By choosing $c = \max(x)$, we ensure the largest exponent becomes 0, preventing overflow while maintaining mathematical correctness.</p>

<h4 id="the-underflow-problem">The Underflow Problem</h4>

<p>Thereâ€™s also a potential underflow issue with very negative numbers:</p>

<p>$e^{-1000} \approx 0$</p>

<p>This underflows to zero in floating-point arithmetic. However, by subtracting the maximum, the largest value becomes 0 ($e^0 = 1$), and only smaller values might underflow. This is acceptable because extremely small exponentials contribute negligibly to the sum anyway.</p>

<p><strong>Demonstration:</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch</span>

<span class="c1"># Very negative logits
</span><span class="n">logits</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([</span><span class="o">-</span><span class="mf">1000.0</span><span class="p">,</span> <span class="o">-</span><span class="mf">999.0</span><span class="p">,</span> <span class="o">-</span><span class="mf">998.0</span><span class="p">])</span>

<span class="c1"># Without stability trick
</span><span class="n">exp_vals</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Exponentials:"</span><span class="p">,</span> <span class="n">exp_vals</span><span class="p">)</span>
<span class="c1"># Output: tensor([0., 0., 0.])  â† All underflow!
</span>
<span class="c1"># With stability trick
</span><span class="n">max_val</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nb">max</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span>
<span class="n">logits_stable</span> <span class="o">=</span> <span class="n">logits</span> <span class="o">-</span> <span class="n">max_val</span>
<span class="n">exp_vals_stable</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="n">logits_stable</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Stable exponentials:"</span><span class="p">,</span> <span class="n">exp_vals_stable</span><span class="p">)</span>
<span class="c1"># Output: tensor([0.1353, 0.3679, 1.0000])  â† Works perfectly!
</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">exp_vals_stable</span> <span class="o">/</span> <span class="n">torch</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">exp_vals_stable</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Result:"</span><span class="p">,</span> <span class="n">result</span><span class="p">)</span>
<span class="c1"># Output: tensor([0.0900, 0.2447, 0.6652])
</span></code></pre></div></div>
<h4 id="why-this-matters-in-deep-learning">Why This Matters in Deep Learning</h4>

<p>In deep learning, logits frequently reach magnitudes of hundreds or thousands, especially in:</p>

<ul>
  <li><strong>Language models</strong> with large vocabulary sizes (tens of thousands of classes)</li>
  <li><strong>Deep networks</strong> where activations accumulate through many layers</li>
  <li><strong>Unnormalized outputs</strong> before the final softmax layer</li>
  <li><strong>Training dynamics</strong> where gradients can push logits to extreme values</li>
</ul>

<p>Without the stability trick, your model would crash with <code class="language-plaintext highlighter-rouge">nan</code> values during training or inference, making it impossible to:</p>
<ul>
  <li>Train the model (gradients become <code class="language-plaintext highlighter-rouge">nan</code>)</li>
  <li>Make predictions (outputs become <code class="language-plaintext highlighter-rouge">nan</code>)</li>
  <li>Debug issues (everything breaks catastrophically)</li>
</ul>

<p>This simple techniqueâ€”subtracting the maximumâ€”keeps all exponentials in a safe computational range (approximately 0 to 1) while computing the exact same mathematical result.</p>

<p><strong>Real-world example from GPT-2:</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Typical logits from a language model
</span><span class="n">logits</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">50257</span><span class="p">)</span> <span class="o">*</span> <span class="mi">10</span>  <span class="c1"># vocab_size = 50,257
</span><span class="k">print</span><span class="p">(</span><span class="s">"Logit range:"</span><span class="p">,</span> <span class="n">logits</span><span class="p">.</span><span class="nb">min</span><span class="p">().</span><span class="n">item</span><span class="p">(),</span> <span class="s">"to"</span><span class="p">,</span> <span class="n">logits</span><span class="p">.</span><span class="nb">max</span><span class="p">().</span><span class="n">item</span><span class="p">())</span>
<span class="c1"># Output: Logit range: -28.3 to 31.7
</span>
<span class="c1"># Without stability trick (would overflow!)
# With stability trick (works perfectly)
</span><span class="n">probs</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Probability range:"</span><span class="p">,</span> <span class="n">probs</span><span class="p">.</span><span class="nb">min</span><span class="p">().</span><span class="n">item</span><span class="p">(),</span> <span class="s">"to"</span><span class="p">,</span> <span class="n">probs</span><span class="p">.</span><span class="nb">max</span><span class="p">().</span><span class="n">item</span><span class="p">())</span>
<span class="c1"># Output: Probability range: 1.2e-27 to 0.0043
</span><span class="k">print</span><span class="p">(</span><span class="s">"Sum:"</span><span class="p">,</span> <span class="n">probs</span><span class="p">.</span><span class="nb">sum</span><span class="p">().</span><span class="n">item</span><span class="p">())</span>
<span class="c1"># Output: Sum: 1.0
</span></code></pre></div></div>
<hr />

<h3 id="pytorch-fundamentals">PyTorch Fundamentals: <code class="language-plaintext highlighter-rouge">dim</code>, <code class="language-plaintext highlighter-rouge">keepdim</code>, and <code class="language-plaintext highlighter-rouge">view</code></h3>

<p>Now that we understand the importance of numerical stability, we need to master the essential PyTorch operations that enable us to implement these stable operations efficiently. Before diving into the implementation details, it is important to understand the three fundamental PyTorch operations on tensors.</p>

<h4 id="1-understanding-dim-dimensionaxis">1. Understanding <code class="language-plaintext highlighter-rouge">dim</code> (Dimension/Axis)</h4>

<p>In PyTorch, tensors can have multiple dimensions. The <code class="language-plaintext highlighter-rouge">dim</code> parameter specifies <strong>which dimension</strong> to operate along.</p>

<h5 id="dimension-indexing">Dimension Indexing</h5>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch</span>

<span class="c1"># A 2D tensor (matrix)
</span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
                  <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">]])</span>

<span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># torch.Size([2, 3])
# dim=0 â†’ rows (size 2)
# dim=1 â†’ columns (size 3)
</span></code></pre></div></div>

<p><strong>Visual representation:</strong></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>        dim=1 â†’
       [1, 2, 3]
dim=0  [4, 5, 6]
  â†“
</code></pre></div></div>

<h5 id="operations-along-different-dimensions">Operations Along Different Dimensions</h5>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Sum along dim=0 (collapse rows, keep columns)
</span><span class="n">result_dim0</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">result_dim0</span><span class="p">)</span>  <span class="c1"># tensor([5, 7, 9])
# Adds: [1+4, 2+5, 3+6]
</span>
<span class="c1"># Sum along dim=1 (collapse columns, keep rows)
</span><span class="n">result_dim1</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">result_dim1</span><span class="p">)</span>  <span class="c1"># tensor([6, 15])
# Adds: [1+2+3, 4+5+6]
</span></code></pre></div></div>

<p><strong>Visual:</strong></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Original:          Sum along dim=0:             Sum along dim=1:
[1, 2, 3]          [5, 7, 9]                    [6, 15]
[4, 5, 6]                                       
</code></pre></div></div>

<h5 id="3d-tensor-example">3D Tensor Example</h5>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Shape: [2, 3, 4] means 2 "matrices" of size 3x4
</span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># torch.Size([2, 3, 4])
# dim=0 â†’ first dimension (size 2)
# dim=1 â†’ second dimension (size 3)
# dim=2 â†’ third dimension (size 4)
# dim=-1 â†’ last dimension (same as dim=2)
# dim=-2 â†’ second to last (same as dim=1)
</span></code></pre></div></div>

<p><strong>Negative indexing:</strong></p>
<ul>
  <li><code class="language-plaintext highlighter-rouge">dim=-1</code> always refers to the <strong>last dimension</strong></li>
  <li><code class="language-plaintext highlighter-rouge">dim=-2</code> refers to the <strong>second to last</strong>, etc.</li>
</ul>

<h4 id="2-understanding-keepdimtrue">2. Understanding <code class="language-plaintext highlighter-rouge">keepdim=True</code></h4>

<p><code class="language-plaintext highlighter-rouge">keepdim</code> controls whether the reduced dimension is <strong>kept</strong> or <strong>removed</strong> after an operation.</p>

<h5 id="example-keepdimfalse-default">Example: <code class="language-plaintext highlighter-rouge">keepdim=False</code> (default)</h5>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
                  <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">]])</span>

<span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># torch.Size([2, 3])
</span>
<span class="c1"># Sum along dim=1 WITHOUT keepdim
</span><span class="n">result</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>        <span class="c1"># tensor([6, 15])
</span><span class="k">print</span><span class="p">(</span><span class="n">result</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># torch.Size([2])  â† dimension collapsed!
</span></code></pre></div></div>

<p>The dimension is <strong>removed</strong>, so shape goes from <code class="language-plaintext highlighter-rouge">[2, 3]</code> â†’ <code class="language-plaintext highlighter-rouge">[2]</code></p>

<h5 id="example-keepdimtrue">Example: <code class="language-plaintext highlighter-rouge">keepdim=True</code></h5>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
                  <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">]])</span>

<span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># torch.Size([2, 3])
</span>
<span class="c1"># Sum along dim=1 WITH keepdim
</span><span class="n">result</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>        <span class="c1"># tensor([[6], [15]])
</span><span class="k">print</span><span class="p">(</span><span class="n">result</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># torch.Size([2, 1])  â† dimension kept!
</span></code></pre></div></div>

<p>The dimension is <strong>preserved</strong> (but size becomes 1), so shape goes from <code class="language-plaintext highlighter-rouge">[2, 3]</code> â†’ <code class="language-plaintext highlighter-rouge">[2, 1]</code></p>

<h5 id="why-keepdimtrue-matters-broadcasting">Why <code class="language-plaintext highlighter-rouge">keepdim=True</code> Matters: Broadcasting</h5>

<p><code class="language-plaintext highlighter-rouge">keepdim=True</code> is crucial for <strong>broadcasting</strong> operations:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">],</span>
                  <span class="p">[</span><span class="mf">4.0</span><span class="p">,</span> <span class="mf">5.0</span><span class="p">,</span> <span class="mf">6.0</span><span class="p">]])</span>

<span class="c1"># Without keepdim
</span><span class="n">mean_no_keep</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">mean_no_keep</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># torch.Size([2])
</span>
<span class="c1"># This will fail! Shapes don't match for broadcasting
# x - mean_no_keep  # Error!
</span>
<span class="c1"># With keepdim
</span><span class="n">mean_keep</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">mean_keep</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># torch.Size([2, 1])
</span>
<span class="c1"># This works! Broadcasting happens correctly
</span><span class="n">normalized</span> <span class="o">=</span> <span class="n">x</span> <span class="o">-</span> <span class="n">mean_keep</span>
<span class="k">print</span><span class="p">(</span><span class="n">normalized</span><span class="p">)</span>
<span class="c1"># tensor([[-1., 0., 1.],
#         [-1., 0., 1.]])
</span></code></pre></div></div>

<p><strong>Visual explanation:</strong></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Original x:        mean (keepdim=True):    Broadcasting x - mean:
[1, 2, 3]          [2]                     [1, 2, 3]   [2]
[4, 5, 6]          [5]                     [4, 5, 6] - [5]
                                                        â†“
Shape [2, 3]       Shape [2, 1]            [1-2, 2-2, 3-2]
                                           [4-5, 5-5, 6-5]
                                            = [[-1, 0, 1],
                                               [-1, 0, 1]]
</code></pre></div></div>

<h5 id="more-examples-with-different-operations">More Examples with Different Operations</h5>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span>
                   <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]],</span>
                  <span class="p">[[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">],</span>
                   <span class="p">[</span><span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">]]])</span>
<span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># torch.Size([2, 2, 2])
# x is a stack of two [2,2] tensors:
# Tensor #0:
# [[1, 2],
#  [3, 4]]
</span>
<span class="c1"># Tensor #1:
# [[5, 6],
#  [7, 8]]
</span>
<span class="c1"># Max along dim=0, we're taking the max across the first dimension (dim=0), meaning we compare corresponding elements between the two [2,2] matrices
# [[max(1,5), max(2,6)],
#  [max(3,7), max(4,8)]]
# That gives
# [[5, 6],
#  [7, 8]]
</span>
<span class="n">max_vals_no_keep</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nb">max</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">False</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
<span class="k">print</span><span class="p">(</span><span class="n">max_vals_no_keep</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># torch.Size([2, 2])
</span>
<span class="n">max_vals_keep</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nb">max</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
<span class="k">print</span><span class="p">(</span><span class="n">max_vals_keep</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># torch.Size([1, 2, 2])
</span></code></pre></div></div>

<h4 id="3-understanding-view---reshaping-tensors">3. Understanding <code class="language-plaintext highlighter-rouge">view()</code> - Reshaping Tensors</h4>

<p><code class="language-plaintext highlighter-rouge">view()</code> reshapes a tensor <strong>without changing its data</strong> : which means that the underlying values stored in memory stay exactly the same â€” PyTorch just interprets that same block of memory in a different shape.</p>
<h5 id="basic-reshaping">Basic Reshaping</h5>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">])</span>
<span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># torch.Size([6])
</span>
<span class="c1"># Reshape to 2x3
</span><span class="n">x_2d</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">x_2d</span><span class="p">)</span>
<span class="c1"># tensor([[1, 2, 3],
#         [4, 5, 6]])
</span><span class="k">print</span><span class="p">(</span><span class="n">x_2d</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># torch.Size([2, 3])
</span>
<span class="c1"># Reshape to 3x2
</span><span class="n">x_3d</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">x_3d</span><span class="p">)</span>
<span class="c1"># tensor([[1, 2],
#         [3, 4],
#         [5, 6]])
</span></code></pre></div></div>

<p><strong>Important:</strong> The <strong>total number of elements must remain the same</strong>!</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">6</span><span class="p">)</span>
<span class="n">x</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>  <span class="c1"># âœ“ Works: 6 = 2 Ã— 3
</span><span class="n">x</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>  <span class="c1"># âœ“ Works: 6 = 3 Ã— 2
</span><span class="n">x</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span>  <span class="c1"># âœ“ Works: 6 = 1 Ã— 6
# x.view(2, 2)  # âœ— Error: 6 â‰  2 Ã— 2
</span></code></pre></div></div>

<h5 id="using--1-for-automatic-dimension-inference">Using <code class="language-plaintext highlighter-rouge">-1</code> for Automatic Dimension Inference</h5>

<p>You can use <code class="language-plaintext highlighter-rouge">-1</code> to let PyTorch <strong>automatically calculate</strong> one dimension:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">12</span><span class="p">)</span>

<span class="c1"># -1 means "figure out this dimension automatically"
</span><span class="n">x</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>   <span class="c1"># torch.Size([3, 4]) - PyTorch calculates 12/3 = 4
</span><span class="n">x</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span>   <span class="c1"># torch.Size([2, 6]) - PyTorch calculates 12/6 = 2
</span><span class="n">x</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>   <span class="c1"># torch.Size([12, 1])
</span><span class="n">x</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>      <span class="c1"># torch.Size([12]) - flattens to 1D
</span></code></pre></div></div>

<p><strong>Rule:</strong> You can only use <code class="language-plaintext highlighter-rouge">-1</code> for <strong>one dimension</strong> at a time.</p>

<h5 id="flattening-tensors">Flattening Tensors</h5>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Common pattern: flatten all dimensions
</span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>  <span class="c1"># Shape: [2, 3, 4]
</span><span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># torch.Size([2, 3, 4])
</span>
<span class="c1"># Flatten to 1D
</span><span class="n">x_flat</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">x_flat</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># torch.Size([24])  (2*3*4 = 24)
</span>
<span class="c1"># Flatten batch dimensions but keep last dimension
</span><span class="n">x_partial_flat</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">x_partial_flat</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># torch.Size([6, 4])  (2*3 = 6)
</span></code></pre></div></div>

<h5 id="practical-example-batch-processing">Practical Example: Batch Processing</h5>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Batch of images: [batch_size, height, width, channels]
</span><span class="n">images</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">images</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># torch.Size([32, 28, 28, 3])
</span>
<span class="c1"># Flatten each image for a fully connected layer
# Keep batch dimension, flatten the rest
</span><span class="n">images_flat</span> <span class="o">=</span> <span class="n">images</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">images_flat</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># torch.Size([32, 2352])  (28*28*3 = 2352)
</span>
<span class="c1"># Or equivalently:
</span><span class="n">images_flat</span> <span class="o">=</span> <span class="n">images</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="n">images</span><span class="p">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">images_flat</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># torch.Size([32, 2352])
</span></code></pre></div></div>

<h5 id="view-vs-reshape"><code class="language-plaintext highlighter-rouge">view()</code> vs <code class="language-plaintext highlighter-rouge">reshape()</code></h5>

<p>PyTorch has both <code class="language-plaintext highlighter-rouge">view()</code> and <code class="language-plaintext highlighter-rouge">reshape()</code>:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>

<span class="c1"># view() - requires contiguous memory
</span><span class="n">y</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="mi">6</span><span class="p">)</span>

<span class="c1"># reshape() - works even if not contiguous (may copy data)
</span><span class="n">z</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">6</span><span class="p">)</span>
</code></pre></div></div>

<p><strong>Key difference:</strong></p>
<ul>
  <li><code class="language-plaintext highlighter-rouge">view()</code>: Only works if tensor is <strong>contiguous in memory</strong>, otherwise raises error</li>
  <li><code class="language-plaintext highlighter-rouge">reshape()</code> is more flexible:
    <ul>
      <li>If the tensor is contiguous, it behaves like .view() (no copy).</li>
      <li>If it isnâ€™t contiguous, it will make a copy behind the scenes to ensure the new tensor has contiguous memory. Thatâ€™s why <code class="language-plaintext highlighter-rouge">reshape()</code> is safer, but sometimes slightly slower (copying costs time &amp; memory).</li>
    </ul>
  </li>
</ul>

<h4 id="quick-reference-summary">Quick Reference Summary</h4>

<table>
  <thead>
    <tr>
      <th>Operation</th>
      <th>What it does</th>
      <th>Example</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">dim=0</code></td>
      <td>Operate along first dimension</td>
      <td><code class="language-plaintext highlighter-rouge">torch.sum(x, dim=0)</code></td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">dim=-1</code></td>
      <td>Operate along last dimension</td>
      <td><code class="language-plaintext highlighter-rouge">torch.max(x, dim=-1)</code></td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">keepdim=True</code></td>
      <td>Keep dimension (sizeâ†’1)</td>
      <td>Shape <code class="language-plaintext highlighter-rouge">[2,3]</code> â†’ <code class="language-plaintext highlighter-rouge">[2,1]</code></td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">keepdim=False</code></td>
      <td>Remove dimension</td>
      <td>Shape <code class="language-plaintext highlighter-rouge">[2,3]</code> â†’ <code class="language-plaintext highlighter-rouge">[2]</code></td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">view(a, b)</code></td>
      <td>Change the view of the data to <code class="language-plaintext highlighter-rouge">[a, b]</code></td>
      <td><code class="language-plaintext highlighter-rouge">x.view(2, 3)</code></td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">view(-1)</code></td>
      <td>Flatten to 1D</td>
      <td>Shape <code class="language-plaintext highlighter-rouge">[2,3]</code> â†’ <code class="language-plaintext highlighter-rouge">[6]</code></td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">view(-1, n)</code></td>
      <td>Auto calculate first dim given <code class="language-plaintext highlighter-rouge">n</code></td>
      <td><code class="language-plaintext highlighter-rouge">x.view(-1, 4)</code></td>
    </tr>
  </tbody>
</table>

<hr />

<h3 id="the-implementation">The Implementation</h3>

<p>With a solid understanding of numerical stability concerns and the fundamental PyTorch operations, weâ€™re now ready to see how these concepts come together in a complete implementation. Below is a sample code snippet showing the complete implementation.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch</span>

<span class="k">def</span> <span class="nf">softmax</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="s">"""
    Apply the softmax operation to a tensor along the specified dimension.

    Uses the numerical stability trick of subtracting the maximum value
    from all elements before applying exponential.

    Args:
        x: torch.Tensor - Input tensor
        dim: int - Dimension along which to apply softmax

    Returns:
        torch.Tensor - Output tensor with same shape as input, with normalized
                      probability distribution along the specified dimension
    """</span>
    <span class="c1"># Subtract maximum for numerical stability
</span>    <span class="c1"># keepdim=True ensures the shape is preserved for broadcasting
</span>    <span class="n">max_vals</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nb">max</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">dim</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">x_stable</span> <span class="o">=</span> <span class="n">x</span> <span class="o">-</span> <span class="n">max_vals</span>

    <span class="c1"># Apply exponential
</span>    <span class="n">exp_vals</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x_stable</span><span class="p">)</span>

    <span class="c1"># Compute sum along the specified dimension
</span>    <span class="n">sum_exp</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">exp_vals</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">dim</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

    <span class="c1"># Normalize to get probabilities
</span>    <span class="k">return</span> <span class="n">exp_vals</span> <span class="o">/</span> <span class="n">sum_exp</span>


<span class="k">def</span> <span class="nf">log_softmax</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="s">"""
    Apply the log-softmax operation to a tensor along the specified dimension.

    log_softmax(x) = log(softmax(x)) = x - log(sum(exp(x)))

    This is more numerically stable than computing log(softmax(x)) separately
    because it cancels the exp and log operations.

    Args:
        x: torch.Tensor - Input tensor
        dim: int - Dimension along which to apply log-softmax

    Returns:
        torch.Tensor - Output tensor with same shape as input, containing
                      log probabilities along the specified dimension
    """</span>
    <span class="c1"># Subtract maximum for numerical stability
</span>    <span class="n">max_vals</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nb">max</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">dim</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">x_stable</span> <span class="o">=</span> <span class="n">x</span> <span class="o">-</span> <span class="n">max_vals</span>

    <span class="c1"># Compute log(sum(exp(x_stable)))
</span>    <span class="n">log_sum_exp</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x_stable</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="n">dim</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">True</span><span class="p">))</span>

    <span class="c1"># log_softmax = x_stable - log(sum(exp(x_stable)))
</span>    <span class="k">return</span> <span class="n">x_stable</span> <span class="o">-</span> <span class="n">log_sum_exp</span>


<span class="k">def</span> <span class="nf">cross_entropy</span><span class="p">(</span><span class="n">logits</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">targets</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="s">"""
    Compute the cross-entropy loss given logits and targets.

    The cross-entropy loss is: -log(softmax(logits)[target])

    This implementation uses the log_softmax function which provides
    numerical stability by canceling log and exp operations.

    Args:
        logits: torch.Tensor of shape (..., vocab_size) - Unnormalized logits
        targets: torch.Tensor of shape (...,) - Target class indices

    Returns:
        torch.Tensor - Scalar tensor with average cross-entropy loss across all examples
    """</span>
    <span class="c1"># Compute log probabilities using log_softmax (numerically stable)
</span>    <span class="n">log_probs</span> <span class="o">=</span> <span class="n">log_softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

    <span class="c1"># Get the log probability for the target class for each example
</span>    <span class="c1"># Flatten batch dimensions to handle any number of batch dims
</span>    <span class="n">batch_shape</span> <span class="o">=</span> <span class="n">logits</span><span class="p">.</span><span class="n">shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">log_probs_flat</span> <span class="o">=</span> <span class="n">log_probs</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">log_probs</span><span class="p">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>
    <span class="n">targets_flat</span> <span class="o">=</span> <span class="n">targets</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>

    <span class="n">batch_indices</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="n">log_probs_flat</span><span class="p">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="n">logits</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>
    <span class="n">target_log_probs</span> <span class="o">=</span> <span class="n">log_probs_flat</span><span class="p">[</span><span class="n">batch_indices</span><span class="p">,</span> <span class="n">targets_flat</span><span class="p">]</span>

    <span class="c1"># Reshape back to original batch shape
</span>    <span class="n">target_log_probs</span> <span class="o">=</span> <span class="n">target_log_probs</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_shape</span><span class="p">)</span>

    <span class="c1"># Cross entropy: -log(softmax(o)[target]) = -log_prob[target]
</span>    <span class="n">cross_entropy_loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">target_log_probs</span>

    <span class="c1"># Return average across all batch dimensions
</span>    <span class="k">return</span> <span class="n">cross_entropy_loss</span><span class="p">.</span><span class="n">mean</span><span class="p">()</span>
</code></pre></div></div>

<hr />

<h3 id="detailed-explanation">Detailed Explanation of the Implementation</h3>

<p>Now that weâ€™ve seen the complete implementation, letâ€™s break down each function to understand how they work.</p>

<h4 id="softmax-function">Softmax Function</h4>

<p><strong>Goal:</strong> Convert raw scores (logits) into probabilities that sum to 1.</p>

<p><strong>Formula:</strong> $\text{softmax}(x_i) = \frac{e^{x_i}}{\sum_{j} e^{x_j}}$</p>

<p><strong>Step-by-step:</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Step 1: Find maximum (for numerical stability)
</span><span class="n">max_vals</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nb">max</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">dim</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
<span class="c1"># keepdim=True keeps shape [2,1] instead of [2] â†’ enables broadcasting
# [0] gets values (not indices)
</span>
<span class="c1"># Step 2: Subtract maximum
</span><span class="n">x_stable</span> <span class="o">=</span> <span class="n">x</span> <span class="o">-</span> <span class="n">max_vals</span>
<span class="c1"># Prevents overflow: softmax(x) = softmax(x - c) mathematically
</span>
<span class="c1"># Step 3: Exponentiate
</span><span class="n">exp_vals</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x_stable</span><span class="p">)</span>
<span class="c1"># Apply e^x to each element
</span>
<span class="c1"># Step 4: Sum and normalize
</span><span class="n">sum_exp</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">exp_vals</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">dim</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="k">return</span> <span class="n">exp_vals</span> <span class="o">/</span> <span class="n">sum_exp</span>  <span class="c1"># Division broadcasts correctly
</span></code></pre></div></div>

<p><strong>Example:</strong> Input <code class="language-plaintext highlighter-rouge">[[1, 2, 3], [4, 5, 6]]</code> â†’ Output <code class="language-plaintext highlighter-rouge">[[0.09, 0.24, 0.67], [0.09, 0.24, 0.67]]</code></p>

<h4 id="log-softmax-function">Log-Softmax Function</h4>

<p><strong>Goal:</strong> Compute $\log(\text{softmax}(x))$ without numerical overflow.</p>

<p><strong>Formula:</strong> $\log(\text{softmax}(x_i)) = x_i - \log\left(\sum_{j} e^{x_j}\right)$</p>

<p><strong>Why not just <code class="language-plaintext highlighter-rouge">log(softmax(x))</code>?</strong> Computing <code class="language-plaintext highlighter-rouge">log(exp(large_number))</code> can overflow. Log-softmax avoids this by staying in log-space.</p>

<p><strong>Step-by-step:</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Step 1 &amp; 2: Subtract maximum (same as softmax)
</span><span class="n">max_vals</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nb">max</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">dim</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">x_stable</span> <span class="o">=</span> <span class="n">x</span> <span class="o">-</span> <span class="n">max_vals</span>

<span class="c1"># Step 3: Compute log(sum(exp(x_stable)))
</span><span class="n">log_sum_exp</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x_stable</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="n">dim</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">True</span><span class="p">))</span>
<span class="c1"># This is the log of the denominator in softmax
</span>
<span class="c1"># Step 4: Subtract to get log probabilities
</span><span class="k">return</span> <span class="n">x_stable</span> <span class="o">-</span> <span class="n">log_sum_exp</span>
</code></pre></div></div>

<p><strong>Example:</strong> Input <code class="language-plaintext highlighter-rouge">[[1, 2, 3], [4, 5, 6]]</code> â†’ Output <code class="language-plaintext highlighter-rouge">[[-2.41, -1.41, -0.41], [-2.41, -1.41, -0.41]]</code></p>

<h4 id="cross-entropy-function">Cross-Entropy Function</h4>

<p><strong>Goal:</strong> Measure how well predictions match the correct labels.</p>

<p><strong>Formula:</strong> $\mathcal{L} = -\log(\text{softmax}(\text{logits})[\text{target}])$</p>

<p><strong>Intuition:</strong> Penalize low probabilities assigned to the correct class. Lower loss = better prediction.</p>

<p><strong>Step-by-step:</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Step 1: Get log probabilities
</span><span class="n">log_probs</span> <span class="o">=</span> <span class="n">log_softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="c1"># Shape: [batch, seq, vocab] â†’ [batch, seq, vocab]
</span>
<span class="c1"># Step 2: Save batch shape for later
</span><span class="n">batch_shape</span> <span class="o">=</span> <span class="n">logits</span><span class="p">.</span><span class="n">shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>  <span class="c1"># e.g., [2, 2] for shape [2, 2, 3]
</span>
<span class="c1"># Step 3: Flatten to 2D for easier indexing
</span><span class="n">log_probs_flat</span> <span class="o">=</span> <span class="n">log_probs</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">log_probs</span><span class="p">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>  <span class="c1"># [batch*seq, vocab]
</span><span class="n">targets_flat</span> <span class="o">=</span> <span class="n">targets</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>                           <span class="c1"># [batch*seq]
# Example: [2, 2, 3] â†’ [4, 3] and [2, 2] â†’ [4]
</span>
<span class="c1"># Step 4: Extract log prob for the correct class of each example
</span><span class="n">batch_indices</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="n">log_probs_flat</span><span class="p">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="n">logits</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>
<span class="n">target_log_probs</span> <span class="o">=</span> <span class="n">log_probs_flat</span><span class="p">[</span><span class="n">batch_indices</span><span class="p">,</span> <span class="n">targets_flat</span><span class="p">]</span>
<span class="c1"># Advanced indexing: log_probs_flat[i, targets_flat[i]] for each i
# Gets the log probability that the model assigned to the correct class
</span>
<span class="c1"># Step 5: Reshape back to original batch dimensions
</span><span class="n">target_log_probs</span> <span class="o">=</span> <span class="n">target_log_probs</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_shape</span><span class="p">)</span>

<span class="c1"># Step 6: Apply negative and average
</span><span class="n">cross_entropy_loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">target_log_probs</span>
<span class="k">return</span> <span class="n">cross_entropy_loss</span><span class="p">.</span><span class="n">mean</span><span class="p">()</span>
</code></pre></div></div>

<p><strong>Understanding Advanced Indexing (Step 4):</strong></p>

<p>This is the trickiest part. Letâ€™s break it down with a concrete example:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Suppose we have:
</span><span class="n">log_probs_flat</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([[</span><span class="o">-</span><span class="mf">2.4</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.4</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4</span><span class="p">],</span>  <span class="c1"># example 0: log probs for 3 classes
</span>                               <span class="p">[</span><span class="o">-</span><span class="mf">2.4</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.4</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4</span><span class="p">],</span>  <span class="c1"># example 1
</span>                               <span class="p">[</span><span class="o">-</span><span class="mf">2.4</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.4</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4</span><span class="p">],</span>  <span class="c1"># example 2
</span>                               <span class="p">[</span><span class="o">-</span><span class="mf">1.1</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.1</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.1</span><span class="p">]])</span> <span class="c1"># example 3
# Shape: [4, 3]
</span>
<span class="n">targets_flat</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>  <span class="c1"># correct class for each example
# Shape: [4]
</span>
<span class="c1"># We want to extract:
# - example 0, class 2 â†’ log_probs_flat[0, 2] = -0.4
# - example 1, class 1 â†’ log_probs_flat[1, 1] = -1.4
# - example 2, class 0 â†’ log_probs_flat[2, 0] = -2.4
# - example 3, class 1 â†’ log_probs_flat[3, 1] = -1.1
</span></code></pre></div></div>

<p><strong>How advanced indexing works:</strong></p>

<p>When we have <code class="language-plaintext highlighter-rouge">tensor[indices1, indices2]</code>, PyTorch pairs up elements from both index arrays:</p>
<ul>
  <li><code class="language-plaintext highlighter-rouge">tensor[indices1[0], indices2[0]]</code></li>
  <li><code class="language-plaintext highlighter-rouge">tensor[indices1[1], indices2[1]]</code></li>
  <li><code class="language-plaintext highlighter-rouge">tensor[indices1[2], indices2[2]]</code></li>
  <li>and so onâ€¦</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">batch_indices</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>  <span class="c1"># row indices
</span><span class="n">targets_flat</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>   <span class="c1"># column indices
</span>
<span class="c1"># This pairing happens:
</span><span class="n">result</span> <span class="o">=</span> <span class="n">log_probs_flat</span><span class="p">[</span><span class="n">batch_indices</span><span class="p">,</span> <span class="n">targets_flat</span><span class="p">]</span>
<span class="c1"># â†’ [log_probs_flat[0,2], log_probs_flat[1,1], log_probs_flat[2,0], log_probs_flat[3,1]]
# â†’ [-0.4, -1.4, -2.4, -1.1]
</span></code></pre></div></div>

<p><strong>Why we need <code class="language-plaintext highlighter-rouge">batch_indices</code>:</strong></p>

<p>Without it, weâ€™d just have <code class="language-plaintext highlighter-rouge">log_probs_flat[:, targets_flat]</code>, which tries to select multiple columns for ALL rowsâ€”not what we want! We need to select ONE element per row (the correct class for that specific example).</p>

<p><strong>Visual representation:</strong></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>log_probs_flat:          targets_flat:         batch_indices:
[[-2.4, -1.4, -0.4]         [2                     [0
 [-2.4, -1.4, -0.4]          1                      1
 [-2.4, -1.4, -0.4]          0                      2
 [-1.1, -1.1, -1.1]]         1]                     3]

Pairing: [0,2] [1,1] [2,0] [3,1]
         â†“     â†“     â†“     â†“
Result: [-0.4, -1.4, -2.4, -1.1]
</code></pre></div></div>

<p><strong>Summary:</strong></p>
<ul>
  <li>Input: <code class="language-plaintext highlighter-rouge">log_probs_flat</code> shape <code class="language-plaintext highlighter-rouge">[4, 3]</code>, <code class="language-plaintext highlighter-rouge">targets_flat = [2, 1, 0, 1]</code></li>
  <li>Output: <code class="language-plaintext highlighter-rouge">target_log_probs</code> shape <code class="language-plaintext highlighter-rouge">[4]</code> with values <code class="language-plaintext highlighter-rouge">[-0.4, -1.4, -2.4, -1.1]</code></li>
  <li>Each value is the log probability of the correct class for that example</li>
</ul>

<h4 id="putting-it-all-together">Putting It All Together</h4>

<p>Hereâ€™s a complete workflow showing how these functions work together:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch</span>

<span class="c1"># Setup: batch=2, sequence=2, vocab=3
</span><span class="n">logits</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([[[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">],</span>
                        <span class="p">[</span><span class="mf">4.0</span><span class="p">,</span> <span class="mf">5.0</span><span class="p">,</span> <span class="mf">6.0</span><span class="p">]],</span>
                       <span class="p">[[</span><span class="mf">7.0</span><span class="p">,</span> <span class="mf">8.0</span><span class="p">,</span> <span class="mf">9.0</span><span class="p">],</span>
                        <span class="p">[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">]]])</span>
<span class="n">targets</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
                        <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span>

<span class="c1"># Compute loss
</span><span class="n">loss</span> <span class="o">=</span> <span class="n">cross_entropy</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Loss: </span><span class="si">{</span><span class="n">loss</span><span class="si">:</span><span class="p">.</span><span class="mi">3</span><span class="n">f</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>  <span class="c1"># Loss: 1.330
</span></code></pre></div></div>

<p><strong>What happens internally:</strong></p>
<ol>
  <li><code class="language-plaintext highlighter-rouge">log_softmax</code> converts logits to log probabilities</li>
  <li>Flatten everything to make indexing easier</li>
  <li>Extract log probability for each correct class</li>
  <li>Reshape back and compute mean loss</li>
</ol>

<p><strong>Key insight:</strong> The entire pipeline is designed to compute $-\log(P(\text{correct class}))$ efficiently and stably.</p>

<hr />

<h3 id="key-takeaways">Key Takeaways</h3>

<ol>
  <li>
    <p><strong>Numerical stability isnâ€™t optional</strong>: Itâ€™s the difference between code that works and code that fails in production</p>
  </li>
  <li>
    <p><strong>Always subtract the maximum</strong> before computing softmax or log-softmax</p>
  </li>
  <li>
    <p><strong>Use log-softmax for cross-entropy</strong>: Computing <code class="language-plaintext highlighter-rouge">log(softmax(x))</code> separately is both slower and less stable than log-softmax</p>
  </li>
  <li>
    <p><strong>The math is equivalent</strong>: These tricks donâ€™t change the resultsâ€”they just make them computable</p>
  </li>
  <li><strong>Modern frameworks do this automatically</strong>: PyTorchâ€™s <code class="language-plaintext highlighter-rouge">torch.nn.functional.softmax()</code> and <code class="language-plaintext highlighter-rouge">torch.nn.functional.cross_entropy()</code>include these optimizations, but understanding them helps you:
    <ul>
      <li>Debug numerical issues</li>
      <li>Implement custom loss functions</li>
      <li>Appreciate the engineering behind deep learning libraries</li>
    </ul>
  </li>
  <li><strong>Test edge cases</strong>: Always test your implementations with extreme values (very large, very small, very negative) to ensure numerical stability</li>
</ol>]]></content><author><name>[&quot;Han Yu&quot;]</name></author><category term="cs336" /><summary type="html"><![CDATA[Understanding Softmax, Log-Softmax, and Cross-Entropy: A Complete Implementation Guide This note explains how to implement Softmax, Log-Softmax, and Cross-Entropy from scratch in PyTorch, highlighting key mathematical tricks to ensure numerical stability. It shows why subtracting the maximum logit before exponentiation prevents overflow and underflow, and walks through essential PyTorch tensor operationsâ€”dim, keepdim, view, and reshapeâ€”that are critical for implementing machine learning algorithms efficiently.]]></summary></entry><entry><title type="html">Study Notes: Stanford CS336 Language Modeling from Scratch [8]</title><link href="http://localhost:4000/cs336/2025/10/05/cs336-training-a-transformer-lm-part-1.html" rel="alternate" type="text/html" title="Study Notes: Stanford CS336 Language Modeling from Scratch [8]" /><published>2025-10-05T00:00:00-07:00</published><updated>2025-10-05T00:00:00-07:00</updated><id>http://localhost:4000/cs336/2025/10/05/cs336-training-a-transformer-lm-part-1</id><content type="html" xml:base="http://localhost:4000/cs336/2025/10/05/cs336-training-a-transformer-lm-part-1.html"><![CDATA[<h2 id="planning-llm-training-cross-entropy-loss-optimizers-memory-and-computational-cost-and-other-practical-levers">Planning LLM Training: Cross-Entropy Loss, Optimizers, Memory and Computational Cost, and other practical levers</h2>

<h3 id="table-of-contents">Table of Contents</h3>
<ol>
  <li><a href="#cross-entropy-loss">Cross-Entropy Loss: Measuring How Wrong We Are</a></li>
  <li><a href="#perplexity">Perplexity: A More Intuitive Metric</a></li>
  <li><a href="#sgd-optimizer">SGD Optimizer: Walking Downhill</a></li>
  <li><a href="#adamw">AdamW: The Smart Optimizer</a></li>
  <li><a href="#memory-requirements">Memory Requirements: Can It Fit?</a></li>
  <li><a href="#computational-cost">Computational Cost: How Long Will This Take?</a></li>
  <li><a href="#learning-rate-schedules">Learning Rate Schedules: Starting Fast, Ending Slow</a></li>
  <li><a href="#gradient-clipping">Gradient Clipping: The Safety Mechanism</a></li>
</ol>

<hr />

<h3 id="cross-entropy-loss">Cross-Entropy Loss: Measuring How Wrong We Are</h3>

<p><em>A language model is trying to <strong>predict the next word</strong> in a sequence. Noted that weâ€™re training a supervised learning model, as we know what is the next correct word in the sequence from the training dataset, and we want to minimize the loss when the model did not predict the correct word.</em></p>

<h4 id="simple-example">Simple Example</h4>

<p>Imagine you have the sentence: â€œThe cat sat on the ___â€</p>

<ul>
  <li>The model needs to predict what comes next</li>
  <li>Maybe the correct word is â€œmatâ€</li>
  <li>The model gives probabilities for all possible words in its vocabulary</li>
</ul>

<h4 id="the-simple-version-of-the-math">The Simple Version of the Math</h4>

<p><strong>Step 1: The model outputs â€œlogitsâ€</strong> (raw scores for each possible word)</p>
<ul>
  <li>Think of logits as unnormalized scores</li>
  <li>Example: â€œmatâ€ gets score 5.2, â€œdogâ€ gets 1.3, â€œtableâ€ gets 3.1, etc.</li>
</ul>

<p><strong>Step 2: Convert logits to probabilities using softmax</strong></p>

\[p(\text{word}) = \frac{e^{\text{score of that word}}}{\text{sum of } e^{\text{score}} \text{ for all words}}\]

<p>Example:</p>
<ul>
  <li>If â€œmatâ€ has score 5.2: probability = $\frac{e^{5.2}}{e^{5.2} + e^{1.3} + e^{3.1} + â€¦}$</li>
</ul>

<p><strong>Step 3: Calculate the loss</strong></p>

\[\text{loss} = -\log(p(\text{correct word}))\]

<p>Assume the correct word is â€œmatâ€, and we want the model to assign high probability to the correct word and low probability to the incorrect word.</p>

<ul>
  <li>If the model gives â€œmatâ€ a probability of 0.8 â†’ loss = -log(0.8) â‰ˆ 0.22 (small loss, good!)</li>
  <li>If the model gives â€œmatâ€ a probability of 0.1 â†’ loss = -log(0.1) â‰ˆ 2.30 (big loss, bad!)</li>
  <li>If the model gives â€œmatâ€ a probability of 1.0 â†’ loss = -log(1.0) = 0 (no loss, perfect!)</li>
</ul>

<h4 id="the-full-version-of-the-math">The Full Version of the Math</h4>

<p>The complete formula averages this loss over:</p>
<ul>
  <li>All positions in a sequence (i = 1 to m)</li>
  <li>All sequences in the training dataset D (all x in D)</li>
</ul>

\[\ell(\theta; D) = \frac{1}{|D|m} \sum_{x \in D} \sum_{i=1}^{m} -\log p_\theta(x_{i+1} | x_{1:i})\]

<p><strong>Bottom line:</strong> Cross-entropy loss is small when the model assigns high probability to the correct next word, and large when it doesnâ€™t. Training tries to minimize this loss!</p>

<hr />

<h3 id="perplexity">Perplexity: A More Intuitive Metric</h3>

<p><strong>Perplexity</strong> is an <strong>evaluation metric</strong> (not a loss function) that provides a more intuitive way to measure how good your language model is. It answers the question: <strong>â€œOn average, how many words is the model confused between?â€</strong>  While we <strong>train</strong> using cross-entropy loss, we <strong>report</strong> perplexity to humans because itâ€™s easier to interpret.</p>

<h4 id="simple-analogy">Simple Analogy</h4>

<p>Imagine a multiple-choice test:</p>

<ul>
  <li><strong>Perplexity = 1</strong>: The model is 100% certain (like having only 1 choice)</li>
  <li><strong>Perplexity = 10</strong>: The model is as confused as if it had to guess among 10 equally likely options</li>
  <li><strong>Perplexity = 100</strong>: The model is as confused as if it had to guess among 100 equally likely options</li>
</ul>

<p><strong>Lower perplexity = Better model!</strong></p>

<h4 id="the-math">The Math</h4>

<p>For a sequence where we make m predictions with cross-entropy losses $\ell_1, \ell_2, â€¦, \ell_m$:</p>

\[\text{perplexity} = \exp\left(\frac{1}{m} \sum_{i=1}^{m} \ell_i\right)\]

<p>This is equivalent to:</p>

\[\text{perplexity} = \exp(\text{average cross-entropy loss})\]

<p><strong>Breaking it down:</strong></p>

<p><strong>Step 1:</strong> Calculate the average cross-entropy loss</p>
<ul>
  <li>Recall that for each position i: 
  \(\ell_i = -\log p(x_{i+1} | x_{1:i})\)</li>
  <li>Add up all the losses: $\ell_1 + \ell_2 + â€¦ + \ell_m$</li>
  <li>Divide by m (the number of token predictions in the sequence)</li>
  <li>Average loss = $\frac{1}{m} \sum_{i=1}^{m} \ell_i$</li>
</ul>

<p><strong>Step 2:</strong> Take the exponential</p>
<ul>
  <li>Apply $\exp()$ to the average loss</li>
  <li>This â€œundoesâ€ the log in the cross-entropy formula</li>
</ul>

<h4 id="why-exponential">Why Exponential?</h4>

<p>The exponential transformation converts the abstract loss value into an interpretable number:</p>

<p><strong>Mathematical intuition:</strong></p>
<ul>
  <li>Cross-entropy loss: $\ell = -\log p(\text{correct word})$</li>
  <li>Perplexity undoes the log: $\exp(\ell) = \exp(-\log p) = \frac{1}{p}$</li>
  <li>If average probability is 0.1, perplexity â‰ˆ 10 (confused among ~10 words)</li>
  <li>If average probability is 0.01, perplexity â‰ˆ 100 (confused among ~100 words)</li>
</ul>

<h4 id="concrete-example">Concrete Example</h4>

<p>Say your model predicts 3 words in a sequence:</p>
<ul>
  <li>Word 1: $\ell_1 = 0$, probability was 1.0 (perfect!)</li>
  <li>Word 2: $\ell_2 = 2.3$, probability was 0.1</li>
  <li>Word 3: $\ell_3 = 0.69$, probability was 0.5</li>
</ul>

<p><strong>Calculation:</strong></p>

<p>Average loss = $\frac{0 + 2.3 + 0.69}{3} = \frac{2.99}{3} \approx 1.0$</p>

<p>Perplexity = $\exp(1.0) \approx 2.72$</p>

<p><strong>Interpretation:</strong> On average, the model is as uncertain as if it had to choose uniformly among about <strong>2.72 equally likely words</strong> at each position.</p>

<h4 id="relationship-to-training">Relationship to Training</h4>

<ul>
  <li><strong>During training:</strong> We minimize cross-entropy loss</li>
  <li><strong>During evaluation:</strong> We report perplexity for interpretability</li>
  <li><strong>Theyâ€™re equivalent:</strong> Lower cross-entropy âŸº Lower perplexity</li>
</ul>

<p>Since perplexity is just an exponential transformation of cross-entropy, optimizing one automatically optimizes the other. We use cross-entropy for training because it has better mathematical properties for gradient-based optimization.</p>

<h4 id="key-takeaway">Key Takeaway</h4>

<p><strong>Perplexity is a user-friendly version of cross-entropy loss:</strong></p>
<ul>
  <li>Lower perplexity = model is more confident and accurate</li>
  <li>Higher perplexity = model is confused and uncertain</li>
  <li>Itâ€™s <strong>not used for training</strong>, only for <strong>reporting results</strong> in a more interpretable way</li>
  <li>Cross-entropy and perplexity are mathematically equivalentâ€”minimizing one minimizes the other</li>
</ul>

<hr />

<h3 id="sgd-optimizer">SGD Optimizer: Which direction to walk to go downhill</h3>

<p>SGD (Stochastic Gradient Descent) is an algorithm that <strong>adjusts your modelâ€™s parameters</strong> during the training process to make the loss smaller. Think of it as teaching the model to make better predictions.</p>
<h4 id="the-mountain-analogy">The Mountain Analogy</h4>

<p>Imagine youâ€™re standing on a mountain in the fog (you canâ€™t see far):</p>
<ul>
  <li>Your <strong>position</strong> = model parameters (Î¸)</li>
  <li>Your <strong>altitude</strong> = loss (how bad the model is)</li>
  <li>Your <strong>goal</strong> = get to the bottom of the valley (minimize loss)</li>
</ul>

<p><strong>SGD tells you which direction to walk to go downhill!</strong></p>

<h4 id="how-sgd-works-step-by-step">How SGD Works (Step by Step)</h4>

<h5 id="step-1-start-randomly">Step 1: Start Randomly</h5>
<ul>
  <li>Î¸â‚€ = random starting position on the mountain</li>
  <li>You donâ€™t know where the bottom is yet</li>
</ul>

<h5 id="step-2-look-around-calculate-gradient">Step 2: Look Around (Calculate Gradient)</h5>
<ul>
  <li>âˆ‡L(Î¸â‚œ; Bâ‚œ) = â€œWhich direction is downhill?â€</li>
  <li>The gradient tells you the steepest uphill direction</li>
  <li>So <strong>negative gradient</strong> points downhill!</li>
</ul>

<h5 id="step-3-take-a-step-downhill">Step 3: Take a Step Downhill</h5>

\[\theta_{t+1} = \theta_t - \alpha_t \nabla L(\theta_t; B_t)\]

<p>Let me break down each part:</p>
<ul>
  <li><strong>Î¸â‚œ</strong> = where you are now</li>
  <li><strong>âˆ‡L(Î¸â‚œ; Bâ‚œ)</strong> = direction of steepest uphill</li>
  <li><strong>-âˆ‡L(Î¸â‚œ; Bâ‚œ)</strong> = direction of steepest downhill (flip the sign!)</li>
  <li><strong>Î±â‚œ</strong> = learning rate (how big a step to take)</li>
  <li><strong>Î¸â‚œâ‚Šâ‚</strong> = your new position</li>
</ul>

<h5 id="step-4-repeat">Step 4: Repeat!</h5>
<ul>
  <li>Keep taking steps downhill until you reach the valley (minimum loss)</li>
</ul>

<h4 id="key-concepts">Key Concepts</h4>

<h5 id="learning-rate-Î±â‚œ">Learning Rate (Î±â‚œ)</h5>
<ul>
  <li><strong>Too large</strong>: You take huge steps and might overshoot the valley</li>
  <li><strong>Too small</strong>: You take tiny steps and it takes forever</li>
  <li><strong>Just right</strong>: You make steady progress</li>
</ul>

<p><strong>Example:</strong></p>
<ul>
  <li>If gradient says â€œgo left by 10 unitsâ€ and Î± = 0.1</li>
  <li>You actually move right by: 10 Ã— 0.1 = 1 unit</li>
</ul>

<h5 id="batch-bâ‚œ">Batch (Bâ‚œ)</h5>
<ul>
  <li>Instead of using ALL your data to calculate the gradient (slow!), use a <strong>random small batch</strong></li>
  <li>This is the â€œstochasticâ€ part - itâ€™s random! Here â€œrandomâ€ means at each training step t, we randomly sample a subset of examples from the full training dataset D.</li>
  <li><strong>Batch size</strong> = how many examples you use each step (this is fixed during training)</li>
</ul>

<p><strong>Why random batches?</strong></p>
<ul>
  <li>Much faster! (calculating gradient on 1 million examples is slow)</li>
  <li>Still gives you a good enough direction</li>
  <li>Adds helpful randomness that can escape bad spots</li>
</ul>

<h4 id="simple-example-1">Simple Example</h4>

<p>Suppose your model has one parameter Î¸ (to keep it simple):</p>

<p><strong>Initial:</strong> Î¸â‚€ = 5, loss = 100</p>

<p><strong>Step 1:</strong></p>
<ul>
  <li>Calculate gradient on a batch: âˆ‡L = 20 (loss increases if we increase Î¸)</li>
  <li>Learning rate: Î± = 0.1</li>
  <li>Update: Î¸â‚ = 5 - 0.1 Ã— 20 = 5 - 2 = <strong>3</strong></li>
</ul>

<p><strong>Step 2:</strong></p>
<ul>
  <li>New gradient: âˆ‡L = 10</li>
  <li>Update: Î¸â‚‚ = 3 - 0.1 Ã— 10 = 3 - 1 = <strong>2</strong></li>
</ul>

<p><strong>Step 3:</strong></p>
<ul>
  <li>New gradient: âˆ‡L = 2</li>
  <li>Update: Î¸â‚ƒ = 2 - 0.1 Ã— 2 = 2 - 0.2 = <strong>1.8</strong></li>
</ul>

<p>You keep going until the loss stops decreasing!</p>

<h4 id="key-takeaway-1">Key Takeaway</h4>

<p><strong>SGD is like walking downhill in small steps:</strong></p>
<ol>
  <li>Check which way is uphill (gradient)</li>
  <li>Take a step in the negative direction (size of the step determined by learning rate)</li>
  <li>Repeat until you reach the bottom (minimum loss)</li>
</ol>

<p>The â€œstochasticâ€ part just means you randomly sample small batches of data instead of using entire dataset when calculate the gradient, making it much faster!</p>

<hr />

<h3 id="adamw">AdamW: The Smart Optimizer</h3>

<h4 id="whats-the-problem-with-sgd">Whatâ€™s the Problem with SGD?</h4>

<p>Remember SGD takes the same size step (Î±) for every parameter. But what if:</p>
<ul>
  <li>Some parameters need <strong>big updates</strong> (theyâ€™re far from optimal)</li>
  <li>Some parameters need <strong>tiny updates</strong> (theyâ€™re almost perfect)</li>
</ul>

<p><strong>AdamW is smarter</strong> via adapting the step size for each parameter individually.</p>

<h4 id="the-big-idea">The Big Idea</h4>

<p>AdamW keeps track of <strong>two pieces of memory</strong> for each parameter:</p>

<ol>
  <li><strong>m (first moment)</strong>: â€œWhich direction has this parameter been moving lately?â€ (like momentum)</li>
  <li><strong>v (second moment)</strong>: â€œHow much has this parameter been jumping around?â€ (like volatility)</li>
</ol>

<p>Then it uses this information to take smarter steps!</p>

<h4 id="how-adamw-works-step-by-step">How AdamW Works (Step by Step)</h4>

<h5 id="setup">Setup</h5>
<ul>
  <li><strong>m = 0</strong>: Start with no momentum</li>
  <li><strong>v = 0</strong>: Start with no volatility estimate</li>
  <li><strong>Î²â‚ = 0.9</strong>: How much to remember past directions (typically 90%)</li>
  <li><strong>Î²â‚‚ = 0.999</strong>: How much to remember past volatility (typically 99.9%)</li>
</ul>

<h5 id="each-training-step">Each Training Step</h5>

<p><strong>Step 1: Calculate gradient</strong> (same as SGD)</p>
<ul>
  <li>g = âˆ‡â„“(Î¸; Bâ‚œ)</li>
  <li>â€œWhich way should we move?â€</li>
</ul>

<p><strong>Step 2: Update momentum (first moment)</strong></p>

\[m = \beta_1 \cdot m + (1-\beta_1) \cdot g\]

<p>Think of this as an <strong>exponential moving average</strong>:</p>
<ul>
  <li>Keep 90% of the old direction (Î²â‚m)</li>
  <li>Add 10% of the new direction ((1-Î²â‚)g)</li>
  <li>This smooths out noisy gradients!</li>
</ul>

<p><strong>Step 3: Update volatility (second moment)</strong></p>

\[v = \beta_2 \cdot v + (1-\beta_2) \cdot g^2\]

<p>Same idea but for squared gradients:</p>
<ul>
  <li>Keep 99.9% of old volatility estimate</li>
  <li>Add 0.1% of new squared gradient</li>
  <li>This tracks how â€œjumpyâ€ the parameter is</li>
</ul>

<p><strong>Step 4: Adjust learning rate</strong></p>

\[\alpha_t = \alpha \frac{\sqrt{1-\beta_2^t}}{1-\beta_1^t}\]

<p>This <strong>bias correction</strong> compensates for starting at m=0 and v=0 (they start biased toward zero!)</p>

<p><strong>Step 5: Update parameters (the smart part!)</strong></p>

\[\theta = \theta - \alpha_t \frac{m}{\sqrt{v} + \epsilon}\]

<p>This is where the magic happens:</p>
<ul>
  <li><strong>m</strong> tells us which direction to go</li>
  <li><strong>âˆšv</strong> tells us how confident we should be</li>
  <li>If v is <strong>large</strong> (parameter is jumpy) â†’ take <strong>smaller</strong> steps</li>
  <li>If v is <strong>small</strong> (parameter is stable) â†’ take <strong>larger</strong> steps</li>
</ul>

<p><strong>Step 6: Weight decay</strong></p>

\[\theta = \theta - \alpha\lambda\theta\]

<p>Pull parameters slightly toward zero to prevent them from getting too large (regularization)</p>

<h4 id="simple-example-2">Simple Example</h4>

<p>Imagine two parameters:</p>

<p><strong>Parameter A:</strong></p>
<ul>
  <li>Gradients: [5, 5.1, 4.9, 5, 5] (very stable!)</li>
  <li>v will be small â†’ AdamW takes <strong>bigger</strong> steps</li>
  <li>Makes sense! Weâ€™re confident about the direction</li>
</ul>

<p><strong>Parameter B:</strong></p>
<ul>
  <li>Gradients: [5, -4, 6, -3, 5] (super noisy!)</li>
  <li>v will be large â†’ AdamW takes <strong>smaller</strong> steps</li>
  <li>Makes sense! Weâ€™re uncertain, so be cautious</li>
</ul>

<h4 id="key-hyperparameters">Key Hyperparameters</h4>

<table>
  <thead>
    <tr>
      <th>Parameter</th>
      <th>Typical Value</th>
      <th>What it does</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Î± (learning rate)</td>
      <td>0.001 or 0.0001</td>
      <td>Base step size</td>
    </tr>
    <tr>
      <td>Î²â‚</td>
      <td>0.9</td>
      <td>How much momentum to keep</td>
    </tr>
    <tr>
      <td>Î²â‚‚</td>
      <td>0.95-0.999</td>
      <td>How much volatility history to keep</td>
    </tr>
    <tr>
      <td>Î» (weight decay)</td>
      <td>0.01</td>
      <td>How much to pull toward zero</td>
    </tr>
    <tr>
      <td>Îµ</td>
      <td>10â»â¸</td>
      <td>Prevent division by zero</td>
    </tr>
  </tbody>
</table>

<h4 id="key-takeaway-2">Key Takeaway</h4>

<p><strong>AdamW is like a smart GPS:</strong></p>
<ul>
  <li><strong>SGD</strong>: â€œAlways drive 50 mph, no matter whatâ€</li>
  <li><strong>AdamW</strong>: â€œDrive faster on smooth highways, slower on bumpy roadsâ€</li>
</ul>

<p>It <strong>adapts the step size</strong> for each parameter based on:</p>
<ol>
  <li>Recent direction (momentum)</li>
  <li>Recent stability (volatility)</li>
</ol>

<p>This makes training <strong>faster and more stable</strong>, which is why all modern language models use it.</p>

<hr />

<h3 id="memory-requirements">Memory Requirements: Can It Fit?</h3>

<p>Letâ€™s calculate how much memory we need to train a model like GPT-2 XL using AdamW with float32 precision.</p>

<h4 id="setup-1">Setup</h4>
<ul>
  <li>Data type: <strong>float32</strong> = <strong>4 bytes</strong> per number</li>
  <li>Batch size: <strong>B</strong></li>
  <li>Sequence length: <strong>L</strong> (context_length)</li>
  <li>Model dimension: <strong>d</strong> (d_model)</li>
  <li>Number of layers: <strong>N</strong> (num_layers)</li>
  <li>Number of heads: <strong>H</strong> (num_heads)</li>
  <li>Vocabulary size: <strong>V</strong> (vocab_size)</li>
  <li>Feed-forward dimension: <strong>d_ff = 4d</strong></li>
</ul>

<h3 id="memory-components">Memory Components</h3>

<p>Training a Transformer model requires four main types of memory. <strong>Parameters</strong> store the modelâ€™s learnable weightsâ€”the numbers that define what the model knows. <strong>Gradients</strong> store the <em>direction</em> and <em>magnitude</em> of how each parameter should change during training, computed during backpropagation. <strong>Optimizer state</strong> keeps AdamWâ€™s running statistics: <em>momentum</em> (which direction parameters have been moving) and <em>volatility</em> (how much parameters have been fluctuating), allowing the optimizer to make smarter, adaptive updates for each parameter. <strong>Activations</strong> store all the intermediate calculations from the forward passâ€”like attention scores, normalized values, and layer outputsâ€”which must be kept in memory so we can compute gradients during backpropagation. While parameters, gradients, and optimizer state have fixed size (Parameters, gradients, and optimizer state represent the modelâ€™s internal structureâ€”they exist regardless of what data you feed into the model), activations are the actual values flowing through the network for the particular batch therefore they scale dramatically with both batch size (B) and sequence length (L), particularly the attention scores which grow quadratically as O(BLÂ²). This is why memory, not computation, is often the bottleneck in training large language modelsâ€”with GPT-2 XL, even an 80GB GPU can only fit a batch size of 3.</p>

<h4 id="1-parameters-memory">1. Parameters Memory</h4>

<p>Letâ€™s count the learnable parameters in each component of the Transformer.</p>

<h5 id="per-transformer-block">Per Transformer Block:</h5>

<p><strong>A. RMSNorm Layers (2 per block)</strong></p>

<p>Each RMSNorm layer has a learnable scale parameter for each dimension:</p>
<ul>
  <li>Pre-attention RMSNorm: <strong>d parameters</strong></li>
  <li>Pre-FFN RMSNorm: <strong>d parameters</strong></li>
  <li><strong>Subtotal: 2d parameters</strong></li>
</ul>

<p><strong>B. Multi-Head Self-Attention</strong></p>

<p>The attention mechanism consists of four projection matrices. Importantly, <strong>the number of heads H does not affect the parameter count</strong>â€”we split the d dimensions across heads rather than expanding them.</p>

<ul>
  <li>Query projection W_Q: (d Ã— d) â†’ <strong>dÂ² parameters</strong></li>
  <li>Key projection W_K: (d Ã— d) â†’ <strong>dÂ² parameters</strong></li>
  <li>Value projection W_V: (d Ã— d) â†’ <strong>dÂ² parameters</strong></li>
  <li>Output projection W_O: (d Ã— d) â†’ <strong>dÂ² parameters</strong></li>
  <li><strong>Subtotal: 4dÂ² parameters</strong></li>
</ul>

<p><em>Note: Modern architectures typically omit bias terms in these projections.</em></p>

<p><strong>C. Feed-Forward Network (FFN)</strong></p>

<p>The FFN expands to an intermediate dimension d_ff = 4d, then projects back:</p>
<ul>
  <li>First layer Wâ‚: (d Ã— 4d) â†’ <strong>4dÂ² parameters</strong></li>
  <li>Activation (SiLU/GELU): <strong>0 parameters</strong> (no learnable weights)</li>
  <li>Second layer Wâ‚‚: (4d Ã— d) â†’ <strong>4dÂ² parameters</strong></li>
  <li><strong>Subtotal: 8dÂ² parameters</strong></li>
</ul>

<p><strong>Total per block: 2d + 4dÂ² + 8dÂ² = 12dÂ² + 2d</strong></p>

<h5 id="all-n-transformer-blocks">All N Transformer Blocks:</h5>

<p><strong>N Ã— (12dÂ² + 2d) = 12NdÂ² + 2Nd</strong></p>

<h5 id="additional-components">Additional Components:</h5>

<p><strong>Token Embedding</strong></p>
<ul>
  <li>Maps each of V vocabulary tokens to a d-dimensional vector</li>
  <li>Shape: (V Ã— d)</li>
  <li><strong>Parameters: Vd</strong></li>
</ul>

<p><strong>Final RMSNorm</strong></p>
<ul>
  <li>One scale parameter per dimension after the last transformer block</li>
  <li><strong>Parameters: d</strong></li>
</ul>

<p><strong>Output Projection</strong></p>
<ul>
  <li>In modern LLMs (GPT-2, LLaMA, etc.), the output projection <strong>shares weights</strong> with the token embedding (weight tying)</li>
  <li><strong>Additional parameters: 0</strong></li>
</ul>

<p><strong>Positional Embeddings</strong> (architecture-dependent)</p>
<ul>
  <li><strong>Modern models (LLaMA, GPT-3+):</strong> Use RoPE or ALiBi â†’ <strong>0 parameters</strong> âœ“</li>
  <li><strong>Older models (GPT-2, BERT):</strong> Learned positional embeddings â†’ <strong>L_max Ã— d parameters</strong></li>
</ul>

<p>For this calculation, we assume modern architecture with no learned positional embeddings.</p>

<h5 id="total-parameters">Total Parameters:</h5>

<p><strong>P = 12NdÂ² + 2Nd + Vd + d</strong></p>

<p>Which can be factored as:</p>

<p><strong>P = 12NdÂ² + d(2N + V + 1)</strong></p>

<p><strong>Important notes:</strong></p>
<ul>
  <li>The sequence length L does <strong>not</strong> affect parameter count (for modern architectures)</li>
  <li>The number of attention heads H does <strong>not</strong> affect parameter count</li>
  <li>The dÂ² term dominates for large models (quadratic scaling with model dimension)</li>
  <li>The Vd term can be significant for large vocabularies</li>
</ul>

<h5 id="memory-requirement">Memory Requirement:</h5>

<p>Since each parameter is stored as <strong>float32</strong> (4 bytes):</p>

<p><strong>Parameters memory = 4P bytes</strong></p>

<p><strong>Example (GPT-2 XL):</strong></p>
<ul>
  <li>N = 48, d = 1,600, V = 50,257</li>
  <li>P = 12(48)(1,600Â²) + 1,600(2Ã—48 + 50,257 + 1)</li>
  <li>P â‰ˆ <strong>1,555,126,400 parameters</strong> (~1.56B)</li>
  <li><strong>Memory = 4 Ã— 1.56B â‰ˆ 6.2 GB</strong></li>
</ul>

<h4 id="2-gradients-memory">2. Gradients Memory</h4>

<p>During backpropagation, we compute gradients for all parameters. AdamW requires storing these gradients to perform parameter updates.</p>

<p><strong>Gradients have the same shape as parameters:</strong></p>
<ul>
  <li>One gradient value per parameter</li>
  <li>Stored as float32 (4 bytes each)</li>
</ul>

<p><strong>Gradients memory = 4P bytes</strong></p>

<h4 id="3-optimizer-state-memory">3. Optimizer State Memory</h4>

<p>AdamW is a <strong>stateful optimizer</strong> that maintains running statistics for each parameter:</p>

<p><strong>First moment (m):</strong> Exponential moving average of gradients (momentum)</p>
<ul>
  <li>Shape: same as parameters</li>
  <li>Storage: 4 bytes per parameter (float32)</li>
  <li><strong>Memory: 4P bytes</strong></li>
</ul>

<p><strong>Second moment (v):</strong> Exponential moving average of squared gradients (volatility)</p>
<ul>
  <li>Shape: same as parameters</li>
  <li>Storage: 4 bytes per parameter (float32)</li>
  <li><strong>Memory: 4P bytes</strong></li>
</ul>

<p><strong>Total optimizer state memory = 4P + 4P = 8P bytes</strong></p>

<p><strong>Note:</strong> Unlike parameters and gradients which all models need, this 8P overhead is specific to Adam-family optimizers. Simpler optimizers like SGD only need gradient storage (4P), while more complex optimizers may require even more state.</p>

<h4 id="4-activations-memory">4. Activations Memory</h4>

<p>Activations are intermediate values computed during the forward pass that must be stored for backpropagation. This is where batch size (B) and sequence length (L) have major impact.</p>

<p><strong>Key factors:</strong></p>
<ul>
  <li>Activations scale with <strong>B</strong> (batch size) and <strong>L</strong> (sequence length)</li>
  <li>We need to store activations at multiple points for gradient computation</li>
  <li>This is typically the <strong>memory bottleneck</strong> for training</li>
</ul>

<h5 id="per-transformer-layer-activations">Per Transformer Layer Activations:</h5>

<p><strong>A. RMSNorm activations:</strong></p>
<ul>
  <li>Input to pre-attention norm: BLd</li>
  <li>Output of pre-attention norm: BLd</li>
  <li>Input to pre-FFN norm: BLd</li>
  <li><strong>Subtotal: ~3BLd</strong></li>
</ul>

<p><strong>B. Attention intermediate values:</strong></p>
<ul>
  <li>Q, K, V projections: 3 Ã— BLd = <strong>3BLd</strong></li>
  <li>Attention scores (before softmax): B Ã— H Ã— L Ã— L = <strong>BHLÂ²</strong></li>
  <li>Attention weights (after softmax): B Ã— H Ã— L Ã— L = <strong>BHLÂ²</strong> (needed for softmax backward)</li>
  <li>Attention output: BLd</li>
</ul>

<p><strong>Subtotal: ~4BLd + 2BHLÂ²</strong></p>

<p><strong>C. Feed-Forward Network activations:</strong></p>
<ul>
  <li>Wâ‚ output (before activation): B Ã— L Ã— 4d = <strong>4BLd</strong></li>
  <li>SiLU/GELU output: B Ã— L Ã— 4d = <strong>4BLd</strong> (needed for activation backward)</li>
  <li>Wâ‚‚ output: BLd</li>
</ul>

<p><strong>Subtotal: ~9BLd</strong></p>

<p><strong>Total per layer: 3BLd + 4BLd + 2BHLÂ² + 9BLd â‰ˆ 16BLd + 2BHLÂ²</strong></p>

<p><em>Note: The original formulaâ€™s 16BLd is an approximation; exact value depends on implementation details like whether certain intermediate values are recomputed vs. stored.</em></p>

<h5 id="all-n-layers">All N Layers:</h5>

<p><strong>N Ã— (16BLd + 2BHLÂ²) â‰ˆ 16NBLd + 2NBHLÂ²</strong></p>

<h5 id="additional-activations-outside-layers">Additional Activations Outside Layers:</h5>

<p><strong>Token embeddings:</strong></p>
<ul>
  <li>Embedding lookup output: <strong>BLd</strong></li>
</ul>

<p><strong>Final RMSNorm:</strong></p>
<ul>
  <li>Negligible (included in layer activations)</li>
</ul>

<p><strong>Output layer (logits):</strong></p>
<ul>
  <li>Softmax probabilities: B Ã— L Ã— V = <strong>BLV</strong> (needed for cross-entropy backward)</li>
</ul>

<p><strong>Total additional: BLd + BLV</strong></p>

<h5 id="total-activation-count">Total Activation Count:</h5>

<p><strong>A = 16NBLd + 2NBHLÂ² + BLd + BLV</strong></p>

<p>Simplified:
<strong>A â‰ˆ NBLd(16 + 1/N) + 2NBHLÂ² + BLV</strong></p>

<p>For large N, this is dominated by: <strong>A â‰ˆ 16NBLd + 2NBHLÂ² + BLV</strong></p>

<p><strong>Activations memory = 4A bytes</strong> (float32)</p>

<p><strong>Key observations:</strong></p>
<ul>
  <li><strong>O(BL) scaling:</strong> Most activations scale linearly with batch size and sequence length</li>
  <li><strong>O(BLÂ²) scaling:</strong> Attention scores create quadratic memory growth with sequence length</li>
  <li><strong>Bottleneck:</strong> For long sequences, the 2NBHLÂ² term (attention scores) dominates</li>
</ul>

<h4 id="total-peak-memory">Total Peak Memory</h4>

<table>
  <thead>
    <tr>
      <th>Component</th>
      <th>Memory (bytes)</th>
      <th>Notes</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Parameters</strong></td>
      <td>4P</td>
      <td>Model weights</td>
    </tr>
    <tr>
      <td><strong>Gradients</strong></td>
      <td>4P</td>
      <td>âˆ‚L/âˆ‚Î¸ for all parameters</td>
    </tr>
    <tr>
      <td><strong>Optimizer State (m, v)</strong></td>
      <td>8P</td>
      <td>AdamW momentum and variance</td>
    </tr>
    <tr>
      <td><strong>Activations</strong></td>
      <td>4A</td>
      <td>Intermediate values for backprop</td>
    </tr>
    <tr>
      <td><strong>TOTAL</strong></td>
      <td><strong>16P + 4A</strong></td>
      <td>Peak during training</td>
    </tr>
  </tbody>
</table>

<p><strong>Breakdown:</strong></p>
<ul>
  <li><strong>Fixed cost (16P):</strong> Independent of batch size and sequence length</li>
  <li><strong>Variable cost (4A):</strong> Scales with B, L, and LÂ²</li>
</ul>

<h4 id="gpt-2-xl-example">GPT-2 XL Example</h4>

<p><strong>Model specifications:</strong></p>
<ul>
  <li>vocab_size (V) = 50,257</li>
  <li>context_length (L) = 1,024</li>
  <li>num_layers (N) = 48</li>
  <li>d_model (d) = 1,600</li>
  <li>num_heads (H) = 25</li>
  <li>d_ff = 4 Ã— d = 6,400</li>
</ul>

<h5 id="step-1-calculate-parameters-p">Step 1: Calculate Parameters (P)</h5>

<p><strong>P = 12NdÂ² + 2Nd + Vd + d</strong></p>

<p>P = 12(48)(1,600Â²) + 2(48)(1,600) + 50,257(1,600) + 1,600</p>

<p><strong>P â‰ˆ 1,555,126,400 â‰ˆ 1.56 Ã— 10â¹ parameters</strong></p>

<h5 id="step-2-calculate-fixed-memory-16p">Step 2: Calculate Fixed Memory (16P)</h5>

<p>Fixed memory = 16 Ã— 1,555,126,400 bytes</p>

<p><strong>Fixed memory â‰ˆ 24.88 GB</strong></p>

<p>This includes parameters (4P), gradients (4P), and optimizer state (8P).</p>

<h5 id="step-3-calculate-activation-memory-per-batch-4a">Step 3: Calculate Activation Memory per Batch (4A)</h5>

<p><strong>A = 16NBLd + 2NBHLÂ² + BLd + BLV</strong></p>

<p>For batch_size = B:</p>

<p>A = 16(48)(B)(1,024)(1,600) + 2(48)(B)(25)(1,024Â²) + B(1,024)(1,600) + B(1,024)(50,257)</p>

<p><strong>Activation memory = 4A â‰ˆ 14.3 Ã— B GB</strong></p>

<h5 id="step-4-total-memory-formula">Step 4: Total Memory Formula</h5>

<p><strong>Total Memory = 24.88 + 14.3 Ã— B GB</strong></p>

<h5 id="step-5-maximum-batch-size-for-80gb-gpu">Step 5: Maximum Batch Size for 80GB GPU</h5>

<p>Solving: 24.88 + 14.3B â‰¤ 80</p>

<p><strong>B â‰¤ 3.85</strong></p>

<p><strong>Maximum batch_size = 3</strong> (must be integer)</p>

<p><strong>Key insight:</strong> So on a single A100 80 GB, GPT-2 XL in pure FP32 training fits a batch size of 3 without further memory optimization. This demonstrates why:</p>
<ol>
  <li>Large-scale training requires massive GPU clusters</li>
  <li>Techniques like gradient accumulation, mixed precision (float16/bfloat16), and activation checkpointing are essential</li>
  <li>Memory, not computation, is often the bottleneck</li>
</ol>

<p>The estimate above assumes naÃ¯ve attention that stores full BÃ—HÃ—LÃ—L score and probability tensors, and a non-fused cross-entropy head. Modern implementations cut this drastically:</p>

<table>
  <thead>
    <tr>
      <th>Technique</th>
      <th>What it Removes</th>
      <th>Result (GPT-2 XL, FP32)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>FlashAttention</strong></td>
      <td>avoids LÂ² attention matrices</td>
      <td>â‰ˆ 5 GB per batch</td>
    </tr>
    <tr>
      <td><strong>Fused CE</strong></td>
      <td>streams logits â†’ softmax</td>
      <td>reduce 0.5â€“1 GB per batch</td>
    </tr>
    <tr>
      <td><strong>Activation checkpointing</strong></td>
      <td>recomputes during backward</td>
      <td>â‰ˆ Ã— 4â€“6 less</td>
    </tr>
    <tr>
      <td><strong>BF16 / FP16</strong></td>
      <td>halves memory per value</td>
      <td>â‰ˆ Ã— 2 less</td>
    </tr>
  </tbody>
</table>

<p>With FlashAttention and combined with other memory optimization techniques, batch_size = 12-16 is achievable for GPT-2 XL on an 80GB GPU with FP32, and this can be scaled to 32-40 with BF16 or 60-70 with activation checkpointing.</p>

<h3 id="computational-cost">Computational Cost: How Long Will This Take?</h3>

<h4 id="the-standard-formula">The Standard Formula</h4>

<p>For Transformer models, thereâ€™s a widely-used approximation:</p>

<p><strong>Training FLOPs per token â‰ˆ 6 Ã— number of parameters</strong></p>

<p>This breaks down as:</p>
<ul>
  <li><strong>Forward pass:</strong> 2P FLOPs per token</li>
  <li><strong>Backward pass:</strong> 4P FLOPs per token (approximately 2Ã— forward)</li>
  <li><strong>Total:</strong> 6P FLOPs per token</li>
</ul>

<p><strong>Why this approximation works:</strong></p>
<ul>
  <li>Dominated by matrix multiplications in attention and FFN layers</li>
  <li>For a matrix multiply of (m Ã— k) @ (k Ã— n), we perform 2mkn FLOPs</li>
  <li>The â€œ2â€ accounts for <em>multiply</em> and <em>add</em> operations</li>
  <li>Backward pass requires computing gradients for all weight matrices (roughly 2Ã— forward)</li>
</ul>

<p><strong>Whatâ€™s excluded:</strong></p>
<ul>
  <li>Optimizer computations (~11 FLOPs per parameter, negligible compared to 6P per token)</li>
  <li>Element-wise operations (LayerNorm, activations)</li>
  <li>Attention softmax</li>
</ul>

<p>These omissions are small compared to the matrix multiplications, making â€œ6P per tokenâ€ a robust rule of thumb.</p>

<h4 id="gpt-2-xl-training-example">GPT-2 XL Training Example</h4>

<p><strong>Given:</strong></p>
<ul>
  <li>Parameters: P â‰ˆ 1.56 Ã— 10â¹</li>
  <li>Training steps: 400,000</li>
  <li><strong>Batch size: 1,024 tokens per step</strong> (total tokens processed)</li>
  <li>Hardware: Single NVIDIA A100 GPU (40GB or 80GB)</li>
  <li>Theoretical peak: 19.5 teraFLOP/s (FP32)</li>
  <li>MFU (Model FLOPs Utilization): 50%</li>
</ul>

<p><strong>Note on batch size:</strong> â€œ1,024 tokensâ€ typically means the <strong>total number of tokens</strong> processed in one training step.</p>

<p><strong>Calculation:</strong></p>

<p><strong>Step 1: FLOPs per token</strong></p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>6 Ã— 1.56 Ã— 10â¹ = 9.36 Ã— 10â¹ FLOPs per token
</code></pre></div></div>

<p><strong>Step 2: FLOPs per training step</strong></p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>9.36 Ã— 10â¹ Ã— 1,024 tokens = 9.585 Ã— 10Â¹Â² FLOPs per step
</code></pre></div></div>

<p><strong>Step 3: Total FLOPs for 400K steps</strong></p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>9.585 Ã— 10Â¹Â² Ã— 400,000 = 3.834 Ã— 10Â¹â¸ FLOPs
</code></pre></div></div>

<p><strong>Step 4: Effective throughput at 50% MFU</strong></p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Theoretical: 19.5 Ã— 10Â¹Â² FLOP/s
Effective: 19.5 Ã— 10Â¹Â² Ã— 0.5 = 9.75 Ã— 10Â¹Â² FLOP/s
</code></pre></div></div>

<p><strong>Step 5: Training time</strong></p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Time = (3.834 Ã— 10Â¹â¸) / (9.75 Ã— 10Â¹Â²) = 393,231 seconds
       â‰ˆ 109.2 hours
       â‰ˆ 4.55 days
</code></pre></div></div>
<h4 id="key-insights">Key Insights</h4>

<p><strong>Why this matters:</strong></p>

<ol>
  <li><strong>Single GPU training is impractical for large models</strong>
    <ul>
      <li>Even â€œmedium-sizedâ€ GPT-2 XL takes <strong>109 hours (~4.5 days)</strong> on a top-tier A100</li>
      <li>Larger models (GPT-3: 175B parameters) would take <strong>months</strong> on a single GPU</li>
      <li>GPT-3 would require: (175B/1.56B) Ã— 109 hours â‰ˆ 12,200 hours â‰ˆ <strong>1.4 years</strong> on one A100!</li>
    </ul>
  </li>
  <li><strong>Parallelism is essential</strong>
    <ul>
      <li><strong>With 100 A100s:</strong> 109.2 / 100 â‰ˆ <strong>1.1 hours</strong> (assuming perfect scaling)</li>
      <li><strong>With 1,000 A100s:</strong> 109.2 / 1,000 â‰ˆ <strong>6.6 minutes</strong> (assuming perfect scaling)</li>
      <li><strong>Real-world scaling efficiency</strong> is typically 60-90% due to:
        <ul>
          <li>Communication overhead (gradient synchronization)</li>
          <li>Load imbalancing</li>
          <li>Pipeline bubbles</li>
          <li>Network bandwidth limitations</li>
        </ul>
      </li>
      <li><strong>Realistic with 100 A100s:</strong> 109.2 / (100 Ã— 0.7) â‰ˆ <strong>1.6 hours</strong> (at 70% efficiency)</li>
    </ul>
  </li>
  <li><strong>Cost considerations</strong>
    <ul>
      <li><strong>A100 cloud cost:</strong> ~$2-4/hour (varies by provider: AWS, GCP, Azure)</li>
      <li><strong>Single A100 training:</strong> 109.2 hours Ã— $2-4 = <strong>$218-437</strong></li>
      <li><strong>100 A100s (70% efficiency):</strong>
        <ul>
          <li>Time: ~1.6 hours</li>
          <li>Cost: 100 GPUs Ã— 1.6 hours Ã— $2-4 = <strong>$320-640</strong></li>
          <li><strong>Trade-off:</strong> Slightly higher cost, but <strong>68Ã— faster!</strong></li>
        </ul>
      </li>
      <li><strong>Cost scales linearly with GPU count, but time scales sub-linearly</strong> (due to overhead)</li>
    </ul>
  </li>
  <li><strong>Memory vs. compute trade-off</strong>
    <ul>
      <li>We calculated <strong>batch_size = 3</strong> fits in 80GB memory (using FP32)</li>
      <li>Larger batches could improve training efficiency (better GPU utilization, more stable gradients)</li>
      <li><strong>Solutions to increase effective batch size:</strong>
        <ul>
          <li><strong>Gradient accumulation:</strong> Simulate larger batches by accumulating gradients over multiple forward/backward passes before updating
            <ul>
              <li>Example: Accumulate 32 micro-batches of size 3 â†’ effective batch size of 96</li>
            </ul>
          </li>
          <li><strong>Mixed precision (FP16/BF16):</strong> Reduce memory by 2Ã—, allowing batch_size â‰ˆ 6-8</li>
          <li><strong>Gradient checkpointing:</strong> Trade compute for memory (recompute activations during backward pass)</li>
          <li><strong>Multi-GPU training:</strong> Distribute batch across GPUs (data parallelism)</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>Why GPT-3 scale requires massive clusters</strong>
    <ul>
      <li>GPT-3 (175B parameters): <strong>~112Ã— larger</strong> than GPT-2 XL</li>
      <li>Single A100 would take: ~1.4 years</li>
      <li>With <strong>10,000 A100s</strong> (at 60% efficiency): ~12,200 / (10,000 Ã— 0.6) â‰ˆ <strong>2 hours</strong></li>
      <li>This explains why frontier models require:
        <ul>
          <li>Tens of thousands of GPUs</li>
          <li>Custom datacenters</li>
          <li>Months of calendar time (even with massive parallelism)</li>
          <li>Millions of dollars in compute costs</li>
        </ul>
      </li>
    </ul>
  </li>
</ol>

<p><strong>Summary Table for training GPT-2 XL</strong></p>

<table>
  <thead>
    <tr>
      <th>Configuration</th>
      <th>Time</th>
      <th>Cost</th>
      <th>Notes</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>1 A100 (FP32)</strong></td>
      <td>109 hours</td>
      <td>$218-437</td>
      <td>Baseline</td>
    </tr>
    <tr>
      <td><strong>1 A100 (FP16)</strong></td>
      <td>~55 hours</td>
      <td>$110-220</td>
      <td>2Ã— faster with mixed precision</td>
    </tr>
    <tr>
      <td><strong>100 A100s (perfect)</strong></td>
      <td>1.1 hours</td>
      <td>$220-440</td>
      <td>Theoretical best case</td>
    </tr>
    <tr>
      <td><strong>100 A100s (70% eff.)</strong></td>
      <td>1.6 hours</td>
      <td>$320-640</td>
      <td>Realistic with overhead</td>
    </tr>
    <tr>
      <td><strong>1,000 A100s (perfect)</strong></td>
      <td>6.6 minutes</td>
      <td>$220-440</td>
      <td>Theoretical best case</td>
    </tr>
    <tr>
      <td><strong>1,000 A100s (60% eff.)</strong></td>
      <td>11 minutes</td>
      <td>$367-733</td>
      <td>Realistic at scale</td>
    </tr>
  </tbody>
</table>

<p><strong>Key takeaway:</strong> Parallelism gives you speed, not cost savings. Using 100 GPUs costs about the same (or slightly more) but finishes <strong>68Ã— faster</strong>, which matters for iteration speed and time-to-market!</p>

<hr />

<h3 id="learning-rate-schedules">Learning Rate Schedules: Starting Fast, Ending Slow</h3>

<h4 id="why-do-we-need-a-schedule">Why Do We Need a Schedule?</h4>

<p>Imagine youâ€™re trying to find the lowest point in a valley while blindfolded:</p>

<ul>
  <li><strong>Beginning</strong>: Youâ€™re far from the goal â†’ take <strong>big steps</strong> to get there quickly</li>
  <li><strong>Middle</strong>: Youâ€™re getting close â†’ take <strong>medium steps</strong> to avoid overshooting</li>
  <li><strong>End</strong>: Youâ€™re very close â†’ take <strong>tiny steps</strong> to settle into the exact lowest point</li>
</ul>

<p>The learning rate schedule does exactly this for training!</p>

<h4 id="the-problem-with-fixed-learning-rate">The Problem with Fixed Learning Rate</h4>

<p><strong>Too high throughout training:</strong></p>
<ul>
  <li>Fast at first, but bounces around the minimum at the end</li>
  <li>Never settles into the best solution</li>
</ul>

<p><strong>Too low throughout training:</strong></p>
<ul>
  <li>Slow progress, takes forever to train</li>
  <li>Might get stuck in bad spots</li>
</ul>

<p><strong>Solution: Start high, gradually decrease!</strong></p>

<h4 id="cosine-annealing-schedule">Cosine Annealing Schedule</h4>

<p>The schedule has <strong>3 phases</strong>:</p>

<h5 id="phase-1-warm-up-t--t_w">Phase 1: Warm-up (t &lt; T_w)</h5>
<p><strong>â€œEase into itâ€</strong></p>

\[\alpha_t = \frac{t}{T_w} \times \alpha_{max}\]

<ul>
  <li>Start from <strong>0</strong> and <strong>linearly increase</strong> to Î±_max</li>
  <li>Example: If T_w = 1,000 and Î±_max = 0.001:
    <ul>
      <li>Step 0: Î± = 0</li>
      <li>Step 500: Î± = 0.0005 (halfway)</li>
      <li>Step 1,000: Î± = 0.001 (full speed!)</li>
    </ul>
  </li>
</ul>

<p><strong>Why warm-up?</strong></p>
<ul>
  <li>Prevents unstable updates at the very beginning</li>
  <li>Gives the model time to â€œorient itselfâ€</li>
  <li>Like warming up before exercise!</li>
</ul>

<h5 id="phase-2-cosine-annealing-t_w--t--t_c">Phase 2: Cosine Annealing (T_w â‰¤ t â‰¤ T_c)</h5>
<p><strong>â€œSmooth slowdownâ€</strong></p>

\[\alpha_t = \alpha_{min} + \frac{1}{2}\left(1 + \cos\left(\frac{t - T_w}{T_c - T_w} \pi\right)\right)(\alpha_{max} - \alpha_{min})\]

<p>This creates a <strong>smooth curve</strong> from Î±_max down to Î±_min!</p>

<p><strong>Breaking it down:</strong></p>

<ol>
  <li><strong>Progress ratio</strong>: How far through annealing are we? (0 to 1)</li>
  <li><strong>Cosine curve</strong>: cos goes from 1 â†’ -1 as we progress</li>
  <li><strong>Scale to [0, 1]</strong>: Transform to go from 1 â†’ 0</li>
  <li><strong>Final value</strong>: Interpolate between Î±_max and Î±_min</li>
</ol>

<p><strong>The result: A smooth decrease from Î±_max to Î±_min</strong></p>

<h5 id="phase-3-post-annealing-t--t_c">Phase 3: Post-Annealing (t &gt; T_c)</h5>
<p><strong>â€œMaintain minimumâ€</strong></p>

\[\alpha_t = \alpha_{min}\]

<ul>
  <li>Keep the learning rate at the minimum value</li>
  <li>Fine-tuning with tiny steps</li>
</ul>

<h4 id="visual-example">Visual Example</h4>

<p>Letâ€™s say:</p>
<ul>
  <li>Î±_max = 0.001</li>
  <li>Î±_min = 0.0001</li>
  <li>T_w = 1,000 (warm-up ends)</li>
  <li>T_c = 10,000 (annealing ends)</li>
</ul>

<p><strong>Learning rate over time:</strong></p>

<table>
  <thead>
    <tr>
      <th>Step</th>
      <th>Phase</th>
      <th>Learning Rate</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>Warm-up</td>
      <td>0</td>
    </tr>
    <tr>
      <td>500</td>
      <td>Warm-up</td>
      <td>0.0005</td>
    </tr>
    <tr>
      <td>1,000</td>
      <td>Warm-up â†’ Annealing</td>
      <td>0.001</td>
    </tr>
    <tr>
      <td>3,000</td>
      <td>Annealing</td>
      <td>~0.00085</td>
    </tr>
    <tr>
      <td>5,500</td>
      <td>Annealing</td>
      <td>~0.00055</td>
    </tr>
    <tr>
      <td>8,000</td>
      <td>Annealing</td>
      <td>~0.00025</td>
    </tr>
    <tr>
      <td>10,000</td>
      <td>Annealing â†’ Post</td>
      <td>0.0001</td>
    </tr>
    <tr>
      <td>15,000</td>
      <td>Post-annealing</td>
      <td>0.0001</td>
    </tr>
  </tbody>
</table>

<h4 id="simple-math-example">Simple Math Example</h4>

<p>Letâ€™s calculate learning rate at t = 5,500 (middle of annealing):</p>

<p><strong>Given:</strong></p>
<ul>
  <li>T_w = 1,000, T_c = 10,000</li>
  <li>Î±_max = 0.001, Î±_min = 0.0001</li>
</ul>

<p><strong>Step 1:</strong> Progress = (5,500 - 1,000) / (10,000 - 1,000) = 0.5</p>

<p><strong>Step 2:</strong> cos(0.5 Ã— Ï€) = 0</p>

<p><strong>Step 3:</strong> Â½(1 + 0) = 0.5</p>

<p><strong>Step 4:</strong> Î± = 0.0001 + 0.5 Ã— (0.001 - 0.0001) = <strong>0.00055</strong></p>

<p>Exactly halfway between min and max!</p>

<h4 id="key-takeaway-3">Key Takeaway</h4>

<p><strong>Learning rate schedule = adaptive step size:</strong></p>
<ol>
  <li><strong>Warm-up</strong>: Gradually increase from 0 â†’ big (safety at start)</li>
  <li><strong>Cosine annealing</strong>: Smoothly decrease from big â†’ small (careful landing)</li>
  <li><strong>Post-annealing</strong>: Stay small (fine-tuning)</li>
</ol>

<p>Itâ€™s like driving: accelerate leaving the driveway, cruise on the highway, then slow down smoothly as you approach your destination!</p>

<hr />

<h3 id="gradient-clipping">Gradient Clipping: The Safety Mechanism</h3>

<h4 id="the-problem-exploding-gradients">The Problem: Exploding Gradients</h4>

<p>Imagine youâ€™re walking downhill with a GPS that tells you how steep the slope is:</p>

<ul>
  <li><strong>Normal case</strong>: â€œSlope is 5 degreesâ€ â†’ take a reasonable step</li>
  <li><strong>Bad case</strong>: â€œSlope is 5,000 degrees!!!â€ â†’ youâ€™d jump off a cliff!</li>
</ul>

<p>Sometimes during training, the model encounters weird examples that produce <strong>huge gradients</strong>. If you take a step proportional to these giant gradients, your model parameters can explode and training crashes!</p>

<h4 id="what-is-gradient-clipping">What is Gradient Clipping?</h4>

<p>Gradient clipping is like having a <strong>speed limiter</strong> on your updates:</p>

<p><strong>â€œNo matter how steep the slope, I wonâ€™t step faster than X unitsâ€</strong></p>

<h4 id="how-it-works-step-by-step">How It Works (Step by Step)</h4>

<h5 id="step-1-calculate-the-gradient-norm">Step 1: Calculate the Gradient Norm</h5>

<p>After the backward pass, measure how â€œbigâ€ the gradients are overall:</p>

\[\|g\|_2 = \sqrt{g_1^2 + g_2^2 + g_3^2 + ... + g_n^2}\]

<p>This is the <strong>L2 norm</strong> (Euclidean distance) - just the length of the gradient vector.</p>

<p><strong>Example:</strong></p>
<ul>
  <li>If gradients are [3, 4]: norm = âˆš(9 + 16) = <strong>5</strong></li>
  <li>If gradients are [30, 40]: norm = âˆš(900 + 1,600) = <strong>50</strong></li>
</ul>

<h5 id="step-2-check-against-maximum">Step 2: Check Against Maximum</h5>

<p>Set a threshold <strong>M</strong> (e.g., M = 1.0):</p>

<p><strong>Is âˆ¥gâˆ¥â‚‚ â‰¤ M?</strong></p>

<ul>
  <li><strong>YES</strong> â†’ Gradients are reasonable, use them as-is âœ“</li>
  <li><strong>NO</strong> â†’ Gradients are too big, need to clip! âœ‚ï¸</li>
</ul>

<h5 id="step-3-scale-down-if-needed">Step 3: Scale Down If Needed</h5>

<p>If the norm exceeds M, <strong>rescale</strong> the entire gradient vector:</p>

\[g_{\text{clipped}} = g \times \frac{M}{\|g\|_2 + \epsilon}\]

<p>Where Îµ â‰ˆ 10â»â¶ is for numerical stability.</p>

<p><strong>What this does:</strong></p>
<ul>
  <li>Keeps the <strong>direction</strong> the same</li>
  <li>Reduces the <strong>magnitude</strong> to exactly M</li>
</ul>

<h4 id="simple-example-3">Simple Example</h4>

<p><strong>Given:</strong></p>
<ul>
  <li>Gradient vector: g = [30, 40]</li>
  <li>Maximum norm: M = 1.0</li>
  <li>Îµ = 10â»â¶ (negligible)</li>
</ul>

<p><strong>Step 1: Calculate norm</strong></p>
<ul>
  <li>âˆ¥gâˆ¥â‚‚ = âˆš(900 + 1,600) = <strong>50</strong></li>
</ul>

<p><strong>Step 2: Check threshold</strong></p>
<ul>
  <li>50 &gt; 1.0 â†’ <strong>Need to clip!</strong></li>
</ul>

<p><strong>Step 3: Scale down</strong></p>
<ul>
  <li>Scaling factor = 1.0 / 50 = <strong>0.02</strong></li>
  <li>g_clipped = [30, 40] Ã— 0.02 = <strong>[0.6, 0.8]</strong></li>
</ul>

<p><strong>Verify new norm:</strong></p>
<ul>
  <li>âˆ¥g_clippedâˆ¥â‚‚ = âˆš(0.36 + 0.64) = <strong>1.0</strong> âœ“</li>
</ul>

<p><strong>Result:</strong> Weâ€™ve scaled from norm 50 to norm 1.0, keeping the same direction!</p>

<h4 id="another-example-no-clipping">Another Example (No Clipping)</h4>

<p><strong>Given:</strong></p>
<ul>
  <li>Gradient: g = [0.3, 0.4]</li>
  <li>Maximum: M = 1.0</li>
</ul>

<p><strong>Norm:</strong> âˆš(0.09 + 0.16) = <strong>0.5</strong></p>

<p><strong>Check:</strong> 0.5 â‰¤ 1.0 â†’ <strong>No clipping needed!</strong></p>

<p>Use gradient as-is: [0.3, 0.4]</p>

<h4 id="why-does-this-work">Why Does This Work?</h4>

<p><strong>Preserves Direction:</strong></p>
<ul>
  <li>We still move in the right direction (downhill)</li>
  <li>Just limit how far we jump</li>
</ul>

<p><strong>Prevents Instability:</strong></p>
<ul>
  <li>Giant gradients can make parameters explode</li>
  <li>Clipping ensures updates stay reasonable</li>
</ul>

<p><strong>Training Stability:</strong></p>
<ul>
  <li>Without clipping: loss might spike or become NaN</li>
  <li>With clipping: training stays smooth</li>
</ul>

<h4 id="pseudocode">Pseudocode</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># After backward pass
</span><span class="n">gradients</span> <span class="o">=</span> <span class="n">compute_gradients</span><span class="p">()</span>

<span class="c1"># Calculate L2 norm of all gradients
</span><span class="n">grad_norm</span> <span class="o">=</span> <span class="n">sqrt</span><span class="p">(</span><span class="nb">sum</span><span class="p">(</span><span class="n">g</span><span class="err">Â²</span> <span class="k">for</span> <span class="nb">all</span> <span class="n">g</span> <span class="ow">in</span> <span class="n">gradients</span><span class="p">))</span>

<span class="c1"># Clip if needed
</span><span class="n">max_norm</span> <span class="o">=</span> <span class="mf">1.0</span>
<span class="k">if</span> <span class="n">grad_norm</span> <span class="o">&gt;</span> <span class="n">max_norm</span><span class="p">:</span>
    <span class="n">scaling_factor</span> <span class="o">=</span> <span class="n">max_norm</span> <span class="o">/</span> <span class="p">(</span><span class="n">grad_norm</span> <span class="o">+</span> <span class="mf">1e-6</span><span class="p">)</span>
    <span class="n">gradients</span> <span class="o">=</span> <span class="n">gradients</span> <span class="o">*</span> <span class="n">scaling_factor</span>

<span class="c1"># Use clipped gradients in optimizer
</span><span class="n">optimizer</span><span class="p">.</span><span class="n">step</span><span class="p">(</span><span class="n">gradients</span><span class="p">)</span>
</code></pre></div></div>

<h4 id="key-takeaway-4">Key Takeaway</h4>

<p><strong>Gradient clipping = speed limiter for training:</strong></p>

<ol>
  <li><strong>Measure</strong> how big the gradients are (L2 norm)</li>
  <li><strong>Check</strong> if they exceed the maximum allowed</li>
  <li><strong>Scale down</strong> if needed (keep direction, reduce magnitude)</li>
</ol>

<p><strong>Result:</strong> Training stays stable even when occasional batches produce huge gradients. Itâ€™s like having a safety governor on a car engine - you can still accelerate and steer normally, but it prevents dangerous speeds that could cause a crash.</p>

<hr />

<h3 id="conclusion">Conclusion</h3>

<p>Training large language models involves carefully balancing multiple components:</p>

<ol>
  <li><strong>Loss functions</strong> (cross-entropy) measure how bad the model did not predict the correct word</li>
  <li><strong>Metrics</strong> (perplexity) make model prediction results more interpretable</li>
  <li><strong>Optimizers</strong> (SGD â†’ AdamW) determine how we update model parameters</li>
  <li><strong>Memory management</strong> dictates what type of hardware we need</li>
  <li><strong>Computational budgets</strong> determine training time, make trade-off between cost and speed</li>
  <li><strong>Learning rate schedules</strong> help us converge smoothly</li>
  <li><strong>Safety mechanisms</strong> (gradient clipping) prevent training instability</li>
</ol>

<p>While the math can seem complex at first, the underlying intuitions are straightforward: weâ€™re teaching a model to predict text by repeatedly showing it examples, measuring its mistakes, and adjusting its parameters to do better next time. The real challenge isnâ€™t understanding any single component - itâ€™s orchestrating all of them together efficiently at massive scale. Thatâ€™s what makes training models like GPT-3 and GPT-4 such remarkable engineering achievements.</p>]]></content><author><name>[&quot;Han Yu&quot;]</name></author><category term="cs336" /><summary type="html"><![CDATA[Planning LLM Training: Cross-Entropy Loss, Optimizers, Memory and Computational Cost, and other practical levers]]></summary></entry><entry><title type="html">Study Notes: Stanford CS336 Language Modeling from Scratch [7]</title><link href="http://localhost:4000/cs336/2025/09/28/cs336-understand-computation-cost-of-transformer-model.html" rel="alternate" type="text/html" title="Study Notes: Stanford CS336 Language Modeling from Scratch [7]" /><published>2025-09-28T00:00:00-07:00</published><updated>2025-09-28T00:00:00-07:00</updated><id>http://localhost:4000/cs336/2025/09/28/cs336-understand-computation-cost-of-transformer-model</id><content type="html" xml:base="http://localhost:4000/cs336/2025/09/28/cs336-understand-computation-cost-of-transformer-model.html"><![CDATA[<h2 id="understanding-where-the-computation-really-goes-in-transformer-language-models">Understanding where the computation really goes in transformer language models*</h2>

<p>When we talk about large language models like GPT-4 or Claude, we often hear impressive numbers: â€œ175 billion parameters,â€ â€œ3.5 trillion FLOPs,â€ â€œtrained on thousands of GPUs.â€ But what do these numbers actually mean? Where does all that computation go during inference? And how do these patterns change as models scale up?</p>

<p>In this notes, weâ€™ll dissect the computational anatomy of GPT-2 models, from the smallest to the largest variants, to understand exactly where the mathematical heavy lifting happens. By the end, hope youâ€™ll have an intuitive understanding of why certain optimization techniques matter more than others, and how the computational landscape shifts dramatically with model size and context length.</p>

<p><em>This analysis was based on GPT-2 architectures, but the principles apply broadly to transformer-based language models. The specific percentages may vary, but the scaling laws and optimization insights remain relevant for understanding modern LLMs.</em></p>

<h3 id="table-of-contents">Table of Contents</h3>

<ol>
  <li><a href="#setting-the-stage-gpt-2-xl-under-the-microscope">Setting the Stage: GPT-2 XL Under the Microscope</a></li>
  <li><a href="#part-1-counting-every-parameter">Part 1: Counting Every Parameter</a>
    <ul>
      <li><a href="#the-156-billion-parameter-breakdown">The 1.56 Billion Parameter Breakdown</a></li>
      <li><a href="#memory-requirements">Memory Requirements</a></li>
    </ul>
  </li>
  <li><a href="#part-2-following-the-flops-floating-point-operations-per-second">Part 2: Following the FLOPS (Floating Point Operations per Second)</a>
    <ul>
      <li><a href="#the-35-trillion-flops-journey">The 3.5 Trillion FLOPs Journey</a></li>
      <li><a href="#the-computational-cost-hierarchy">The Computational Cost Hierarchy</a></li>
    </ul>
  </li>
  <li><a href="#part-3-how-computational-patterns-change-with-model-scale">Part 3: How Computational Patterns Change with Model Scale</a>
    <ul>
      <li><a href="#scaling-trends">Scaling Trends</a></li>
      <li><a href="#optimization-strategy-by-model-size">Optimization Strategy by Model Size</a></li>
    </ul>
  </li>
  <li><a href="#part-4-the-long-context-revolution">Part 4: The Long Context Revolution</a>
    <ul>
      <li><a href="#the-38-computational-explosion">The 38Ã— Computational Explosion</a></li>
      <li><a href="#the-quadratic-takeover">The Quadratic Takeover</a></li>
      <li><a href="#memory-implications">Memory Implications</a></li>
    </ul>
  </li>
  <li><a href="#part-5-understanding-mixture-of-experts-moe">Part 5: Understanding Mixture of Experts (MoE)</a>
    <ul>
      <li><a href="#the-restaurant-kitchen-analogy">The Restaurant Kitchen Analogy</a></li>
      <li><a href="#how-moe-works-in-practice">How MoE Works in Practice</a></li>
      <li><a href="#why-moe-provides-massive-savings">Why MoE Provides Massive Savings</a></li>
    </ul>
  </li>
  <li><a href="#part-6-key-insights-and-practical-implications">Part 6: Key Insights and Practical Implications</a>
    <ul>
      <li><a href="#for-model-developers">For Model Developers</a></li>
      <li><a href="#for-infrastructure-teams">For Infrastructure Teams</a></li>
    </ul>
  </li>
  <li><a href="#conclusion">Conclusion</a></li>
</ol>

<h3 id="setting-the-stage-gpt-2-xl-under-the-microscope"><strong>Setting the Stage: GPT-2 XL Under the Microscope</strong></h3>

<p>Letâ€™s start by examining GPT-2 XL, one of the largest publicly available GPT-2 models, with these specifications:</p>

<ul>
  <li><strong>Vocabulary size</strong>: 50,257 tokens</li>
  <li><strong>Context/sequence length</strong>: 1,024 tokens</li>
  <li><strong>Layers</strong>: 48 transformer blocks</li>
  <li><strong>Embedding Model dimension (d_model)</strong>: 1,600</li>
  <li><strong>Attention heads</strong>: 25</li>
  <li><strong>Feed forward dimension (d_ff)</strong>: 6,400</li>
</ul>

<h3 id="part-1-counting-every-parameter"><strong>Part 1: Counting Every Parameter</strong></h3>

<h4 id="the-156-billion-parameter-breakdown"><strong>The 1.56 Billion Parameter Breakdown</strong></h4>

<p>When we say GPT-2 XL has <code class="language-plaintext highlighter-rouge">1.56 billion</code> parameters, where exactly do they all go?</p>

<p><strong>Token &amp; Position Embeddings: 82M parameters (5.3%)</strong></p>
<ul>
  <li><strong>Token embeddings</strong>: 50,257 tokens Ã— 1,600 dimensions -&gt; 80.4M parameters</li>
  <li><strong>Position embeddings</strong>: 1,024 positions Ã— 1,600 dimensions -&gt; 1.6M parameters</li>
</ul>

<p><em>Think of these as lookup tables: each token gets its own 1,600-dimensional vector, and each position (1st word, 2nd word, etc.) gets its own vector too.</em></p>

<p><strong>Transformer Layers: 1.47B parameters (94.7%)</strong></p>

<p>Each of the 48 layers contains:</p>

<ul>
  <li><em>Multi-Head Attention (10.2M parameters per layer):</em>
    <ul>
      <li><strong>Q, K, V projections</strong>: 3 Ã— (1,600 Ã— 1,600) = 7.68M parameters</li>
      <li><strong>Output projection</strong>: 1,600 Ã— 1,600 = 2.56M parameters</li>
    </ul>
  </li>
  <li><em>Feed Forward Network (20.5M parameters per layer):</em>
    <ul>
      <li><strong>Expansion layer (linear projection up)</strong>: 1,600 Ã— 6,400 + 6,400 bias = 10.24M parameters</li>
      <li><strong>Contraction layer (linear project down)</strong>: 6,400 Ã— 1,600 + 1,600 bias = 10.24M parameters</li>
    </ul>
  </li>
  <li><em>Layer Normalization (6,400 parameters per layer):</em>
    <ul>
      <li><strong>Two layer norms (e.g.,RMSNorm)</strong> Ã— 2 parameters each (e.g., two learnable parameters used in RMSNorm) Ã— 1,600 dimensions = 6,400 parameters</li>
    </ul>
  </li>
</ul>

<p><strong>Final Layer Norm (e.g., 1 RMSNorm layer with 2 parameters, 1,600 dimensions): 3,200 parameters</strong></p>

<p><strong>Key Insight</strong>: The feed forward networks contain <code class="language-plaintext highlighter-rouge">67%</code> of parameters per layer, while attention uses <code class="language-plaintext highlighter-rouge">33%</code>. This <code class="language-plaintext highlighter-rouge">2:1</code> ratio will become important when we analyze computation.</p>

<h4 id="memory-requirements"><strong>Memory Requirements</strong></h4>

<p>With single-precision floating point (each parameter is represented by 32 bits which is 4 bytes per parameter):</p>
<ul>
  <li><strong>Total memory</strong>: <code class="language-plaintext highlighter-rouge">1.56B Ã— 4 bytes</code> -&gt; <strong><code class="language-plaintext highlighter-rouge">6.2 GB</code></strong></li>
</ul>

<p>This is just for storing the model weightsâ€”actual inference requires additional memory for activations, gradients (if training), intermediate computations, and other overhead for using frameworks.</p>

<h3 id="part-2-following-the-flops-floating-point-operations-per-second"><strong>Part 2: Following the FLOPS (Floating Point Operations per Second)</strong></h3>

<p>Now comes the crucial question: during a forward pass with 1,024 input tokens, where does the computational work actually happen?</p>

<h4 id="the-35-trillion-flops-journey"><strong>The 3.5 Trillion FLOPs Journey</strong></h4>
<p>A FLOP stands for Floating Point Operation. It means one basic arithmetic operation (addition, subtraction, multiplication, division, etc.) performed on floating-point numbers (like 32-bit floats in neural nets).</p>

<p>Example:</p>
<blockquote>
  <p>3.14Ã—2.71 â†’ 1 FLOP</p>

  <p>(3.14 Ã— 2.71) + 1.23 â†’ 2 FLOPs</p>
</blockquote>

<p>FLOPS vs FLOP:</p>

<blockquote>
  <p>FLOP = one operation.</p>

  <p>FLOPS = FLOPs per second â†’ a measure of compute speed.</p>
</blockquote>

<p>Example:</p>
<blockquote>
  <p>A GPU that can perform 1Ã—10^12 FLOPs per second = 1 TFLOPS.</p>
</blockquote>

<p>When we talk about FLOPs of a model: it means the total number of floating-point operations required for one forward pass (sometimes forward + backward during training).</p>

<p>Example:</p>
<blockquote>
  <p>A Transformer block does a lot of matrix multiplications (like attention and feedforward layers). Counting their FLOPs helps estimate compute cost and compare model efficiency.</p>
</blockquote>

<p>Note:</p>
<blockquote>
  <p>For matrix multiplication (M Ã— N) @ (N Ã— K), the FLOPs = 2 Ã— M Ã— N Ã— K</p>
</blockquote>

<p><strong>Feed Forward Networks: 2.01 TFLOPs (57.4%)</strong></p>
<ul>
  <li>Two matrix multiplications per layer: d_model â†” d_ff expansion and contraction</li>
  <li>Why so expensive: The <code class="language-plaintext highlighter-rouge">4Ã—</code> expansion (1,600 â†’ 6,400), then contraction (6,400 â†’ 1,600) creates huge matrix operations</li>
  <li>Per layer cost: 2 Ã— 1,600 Ã— 6,400 + 2 Ã— 6,400 Ã— 1,600 -&gt; 41.9 GFLOPs</li>
  <li>Total across all layers: 41.9 GFLOPs Ã— 48 layers -&gt; 2.01 TFLOPs</li>
</ul>

<p><strong>Attention Linear Projections: 1.01 TFLOPs (28.7%)</strong></p>
<ul>
  <li>Four projections per layer: Query, Key, Value, and Output matrices</li>
  <li>Each projection: (1,024 Ã— 1,600) @ (1,600 Ã— 1,600) matrix multiplication</li>
  <li>Per projection cost: 2 Ã— 1,024 Ã— 1,600 Ã— 1,600 = 5.24 GFLOPs</li>
  <li>Per layer cost (4 projections): 4 Ã— 5.24 GFLOPs = 20.97 GFLOPs</li>
  <li>Total across all layers: 20.97 GFLOPs Ã— 48 layers = 1.007 TFLOPs</li>
</ul>

<p><strong>Attention Computation: 0.32 TFLOPs (9.2%)</strong></p>
<ul>
  <li><strong>Q@K^T</strong>: Computing attention scores
    <ul>
      <li>Creates the attention matrix that determines which tokens attend to which</li>
      <li>Matrix multiplication: (1,024 Ã— 1,600) @ (1,600 Ã— 1,024)</li>
      <li>FLOPs per layer = 2 Ã— 1024 Ã— 1600 Ã— 1024 -&gt;  3,355,443,200 FLOPs per layer</li>
      <li>Total across all layers: 3,355,443,200 Ã— 48 layers -&gt; 0.16 TFLOPs</li>
    </ul>
  </li>
  <li><strong>Attention@V</strong>: Applying attention weights to values
    <ul>
      <li>Applies attention weights to value vectors to get final attended representations</li>
      <li>Matrix multiplication: (1,024 Ã— 1,024)  @  (1,024 Ã— 1,600)</li>
      <li>FLOPs per layer = 2 Ã— 1024 Ã— 1024 Ã— 1600 -&gt;  3,355,443,200 FLOPs per layer</li>
      <li>Total across all layers: 3,355,443,200 Ã— 48 layers -&gt; 0.16 TFLOPs</li>
    </ul>
  </li>
  <li><strong>Currently small</strong> but <em>grows quadratically with longer sequences</em></li>
</ul>

<p><strong>Output Projection: 0.16 TFLOPs (4.7%)</strong></p>
<ul>
  <li>Final projection from hidden states to 50,257 vocabulary logits
    <ul>
      <li>Input shape: (1,024 Ã— 1,600) hidden states</li>
      <li>Weight shape: (1,600 Ã— 50,257) vocabulary projection</li>
      <li>Output shape: (1,024 Ã— 50,257) logits for each position</li>
      <li>FLOPs: 2 Ã— 1,024 Ã— 1,600 Ã— 50,257 -&gt; 0.16 TFLOPs</li>
    </ul>
  </li>
  <li>Large vocabulary makes this significant despite being a single operation</li>
</ul>

<h4 id="the-computational-cost-hierarchy"><strong>The Computational Cost Hierarchy</strong></h4>

<p><strong>ğŸ¥‡ Feed Forward is the King (57.4%)</strong></p>

<p>This is the most important finding: <em>feed forward networks consume more computation than everything else combined</em>. The 4Ã— expansion factor creates the largest matrix operations in the entire model.</p>

<p><strong>ğŸ¥ˆ Attention Linear Projections Runner-Up (28.7%)</strong></p>

<p>The four linear projections (Q, K, V, O) that prepare attention computations use nearly 30% of all FLOPs.</p>

<p><strong>ğŸ¥‰ The Famous Attention Mechanism (9.2%)</strong></p>

<p>Despite getting most research attention, the actual attention computation (the part that makes transformers special) uses less than 10% of computation for typical sequence lengths. But, it can <em>grow quadratically with longer sequences or context window</em>.</p>

<p><strong>ğŸ… Vocabulary Bottleneck (4.7%)</strong></p>

<p>The final projection to vocabulary logits is notable due to GPT-2 XLâ€™s large vocabulary.</p>

<p><strong>Why This Distribution Matters</strong></p>

<p>This analysis reveals that <strong>feed forward networks are the computational elephant in the room</strong>. For current sequence lengths, optimizing feed forward networks provides bigger computational savings than optimizing attention mechanisms.</p>

<h3 id="part-3-how-computational-patterns-change-with-model-scale"><strong>Part 3: How Computational Patterns Change with Model Scale</strong></h3>

<p>Letâ€™s examine how FLOP distribution evolves across the entire GPT-2 family:</p>

<table>
  <thead>
    <tr>
      <th>Model</th>
      <th>Layers</th>
      <th>d_model</th>
      <th>Total FLOPs</th>
      <th>Feed Forward %</th>
      <th>Attention Linear Proj %</th>
      <th>Attention Computation %</th>
      <th>Output Projection %</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Small</strong></td>
      <td>12</td>
      <td>768</td>
      <td>0.29 TFLOPs</td>
      <td>39.8%</td>
      <td>19.9%</td>
      <td>13.3%</td>
      <td><strong>27.1%</strong></td>
    </tr>
    <tr>
      <td><strong>Medium</strong></td>
      <td>24</td>
      <td>1024</td>
      <td>0.83 TFLOPs</td>
      <td>49.9%</td>
      <td>24.9%</td>
      <td>12.5%</td>
      <td>12.7%</td>
    </tr>
    <tr>
      <td><strong>Large</strong></td>
      <td>36</td>
      <td>1280</td>
      <td>1.78 TFLOPs</td>
      <td>54.5%</td>
      <td>27.2%</td>
      <td>10.9%</td>
      <td>7.4%</td>
    </tr>
    <tr>
      <td><strong>XL</strong></td>
      <td>48</td>
      <td>1600</td>
      <td>3.51 TFLOPs</td>
      <td><strong>57.4%</strong></td>
      <td><strong>28.7%</strong></td>
      <td>9.2%</td>
      <td>4.7%</td>
    </tr>
  </tbody>
</table>

<h4 id="scaling-trends"><strong>Scaling Trends</strong></h4>

<p><strong>ğŸ“ˆ Feed Forward Dominance Grows</strong></p>
<ul>
  <li>Small: 39.8% â†’ XL: 57.4% (+17.7 percentage points)</li>
  <li><strong>Why</strong>: Scales as O(d_modelÂ² Ã— layers), growing faster than other components, and <em>growing quadratically with larger embedding model</em>.</li>
</ul>

<p><strong>ğŸ“‰ Output Projection Becomes Negligible</strong></p>
<ul>
  <li>Small: 27.1% â†’ XL: 4.7% (-22.4 percentage points)</li>
  <li><strong>Why</strong>: Scales as O(d_model) while vocabulary size stays constant</li>
</ul>

<p><strong>ğŸ“‰ Attention Computation Relatively Shrinks</strong></p>
<ul>
  <li>Small: 13.3% â†’ XL: 9.2% (-4.1 percentage points)</li>
  <li><strong>Why</strong>: Scales as O(d_model Ã— layers), but growing only linear with larger embedding model.</li>
</ul>

<p><strong>ğŸ“ˆ Attention Projections Grow Steadily</strong></p>
<ul>
  <li>Small: 19.9% â†’ XL: 28.7% (+8.8 percentage points)</li>
  <li><strong>Why</strong>: Scales same as feed forward: O(d_modelÂ² Ã— layers), and <em>growing quadratically with larger embedding model</em>.</li>
</ul>

<h4 id="optimization-strategy-by-model-size"><strong>Optimization Strategy by Model Size</strong></h4>

<p><strong>Small Models</strong>: Output projection matters most (27% of computation)</p>
<ul>
  <li>Focus on vocabulary efficiency and embedding optimizations</li>
</ul>

<p><strong>Large Models</strong>: Feed forward networks dominate (57% of computation)</p>
<ul>
  <li>Focus on Mixture of Experts (MoE), pruning, and quantization</li>
</ul>

<p>This scaling analysis explains why techniques like <strong>Mixture of Experts</strong>â€”which primarily optimize feed forward networksâ€”become increasingly important for large models (we will explain that later in this notes).</p>

<h3 id="part-4-the-long-context-revolution"><strong>Part 4: The Long Context Revolution</strong></h3>

<p>What happens when we extend GPT-2 XLâ€™s context from 1,024 to 16,384 tokens? The results are dramatic.</p>

<h4 id="the-38-computational-explosion"><strong>The 38Ã— Computational Explosion</strong></h4>

<table>
  <thead>
    <tr>
      <th>Component</th>
      <th>1K Context</th>
      <th>16K Context</th>
      <th>Scaling</th>
      <th>Change</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Attention Computation</strong></td>
      <td><strong>9.2%</strong></td>
      <td><strong>61.8%</strong></td>
      <td><strong>256Ã—</strong></td>
      <td><strong>+52.6 pts</strong></td>
    </tr>
    <tr>
      <td><strong>Feed Forward</strong></td>
      <td><strong>57.4%</strong></td>
      <td><strong>24.1%</strong></td>
      <td><strong>16Ã—</strong></td>
      <td><strong>-33.3 pts</strong></td>
    </tr>
    <tr>
      <td><strong>Attention Projections</strong></td>
      <td><strong>28.7%</strong></td>
      <td><strong>12.1%</strong></td>
      <td><strong>16Ã—</strong></td>
      <td><strong>-16.6 pts</strong></td>
    </tr>
    <tr>
      <td><strong>Output Projection</strong></td>
      <td><strong>4.7%</strong></td>
      <td><strong>2.0%</strong></td>
      <td><strong>16Ã—</strong></td>
      <td><strong>-2.7 pts</strong></td>
    </tr>
  </tbody>
</table>

<h4 id="the-quadratic-takeover"><strong>The Quadratic Takeover</strong></h4>

<p><strong>Total FLOPs</strong>: 3.5 TFLOPs â†’ 133.4 TFLOPs (38Ã— increase)</p>

<p>The most shocking change: <strong>attention computation explodes from 9% to 62%</strong> of total computation. Hereâ€™s why:</p>

<ul>
  <li><strong>Sequence length scaling</strong>: 1,024 â†’ 16,384 tokens (16Ã— increase)</li>
  <li><strong>Linear components</strong> (feed forward, projections): Scale by 16Ã—</li>
  <li><strong>Quadratic components</strong> (attention computation): Scale by 16Â² = 256Ã—</li>
</ul>

<p>The mathematical culprits are the <code class="language-plaintext highlighter-rouge">Q@K^T</code> and <code class="language-plaintext highlighter-rouge">Attention@V</code> operations, which both scale as <code class="language-plaintext highlighter-rouge">O(sequence_lengthÂ²)</code>.</p>

<h4 id="memory-implications"><strong>Memory Implications</strong></h4>

<p>The memory story is even more dramatic:</p>
<ul>
  <li><strong>Attention matrices</strong>: 50M â†’ 12,885M elements (256Ã— increase)</li>
  <li><strong>Storage requirement</strong>: Each attention head must store a 16KÃ—16K matrix</li>
</ul>

<p>This reveals why <strong>long context is the next major frontier</strong> in LLM optimization, requiring techniques like:</p>
<ul>
  <li><strong>Flash Attention</strong>: Memory-efficient attention computation</li>
  <li><strong>Sparse Attention</strong>: Only compute attention for relevant tokens</li>
  <li><strong>Linear Attention</strong>: Approximate attention with linear complexity</li>
  <li><strong>Sliding Window</strong>: Limit attention to recent tokens</li>
</ul>

<h3 id="part-5-understanding-mixture-of-experts-moe"><strong>Part 5: Understanding Mixture of Experts (MoE)</strong></h3>

<p>Given that feed forward networks dominate computation (57% in large models), letâ€™s understand why <strong>Mixture of Experts</strong> has become a game-changing optimization technique.</p>

<h4 id="the-restaurant-kitchen-analogy"><strong>The Restaurant Kitchen Analogy</strong></h4>

<p><strong>Traditional Model</strong>: One super-chef tries to cook everythingâ€”pizza, sushi, pasta, desserts. The chef gets exhausted and slower as the menu grows.</p>

<p><strong>MoE Model</strong>: Multiple specialist chefs (pizza expert, sushi master, pasta chef, dessert specialist) with a smart dispatcher who sends each order to the right specialist. Only the relevant chef works on each dish.</p>

<h4 id="how-moe-works-in-practice">How MoE Works in Practice</h4>
<p>Instead of one giant feedforward network, we have many smaller expert networks (say 64 FFNs). A router (small gating network) decides which subset of experts (often 1 or 2) each token should use. Thus, for each token: Only ~1â€“2 experts are activated
The others are inactive (no compute for them).</p>
<h4 id="why-moe-provides-massive-savings">Why MoE Provides Massive Savings</h4>
<p><strong>Computational Savings</strong>:</p>
<ul>
  <li><strong>Before</strong>: Use 100% of the giant network for every input</li>
  <li><strong>After</strong>: Use only 10-20% of total network capacity</li>
  <li><strong>Result</strong>: 5-10Ã— faster inference with same quality</li>
</ul>

<p><strong>Specialization Benefits</strong>:</p>
<ul>
  <li><strong>Expert 1</strong>: Math and science</li>
  <li><strong>Expert 2</strong>: Creative writing</li>
  <li><strong>Expert 3</strong>: Code and programming</li>
  <li><strong>Expert 4</strong>: Languages and translation</li>
</ul>

<p><strong>Scale Without Pain</strong>:</p>
<ul>
  <li><strong>Traditional</strong>: 2Ã— bigger model = 2Ã— more computation</li>
  <li><strong>MoE</strong>: 2Ã— more experts â‰ˆ same computation (since only 1-2 active)</li>
</ul>

<p>Since feed forward networks use 57% of computation in large models, MoE can reduce this to 6-12%, eliminating the primary bottleneck.</p>

<h3 id="part-6-key-insights-and-practical-implications"><strong>Part 6: Key Insights and Practical Implications</strong></h3>

<h4 id="for-model-developers"><strong>For Model Developers</strong></h4>

<p><strong>Small Models (&lt; 1B parameters)</strong>:</p>
<ul>
  <li>Output projection optimization matters most</li>
  <li>Vocabulary efficiency and embedding techniques provide biggest gains</li>
  <li>Feed forward optimization secondary</li>
</ul>

<p><strong>Large Models (&gt; 10B parameters)</strong>:</p>
<ul>
  <li>Feed forward networks are the primary target (MoE, quantization, pruning)</li>
  <li>Attention projection optimizations become important</li>
  <li>Output projection becomes negligible</li>
</ul>

<p><strong>Long Context Models</strong>:</p>
<ul>
  <li>Attention computation becomes dominant bottleneck</li>
  <li>Memory optimization equally critical as computation optimization</li>
  <li>Linear attention mechanisms essential</li>
</ul>

<h4 id="for-infrastructure-teams"><strong>For Infrastructure Teams</strong></h4>

<p><strong>Hardware Requirements Scale Predictably</strong>:</p>

<p>Computational Scaling Patterns:</p>

<ul>
  <li>Feed forward: Scales as O(d_modelÂ² Ã— layers)
    <ul>
      <li>GPT-2 Small â†’ XL: d_model grows 2.1x, layers grow 4x â†’ FF computation grows ~17x</li>
      <li>Dominates short-sequence/context window workloads (57% of computation)</li>
      <li>Requires high tensor core utilization for matrix multiplication</li>
    </ul>
  </li>
  <li>Attention: Scales as O(sequence_lengthÂ² Ã— layers)
    <ul>
      <li>1K â†’ 16K context window: sequence_length grows 16x â†’ attention grows 256x</li>
      <li>Becomes dominant for long sequences (62% computation at 16K context)</li>
      <li>Requires specialized attention kernels and memory optimization</li>
    </ul>
  </li>
</ul>

<p>Memory Scaling Reality:</p>
<ul>
  <li>Model weights: Static ~6 GB for GPT-2 XL (predictable)</li>
  <li>Activations: Variable based on sequence length
    <ul>
      <li>Short sequences (1K): ~0.5 GB activations</li>
      <li>Long sequences (16K): ~57 GB activations (100x more!)</li>
    </ul>
  </li>
  <li>Attention matrices: The memory killer at long sequences
    <ul>
      <li>1K context: 0.2 GB attention matrices</li>
      <li>16K context: 51.5 GB attention matrices (256x increase)</li>
    </ul>
  </li>
</ul>

<p><em>Activations</em> are the intermediate values computed during the forward pass that must be stored in memory for:</p>
<ul>
  <li>Computing the next layerâ€™s input</li>
  <li>Backpropagation (during training)</li>
  <li>Gradient computation</li>
</ul>

<p><strong>Optimization Priorities</strong>:</p>
<ul>
  <li><strong>Short sequences</strong>: Focus on feed forward efficiency</li>
  <li><strong>Long sequences</strong>: Focus on attention efficiency</li>
  <li><strong>Both</strong>: Memory bandwidth becomes critical</li>
</ul>

<h3 id="conclusion"><strong>Conclusion</strong></h3>

<p>Large language models may seem like black boxes, but their computational patterns follow clear mathematical principles:</p>

<ol>
  <li><strong>Feed forward networks dominate</strong> computation in most scenarios (57% for large models)</li>
  <li><strong>Model scaling</strong> predictably shifts optimization priorities from vocabulary to feed forward efficiency</li>
  <li><strong>Long context</strong> fundamentally changes the game, making attention the primary bottleneck</li>
  <li><strong>Memory requirements</strong> often exceed computational requirements for optimization</li>
</ol>

<p>Understanding these patterns isnâ€™t just academicâ€”it directly informs optimization strategies, hardware requirements, and research directions. As we push toward even larger models and longer contexts, these computational realities will increasingly determine whatâ€™s possible and whatâ€™s practical in the world of large language models.</p>]]></content><author><name>[&quot;Han Yu&quot;]</name></author><category term="cs336" /><summary type="html"><![CDATA[Understanding where the computation really goes in transformer language models*]]></summary></entry><entry><title type="html">Study Notes: Stanford CS336 Language Modeling from Scratch [6]</title><link href="http://localhost:4000/cs336/2025/09/17/cs336-transformer-architecture-overview.html" rel="alternate" type="text/html" title="Study Notes: Stanford CS336 Language Modeling from Scratch [6]" /><published>2025-09-17T00:00:00-07:00</published><updated>2025-09-17T00:00:00-07:00</updated><id>http://localhost:4000/cs336/2025/09/17/cs336-transformer-architecture-overview</id><content type="html" xml:base="http://localhost:4000/cs336/2025/09/17/cs336-transformer-architecture-overview.html"><![CDATA[<h2 id="an-overview-of-popular-transformer-architectures">An Overview of Popular Transformer Architectures</h2>

<p>While working on the Transformer LM assignments, I realized it would be helpful to also step back and look at some of the most popular Transformer architectures. Here are my notes and takeaways.</p>

<h3 id="table-of-contents">Table of Contents</h3>
<ol>
  <li><a href="#introduction">Introduction</a></li>
  <li><a href="#architecture-overview">Architecture Overview</a></li>
  <li><a href="#encoder-decoder-transformers">Encoder-Decoder Transformers</a></li>
  <li><a href="#decoder-only-transformers">Decoder-Only Transformers</a></li>
  <li><a href="#encoder-only-transformers">Encoder-Only Transformers</a></li>
  <li><a href="#comparison-summary">Comparison Summary</a></li>
  <li><a href="#modern-trends-and-applications">Modern Trends and Applications</a></li>
</ol>

<h3 id="introduction">Introduction</h3>

<p>Transformer architectures have revolutionized natural language processing and machine learning. Since the original â€œAttention is All You Needâ€ paper in 2017, three main architectural variants have emerged, each optimized for different types of tasks:</p>

<ul>
  <li><strong>Encoder-Decoder</strong>: Sequence-to-sequence transformations</li>
  <li><strong>Decoder-Only</strong>: Autoregressive text generation</li>
  <li><strong>Encoder-Only</strong>: Text understanding and classification</li>
</ul>

<p>This note provides an overview of how each architecture works, their training methodologies, evaluation approaches, and practical applications.</p>

<hr />

<h3 id="architecture-overview">Architecture Overview</h3>

<h4 id="core-components">Core Components</h4>

<p>All transformer architectures share fundamental building blocks:</p>

<ul>
  <li><strong>Self-Attention Mechanism</strong>: Allows tokens to attend to other tokens</li>
  <li><strong>Feed-Forward Networks</strong>: Position-wise processing layers</li>
  <li><strong>Layer Normalization</strong>: Stabilizes training</li>
  <li><strong>Residual Connections</strong>: Enables deep architectures</li>
  <li><strong>Positional Encodings</strong>: Provides sequence position information</li>
</ul>

<h4 id="multi-head-self-attention-deep-dive">Multi-Head Self-Attention Deep Dive</h4>

<p>The multi-head self-attention mechanism is the core innovation of transformers. Hereâ€™s how it works in detail:</p>

<p><img src="/assets/picture/2025-09-17-cs336-transformer-architecture-overview/encoder-multi-head-self-attention.png" alt="Multi-Head Self-Attention Mechanism" width="1080" /></p>

<p><strong>Key Steps:</strong></p>
<ol>
  <li><strong>Linear Projections</strong>: Input embeddings are transformed into Query (Q), Key (K), and Value (V) matrices</li>
  <li><strong>Head Splitting</strong>: Q, K, V matrices are reshaped and split into multiple attention heads</li>
  <li><strong>Parallel Attention</strong>: Each head computes attention independently using scaled dot-product attention</li>
  <li><strong>Concatenation</strong>: All head outputs are concatenated back together</li>
  <li><strong>Final Projection</strong>: A final linear layer projects the concatenated result back to the model dimension</li>
</ol>

<p>This parallel processing allows the model to attend to different types of relationships simultaneously - some heads might focus on syntactic relationships while others capture semantic connections.</p>

<h4 id="key-differences">Key Differences</h4>

<p>The main distinction lies in the <strong>attention patterns</strong>:</p>

<table>
  <thead>
    <tr>
      <th>Architecture</th>
      <th>Attention Pattern</th>
      <th>Primary Use Case</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Encoder-Decoder</td>
      <td>Bidirectional (encoder) + Causal (decoder)</td>
      <td>Sequence-to-sequence tasks</td>
    </tr>
    <tr>
      <td>Decoder-Only</td>
      <td>Causal only</td>
      <td>Autoregressive generation</td>
    </tr>
    <tr>
      <td>Encoder-Only</td>
      <td>Bidirectional only</td>
      <td>Understanding and classification</td>
    </tr>
  </tbody>
</table>

<hr />

<h3 id="encoder-decoder-transformers">Encoder-Decoder Transformers</h3>

<h4 id="architecture-design">Architecture Design</h4>

<p>The encoder-decoder architecture consists of two separate stacks connected through cross-attention:</p>

<p><img src="/assets/picture/2025-09-17-cs336-transformer-architecture-overview/encoder-decoder-transformer-architecture.png" alt="Encoder-Decoder Transformer Architecture" width="1080" /></p>

<p><strong>Key Components:</strong></p>
<ul>
  <li><strong>Encoder</strong>: Uses bidirectional self-attention to process input sequence with full context</li>
  <li><strong>Decoder</strong>: Uses causal self-attention + cross-attention to generate output sequence</li>
  <li><strong>Cross-Attention</strong>: Allows decoder to attend to encoder representations at each layer</li>
  <li><strong>Layer-by-Layer Processing</strong>: Each decoder layer receives information from the corresponding encoder layer</li>
</ul>

<p><strong>Key Features:</strong></p>
<ul>
  <li><strong>ğŸ”„ Bidirectional Encoder</strong>: Full context understanding for source sequence</li>
  <li><strong>ğŸ”— Cross-Attention</strong>: Decoder attends to encoder representations</li>
  <li><strong>ğŸ“ Sequence-to-Sequence</strong>: Perfect for translation, summarization, and question answering</li>
</ul>

<p>This architecture excels at tasks requiring structured input-output transformations where the model needs to understand the entire input before generating the output.</p>

<h4 id="training-methodology">Training Methodology</h4>

<p><strong>Objective</strong>: Learn to map input sequences to output sequences</p>

<p><strong>Training Process:</strong></p>
<ol>
  <li><strong>Teacher Forcing</strong>: Use ground truth target tokens as decoder input</li>
  <li><strong>Parallel Training</strong>: All target positions trained simultaneously</li>
  <li><strong>Cross-Entropy Loss</strong>: Computed over target vocabulary</li>
</ol>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Training pseudocode
</span><span class="k">def</span> <span class="nf">train_encoder_decoder</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">dataloader</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">dataloader</span><span class="p">:</span>
        <span class="n">src_tokens</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="s">'source'</span><span class="p">]</span>      <span class="c1"># Input sequence
</span>        <span class="n">tgt_tokens</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="s">'target'</span><span class="p">]</span>      <span class="c1"># Target sequence
</span>        
        <span class="c1"># Teacher forcing setup
</span>        <span class="n">tgt_input</span> <span class="o">=</span> <span class="n">tgt_tokens</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>       <span class="c1"># Decoder input
</span>        <span class="n">tgt_output</span> <span class="o">=</span> <span class="n">tgt_tokens</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>       <span class="c1"># Expected output
</span>        
        <span class="c1"># Forward pass
</span>        <span class="n">logits</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">src_tokens</span><span class="p">,</span> <span class="n">tgt_input</span><span class="p">)</span>
        
        <span class="c1"># Compute loss
</span>        <span class="n">loss</span> <span class="o">=</span> <span class="n">cross_entropy</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">tgt_output</span><span class="p">)</span>
        
        <span class="c1"># Backpropagation
</span>        <span class="n">loss</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="p">.</span><span class="n">step</span><span class="p">()</span>
</code></pre></div></div>

<p><strong>Training Data Requirements:</strong></p>
<ul>
  <li><strong>Parallel Corpora</strong>: Paired input-output sequences</li>
  <li><strong>Domain-Specific</strong>: Task-dependent datasets</li>
  <li><strong>Quality</strong>: High-quality alignments crucial for performance</li>
</ul>

<h4 id="evaluation-methods">Evaluation Methods</h4>

<p><strong>Generation-Based Evaluation:</strong></p>

<ol>
  <li><strong>Automatic Metrics</strong>:
    <ul>
      <li><strong>BLEU</strong>: N-gram overlap for translation</li>
      <li><strong>ROUGE</strong>: Recall-oriented for summarization</li>
      <li><strong>METEOR</strong>: Semantic similarity measures</li>
      <li><strong>BERTScore</strong>: Contextual embeddings comparison</li>
    </ul>
  </li>
  <li><strong>Human Evaluation</strong>:
    <ul>
      <li><strong>Fluency</strong>: How natural the output sounds</li>
      <li><strong>Adequacy</strong>: How well meaning is preserved</li>
      <li><strong>Faithfulness</strong>: Accuracy to source content</li>
    </ul>
  </li>
</ol>

<p><strong>Task-Specific Benchmarks:</strong></p>
<ul>
  <li><strong>Translation</strong>: WMT datasets, OPUS corpora</li>
  <li><strong>Summarization</strong>: CNN/DailyMail, XSum</li>
  <li><strong>Question Answering</strong>: SQuAD variants</li>
</ul>

<h4 id="use-cases-and-applications">Use Cases and Applications</h4>

<p><strong>Primary Applications:</strong></p>
<ul>
  <li><strong>Machine Translation</strong>: Language pair transformations</li>
  <li><strong>Text Summarization</strong>: Document to summary conversion</li>
  <li><strong>Dialogue Systems</strong>: Context-aware response generation</li>
  <li><strong>Code Translation</strong>: Between programming languages</li>
  <li><strong>Data-to-Text</strong>: Structured data to natural language</li>
</ul>

<p><strong>Examples:</strong></p>
<ul>
  <li>Google Translate (earlier versions)</li>
  <li>T5 (Text-to-Text Transfer Transformer)</li>
  <li>BART (Bidirectional and Auto-Regressive Transformers)</li>
  <li>mT5 (Multilingual T5)</li>
</ul>

<hr />

<h3 id="decoder-only-transformers">Decoder-Only Transformers</h3>

<h4 id="architecture-design-1">Architecture Design</h4>

<p>Decoder-only models use a single stack with causal attention:</p>

<p><img src="/assets/picture/2025-09-17-cs336-transformer-architecture-overview/decoder-only-transformer-lm.png" alt="Decoder-Only Transformer Architecture" width="880" /></p>

<p><strong>Key Characteristics:</strong></p>
<ul>
  <li><strong>Causal Masking</strong>: Prevents attention to future tokens during training and inference</li>
  <li><strong>Autoregressive Generation</strong>: Produces one token at a time during generation</li>
  <li><strong>Unified Architecture</strong>: Same model architecture handles various tasks through different prompting strategies</li>
  <li><strong>Scalability</strong>: Architecture scales well to very large model sizes (billions of parameters)</li>
</ul>

<p><strong>Key Features:</strong></p>
<ul>
  <li><strong>ğŸ”’ Causal Masking</strong>: Can only attend to previous tokens</li>
  <li><strong>ğŸ”„ Autoregressive</strong>: Generates tokens one at a time</li>
  <li><strong>ğŸ’¬ Text Generation</strong>: Chat, completion, and code generation</li>
</ul>

<p>This architecture has become the foundation for modern large language models like GPT, excelling at open-ended text generation and few-shot learning through prompting.</p>

<h4 id="training-methodology-1">Training Methodology</h4>

<p><strong>Objective</strong>: Learn to predict the next token given previous context</p>

<p><strong>Training Process:</strong></p>
<ol>
  <li><strong>Next Token Prediction</strong>: Core training objective</li>
  <li><strong>Causal Masking</strong>: Maintains autoregressive property during training</li>
  <li><strong>Large-Scale Data</strong>: Trained on massive text corpora</li>
</ol>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Training pseudocode
</span><span class="k">def</span> <span class="nf">train_decoder_only</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">dataloader</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">dataloader</span><span class="p">:</span>
        <span class="c1"># Sequence: "The cat sat on the mat"
</span>        <span class="n">input_tokens</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="s">'tokens'</span><span class="p">][:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>   <span class="c1"># "The cat sat on the"
</span>        <span class="n">target_tokens</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="s">'tokens'</span><span class="p">][</span><span class="mi">1</span><span class="p">:]</span>   <span class="c1"># "cat sat on the mat"
</span>        
        <span class="c1"># Forward pass with causal masking
</span>        <span class="n">logits</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">input_tokens</span><span class="p">)</span>
        
        <span class="c1"># Next token prediction loss
</span>        <span class="n">loss</span> <span class="o">=</span> <span class="n">cross_entropy</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">target_tokens</span><span class="p">)</span>
        
        <span class="n">loss</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="p">.</span><span class="n">step</span><span class="p">()</span>
</code></pre></div></div>

<p><strong>Multi-Stage Training:</strong></p>

<ol>
  <li><strong>Pre-training</strong>:
    <ul>
      <li><strong>Data</strong>: Large-scale web text, books, articles</li>
      <li><strong>Objective</strong>: Next token prediction</li>
      <li><strong>Scale</strong>: Billions to trillions of tokens</li>
    </ul>
  </li>
  <li><strong>Instruction Fine-tuning</strong>:
    <ul>
      <li><strong>Data</strong>: Human-written instruction-response pairs</li>
      <li><strong>Objective</strong>: Follow instructions accurately</li>
      <li><strong>Benefits</strong>: Improved task performance</li>
    </ul>
  </li>
  <li><strong>Reinforcement Learning from Human Feedback (RLHF)</strong>:
    <ul>
      <li><strong>Data</strong>: Human preference comparisons</li>
      <li><strong>Objective</strong>: Align with human values</li>
      <li><strong>Benefits</strong>: Safer, more helpful responses</li>
    </ul>
  </li>
</ol>

<h4 id="evaluation-methods-1">Evaluation Methods</h4>

<p><strong>Multiple Evaluation Paradigms:</strong></p>

<ol>
  <li><strong>Perplexity Measurement</strong>:
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">compute_perplexity</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">test_data</span><span class="p">):</span>
 <span class="n">total_loss</span> <span class="o">=</span> <span class="mi">0</span>
 <span class="n">total_tokens</span> <span class="o">=</span> <span class="mi">0</span>
    
 <span class="k">for</span> <span class="n">sequence</span> <span class="ow">in</span> <span class="n">test_data</span><span class="p">:</span>
     <span class="n">loss</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">compute_loss</span><span class="p">(</span><span class="n">sequence</span><span class="p">)</span>
     <span class="n">total_loss</span> <span class="o">+=</span> <span class="n">loss</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">sequence</span><span class="p">)</span>
     <span class="n">total_tokens</span> <span class="o">+=</span> <span class="nb">len</span><span class="p">(</span><span class="n">sequence</span><span class="p">)</span>
    
 <span class="k">return</span> <span class="n">exp</span><span class="p">(</span><span class="n">total_loss</span> <span class="o">/</span> <span class="n">total_tokens</span><span class="p">)</span>
</code></pre></div>    </div>
  </li>
  <li><strong>Generation Quality</strong>:
    <ul>
      <li><strong>Human Evaluation</strong>: Coherence, relevance, helpfulness</li>
      <li><strong>Automatic Metrics</strong>: Diversity, repetition, toxicity</li>
      <li><strong>Task-Specific</strong>: BLEU for translation, ROUGE for summarization</li>
    </ul>
  </li>
  <li><strong>Benchmark Evaluation</strong>:
    <ul>
      <li><strong>GLUE/SuperGLUE</strong>: General language understanding</li>
      <li><strong>MMLU</strong>: Massive multitask language understanding</li>
      <li><strong>HumanEval</strong>: Code generation capabilities</li>
      <li><strong>HellaSwag</strong>: Commonsense reasoning</li>
    </ul>
  </li>
</ol>

<h4 id="use-cases-and-applications-1">Use Cases and Applications</h4>

<p><strong>Primary Applications:</strong></p>
<ul>
  <li><strong>Text Generation</strong>: Creative writing, content creation</li>
  <li><strong>Conversational AI</strong>: Chatbots, virtual assistants</li>
  <li><strong>Code Generation</strong>: Programming assistance</li>
  <li><strong>Question Answering</strong>: Information retrieval and reasoning</li>
  <li><strong>Few-Shot Learning</strong>: Task adaptation through prompting</li>
</ul>

<p><strong>Examples:</strong></p>
<ul>
  <li>GPT family (GPT-2, GPT-3, GPT-4)</li>
  <li>LLaMA (Large Language Model Meta AI)</li>
  <li>PaLM (Pathways Language Model)</li>
  <li>Claude (Anthropicâ€™s assistant)</li>
  <li>ChatGPT and GPT-4</li>
</ul>

<hr />

<h3 id="encoder-only-transformers">Encoder-Only Transformers</h3>

<h4 id="architecture-design-2">Architecture Design</h4>

<p>Encoder-only models use bidirectional attention for understanding:</p>

<p><img src="/assets/picture/2025-09-17-cs336-transformer-architecture-overview/encoder-only-transformer-lm.png" alt="Encoder-Only Transformer Architecture" width="880" /></p>

<p><strong>Key Characteristics:</strong></p>
<ul>
  <li><strong>Bidirectional Context</strong>: Can attend to all positions in the sequence simultaneously</li>
  <li><strong>Rich Representations</strong>: Deep contextual understanding from both left and right context</li>
  <li><strong>Task Adaptation</strong>: Requires fine-tuning for downstream tasks but excels at understanding</li>
  <li><strong>Special Tokens</strong>: Uses [CLS] and [SEP] tokens for sequence classification and separation</li>
</ul>

<p><strong>Key Features:</strong></p>
<ul>
  <li><strong>ğŸ”„ Bidirectional Attention</strong>: Full context understanding from both directions</li>
  <li><strong>ğŸ§  Understanding Tasks</strong>: Classification, extraction, comprehension</li>
  <li><strong>ğŸ“š Pre-training + Fine-tuning</strong>: Masked language modeling then task-specific training</li>
</ul>

<p>This architecture excels at tasks requiring deep understanding of text, where the model benefits from seeing the entire context before making predictions. The bidirectional nature makes it particularly powerful for classification and extraction tasks.</p>

<h4 id="training-methodology-2">Training Methodology</h4>

<p><strong>Pre-training Objective</strong>: Masked Language Modeling (MLM)</p>

<p><strong>Training Process:</strong></p>
<ol>
  <li><strong>Token Masking</strong>: Randomly mask 15% of input tokens</li>
  <li><strong>Bidirectional Processing</strong>: Full context available for predictions</li>
  <li><strong>Mask Prediction</strong>: Reconstruct original tokens</li>
</ol>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Training pseudocode
</span><span class="k">def</span> <span class="nf">train_encoder_only</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">dataloader</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">dataloader</span><span class="p">:</span>
        <span class="c1"># Original: "The cat sat on the mat"
</span>        <span class="c1"># Masked:   "The [MASK] sat on the [MASK]"
</span>        
        <span class="n">masked_tokens</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="s">'masked_input'</span><span class="p">]</span>
        <span class="n">original_tokens</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="s">'original_input'</span><span class="p">]</span>
        <span class="n">mask_positions</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="s">'mask_positions'</span><span class="p">]</span>
        
        <span class="c1"># Bidirectional encoding
</span>        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">masked_tokens</span><span class="p">)</span>
        
        <span class="c1"># Predict only at masked positions
</span>        <span class="n">masked_predictions</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="p">[</span><span class="n">mask_positions</span><span class="p">]</span>
        <span class="n">masked_targets</span> <span class="o">=</span> <span class="n">original_tokens</span><span class="p">[</span><span class="n">mask_positions</span><span class="p">]</span>
        
        <span class="n">loss</span> <span class="o">=</span> <span class="n">cross_entropy</span><span class="p">(</span><span class="n">masked_predictions</span><span class="p">,</span> <span class="n">masked_targets</span><span class="p">)</span>
        
        <span class="n">loss</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="p">.</span><span class="n">step</span><span class="p">()</span>
</code></pre></div></div>

<p><strong>Masking Strategy:</strong></p>
<ul>
  <li><strong>80%</strong>: Replace with [MASK] token</li>
  <li><strong>10%</strong>: Replace with random token</li>
  <li><strong>10%</strong>: Keep original token</li>
</ul>

<p><strong>Fine-tuning for Downstream Tasks:</strong></p>

<p>After pre-training, models are fine-tuned for specific applications:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">finetune_classification</span><span class="p">(</span><span class="n">pretrained_model</span><span class="p">,</span> <span class="n">task_data</span><span class="p">):</span>
    <span class="c1"># Add task-specific classification head
</span>    <span class="n">classifier</span> <span class="o">=</span> <span class="n">Linear</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">)</span>
    
    <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">task_data</span><span class="p">:</span>
        <span class="n">inputs</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="s">'text'</span><span class="p">],</span> <span class="n">batch</span><span class="p">[</span><span class="s">'labels'</span><span class="p">]</span>
        
        <span class="c1"># Get contextual representations
</span>        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">pretrained_model</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
        
        <span class="c1"># Use [CLS] token for classification
</span>        <span class="n">cls_representation</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span>
        
        <span class="c1"># Classification prediction
</span>        <span class="n">logits</span> <span class="o">=</span> <span class="n">classifier</span><span class="p">(</span><span class="n">cls_representation</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">cross_entropy</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
        
        <span class="n">loss</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="p">.</span><span class="n">step</span><span class="p">()</span>
</code></pre></div></div>

<h4 id="evaluation-methods-2">Evaluation Methods</h4>

<p><strong>Task-Specific Evaluation:</strong></p>

<ol>
  <li><strong>Classification Tasks</strong>:
    <ul>
      <li><strong>Accuracy</strong>: Percentage of correct predictions</li>
      <li><strong>F1-Score</strong>: Harmonic mean of precision and recall</li>
      <li><strong>Matthews Correlation</strong>: Balanced measure for imbalanced data</li>
    </ul>
  </li>
  <li><strong>Token-Level Tasks</strong>:
    <ul>
      <li><strong>Named Entity Recognition</strong>: Entity-level F1</li>
      <li><strong>Part-of-Speech Tagging</strong>: Token-level accuracy</li>
      <li><strong>Dependency Parsing</strong>: Unlabeled/labeled attachment scores</li>
    </ul>
  </li>
  <li><strong>Span-Based Tasks</strong>:
    <ul>
      <li><strong>Question Answering</strong>: Exact match and F1 scores</li>
      <li><strong>Reading Comprehension</strong>: Answer extraction accuracy</li>
    </ul>
  </li>
</ol>

<p><strong>Benchmark Suites:</strong></p>
<ul>
  <li><strong>GLUE</strong>: General Language Understanding Evaluation</li>
  <li><strong>SuperGLUE</strong>: More challenging language understanding tasks</li>
  <li><strong>SentEval</strong>: Sentence representation evaluation</li>
</ul>

<h4 id="use-cases-and-applications-2">Use Cases and Applications</h4>

<p><strong>Primary Applications:</strong></p>
<ul>
  <li><strong>Text Classification</strong>: Sentiment analysis, topic classification</li>
  <li><strong>Named Entity Recognition</strong>: Information extraction</li>
  <li><strong>Question Answering</strong>: Extractive QA systems</li>
  <li><strong>Semantic Similarity</strong>: Text matching and retrieval</li>
  <li><strong>Language Understanding</strong>: Intent classification, slot filling</li>
</ul>

<p><strong>Examples:</strong></p>
<ul>
  <li>BERT (Bidirectional Encoder Representations from Transformers)</li>
  <li>RoBERTa (Robustly Optimized BERT Pretraining Approach)</li>
  <li>DeBERTa (Decoding-enhanced BERT with Disentangled Attention)</li>
  <li>ELECTRA (Efficiently Learning an Encoder that Classifies Token Replacements Accurately)</li>
</ul>

<hr />

<h3 id="comparison-summary">Comparison Summary</h3>

<h4 id="architecture-comparison">Architecture Comparison</h4>

<p>The three transformer architectures shown in the diagrams above have distinct characteristics that make them suitable for different tasks:</p>

<table>
  <thead>
    <tr>
      <th>Aspect</th>
      <th>Encoder-Decoder</th>
      <th>Decoder-Only</th>
      <th>Encoder-Only</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Attention Pattern</strong></td>
      <td>Bidirectional + Causal</td>
      <td>Causal Only</td>
      <td>Bidirectional Only</td>
    </tr>
    <tr>
      <td><strong>Primary Strength</strong></td>
      <td>Seq2seq transformation</td>
      <td>Text generation</td>
      <td>Text understanding</td>
    </tr>
    <tr>
      <td><strong>Training Data</strong></td>
      <td>Parallel sequences</td>
      <td>Raw text</td>
      <td>Raw text + labels</td>
    </tr>
    <tr>
      <td><strong>Evaluation Focus</strong></td>
      <td>Generation quality</td>
      <td>Perplexity + tasks</td>
      <td>Task performance</td>
    </tr>
    <tr>
      <td><strong>Inference</strong></td>
      <td>Autoregressive</td>
      <td>Autoregressive</td>
      <td>Single forward pass</td>
    </tr>
    <tr>
      <td><strong>Architecture Complexity</strong></td>
      <td>Most complex (2 stacks)</td>
      <td>Simple (1 stack)</td>
      <td>Simple (1 stack)</td>
    </tr>
    <tr>
      <td><strong>Cross-Attention</strong></td>
      <td>âœ… Required</td>
      <td>âŒ None</td>
      <td>âŒ None</td>
    </tr>
  </tbody>
</table>

<h4 id="training-requirements">Training Requirements</h4>

<table>
  <thead>
    <tr>
      <th>Requirement</th>
      <th>Encoder-Decoder</th>
      <th>Decoder-Only</th>
      <th>Encoder-Only</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Data Quantity</strong></td>
      <td>Moderate (paired data)</td>
      <td>Large (raw text)</td>
      <td>Moderate (raw + labeled)</td>
    </tr>
    <tr>
      <td><strong>Data Quality</strong></td>
      <td>High (alignment crucial)</td>
      <td>Variable (web-scale)</td>
      <td>High (clean text)</td>
    </tr>
    <tr>
      <td><strong>Compute Cost</strong></td>
      <td>Moderate</td>
      <td>Very High</td>
      <td>Moderate</td>
    </tr>
    <tr>
      <td><strong>Training Time</strong></td>
      <td>Days to weeks</td>
      <td>Weeks to months</td>
      <td>Days to weeks</td>
    </tr>
  </tbody>
</table>

<h4 id="use-case-suitability">Use Case Suitability</h4>

<table>
  <thead>
    <tr>
      <th>Task Type</th>
      <th>Best Architecture</th>
      <th>Rationale</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Translation</strong></td>
      <td>Encoder-Decoder</td>
      <td>Structured input-output mapping with cross-attention</td>
    </tr>
    <tr>
      <td><strong>Text Generation</strong></td>
      <td>Decoder-Only</td>
      <td>Autoregressive nature with causal masking</td>
    </tr>
    <tr>
      <td><strong>Classification</strong></td>
      <td>Encoder-Only</td>
      <td>Bidirectional understanding with task-specific heads</td>
    </tr>
    <tr>
      <td><strong>Summarization</strong></td>
      <td>Encoder-Decoder / Decoder-Only</td>
      <td>Both work well - encoder-decoder for extractive, decoder-only for abstractive</td>
    </tr>
    <tr>
      <td><strong>Question Answering</strong></td>
      <td>All three</td>
      <td>Encoder-only for extractive, decoder-only for generative, encoder-decoder for complex reasoning</td>
    </tr>
    <tr>
      <td><strong>Dialogue</strong></td>
      <td>Decoder-Only</td>
      <td>Generative conversation with context understanding</td>
    </tr>
    <tr>
      <td><strong>Code Generation</strong></td>
      <td>Decoder-Only</td>
      <td>Sequential token generation with programming syntax</td>
    </tr>
    <tr>
      <td><strong>Sentiment Analysis</strong></td>
      <td>Encoder-Only</td>
      <td>Classification task with bidirectional context</td>
    </tr>
    <tr>
      <td><strong>Named Entity Recognition</strong></td>
      <td>Encoder-Only</td>
      <td>Token-level classification with full context</td>
    </tr>
  </tbody>
</table>

<h4 id="architecture-selection-guide">Architecture Selection Guide</h4>

<p><strong>Choose Encoder-Decoder when:</strong></p>
<ul>
  <li>You have paired input-output data (parallel corpora)</li>
  <li>Tasks require understanding input completely before generating output</li>
  <li>You need structured transformations (translation, summarization)</li>
  <li>Cross-attention between source and target is beneficial</li>
</ul>

<p><strong>Choose Decoder-Only when:</strong></p>
<ul>
  <li>You want a unified model for multiple tasks</li>
  <li>Open-ended text generation is the primary goal</li>
  <li>You have large amounts of raw text data</li>
  <li>You want to leverage in-context learning and prompting</li>
</ul>

<p><strong>Choose Encoder-Only when:</strong></p>
<ul>
  <li>Understanding and classification are the primary goals</li>
  <li>You donâ€™t need to generate long sequences</li>
  <li>You have labeled data for fine-tuning</li>
  <li>Bidirectional context improves performance significantly</li>
</ul>

<p>This is just a quick note ğŸ“ â€” to dive into the details, youâ€™d probably need to read some relevant papers ğŸ“š, but I hope it still shared something useful âœ¨</p>]]></content><author><name>[&quot;Han Yu&quot;]</name></author><category term="cs336" /><summary type="html"><![CDATA[An Overview of Popular Transformer Architectures]]></summary></entry></feed>