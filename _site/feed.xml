<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2025-10-05T15:18:59-07:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">üçí Han‚Äôs Generative AI Quest</title><subtitle>I chronicle my captivating journey through Generative AI, sharing insights,  breakthroughs, and learnings from my enthralling side projects in the field. 
</subtitle><entry><title type="html">Study Notes: Stanford CS336 Language Modeling from Scratch [8]</title><link href="http://localhost:4000/cs336/2025/10/05/cs336-training-a-transformer-lm-part-1.html" rel="alternate" type="text/html" title="Study Notes: Stanford CS336 Language Modeling from Scratch [8]" /><published>2025-10-05T00:00:00-07:00</published><updated>2025-10-05T00:00:00-07:00</updated><id>http://localhost:4000/cs336/2025/10/05/cs336-training-a-transformer-lm-part-1</id><content type="html" xml:base="http://localhost:4000/cs336/2025/10/05/cs336-training-a-transformer-lm-part-1.html"><![CDATA[<h2 id="planning-llm-training-cross-entropy-loss-optimizers-memory-and-computational-cost-and-other-practical-levers">Planning LLM Training: Cross-Entropy Loss, Optimizers, Memory and Computational Cost, and other practical levers</h2>

<h3 id="table-of-contents">Table of Contents</h3>
<ol>
  <li><a href="#cross-entropy-loss">Cross-Entropy Loss: Measuring How Wrong We Are</a></li>
  <li><a href="#perplexity">Perplexity: A More Intuitive Metric</a></li>
  <li><a href="#sgd-optimizer">SGD Optimizer: Walking Downhill</a></li>
  <li><a href="#adamw">AdamW: The Smart Optimizer</a></li>
  <li><a href="#memory-requirements">Memory Requirements: Can It Fit?</a></li>
  <li><a href="#computational-cost">Computational Cost: How Long Will This Take?</a></li>
  <li><a href="#learning-rate-schedules">Learning Rate Schedules: Starting Fast, Ending Slow</a></li>
  <li><a href="#gradient-clipping">Gradient Clipping: The Safety Mechanism</a></li>
</ol>

<hr />

<h3 id="cross-entropy-loss">Cross-Entropy Loss: Measuring How Wrong We Are</h3>

<p><em>A language model is trying to <strong>predict the next word</strong> in a sequence. Noted that we‚Äôre training a supervised learning model, as we know what is the next correct word in the sequence from the training dataset, and we want to minimize the loss when the model did not predict the correct word.</em></p>

<h4 id="simple-example">Simple Example</h4>

<p>Imagine you have the sentence: ‚ÄúThe cat sat on the ___‚Äù</p>

<ul>
  <li>The model needs to predict what comes next</li>
  <li>Maybe the correct word is ‚Äúmat‚Äù</li>
  <li>The model gives probabilities for all possible words in its vocabulary</li>
</ul>

<h4 id="the-simple-version-of-the-math">The Simple Version of the Math</h4>

<p><strong>Step 1: The model outputs ‚Äúlogits‚Äù</strong> (raw scores for each possible word)</p>
<ul>
  <li>Think of logits as unnormalized scores</li>
  <li>Example: ‚Äúmat‚Äù gets score 5.2, ‚Äúdog‚Äù gets 1.3, ‚Äútable‚Äù gets 3.1, etc.</li>
</ul>

<p><strong>Step 2: Convert logits to probabilities using softmax</strong></p>

\[p(\text{word}) = \frac{e^{\text{score of that word}}}{\text{sum of } e^{\text{score}} \text{ for all words}}\]

<p>Example:</p>
<ul>
  <li>If ‚Äúmat‚Äù has score 5.2: probability = $\frac{e^{5.2}}{e^{5.2} + e^{1.3} + e^{3.1} + ‚Ä¶}$</li>
</ul>

<p><strong>Step 3: Calculate the loss</strong></p>

\[\text{loss} = -\log(p(\text{correct word}))\]

<p>Assume the correct word is ‚Äúmat‚Äù, and we want the model to assign high probability to the correct word and low probability to the incorrect word.</p>

<ul>
  <li>If the model gives ‚Äúmat‚Äù a probability of 0.8 ‚Üí loss = -log(0.8) ‚âà 0.22 (small loss, good!)</li>
  <li>If the model gives ‚Äúmat‚Äù a probability of 0.1 ‚Üí loss = -log(0.1) ‚âà 2.30 (big loss, bad!)</li>
  <li>If the model gives ‚Äúmat‚Äù a probability of 1.0 ‚Üí loss = -log(1.0) = 0 (no loss, perfect!)</li>
</ul>

<h4 id="the-full-version-of-the-math">The Full Version of the Math</h4>

<p>The complete formula averages this loss over:</p>
<ul>
  <li>All positions in a sequence (i = 1 to m)</li>
  <li>All sequences in the training dataset D (all x in D)</li>
</ul>

\[\ell(\theta; D) = \frac{1}{|D|m} \sum_{x \in D} \sum_{i=1}^{m} -\log p_\theta(x_{i+1} | x_{1:i})\]

<p><strong>Bottom line:</strong> Cross-entropy loss is small when the model assigns high probability to the correct next word, and large when it doesn‚Äôt. Training tries to minimize this loss!</p>

<hr />

<h3 id="perplexity">Perplexity: A More Intuitive Metric</h3>

<p><strong>Perplexity</strong> is an <strong>evaluation metric</strong> (not a loss function) that provides a more intuitive way to measure how good your language model is. It answers the question: <strong>‚ÄúOn average, how many words is the model confused between?‚Äù</strong>  While we <strong>train</strong> using cross-entropy loss, we <strong>report</strong> perplexity to humans because it‚Äôs easier to interpret.</p>

<h4 id="simple-analogy">Simple Analogy</h4>

<p>Imagine a multiple-choice test:</p>

<ul>
  <li><strong>Perplexity = 1</strong>: The model is 100% certain (like having only 1 choice)</li>
  <li><strong>Perplexity = 10</strong>: The model is as confused as if it had to guess among 10 equally likely options</li>
  <li><strong>Perplexity = 100</strong>: The model is as confused as if it had to guess among 100 equally likely options</li>
</ul>

<p><strong>Lower perplexity = Better model!</strong></p>

<h4 id="the-math">The Math</h4>

<p>For a sequence where we make m predictions with cross-entropy losses $\ell_1, \ell_2, ‚Ä¶, \ell_m$:</p>

\[\text{perplexity} = \exp\left(\frac{1}{m} \sum_{i=1}^{m} \ell_i\right)\]

<p>This is equivalent to:</p>

\[\text{perplexity} = \exp(\text{average cross-entropy loss})\]

<p><strong>Breaking it down:</strong></p>

<p><strong>Step 1:</strong> Calculate the average cross-entropy loss</p>
<ul>
  <li>Recall that for each position i: 
  \(\ell_i = -\log p(x_{i+1} | x_{1:i})\)</li>
  <li>Add up all the losses: $\ell_1 + \ell_2 + ‚Ä¶ + \ell_m$</li>
  <li>Divide by m (the number of token predictions in the sequence)</li>
  <li>Average loss = $\frac{1}{m} \sum_{i=1}^{m} \ell_i$</li>
</ul>

<p><strong>Step 2:</strong> Take the exponential</p>
<ul>
  <li>Apply $\exp()$ to the average loss</li>
  <li>This ‚Äúundoes‚Äù the log in the cross-entropy formula</li>
</ul>

<h4 id="why-exponential">Why Exponential?</h4>

<p>The exponential transformation converts the abstract loss value into an interpretable number:</p>

<p><strong>Mathematical intuition:</strong></p>
<ul>
  <li>Cross-entropy loss: $\ell = -\log p(\text{correct word})$</li>
  <li>Perplexity undoes the log: $\exp(\ell) = \exp(-\log p) = \frac{1}{p}$</li>
  <li>If average probability is 0.1, perplexity ‚âà 10 (confused among ~10 words)</li>
  <li>If average probability is 0.01, perplexity ‚âà 100 (confused among ~100 words)</li>
</ul>

<h4 id="concrete-example">Concrete Example</h4>

<p>Say your model predicts 3 words in a sequence:</p>
<ul>
  <li>Word 1: $\ell_1 = 0$, probability was 1.0 (perfect!)</li>
  <li>Word 2: $\ell_2 = 2.3$, probability was 0.1</li>
  <li>Word 3: $\ell_3 = 0.69$, probability was 0.5</li>
</ul>

<p><strong>Calculation:</strong></p>

<p>Average loss = $\frac{0 + 2.3 + 0.69}{3} = \frac{2.99}{3} \approx 1.0$</p>

<p>Perplexity = $\exp(1.0) \approx 2.72$</p>

<p><strong>Interpretation:</strong> On average, the model is as uncertain as if it had to choose uniformly among about <strong>2.72 equally likely words</strong> at each position.</p>

<h4 id="relationship-to-training">Relationship to Training</h4>

<ul>
  <li><strong>During training:</strong> We minimize cross-entropy loss</li>
  <li><strong>During evaluation:</strong> We report perplexity for interpretability</li>
  <li><strong>They‚Äôre equivalent:</strong> Lower cross-entropy ‚ü∫ Lower perplexity</li>
</ul>

<p>Since perplexity is just an exponential transformation of cross-entropy, optimizing one automatically optimizes the other. We use cross-entropy for training because it has better mathematical properties for gradient-based optimization.</p>

<h4 id="key-takeaway">Key Takeaway</h4>

<p><strong>Perplexity is a user-friendly version of cross-entropy loss:</strong></p>
<ul>
  <li>Lower perplexity = model is more confident and accurate</li>
  <li>Higher perplexity = model is confused and uncertain</li>
  <li>It‚Äôs <strong>not used for training</strong>, only for <strong>reporting results</strong> in a more interpretable way</li>
  <li>Cross-entropy and perplexity are mathematically equivalent‚Äîminimizing one minimizes the other</li>
</ul>

<hr />

<h3 id="sgd-optimizer">SGD Optimizer: Which direction to walk to go downhill</h3>

<p>SGD (Stochastic Gradient Descent) is an algorithm that <strong>adjusts your model‚Äôs parameters</strong> during the training process to make the loss smaller. Think of it as teaching the model to make better predictions.</p>
<h4 id="the-mountain-analogy">The Mountain Analogy</h4>

<p>Imagine you‚Äôre standing on a mountain in the fog (you can‚Äôt see far):</p>
<ul>
  <li>Your <strong>position</strong> = model parameters (Œ∏)</li>
  <li>Your <strong>altitude</strong> = loss (how bad the model is)</li>
  <li>Your <strong>goal</strong> = get to the bottom of the valley (minimize loss)</li>
</ul>

<p><strong>SGD tells you which direction to walk to go downhill!</strong></p>

<h4 id="how-sgd-works-step-by-step">How SGD Works (Step by Step)</h4>

<h5 id="step-1-start-randomly">Step 1: Start Randomly</h5>
<ul>
  <li>Œ∏‚ÇÄ = random starting position on the mountain</li>
  <li>You don‚Äôt know where the bottom is yet</li>
</ul>

<h5 id="step-2-look-around-calculate-gradient">Step 2: Look Around (Calculate Gradient)</h5>
<ul>
  <li>‚àáL(Œ∏‚Çú; B‚Çú) = ‚ÄúWhich direction is downhill?‚Äù</li>
  <li>The gradient tells you the steepest uphill direction</li>
  <li>So <strong>negative gradient</strong> points downhill!</li>
</ul>

<h5 id="step-3-take-a-step-downhill">Step 3: Take a Step Downhill</h5>

\[\theta_{t+1} = \theta_t - \alpha_t \nabla L(\theta_t; B_t)\]

<p>Let me break down each part:</p>
<ul>
  <li><strong>Œ∏‚Çú</strong> = where you are now</li>
  <li><strong>‚àáL(Œ∏‚Çú; B‚Çú)</strong> = direction of steepest uphill</li>
  <li><strong>-‚àáL(Œ∏‚Çú; B‚Çú)</strong> = direction of steepest downhill (flip the sign!)</li>
  <li><strong>Œ±‚Çú</strong> = learning rate (how big a step to take)</li>
  <li><strong>Œ∏‚Çú‚Çä‚ÇÅ</strong> = your new position</li>
</ul>

<h5 id="step-4-repeat">Step 4: Repeat!</h5>
<ul>
  <li>Keep taking steps downhill until you reach the valley (minimum loss)</li>
</ul>

<h4 id="key-concepts">Key Concepts</h4>

<h5 id="learning-rate-Œ±‚Çú">Learning Rate (Œ±‚Çú)</h5>
<ul>
  <li><strong>Too large</strong>: You take huge steps and might overshoot the valley</li>
  <li><strong>Too small</strong>: You take tiny steps and it takes forever</li>
  <li><strong>Just right</strong>: You make steady progress</li>
</ul>

<p><strong>Example:</strong></p>
<ul>
  <li>If gradient says ‚Äúgo left by 10 units‚Äù and Œ± = 0.1</li>
  <li>You actually move right by: 10 √ó 0.1 = 1 unit</li>
</ul>

<h5 id="batch-b‚Çú">Batch (B‚Çú)</h5>
<ul>
  <li>Instead of using ALL your data to calculate the gradient (slow!), use a <strong>random small batch</strong></li>
  <li>This is the ‚Äústochastic‚Äù part - it‚Äôs random! Here ‚Äúrandom‚Äù means at each training step t, we randomly sample a subset of examples from the full training dataset D.</li>
  <li><strong>Batch size</strong> = how many examples you use each step (this is fixed during training)</li>
</ul>

<p><strong>Why random batches?</strong></p>
<ul>
  <li>Much faster! (calculating gradient on 1 million examples is slow)</li>
  <li>Still gives you a good enough direction</li>
  <li>Adds helpful randomness that can escape bad spots</li>
</ul>

<h4 id="simple-example-1">Simple Example</h4>

<p>Suppose your model has one parameter Œ∏ (to keep it simple):</p>

<p><strong>Initial:</strong> Œ∏‚ÇÄ = 5, loss = 100</p>

<p><strong>Step 1:</strong></p>
<ul>
  <li>Calculate gradient on a batch: ‚àáL = 20 (loss increases if we increase Œ∏)</li>
  <li>Learning rate: Œ± = 0.1</li>
  <li>Update: Œ∏‚ÇÅ = 5 - 0.1 √ó 20 = 5 - 2 = <strong>3</strong></li>
</ul>

<p><strong>Step 2:</strong></p>
<ul>
  <li>New gradient: ‚àáL = 10</li>
  <li>Update: Œ∏‚ÇÇ = 3 - 0.1 √ó 10 = 3 - 1 = <strong>2</strong></li>
</ul>

<p><strong>Step 3:</strong></p>
<ul>
  <li>New gradient: ‚àáL = 2</li>
  <li>Update: Œ∏‚ÇÉ = 2 - 0.1 √ó 2 = 2 - 0.2 = <strong>1.8</strong></li>
</ul>

<p>You keep going until the loss stops decreasing!</p>

<h4 id="key-takeaway-1">Key Takeaway</h4>

<p><strong>SGD is like walking downhill in small steps:</strong></p>
<ol>
  <li>Check which way is uphill (gradient)</li>
  <li>Take a step in the negative direction (size of the step determined by learning rate)</li>
  <li>Repeat until you reach the bottom (minimum loss)</li>
</ol>

<p>The ‚Äústochastic‚Äù part just means you randomly sample small batches of data instead of using entire dataset when calculate the gradient, making it much faster!</p>

<hr />

<h3 id="adamw">AdamW: The Smart Optimizer</h3>

<h4 id="whats-the-problem-with-sgd">What‚Äôs the Problem with SGD?</h4>

<p>Remember SGD takes the same size step (Œ±) for every parameter. But what if:</p>
<ul>
  <li>Some parameters need <strong>big updates</strong> (they‚Äôre far from optimal)</li>
  <li>Some parameters need <strong>tiny updates</strong> (they‚Äôre almost perfect)</li>
</ul>

<p><strong>AdamW is smarter</strong> via adapting the step size for each parameter individually.</p>

<h4 id="the-big-idea">The Big Idea</h4>

<p>AdamW keeps track of <strong>two pieces of memory</strong> for each parameter:</p>

<ol>
  <li><strong>m (first moment)</strong>: ‚ÄúWhich direction has this parameter been moving lately?‚Äù (like momentum)</li>
  <li><strong>v (second moment)</strong>: ‚ÄúHow much has this parameter been jumping around?‚Äù (like volatility)</li>
</ol>

<p>Then it uses this information to take smarter steps!</p>

<h4 id="how-adamw-works-step-by-step">How AdamW Works (Step by Step)</h4>

<h5 id="setup">Setup</h5>
<ul>
  <li><strong>m = 0</strong>: Start with no momentum</li>
  <li><strong>v = 0</strong>: Start with no volatility estimate</li>
  <li><strong>Œ≤‚ÇÅ = 0.9</strong>: How much to remember past directions (typically 90%)</li>
  <li><strong>Œ≤‚ÇÇ = 0.999</strong>: How much to remember past volatility (typically 99.9%)</li>
</ul>

<h5 id="each-training-step">Each Training Step</h5>

<p><strong>Step 1: Calculate gradient</strong> (same as SGD)</p>
<ul>
  <li>g = ‚àá‚Ñì(Œ∏; B‚Çú)</li>
  <li>‚ÄúWhich way should we move?‚Äù</li>
</ul>

<p><strong>Step 2: Update momentum (first moment)</strong></p>

\[m = \beta_1 \cdot m + (1-\beta_1) \cdot g\]

<p>Think of this as an <strong>exponential moving average</strong>:</p>
<ul>
  <li>Keep 90% of the old direction (Œ≤‚ÇÅm)</li>
  <li>Add 10% of the new direction ((1-Œ≤‚ÇÅ)g)</li>
  <li>This smooths out noisy gradients!</li>
</ul>

<p><strong>Step 3: Update volatility (second moment)</strong></p>

\[v = \beta_2 \cdot v + (1-\beta_2) \cdot g^2\]

<p>Same idea but for squared gradients:</p>
<ul>
  <li>Keep 99.9% of old volatility estimate</li>
  <li>Add 0.1% of new squared gradient</li>
  <li>This tracks how ‚Äújumpy‚Äù the parameter is</li>
</ul>

<p><strong>Step 4: Adjust learning rate</strong></p>

\[\alpha_t = \alpha \frac{\sqrt{1-\beta_2^t}}{1-\beta_1^t}\]

<p>This <strong>bias correction</strong> compensates for starting at m=0 and v=0 (they start biased toward zero!)</p>

<p><strong>Step 5: Update parameters (the smart part!)</strong></p>

\[\theta = \theta - \alpha_t \frac{m}{\sqrt{v} + \epsilon}\]

<p>This is where the magic happens:</p>
<ul>
  <li><strong>m</strong> tells us which direction to go</li>
  <li><strong>‚àöv</strong> tells us how confident we should be</li>
  <li>If v is <strong>large</strong> (parameter is jumpy) ‚Üí take <strong>smaller</strong> steps</li>
  <li>If v is <strong>small</strong> (parameter is stable) ‚Üí take <strong>larger</strong> steps</li>
</ul>

<p><strong>Step 6: Weight decay</strong></p>

\[\theta = \theta - \alpha\lambda\theta\]

<p>Pull parameters slightly toward zero to prevent them from getting too large (regularization)</p>

<h4 id="simple-example-2">Simple Example</h4>

<p>Imagine two parameters:</p>

<p><strong>Parameter A:</strong></p>
<ul>
  <li>Gradients: [5, 5.1, 4.9, 5, 5] (very stable!)</li>
  <li>v will be small ‚Üí AdamW takes <strong>bigger</strong> steps</li>
  <li>Makes sense! We‚Äôre confident about the direction</li>
</ul>

<p><strong>Parameter B:</strong></p>
<ul>
  <li>Gradients: [5, -4, 6, -3, 5] (super noisy!)</li>
  <li>v will be large ‚Üí AdamW takes <strong>smaller</strong> steps</li>
  <li>Makes sense! We‚Äôre uncertain, so be cautious</li>
</ul>

<h4 id="key-hyperparameters">Key Hyperparameters</h4>

<table>
  <thead>
    <tr>
      <th>Parameter</th>
      <th>Typical Value</th>
      <th>What it does</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Œ± (learning rate)</td>
      <td>0.001 or 0.0001</td>
      <td>Base step size</td>
    </tr>
    <tr>
      <td>Œ≤‚ÇÅ</td>
      <td>0.9</td>
      <td>How much momentum to keep</td>
    </tr>
    <tr>
      <td>Œ≤‚ÇÇ</td>
      <td>0.95-0.999</td>
      <td>How much volatility history to keep</td>
    </tr>
    <tr>
      <td>Œª (weight decay)</td>
      <td>0.01</td>
      <td>How much to pull toward zero</td>
    </tr>
    <tr>
      <td>Œµ</td>
      <td>10‚Åª‚Å∏</td>
      <td>Prevent division by zero</td>
    </tr>
  </tbody>
</table>

<h4 id="key-takeaway-2">Key Takeaway</h4>

<p><strong>AdamW is like a smart GPS:</strong></p>
<ul>
  <li><strong>SGD</strong>: ‚ÄúAlways drive 50 mph, no matter what‚Äù</li>
  <li><strong>AdamW</strong>: ‚ÄúDrive faster on smooth highways, slower on bumpy roads‚Äù</li>
</ul>

<p>It <strong>adapts the step size</strong> for each parameter based on:</p>
<ol>
  <li>Recent direction (momentum)</li>
  <li>Recent stability (volatility)</li>
</ol>

<p>This makes training <strong>faster and more stable</strong>, which is why all modern language models use it.</p>

<hr />

<h3 id="memory-requirements">Memory Requirements: Can It Fit?</h3>

<p>Let‚Äôs calculate how much memory we need to train a model like GPT-2 XL using AdamW with float32 precision.</p>

<h4 id="setup-1">Setup</h4>
<ul>
  <li>Data type: <strong>float32</strong> = <strong>4 bytes</strong> per number</li>
  <li>Batch size: <strong>B</strong></li>
  <li>Sequence length: <strong>L</strong> (context_length)</li>
  <li>Model dimension: <strong>d</strong> (d_model)</li>
  <li>Number of layers: <strong>N</strong> (num_layers)</li>
  <li>Number of heads: <strong>H</strong> (num_heads)</li>
  <li>Vocabulary size: <strong>V</strong> (vocab_size)</li>
  <li>Feed-forward dimension: <strong>d_ff = 4d</strong></li>
</ul>

<h3 id="memory-components">Memory Components</h3>

<p>Training a Transformer model requires four main types of memory. <strong>Parameters</strong> store the model‚Äôs learnable weights‚Äîthe numbers that define what the model knows. <strong>Gradients</strong> store the <em>direction</em> and <em>magnitude</em> of how each parameter should change during training, computed during backpropagation. <strong>Optimizer state</strong> keeps AdamW‚Äôs running statistics: <em>momentum</em> (which direction parameters have been moving) and <em>volatility</em> (how much parameters have been fluctuating), allowing the optimizer to make smarter, adaptive updates for each parameter. <strong>Activations</strong> store all the intermediate calculations from the forward pass‚Äîlike attention scores, normalized values, and layer outputs‚Äîwhich must be kept in memory so we can compute gradients during backpropagation. While parameters, gradients, and optimizer state have fixed size (Parameters, gradients, and optimizer state represent the model‚Äôs internal structure‚Äîthey exist regardless of what data you feed into the model), activations are the actual values flowing through the network for the particular batch therefore they scale dramatically with both batch size (B) and sequence length (L), particularly the attention scores which grow quadratically as O(BL¬≤). This is why memory, not computation, is often the bottleneck in training large language models‚Äîwith GPT-2 XL, even an 80GB GPU can only fit a batch size of 3.</p>

<h4 id="1-parameters-memory">1. Parameters Memory</h4>

<p>Let‚Äôs count the learnable parameters in each component of the Transformer.</p>

<h5 id="per-transformer-block">Per Transformer Block:</h5>

<p><strong>A. RMSNorm Layers (2 per block)</strong></p>

<p>Each RMSNorm layer has a learnable scale parameter for each dimension:</p>
<ul>
  <li>Pre-attention RMSNorm: <strong>d parameters</strong></li>
  <li>Pre-FFN RMSNorm: <strong>d parameters</strong></li>
  <li><strong>Subtotal: 2d parameters</strong></li>
</ul>

<p><strong>B. Multi-Head Self-Attention</strong></p>

<p>The attention mechanism consists of four projection matrices. Importantly, <strong>the number of heads H does not affect the parameter count</strong>‚Äîwe split the d dimensions across heads rather than expanding them.</p>

<ul>
  <li>Query projection W_Q: (d √ó d) ‚Üí <strong>d¬≤ parameters</strong></li>
  <li>Key projection W_K: (d √ó d) ‚Üí <strong>d¬≤ parameters</strong></li>
  <li>Value projection W_V: (d √ó d) ‚Üí <strong>d¬≤ parameters</strong></li>
  <li>Output projection W_O: (d √ó d) ‚Üí <strong>d¬≤ parameters</strong></li>
  <li><strong>Subtotal: 4d¬≤ parameters</strong></li>
</ul>

<p><em>Note: Modern architectures typically omit bias terms in these projections.</em></p>

<p><strong>C. Feed-Forward Network (FFN)</strong></p>

<p>The FFN expands to an intermediate dimension d_ff = 4d, then projects back:</p>
<ul>
  <li>First layer W‚ÇÅ: (d √ó 4d) ‚Üí <strong>4d¬≤ parameters</strong></li>
  <li>Activation (SiLU/GELU): <strong>0 parameters</strong> (no learnable weights)</li>
  <li>Second layer W‚ÇÇ: (4d √ó d) ‚Üí <strong>4d¬≤ parameters</strong></li>
  <li><strong>Subtotal: 8d¬≤ parameters</strong></li>
</ul>

<p><strong>Total per block: 2d + 4d¬≤ + 8d¬≤ = 12d¬≤ + 2d</strong></p>

<h5 id="all-n-transformer-blocks">All N Transformer Blocks:</h5>

<p><strong>N √ó (12d¬≤ + 2d) = 12Nd¬≤ + 2Nd</strong></p>

<h5 id="additional-components">Additional Components:</h5>

<p><strong>Token Embedding</strong></p>
<ul>
  <li>Maps each of V vocabulary tokens to a d-dimensional vector</li>
  <li>Shape: (V √ó d)</li>
  <li><strong>Parameters: Vd</strong></li>
</ul>

<p><strong>Final RMSNorm</strong></p>
<ul>
  <li>One scale parameter per dimension after the last transformer block</li>
  <li><strong>Parameters: d</strong></li>
</ul>

<p><strong>Output Projection</strong></p>
<ul>
  <li>In modern LLMs (GPT-2, LLaMA, etc.), the output projection <strong>shares weights</strong> with the token embedding (weight tying)</li>
  <li><strong>Additional parameters: 0</strong></li>
</ul>

<p><strong>Positional Embeddings</strong> (architecture-dependent)</p>
<ul>
  <li><strong>Modern models (LLaMA, GPT-3+):</strong> Use RoPE or ALiBi ‚Üí <strong>0 parameters</strong> ‚úì</li>
  <li><strong>Older models (GPT-2, BERT):</strong> Learned positional embeddings ‚Üí <strong>L_max √ó d parameters</strong></li>
</ul>

<p>For this calculation, we assume modern architecture with no learned positional embeddings.</p>

<h5 id="total-parameters">Total Parameters:</h5>

<p><strong>P = 12Nd¬≤ + 2Nd + Vd + d</strong></p>

<p>Which can be factored as:</p>

<p><strong>P = 12Nd¬≤ + d(2N + V + 1)</strong></p>

<p><strong>Important notes:</strong></p>
<ul>
  <li>The sequence length L does <strong>not</strong> affect parameter count (for modern architectures)</li>
  <li>The number of attention heads H does <strong>not</strong> affect parameter count</li>
  <li>The d¬≤ term dominates for large models (quadratic scaling with model dimension)</li>
  <li>The Vd term can be significant for large vocabularies</li>
</ul>

<h5 id="memory-requirement">Memory Requirement:</h5>

<p>Since each parameter is stored as <strong>float32</strong> (4 bytes):</p>

<p><strong>Parameters memory = 4P bytes</strong></p>

<p><strong>Example (GPT-2 XL):</strong></p>
<ul>
  <li>N = 48, d = 1,600, V = 50,257</li>
  <li>P = 12(48)(1,600¬≤) + 1,600(2√ó48 + 50,257 + 1)</li>
  <li>P ‚âà <strong>1,555,126,400 parameters</strong> (~1.56B)</li>
  <li><strong>Memory = 4 √ó 1.56B ‚âà 6.2 GB</strong></li>
</ul>

<h4 id="2-gradients-memory">2. Gradients Memory</h4>

<p>During backpropagation, we compute gradients for all parameters. AdamW requires storing these gradients to perform parameter updates.</p>

<p><strong>Gradients have the same shape as parameters:</strong></p>
<ul>
  <li>One gradient value per parameter</li>
  <li>Stored as float32 (4 bytes each)</li>
</ul>

<p><strong>Gradients memory = 4P bytes</strong></p>

<h4 id="3-optimizer-state-memory">3. Optimizer State Memory</h4>

<p>AdamW is a <strong>stateful optimizer</strong> that maintains running statistics for each parameter:</p>

<p><strong>First moment (m):</strong> Exponential moving average of gradients (momentum)</p>
<ul>
  <li>Shape: same as parameters</li>
  <li>Storage: 4 bytes per parameter (float32)</li>
  <li><strong>Memory: 4P bytes</strong></li>
</ul>

<p><strong>Second moment (v):</strong> Exponential moving average of squared gradients (volatility)</p>
<ul>
  <li>Shape: same as parameters</li>
  <li>Storage: 4 bytes per parameter (float32)</li>
  <li><strong>Memory: 4P bytes</strong></li>
</ul>

<p><strong>Total optimizer state memory = 4P + 4P = 8P bytes</strong></p>

<p><strong>Note:</strong> Unlike parameters and gradients which all models need, this 8P overhead is specific to Adam-family optimizers. Simpler optimizers like SGD only need gradient storage (4P), while more complex optimizers may require even more state.</p>

<h4 id="4-activations-memory">4. Activations Memory</h4>

<p>Activations are intermediate values computed during the forward pass that must be stored for backpropagation. This is where batch size (B) and sequence length (L) have major impact.</p>

<p><strong>Key factors:</strong></p>
<ul>
  <li>Activations scale with <strong>B</strong> (batch size) and <strong>L</strong> (sequence length)</li>
  <li>We need to store activations at multiple points for gradient computation</li>
  <li>This is typically the <strong>memory bottleneck</strong> for training</li>
</ul>

<h5 id="per-transformer-layer-activations">Per Transformer Layer Activations:</h5>

<p><strong>A. RMSNorm activations:</strong></p>
<ul>
  <li>Input to pre-attention norm: BLd</li>
  <li>Output of pre-attention norm: BLd</li>
  <li>Input to pre-FFN norm: BLd</li>
  <li><strong>Subtotal: ~3BLd</strong></li>
</ul>

<p><strong>B. Attention intermediate values:</strong></p>
<ul>
  <li>Q, K, V projections: 3 √ó BLd = <strong>3BLd</strong></li>
  <li>Attention scores (before softmax): B √ó H √ó L √ó L = <strong>BHL¬≤</strong></li>
  <li>Attention weights (after softmax): B √ó H √ó L √ó L = <strong>BHL¬≤</strong> (needed for softmax backward)</li>
  <li>Attention output: BLd</li>
</ul>

<p><strong>Subtotal: ~4BLd + 2BHL¬≤</strong></p>

<p><strong>C. Feed-Forward Network activations:</strong></p>
<ul>
  <li>W‚ÇÅ output (before activation): B √ó L √ó 4d = <strong>4BLd</strong></li>
  <li>SiLU/GELU output: B √ó L √ó 4d = <strong>4BLd</strong> (needed for activation backward)</li>
  <li>W‚ÇÇ output: BLd</li>
</ul>

<p><strong>Subtotal: ~9BLd</strong></p>

<p><strong>Total per layer: 3BLd + 4BLd + 2BHL¬≤ + 9BLd ‚âà 16BLd + 2BHL¬≤</strong></p>

<p><em>Note: The original formula‚Äôs 16BLd is an approximation; exact value depends on implementation details like whether certain intermediate values are recomputed vs. stored.</em></p>

<h5 id="all-n-layers">All N Layers:</h5>

<p><strong>N √ó (16BLd + 2BHL¬≤) ‚âà 16NBLd + 2NBHL¬≤</strong></p>

<h5 id="additional-activations-outside-layers">Additional Activations Outside Layers:</h5>

<p><strong>Token embeddings:</strong></p>
<ul>
  <li>Embedding lookup output: <strong>BLd</strong></li>
</ul>

<p><strong>Final RMSNorm:</strong></p>
<ul>
  <li>Negligible (included in layer activations)</li>
</ul>

<p><strong>Output layer (logits):</strong></p>
<ul>
  <li>Softmax probabilities: B √ó L √ó V = <strong>BLV</strong> (needed for cross-entropy backward)</li>
</ul>

<p><strong>Total additional: BLd + BLV</strong></p>

<h5 id="total-activation-count">Total Activation Count:</h5>

<p><strong>A = 16NBLd + 2NBHL¬≤ + BLd + BLV</strong></p>

<p>Simplified:
<strong>A ‚âà NBLd(16 + 1/N) + 2NBHL¬≤ + BLV</strong></p>

<p>For large N, this is dominated by: <strong>A ‚âà 16NBLd + 2NBHL¬≤ + BLV</strong></p>

<p><strong>Activations memory = 4A bytes</strong> (float32)</p>

<p><strong>Key observations:</strong></p>
<ul>
  <li><strong>O(BL) scaling:</strong> Most activations scale linearly with batch size and sequence length</li>
  <li><strong>O(BL¬≤) scaling:</strong> Attention scores create quadratic memory growth with sequence length</li>
  <li><strong>Bottleneck:</strong> For long sequences, the 2NBHL¬≤ term (attention scores) dominates</li>
</ul>

<h4 id="total-peak-memory">Total Peak Memory</h4>

<table>
  <thead>
    <tr>
      <th>Component</th>
      <th>Memory (bytes)</th>
      <th>Notes</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Parameters</strong></td>
      <td>4P</td>
      <td>Model weights</td>
    </tr>
    <tr>
      <td><strong>Gradients</strong></td>
      <td>4P</td>
      <td>‚àÇL/‚àÇŒ∏ for all parameters</td>
    </tr>
    <tr>
      <td><strong>Optimizer State (m, v)</strong></td>
      <td>8P</td>
      <td>AdamW momentum and variance</td>
    </tr>
    <tr>
      <td><strong>Activations</strong></td>
      <td>4A</td>
      <td>Intermediate values for backprop</td>
    </tr>
    <tr>
      <td><strong>TOTAL</strong></td>
      <td><strong>16P + 4A</strong></td>
      <td>Peak during training</td>
    </tr>
  </tbody>
</table>

<p><strong>Breakdown:</strong></p>
<ul>
  <li><strong>Fixed cost (16P):</strong> Independent of batch size and sequence length</li>
  <li><strong>Variable cost (4A):</strong> Scales with B, L, and L¬≤</li>
</ul>

<h4 id="gpt-2-xl-example">GPT-2 XL Example</h4>

<p><strong>Model specifications:</strong></p>
<ul>
  <li>vocab_size (V) = 50,257</li>
  <li>context_length (L) = 1,024</li>
  <li>num_layers (N) = 48</li>
  <li>d_model (d) = 1,600</li>
  <li>num_heads (H) = 25</li>
  <li>d_ff = 4 √ó d = 6,400</li>
</ul>

<h5 id="step-1-calculate-parameters-p">Step 1: Calculate Parameters (P)</h5>

<p><strong>P = 12Nd¬≤ + 2Nd + Vd + d</strong></p>

<p>P = 12(48)(1,600¬≤) + 2(48)(1,600) + 50,257(1,600) + 1,600</p>

<p><strong>P ‚âà 1,555,126,400 ‚âà 1.56 √ó 10‚Åπ parameters</strong></p>

<h5 id="step-2-calculate-fixed-memory-16p">Step 2: Calculate Fixed Memory (16P)</h5>

<p>Fixed memory = 16 √ó 1,555,126,400 bytes</p>

<p><strong>Fixed memory ‚âà 24.88 GB</strong></p>

<p>This includes parameters (4P), gradients (4P), and optimizer state (8P).</p>

<h5 id="step-3-calculate-activation-memory-per-batch-4a">Step 3: Calculate Activation Memory per Batch (4A)</h5>

<p><strong>A = 16NBLd + 2NBHL¬≤ + BLd + BLV</strong></p>

<p>For batch_size = B:</p>

<p>A = 16(48)(B)(1,024)(1,600) + 2(48)(B)(25)(1,024¬≤) + B(1,024)(1,600) + B(1,024)(50,257)</p>

<p><strong>Activation memory = 4A ‚âà 14.3 √ó B GB</strong></p>

<h5 id="step-4-total-memory-formula">Step 4: Total Memory Formula</h5>

<p><strong>Total Memory = 24.88 + 14.3 √ó B GB</strong></p>

<h5 id="step-5-maximum-batch-size-for-80gb-gpu">Step 5: Maximum Batch Size for 80GB GPU</h5>

<p>Solving: 24.88 + 14.3B ‚â§ 80</p>

<p><strong>B ‚â§ 3.85</strong></p>

<p><strong>Maximum batch_size = 3</strong> (must be integer)</p>

<p><strong>Key insight:</strong> So on a single A100 80 GB, GPT-2 XL in pure FP32 training fits a batch size of 3 without further memory optimization. This demonstrates why:</p>
<ol>
  <li>Large-scale training requires massive GPU clusters</li>
  <li>Techniques like gradient accumulation, mixed precision (float16/bfloat16), and activation checkpointing are essential</li>
  <li>Memory, not computation, is often the bottleneck</li>
</ol>

<p>The estimate above assumes na√Øve attention that stores full B√óH√óL√óL score and probability tensors, and a non-fused cross-entropy head. Modern implementations cut this drastically:</p>

<table>
  <thead>
    <tr>
      <th>Technique</th>
      <th>What it Removes</th>
      <th>Result (GPT-2 XL, FP32)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>FlashAttention</strong></td>
      <td>avoids L¬≤ attention matrices</td>
      <td>‚âà 5 GB per batch</td>
    </tr>
    <tr>
      <td><strong>Fused CE</strong></td>
      <td>streams logits ‚Üí softmax</td>
      <td>reduce 0.5‚Äì1 GB per batch</td>
    </tr>
    <tr>
      <td><strong>Activation checkpointing</strong></td>
      <td>recomputes during backward</td>
      <td>‚âà √ó 4‚Äì6 less</td>
    </tr>
    <tr>
      <td><strong>BF16 / FP16</strong></td>
      <td>halves memory per value</td>
      <td>‚âà √ó 2 less</td>
    </tr>
  </tbody>
</table>

<p>With FlashAttention and combined with other memory optimization techniques, batch_size = 12-16 is achievable for GPT-2 XL on an 80GB GPU with FP32, and this can be scaled to 32-40 with BF16 or 60-70 with activation checkpointing.</p>

<h3 id="computational-cost">Computational Cost: How Long Will This Take?</h3>

<h4 id="the-standard-formula">The Standard Formula</h4>

<p>For Transformer models, there‚Äôs a widely-used approximation:</p>

<p><strong>Training FLOPs per token ‚âà 6 √ó number of parameters</strong></p>

<p>This breaks down as:</p>
<ul>
  <li><strong>Forward pass:</strong> 2P FLOPs per token</li>
  <li><strong>Backward pass:</strong> 4P FLOPs per token (approximately 2√ó forward)</li>
  <li><strong>Total:</strong> 6P FLOPs per token</li>
</ul>

<p><strong>Why this approximation works:</strong></p>
<ul>
  <li>Dominated by matrix multiplications in attention and FFN layers</li>
  <li>For a matrix multiply of (m √ó k) @ (k √ó n), we perform 2mkn FLOPs</li>
  <li>The ‚Äú2‚Äù accounts for <em>multiply</em> and <em>add</em> operations</li>
  <li>Backward pass requires computing gradients for all weight matrices (roughly 2√ó forward)</li>
</ul>

<p><strong>What‚Äôs excluded:</strong></p>
<ul>
  <li>Optimizer computations (~11 FLOPs per parameter, negligible compared to 6P per token)</li>
  <li>Element-wise operations (LayerNorm, activations)</li>
  <li>Attention softmax</li>
</ul>

<p>These omissions are small compared to the matrix multiplications, making ‚Äú6P per token‚Äù a robust rule of thumb.</p>

<h4 id="gpt-2-xl-training-example">GPT-2 XL Training Example</h4>

<p><strong>Given:</strong></p>
<ul>
  <li>Parameters: P ‚âà 1.56 √ó 10‚Åπ</li>
  <li>Training steps: 400,000</li>
  <li><strong>Batch size: 1,024 tokens per step</strong> (total tokens processed)</li>
  <li>Hardware: Single NVIDIA A100 GPU (40GB or 80GB)</li>
  <li>Theoretical peak: 19.5 teraFLOP/s (FP32)</li>
  <li>MFU (Model FLOPs Utilization): 50%</li>
</ul>

<p><strong>Note on batch size:</strong> ‚Äú1,024 tokens‚Äù typically means the <strong>total number of tokens</strong> processed in one training step.</p>

<p><strong>Calculation:</strong></p>

<p><strong>Step 1: FLOPs per token</strong></p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>6 √ó 1.56 √ó 10‚Åπ = 9.36 √ó 10‚Åπ FLOPs per token
</code></pre></div></div>

<p><strong>Step 2: FLOPs per training step</strong></p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>9.36 √ó 10‚Åπ √ó 1,024 tokens = 9.585 √ó 10¬π¬≤ FLOPs per step
</code></pre></div></div>

<p><strong>Step 3: Total FLOPs for 400K steps</strong></p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>9.585 √ó 10¬π¬≤ √ó 400,000 = 3.834 √ó 10¬π‚Å∏ FLOPs
</code></pre></div></div>

<p><strong>Step 4: Effective throughput at 50% MFU</strong></p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Theoretical: 19.5 √ó 10¬π¬≤ FLOP/s
Effective: 19.5 √ó 10¬π¬≤ √ó 0.5 = 9.75 √ó 10¬π¬≤ FLOP/s
</code></pre></div></div>

<p><strong>Step 5: Training time</strong></p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Time = (3.834 √ó 10¬π‚Å∏) / (9.75 √ó 10¬π¬≤) = 393,231 seconds
       ‚âà 109.2 hours
       ‚âà 4.55 days
</code></pre></div></div>
<h4 id="key-insights">Key Insights</h4>

<p><strong>Why this matters:</strong></p>

<ol>
  <li><strong>Single GPU training is impractical for large models</strong>
    <ul>
      <li>Even ‚Äúmedium-sized‚Äù GPT-2 XL takes <strong>109 hours (~4.5 days)</strong> on a top-tier A100</li>
      <li>Larger models (GPT-3: 175B parameters) would take <strong>months</strong> on a single GPU</li>
      <li>GPT-3 would require: (175B/1.56B) √ó 109 hours ‚âà 12,200 hours ‚âà <strong>1.4 years</strong> on one A100!</li>
    </ul>
  </li>
  <li><strong>Parallelism is essential</strong>
    <ul>
      <li><strong>With 100 A100s:</strong> 109.2 / 100 ‚âà <strong>1.1 hours</strong> (assuming perfect scaling)</li>
      <li><strong>With 1,000 A100s:</strong> 109.2 / 1,000 ‚âà <strong>6.6 minutes</strong> (assuming perfect scaling)</li>
      <li><strong>Real-world scaling efficiency</strong> is typically 60-90% due to:
        <ul>
          <li>Communication overhead (gradient synchronization)</li>
          <li>Load imbalancing</li>
          <li>Pipeline bubbles</li>
          <li>Network bandwidth limitations</li>
        </ul>
      </li>
      <li><strong>Realistic with 100 A100s:</strong> 109.2 / (100 √ó 0.7) ‚âà <strong>1.6 hours</strong> (at 70% efficiency)</li>
    </ul>
  </li>
  <li><strong>Cost considerations</strong>
    <ul>
      <li><strong>A100 cloud cost:</strong> ~$2-4/hour (varies by provider: AWS, GCP, Azure)</li>
      <li><strong>Single A100 training:</strong> 109.2 hours √ó $2-4 = <strong>$218-437</strong></li>
      <li><strong>100 A100s (70% efficiency):</strong>
        <ul>
          <li>Time: ~1.6 hours</li>
          <li>Cost: 100 GPUs √ó 1.6 hours √ó $2-4 = <strong>$320-640</strong></li>
          <li><strong>Trade-off:</strong> Slightly higher cost, but <strong>68√ó faster!</strong></li>
        </ul>
      </li>
      <li><strong>Cost scales linearly with GPU count, but time scales sub-linearly</strong> (due to overhead)</li>
    </ul>
  </li>
  <li><strong>Memory vs. compute trade-off</strong>
    <ul>
      <li>We calculated <strong>batch_size = 3</strong> fits in 80GB memory (using FP32)</li>
      <li>Larger batches could improve training efficiency (better GPU utilization, more stable gradients)</li>
      <li><strong>Solutions to increase effective batch size:</strong>
        <ul>
          <li><strong>Gradient accumulation:</strong> Simulate larger batches by accumulating gradients over multiple forward/backward passes before updating
            <ul>
              <li>Example: Accumulate 32 micro-batches of size 3 ‚Üí effective batch size of 96</li>
            </ul>
          </li>
          <li><strong>Mixed precision (FP16/BF16):</strong> Reduce memory by 2√ó, allowing batch_size ‚âà 6-8</li>
          <li><strong>Gradient checkpointing:</strong> Trade compute for memory (recompute activations during backward pass)</li>
          <li><strong>Multi-GPU training:</strong> Distribute batch across GPUs (data parallelism)</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>Why GPT-3 scale requires massive clusters</strong>
    <ul>
      <li>GPT-3 (175B parameters): <strong>~112√ó larger</strong> than GPT-2 XL</li>
      <li>Single A100 would take: ~1.4 years</li>
      <li>With <strong>10,000 A100s</strong> (at 60% efficiency): ~12,200 / (10,000 √ó 0.6) ‚âà <strong>2 hours</strong></li>
      <li>This explains why frontier models require:
        <ul>
          <li>Tens of thousands of GPUs</li>
          <li>Custom datacenters</li>
          <li>Months of calendar time (even with massive parallelism)</li>
          <li>Millions of dollars in compute costs</li>
        </ul>
      </li>
    </ul>
  </li>
</ol>

<p><strong>Summary Table for training GPT-2 XL</strong></p>

<table>
  <thead>
    <tr>
      <th>Configuration</th>
      <th>Time</th>
      <th>Cost</th>
      <th>Notes</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>1 A100 (FP32)</strong></td>
      <td>109 hours</td>
      <td>$218-437</td>
      <td>Baseline</td>
    </tr>
    <tr>
      <td><strong>1 A100 (FP16)</strong></td>
      <td>~55 hours</td>
      <td>$110-220</td>
      <td>2√ó faster with mixed precision</td>
    </tr>
    <tr>
      <td><strong>100 A100s (perfect)</strong></td>
      <td>1.1 hours</td>
      <td>$220-440</td>
      <td>Theoretical best case</td>
    </tr>
    <tr>
      <td><strong>100 A100s (70% eff.)</strong></td>
      <td>1.6 hours</td>
      <td>$320-640</td>
      <td>Realistic with overhead</td>
    </tr>
    <tr>
      <td><strong>1,000 A100s (perfect)</strong></td>
      <td>6.6 minutes</td>
      <td>$220-440</td>
      <td>Theoretical best case</td>
    </tr>
    <tr>
      <td><strong>1,000 A100s (60% eff.)</strong></td>
      <td>11 minutes</td>
      <td>$367-733</td>
      <td>Realistic at scale</td>
    </tr>
  </tbody>
</table>

<p><strong>Key takeaway:</strong> Parallelism gives you speed, not cost savings. Using 100 GPUs costs about the same (or slightly more) but finishes <strong>68√ó faster</strong>, which matters for iteration speed and time-to-market!</p>

<hr />

<h3 id="learning-rate-schedules">Learning Rate Schedules: Starting Fast, Ending Slow</h3>

<h4 id="why-do-we-need-a-schedule">Why Do We Need a Schedule?</h4>

<p>Imagine you‚Äôre trying to find the lowest point in a valley while blindfolded:</p>

<ul>
  <li><strong>Beginning</strong>: You‚Äôre far from the goal ‚Üí take <strong>big steps</strong> to get there quickly</li>
  <li><strong>Middle</strong>: You‚Äôre getting close ‚Üí take <strong>medium steps</strong> to avoid overshooting</li>
  <li><strong>End</strong>: You‚Äôre very close ‚Üí take <strong>tiny steps</strong> to settle into the exact lowest point</li>
</ul>

<p>The learning rate schedule does exactly this for training!</p>

<h4 id="the-problem-with-fixed-learning-rate">The Problem with Fixed Learning Rate</h4>

<p><strong>Too high throughout training:</strong></p>
<ul>
  <li>Fast at first, but bounces around the minimum at the end</li>
  <li>Never settles into the best solution</li>
</ul>

<p><strong>Too low throughout training:</strong></p>
<ul>
  <li>Slow progress, takes forever to train</li>
  <li>Might get stuck in bad spots</li>
</ul>

<p><strong>Solution: Start high, gradually decrease!</strong></p>

<h4 id="cosine-annealing-schedule">Cosine Annealing Schedule</h4>

<p>The schedule has <strong>3 phases</strong>:</p>

<h5 id="phase-1-warm-up-t--t_w">Phase 1: Warm-up (t &lt; T_w)</h5>
<p><strong>‚ÄúEase into it‚Äù</strong></p>

\[\alpha_t = \frac{t}{T_w} \times \alpha_{max}\]

<ul>
  <li>Start from <strong>0</strong> and <strong>linearly increase</strong> to Œ±_max</li>
  <li>Example: If T_w = 1,000 and Œ±_max = 0.001:
    <ul>
      <li>Step 0: Œ± = 0</li>
      <li>Step 500: Œ± = 0.0005 (halfway)</li>
      <li>Step 1,000: Œ± = 0.001 (full speed!)</li>
    </ul>
  </li>
</ul>

<p><strong>Why warm-up?</strong></p>
<ul>
  <li>Prevents unstable updates at the very beginning</li>
  <li>Gives the model time to ‚Äúorient itself‚Äù</li>
  <li>Like warming up before exercise!</li>
</ul>

<h5 id="phase-2-cosine-annealing-t_w--t--t_c">Phase 2: Cosine Annealing (T_w ‚â§ t ‚â§ T_c)</h5>
<p><strong>‚ÄúSmooth slowdown‚Äù</strong></p>

\[\alpha_t = \alpha_{min} + \frac{1}{2}\left(1 + \cos\left(\frac{t - T_w}{T_c - T_w} \pi\right)\right)(\alpha_{max} - \alpha_{min})\]

<p>This creates a <strong>smooth curve</strong> from Œ±_max down to Œ±_min!</p>

<p><strong>Breaking it down:</strong></p>

<ol>
  <li><strong>Progress ratio</strong>: How far through annealing are we? (0 to 1)</li>
  <li><strong>Cosine curve</strong>: cos goes from 1 ‚Üí -1 as we progress</li>
  <li><strong>Scale to [0, 1]</strong>: Transform to go from 1 ‚Üí 0</li>
  <li><strong>Final value</strong>: Interpolate between Œ±_max and Œ±_min</li>
</ol>

<p><strong>The result: A smooth decrease from Œ±_max to Œ±_min</strong></p>

<h5 id="phase-3-post-annealing-t--t_c">Phase 3: Post-Annealing (t &gt; T_c)</h5>
<p><strong>‚ÄúMaintain minimum‚Äù</strong></p>

\[\alpha_t = \alpha_{min}\]

<ul>
  <li>Keep the learning rate at the minimum value</li>
  <li>Fine-tuning with tiny steps</li>
</ul>

<h4 id="visual-example">Visual Example</h4>

<p>Let‚Äôs say:</p>
<ul>
  <li>Œ±_max = 0.001</li>
  <li>Œ±_min = 0.0001</li>
  <li>T_w = 1,000 (warm-up ends)</li>
  <li>T_c = 10,000 (annealing ends)</li>
</ul>

<p><strong>Learning rate over time:</strong></p>

<table>
  <thead>
    <tr>
      <th>Step</th>
      <th>Phase</th>
      <th>Learning Rate</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>Warm-up</td>
      <td>0</td>
    </tr>
    <tr>
      <td>500</td>
      <td>Warm-up</td>
      <td>0.0005</td>
    </tr>
    <tr>
      <td>1,000</td>
      <td>Warm-up ‚Üí Annealing</td>
      <td>0.001</td>
    </tr>
    <tr>
      <td>3,000</td>
      <td>Annealing</td>
      <td>~0.00085</td>
    </tr>
    <tr>
      <td>5,500</td>
      <td>Annealing</td>
      <td>~0.00055</td>
    </tr>
    <tr>
      <td>8,000</td>
      <td>Annealing</td>
      <td>~0.00025</td>
    </tr>
    <tr>
      <td>10,000</td>
      <td>Annealing ‚Üí Post</td>
      <td>0.0001</td>
    </tr>
    <tr>
      <td>15,000</td>
      <td>Post-annealing</td>
      <td>0.0001</td>
    </tr>
  </tbody>
</table>

<h4 id="simple-math-example">Simple Math Example</h4>

<p>Let‚Äôs calculate learning rate at t = 5,500 (middle of annealing):</p>

<p><strong>Given:</strong></p>
<ul>
  <li>T_w = 1,000, T_c = 10,000</li>
  <li>Œ±_max = 0.001, Œ±_min = 0.0001</li>
</ul>

<p><strong>Step 1:</strong> Progress = (5,500 - 1,000) / (10,000 - 1,000) = 0.5</p>

<p><strong>Step 2:</strong> cos(0.5 √ó œÄ) = 0</p>

<p><strong>Step 3:</strong> ¬Ω(1 + 0) = 0.5</p>

<p><strong>Step 4:</strong> Œ± = 0.0001 + 0.5 √ó (0.001 - 0.0001) = <strong>0.00055</strong></p>

<p>Exactly halfway between min and max!</p>

<h4 id="key-takeaway-3">Key Takeaway</h4>

<p><strong>Learning rate schedule = adaptive step size:</strong></p>
<ol>
  <li><strong>Warm-up</strong>: Gradually increase from 0 ‚Üí big (safety at start)</li>
  <li><strong>Cosine annealing</strong>: Smoothly decrease from big ‚Üí small (careful landing)</li>
  <li><strong>Post-annealing</strong>: Stay small (fine-tuning)</li>
</ol>

<p>It‚Äôs like driving: accelerate leaving the driveway, cruise on the highway, then slow down smoothly as you approach your destination!</p>

<hr />

<h3 id="gradient-clipping">Gradient Clipping: The Safety Mechanism</h3>

<h4 id="the-problem-exploding-gradients">The Problem: Exploding Gradients</h4>

<p>Imagine you‚Äôre walking downhill with a GPS that tells you how steep the slope is:</p>

<ul>
  <li><strong>Normal case</strong>: ‚ÄúSlope is 5 degrees‚Äù ‚Üí take a reasonable step</li>
  <li><strong>Bad case</strong>: ‚ÄúSlope is 5,000 degrees!!!‚Äù ‚Üí you‚Äôd jump off a cliff!</li>
</ul>

<p>Sometimes during training, the model encounters weird examples that produce <strong>huge gradients</strong>. If you take a step proportional to these giant gradients, your model parameters can explode and training crashes!</p>

<h4 id="what-is-gradient-clipping">What is Gradient Clipping?</h4>

<p>Gradient clipping is like having a <strong>speed limiter</strong> on your updates:</p>

<p><strong>‚ÄúNo matter how steep the slope, I won‚Äôt step faster than X units‚Äù</strong></p>

<h4 id="how-it-works-step-by-step">How It Works (Step by Step)</h4>

<h5 id="step-1-calculate-the-gradient-norm">Step 1: Calculate the Gradient Norm</h5>

<p>After the backward pass, measure how ‚Äúbig‚Äù the gradients are overall:</p>

\[\|g\|_2 = \sqrt{g_1^2 + g_2^2 + g_3^2 + ... + g_n^2}\]

<p>This is the <strong>L2 norm</strong> (Euclidean distance) - just the length of the gradient vector.</p>

<p><strong>Example:</strong></p>
<ul>
  <li>If gradients are [3, 4]: norm = ‚àö(9 + 16) = <strong>5</strong></li>
  <li>If gradients are [30, 40]: norm = ‚àö(900 + 1,600) = <strong>50</strong></li>
</ul>

<h5 id="step-2-check-against-maximum">Step 2: Check Against Maximum</h5>

<p>Set a threshold <strong>M</strong> (e.g., M = 1.0):</p>

<p><strong>Is ‚à•g‚à•‚ÇÇ ‚â§ M?</strong></p>

<ul>
  <li><strong>YES</strong> ‚Üí Gradients are reasonable, use them as-is ‚úì</li>
  <li><strong>NO</strong> ‚Üí Gradients are too big, need to clip! ‚úÇÔ∏è</li>
</ul>

<h5 id="step-3-scale-down-if-needed">Step 3: Scale Down If Needed</h5>

<p>If the norm exceeds M, <strong>rescale</strong> the entire gradient vector:</p>

\[g_{\text{clipped}} = g \times \frac{M}{\|g\|_2 + \epsilon}\]

<p>Where Œµ ‚âà 10‚Åª‚Å∂ is for numerical stability.</p>

<p><strong>What this does:</strong></p>
<ul>
  <li>Keeps the <strong>direction</strong> the same</li>
  <li>Reduces the <strong>magnitude</strong> to exactly M</li>
</ul>

<h4 id="simple-example-3">Simple Example</h4>

<p><strong>Given:</strong></p>
<ul>
  <li>Gradient vector: g = [30, 40]</li>
  <li>Maximum norm: M = 1.0</li>
  <li>Œµ = 10‚Åª‚Å∂ (negligible)</li>
</ul>

<p><strong>Step 1: Calculate norm</strong></p>
<ul>
  <li>‚à•g‚à•‚ÇÇ = ‚àö(900 + 1,600) = <strong>50</strong></li>
</ul>

<p><strong>Step 2: Check threshold</strong></p>
<ul>
  <li>50 &gt; 1.0 ‚Üí <strong>Need to clip!</strong></li>
</ul>

<p><strong>Step 3: Scale down</strong></p>
<ul>
  <li>Scaling factor = 1.0 / 50 = <strong>0.02</strong></li>
  <li>g_clipped = [30, 40] √ó 0.02 = <strong>[0.6, 0.8]</strong></li>
</ul>

<p><strong>Verify new norm:</strong></p>
<ul>
  <li>‚à•g_clipped‚à•‚ÇÇ = ‚àö(0.36 + 0.64) = <strong>1.0</strong> ‚úì</li>
</ul>

<p><strong>Result:</strong> We‚Äôve scaled from norm 50 to norm 1.0, keeping the same direction!</p>

<h4 id="another-example-no-clipping">Another Example (No Clipping)</h4>

<p><strong>Given:</strong></p>
<ul>
  <li>Gradient: g = [0.3, 0.4]</li>
  <li>Maximum: M = 1.0</li>
</ul>

<p><strong>Norm:</strong> ‚àö(0.09 + 0.16) = <strong>0.5</strong></p>

<p><strong>Check:</strong> 0.5 ‚â§ 1.0 ‚Üí <strong>No clipping needed!</strong></p>

<p>Use gradient as-is: [0.3, 0.4]</p>

<h4 id="why-does-this-work">Why Does This Work?</h4>

<p><strong>Preserves Direction:</strong></p>
<ul>
  <li>We still move in the right direction (downhill)</li>
  <li>Just limit how far we jump</li>
</ul>

<p><strong>Prevents Instability:</strong></p>
<ul>
  <li>Giant gradients can make parameters explode</li>
  <li>Clipping ensures updates stay reasonable</li>
</ul>

<p><strong>Training Stability:</strong></p>
<ul>
  <li>Without clipping: loss might spike or become NaN</li>
  <li>With clipping: training stays smooth</li>
</ul>

<h4 id="pseudocode">Pseudocode</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># After backward pass
</span><span class="n">gradients</span> <span class="o">=</span> <span class="n">compute_gradients</span><span class="p">()</span>

<span class="c1"># Calculate L2 norm of all gradients
</span><span class="n">grad_norm</span> <span class="o">=</span> <span class="n">sqrt</span><span class="p">(</span><span class="nb">sum</span><span class="p">(</span><span class="n">g</span><span class="err">¬≤</span> <span class="k">for</span> <span class="nb">all</span> <span class="n">g</span> <span class="ow">in</span> <span class="n">gradients</span><span class="p">))</span>

<span class="c1"># Clip if needed
</span><span class="n">max_norm</span> <span class="o">=</span> <span class="mf">1.0</span>
<span class="k">if</span> <span class="n">grad_norm</span> <span class="o">&gt;</span> <span class="n">max_norm</span><span class="p">:</span>
    <span class="n">scaling_factor</span> <span class="o">=</span> <span class="n">max_norm</span> <span class="o">/</span> <span class="p">(</span><span class="n">grad_norm</span> <span class="o">+</span> <span class="mf">1e-6</span><span class="p">)</span>
    <span class="n">gradients</span> <span class="o">=</span> <span class="n">gradients</span> <span class="o">*</span> <span class="n">scaling_factor</span>

<span class="c1"># Use clipped gradients in optimizer
</span><span class="n">optimizer</span><span class="p">.</span><span class="n">step</span><span class="p">(</span><span class="n">gradients</span><span class="p">)</span>
</code></pre></div></div>

<h4 id="key-takeaway-4">Key Takeaway</h4>

<p><strong>Gradient clipping = speed limiter for training:</strong></p>

<ol>
  <li><strong>Measure</strong> how big the gradients are (L2 norm)</li>
  <li><strong>Check</strong> if they exceed the maximum allowed</li>
  <li><strong>Scale down</strong> if needed (keep direction, reduce magnitude)</li>
</ol>

<p><strong>Result:</strong> Training stays stable even when occasional batches produce huge gradients. It‚Äôs like having a safety governor on a car engine - you can still accelerate and steer normally, but it prevents dangerous speeds that could cause a crash.</p>

<hr />

<h3 id="conclusion">Conclusion</h3>

<p>Training large language models involves carefully balancing multiple components:</p>

<ol>
  <li><strong>Loss functions</strong> (cross-entropy) measure how bad the model did not predict the correct word</li>
  <li><strong>Metrics</strong> (perplexity) make model prediction results more interpretable</li>
  <li><strong>Optimizers</strong> (SGD ‚Üí AdamW) determine how we update model parameters</li>
  <li><strong>Memory management</strong> dictates what type of hardware we need</li>
  <li><strong>Computational budgets</strong> determine training time, make trade-off between cost and speed</li>
  <li><strong>Learning rate schedules</strong> help us converge smoothly</li>
  <li><strong>Safety mechanisms</strong> (gradient clipping) prevent training instability</li>
</ol>

<p>While the math can seem complex at first, the underlying intuitions are straightforward: we‚Äôre teaching a model to predict text by repeatedly showing it examples, measuring its mistakes, and adjusting its parameters to do better next time. The real challenge isn‚Äôt understanding any single component - it‚Äôs orchestrating all of them together efficiently at massive scale. That‚Äôs what makes training models like GPT-3 and GPT-4 such remarkable engineering achievements.</p>]]></content><author><name>[&quot;Han Yu&quot;]</name></author><category term="cs336" /><summary type="html"><![CDATA[Planning LLM Training: Cross-Entropy Loss, Optimizers, Memory and Computational Cost, and other practical levers]]></summary></entry><entry><title type="html">Study Notes: Stanford CS336 Language Modeling from Scratch [7]</title><link href="http://localhost:4000/cs336/2025/09/28/cs336-understand-computation-cost-of-transformer-model.html" rel="alternate" type="text/html" title="Study Notes: Stanford CS336 Language Modeling from Scratch [7]" /><published>2025-09-28T00:00:00-07:00</published><updated>2025-09-28T00:00:00-07:00</updated><id>http://localhost:4000/cs336/2025/09/28/cs336-understand-computation-cost-of-transformer-model</id><content type="html" xml:base="http://localhost:4000/cs336/2025/09/28/cs336-understand-computation-cost-of-transformer-model.html"><![CDATA[<h2 id="understanding-where-the-computation-really-goes-in-transformer-language-models">Understanding where the computation really goes in transformer language models*</h2>

<p>When we talk about large language models like GPT-4 or Claude, we often hear impressive numbers: ‚Äú175 billion parameters,‚Äù ‚Äú3.5 trillion FLOPs,‚Äù ‚Äútrained on thousands of GPUs.‚Äù But what do these numbers actually mean? Where does all that computation go during inference? And how do these patterns change as models scale up?</p>

<p>In this notes, we‚Äôll dissect the computational anatomy of GPT-2 models, from the smallest to the largest variants, to understand exactly where the mathematical heavy lifting happens. By the end, hope you‚Äôll have an intuitive understanding of why certain optimization techniques matter more than others, and how the computational landscape shifts dramatically with model size and context length.</p>

<p><em>This analysis was based on GPT-2 architectures, but the principles apply broadly to transformer-based language models. The specific percentages may vary, but the scaling laws and optimization insights remain relevant for understanding modern LLMs.</em></p>

<h3 id="table-of-contents">Table of Contents</h3>

<ol>
  <li><a href="#setting-the-stage-gpt-2-xl-under-the-microscope">Setting the Stage: GPT-2 XL Under the Microscope</a></li>
  <li><a href="#part-1-counting-every-parameter">Part 1: Counting Every Parameter</a>
    <ul>
      <li><a href="#the-156-billion-parameter-breakdown">The 1.56 Billion Parameter Breakdown</a></li>
      <li><a href="#memory-requirements">Memory Requirements</a></li>
    </ul>
  </li>
  <li><a href="#part-2-following-the-flops-floating-point-operations-per-second">Part 2: Following the FLOPS (Floating Point Operations per Second)</a>
    <ul>
      <li><a href="#the-35-trillion-flops-journey">The 3.5 Trillion FLOPs Journey</a></li>
      <li><a href="#the-computational-cost-hierarchy">The Computational Cost Hierarchy</a></li>
    </ul>
  </li>
  <li><a href="#part-3-how-computational-patterns-change-with-model-scale">Part 3: How Computational Patterns Change with Model Scale</a>
    <ul>
      <li><a href="#scaling-trends">Scaling Trends</a></li>
      <li><a href="#optimization-strategy-by-model-size">Optimization Strategy by Model Size</a></li>
    </ul>
  </li>
  <li><a href="#part-4-the-long-context-revolution">Part 4: The Long Context Revolution</a>
    <ul>
      <li><a href="#the-38-computational-explosion">The 38√ó Computational Explosion</a></li>
      <li><a href="#the-quadratic-takeover">The Quadratic Takeover</a></li>
      <li><a href="#memory-implications">Memory Implications</a></li>
    </ul>
  </li>
  <li><a href="#part-5-understanding-mixture-of-experts-moe">Part 5: Understanding Mixture of Experts (MoE)</a>
    <ul>
      <li><a href="#the-restaurant-kitchen-analogy">The Restaurant Kitchen Analogy</a></li>
      <li><a href="#how-moe-works-in-practice">How MoE Works in Practice</a></li>
      <li><a href="#why-moe-provides-massive-savings">Why MoE Provides Massive Savings</a></li>
    </ul>
  </li>
  <li><a href="#part-6-key-insights-and-practical-implications">Part 6: Key Insights and Practical Implications</a>
    <ul>
      <li><a href="#for-model-developers">For Model Developers</a></li>
      <li><a href="#for-infrastructure-teams">For Infrastructure Teams</a></li>
    </ul>
  </li>
  <li><a href="#conclusion">Conclusion</a></li>
</ol>

<h3 id="setting-the-stage-gpt-2-xl-under-the-microscope"><strong>Setting the Stage: GPT-2 XL Under the Microscope</strong></h3>

<p>Let‚Äôs start by examining GPT-2 XL, one of the largest publicly available GPT-2 models, with these specifications:</p>

<ul>
  <li><strong>Vocabulary size</strong>: 50,257 tokens</li>
  <li><strong>Context/sequence length</strong>: 1,024 tokens</li>
  <li><strong>Layers</strong>: 48 transformer blocks</li>
  <li><strong>Embedding Model dimension (d_model)</strong>: 1,600</li>
  <li><strong>Attention heads</strong>: 25</li>
  <li><strong>Feed forward dimension (d_ff)</strong>: 6,400</li>
</ul>

<h3 id="part-1-counting-every-parameter"><strong>Part 1: Counting Every Parameter</strong></h3>

<h4 id="the-156-billion-parameter-breakdown"><strong>The 1.56 Billion Parameter Breakdown</strong></h4>

<p>When we say GPT-2 XL has <code class="language-plaintext highlighter-rouge">1.56 billion</code> parameters, where exactly do they all go?</p>

<p><strong>Token &amp; Position Embeddings: 82M parameters (5.3%)</strong></p>
<ul>
  <li><strong>Token embeddings</strong>: 50,257 tokens √ó 1,600 dimensions -&gt; 80.4M parameters</li>
  <li><strong>Position embeddings</strong>: 1,024 positions √ó 1,600 dimensions -&gt; 1.6M parameters</li>
</ul>

<p><em>Think of these as lookup tables: each token gets its own 1,600-dimensional vector, and each position (1st word, 2nd word, etc.) gets its own vector too.</em></p>

<p><strong>Transformer Layers: 1.47B parameters (94.7%)</strong></p>

<p>Each of the 48 layers contains:</p>

<ul>
  <li><em>Multi-Head Attention (10.2M parameters per layer):</em>
    <ul>
      <li><strong>Q, K, V projections</strong>: 3 √ó (1,600 √ó 1,600) = 7.68M parameters</li>
      <li><strong>Output projection</strong>: 1,600 √ó 1,600 = 2.56M parameters</li>
    </ul>
  </li>
  <li><em>Feed Forward Network (20.5M parameters per layer):</em>
    <ul>
      <li><strong>Expansion layer (linear projection up)</strong>: 1,600 √ó 6,400 + 6,400 bias = 10.24M parameters</li>
      <li><strong>Contraction layer (linear project down)</strong>: 6,400 √ó 1,600 + 1,600 bias = 10.24M parameters</li>
    </ul>
  </li>
  <li><em>Layer Normalization (6,400 parameters per layer):</em>
    <ul>
      <li><strong>Two layer norms (e.g.,RMSNorm)</strong> √ó 2 parameters each (e.g., two learnable parameters used in RMSNorm) √ó 1,600 dimensions = 6,400 parameters</li>
    </ul>
  </li>
</ul>

<p><strong>Final Layer Norm (e.g., 1 RMSNorm layer with 2 parameters, 1,600 dimensions): 3,200 parameters</strong></p>

<p><strong>Key Insight</strong>: The feed forward networks contain <code class="language-plaintext highlighter-rouge">67%</code> of parameters per layer, while attention uses <code class="language-plaintext highlighter-rouge">33%</code>. This <code class="language-plaintext highlighter-rouge">2:1</code> ratio will become important when we analyze computation.</p>

<h4 id="memory-requirements"><strong>Memory Requirements</strong></h4>

<p>With single-precision floating point (each parameter is represented by 32 bits which is 4 bytes per parameter):</p>
<ul>
  <li><strong>Total memory</strong>: <code class="language-plaintext highlighter-rouge">1.56B √ó 4 bytes</code> -&gt; <strong><code class="language-plaintext highlighter-rouge">6.2 GB</code></strong></li>
</ul>

<p>This is just for storing the model weights‚Äîactual inference requires additional memory for activations, gradients (if training), intermediate computations, and other overhead for using frameworks.</p>

<h3 id="part-2-following-the-flops-floating-point-operations-per-second"><strong>Part 2: Following the FLOPS (Floating Point Operations per Second)</strong></h3>

<p>Now comes the crucial question: during a forward pass with 1,024 input tokens, where does the computational work actually happen?</p>

<h4 id="the-35-trillion-flops-journey"><strong>The 3.5 Trillion FLOPs Journey</strong></h4>
<p>A FLOP stands for Floating Point Operation. It means one basic arithmetic operation (addition, subtraction, multiplication, division, etc.) performed on floating-point numbers (like 32-bit floats in neural nets).</p>

<p>Example:</p>
<blockquote>
  <p>3.14√ó2.71 ‚Üí 1 FLOP</p>

  <p>(3.14 √ó 2.71) + 1.23 ‚Üí 2 FLOPs</p>
</blockquote>

<p>FLOPS vs FLOP:</p>

<blockquote>
  <p>FLOP = one operation.</p>

  <p>FLOPS = FLOPs per second ‚Üí a measure of compute speed.</p>
</blockquote>

<p>Example:</p>
<blockquote>
  <p>A GPU that can perform 1√ó10^12 FLOPs per second = 1 TFLOPS.</p>
</blockquote>

<p>When we talk about FLOPs of a model: it means the total number of floating-point operations required for one forward pass (sometimes forward + backward during training).</p>

<p>Example:</p>
<blockquote>
  <p>A Transformer block does a lot of matrix multiplications (like attention and feedforward layers). Counting their FLOPs helps estimate compute cost and compare model efficiency.</p>
</blockquote>

<p>Note:</p>
<blockquote>
  <p>For matrix multiplication (M √ó N) @ (N √ó K), the FLOPs = 2 √ó M √ó N √ó K</p>
</blockquote>

<p><strong>Feed Forward Networks: 2.01 TFLOPs (57.4%)</strong></p>
<ul>
  <li>Two matrix multiplications per layer: d_model ‚Üî d_ff expansion and contraction</li>
  <li>Why so expensive: The <code class="language-plaintext highlighter-rouge">4√ó</code> expansion (1,600 ‚Üí 6,400), then contraction (6,400 ‚Üí 1,600) creates huge matrix operations</li>
  <li>Per layer cost: 2 √ó 1,600 √ó 6,400 + 2 √ó 6,400 √ó 1,600 -&gt; 41.9 GFLOPs</li>
  <li>Total across all layers: 41.9 GFLOPs √ó 48 layers -&gt; 2.01 TFLOPs</li>
</ul>

<p><strong>Attention Linear Projections: 1.01 TFLOPs (28.7%)</strong></p>
<ul>
  <li>Four projections per layer: Query, Key, Value, and Output matrices</li>
  <li>Each projection: (1,024 √ó 1,600) @ (1,600 √ó 1,600) matrix multiplication</li>
  <li>Per projection cost: 2 √ó 1,024 √ó 1,600 √ó 1,600 = 5.24 GFLOPs</li>
  <li>Per layer cost (4 projections): 4 √ó 5.24 GFLOPs = 20.97 GFLOPs</li>
  <li>Total across all layers: 20.97 GFLOPs √ó 48 layers = 1.007 TFLOPs</li>
</ul>

<p><strong>Attention Computation: 0.32 TFLOPs (9.2%)</strong></p>
<ul>
  <li><strong>Q@K^T</strong>: Computing attention scores
    <ul>
      <li>Creates the attention matrix that determines which tokens attend to which</li>
      <li>Matrix multiplication: (1,024 √ó 1,600) @ (1,600 √ó 1,024)</li>
      <li>FLOPs per layer = 2 √ó 1024 √ó 1600 √ó 1024 -&gt;  3,355,443,200 FLOPs per layer</li>
      <li>Total across all layers: 3,355,443,200 √ó 48 layers -&gt; 0.16 TFLOPs</li>
    </ul>
  </li>
  <li><strong>Attention@V</strong>: Applying attention weights to values
    <ul>
      <li>Applies attention weights to value vectors to get final attended representations</li>
      <li>Matrix multiplication: (1,024 √ó 1,024)  @  (1,024 √ó 1,600)</li>
      <li>FLOPs per layer = 2 √ó 1024 √ó 1024 √ó 1600 -&gt;  3,355,443,200 FLOPs per layer</li>
      <li>Total across all layers: 3,355,443,200 √ó 48 layers -&gt; 0.16 TFLOPs</li>
    </ul>
  </li>
  <li><strong>Currently small</strong> but <em>grows quadratically with longer sequences</em></li>
</ul>

<p><strong>Output Projection: 0.16 TFLOPs (4.7%)</strong></p>
<ul>
  <li>Final projection from hidden states to 50,257 vocabulary logits
    <ul>
      <li>Input shape: (1,024 √ó 1,600) hidden states</li>
      <li>Weight shape: (1,600 √ó 50,257) vocabulary projection</li>
      <li>Output shape: (1,024 √ó 50,257) logits for each position</li>
      <li>FLOPs: 2 √ó 1,024 √ó 1,600 √ó 50,257 -&gt; 0.16 TFLOPs</li>
    </ul>
  </li>
  <li>Large vocabulary makes this significant despite being a single operation</li>
</ul>

<h4 id="the-computational-cost-hierarchy"><strong>The Computational Cost Hierarchy</strong></h4>

<p><strong>ü•á Feed Forward is the King (57.4%)</strong></p>

<p>This is the most important finding: <em>feed forward networks consume more computation than everything else combined</em>. The 4√ó expansion factor creates the largest matrix operations in the entire model.</p>

<p><strong>ü•à Attention Linear Projections Runner-Up (28.7%)</strong></p>

<p>The four linear projections (Q, K, V, O) that prepare attention computations use nearly 30% of all FLOPs.</p>

<p><strong>ü•â The Famous Attention Mechanism (9.2%)</strong></p>

<p>Despite getting most research attention, the actual attention computation (the part that makes transformers special) uses less than 10% of computation for typical sequence lengths. But, it can <em>grow quadratically with longer sequences or context window</em>.</p>

<p><strong>üèÖ Vocabulary Bottleneck (4.7%)</strong></p>

<p>The final projection to vocabulary logits is notable due to GPT-2 XL‚Äôs large vocabulary.</p>

<p><strong>Why This Distribution Matters</strong></p>

<p>This analysis reveals that <strong>feed forward networks are the computational elephant in the room</strong>. For current sequence lengths, optimizing feed forward networks provides bigger computational savings than optimizing attention mechanisms.</p>

<h3 id="part-3-how-computational-patterns-change-with-model-scale"><strong>Part 3: How Computational Patterns Change with Model Scale</strong></h3>

<p>Let‚Äôs examine how FLOP distribution evolves across the entire GPT-2 family:</p>

<table>
  <thead>
    <tr>
      <th>Model</th>
      <th>Layers</th>
      <th>d_model</th>
      <th>Total FLOPs</th>
      <th>Feed Forward %</th>
      <th>Attention Linear Proj %</th>
      <th>Attention Computation %</th>
      <th>Output Projection %</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Small</strong></td>
      <td>12</td>
      <td>768</td>
      <td>0.29 TFLOPs</td>
      <td>39.8%</td>
      <td>19.9%</td>
      <td>13.3%</td>
      <td><strong>27.1%</strong></td>
    </tr>
    <tr>
      <td><strong>Medium</strong></td>
      <td>24</td>
      <td>1024</td>
      <td>0.83 TFLOPs</td>
      <td>49.9%</td>
      <td>24.9%</td>
      <td>12.5%</td>
      <td>12.7%</td>
    </tr>
    <tr>
      <td><strong>Large</strong></td>
      <td>36</td>
      <td>1280</td>
      <td>1.78 TFLOPs</td>
      <td>54.5%</td>
      <td>27.2%</td>
      <td>10.9%</td>
      <td>7.4%</td>
    </tr>
    <tr>
      <td><strong>XL</strong></td>
      <td>48</td>
      <td>1600</td>
      <td>3.51 TFLOPs</td>
      <td><strong>57.4%</strong></td>
      <td><strong>28.7%</strong></td>
      <td>9.2%</td>
      <td>4.7%</td>
    </tr>
  </tbody>
</table>

<h4 id="scaling-trends"><strong>Scaling Trends</strong></h4>

<p><strong>üìà Feed Forward Dominance Grows</strong></p>
<ul>
  <li>Small: 39.8% ‚Üí XL: 57.4% (+17.7 percentage points)</li>
  <li><strong>Why</strong>: Scales as O(d_model¬≤ √ó layers), growing faster than other components, and <em>growing quadratically with larger embedding model</em>.</li>
</ul>

<p><strong>üìâ Output Projection Becomes Negligible</strong></p>
<ul>
  <li>Small: 27.1% ‚Üí XL: 4.7% (-22.4 percentage points)</li>
  <li><strong>Why</strong>: Scales as O(d_model) while vocabulary size stays constant</li>
</ul>

<p><strong>üìâ Attention Computation Relatively Shrinks</strong></p>
<ul>
  <li>Small: 13.3% ‚Üí XL: 9.2% (-4.1 percentage points)</li>
  <li><strong>Why</strong>: Scales as O(d_model √ó layers), but growing only linear with larger embedding model.</li>
</ul>

<p><strong>üìà Attention Projections Grow Steadily</strong></p>
<ul>
  <li>Small: 19.9% ‚Üí XL: 28.7% (+8.8 percentage points)</li>
  <li><strong>Why</strong>: Scales same as feed forward: O(d_model¬≤ √ó layers), and <em>growing quadratically with larger embedding model</em>.</li>
</ul>

<h4 id="optimization-strategy-by-model-size"><strong>Optimization Strategy by Model Size</strong></h4>

<p><strong>Small Models</strong>: Output projection matters most (27% of computation)</p>
<ul>
  <li>Focus on vocabulary efficiency and embedding optimizations</li>
</ul>

<p><strong>Large Models</strong>: Feed forward networks dominate (57% of computation)</p>
<ul>
  <li>Focus on Mixture of Experts (MoE), pruning, and quantization</li>
</ul>

<p>This scaling analysis explains why techniques like <strong>Mixture of Experts</strong>‚Äîwhich primarily optimize feed forward networks‚Äîbecome increasingly important for large models (we will explain that later in this notes).</p>

<h3 id="part-4-the-long-context-revolution"><strong>Part 4: The Long Context Revolution</strong></h3>

<p>What happens when we extend GPT-2 XL‚Äôs context from 1,024 to 16,384 tokens? The results are dramatic.</p>

<h4 id="the-38-computational-explosion"><strong>The 38√ó Computational Explosion</strong></h4>

<table>
  <thead>
    <tr>
      <th>Component</th>
      <th>1K Context</th>
      <th>16K Context</th>
      <th>Scaling</th>
      <th>Change</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Attention Computation</strong></td>
      <td><strong>9.2%</strong></td>
      <td><strong>61.8%</strong></td>
      <td><strong>256√ó</strong></td>
      <td><strong>+52.6 pts</strong></td>
    </tr>
    <tr>
      <td><strong>Feed Forward</strong></td>
      <td><strong>57.4%</strong></td>
      <td><strong>24.1%</strong></td>
      <td><strong>16√ó</strong></td>
      <td><strong>-33.3 pts</strong></td>
    </tr>
    <tr>
      <td><strong>Attention Projections</strong></td>
      <td><strong>28.7%</strong></td>
      <td><strong>12.1%</strong></td>
      <td><strong>16√ó</strong></td>
      <td><strong>-16.6 pts</strong></td>
    </tr>
    <tr>
      <td><strong>Output Projection</strong></td>
      <td><strong>4.7%</strong></td>
      <td><strong>2.0%</strong></td>
      <td><strong>16√ó</strong></td>
      <td><strong>-2.7 pts</strong></td>
    </tr>
  </tbody>
</table>

<h4 id="the-quadratic-takeover"><strong>The Quadratic Takeover</strong></h4>

<p><strong>Total FLOPs</strong>: 3.5 TFLOPs ‚Üí 133.4 TFLOPs (38√ó increase)</p>

<p>The most shocking change: <strong>attention computation explodes from 9% to 62%</strong> of total computation. Here‚Äôs why:</p>

<ul>
  <li><strong>Sequence length scaling</strong>: 1,024 ‚Üí 16,384 tokens (16√ó increase)</li>
  <li><strong>Linear components</strong> (feed forward, projections): Scale by 16√ó</li>
  <li><strong>Quadratic components</strong> (attention computation): Scale by 16¬≤ = 256√ó</li>
</ul>

<p>The mathematical culprits are the <code class="language-plaintext highlighter-rouge">Q@K^T</code> and <code class="language-plaintext highlighter-rouge">Attention@V</code> operations, which both scale as <code class="language-plaintext highlighter-rouge">O(sequence_length¬≤)</code>.</p>

<h4 id="memory-implications"><strong>Memory Implications</strong></h4>

<p>The memory story is even more dramatic:</p>
<ul>
  <li><strong>Attention matrices</strong>: 50M ‚Üí 12,885M elements (256√ó increase)</li>
  <li><strong>Storage requirement</strong>: Each attention head must store a 16K√ó16K matrix</li>
</ul>

<p>This reveals why <strong>long context is the next major frontier</strong> in LLM optimization, requiring techniques like:</p>
<ul>
  <li><strong>Flash Attention</strong>: Memory-efficient attention computation</li>
  <li><strong>Sparse Attention</strong>: Only compute attention for relevant tokens</li>
  <li><strong>Linear Attention</strong>: Approximate attention with linear complexity</li>
  <li><strong>Sliding Window</strong>: Limit attention to recent tokens</li>
</ul>

<h3 id="part-5-understanding-mixture-of-experts-moe"><strong>Part 5: Understanding Mixture of Experts (MoE)</strong></h3>

<p>Given that feed forward networks dominate computation (57% in large models), let‚Äôs understand why <strong>Mixture of Experts</strong> has become a game-changing optimization technique.</p>

<h4 id="the-restaurant-kitchen-analogy"><strong>The Restaurant Kitchen Analogy</strong></h4>

<p><strong>Traditional Model</strong>: One super-chef tries to cook everything‚Äîpizza, sushi, pasta, desserts. The chef gets exhausted and slower as the menu grows.</p>

<p><strong>MoE Model</strong>: Multiple specialist chefs (pizza expert, sushi master, pasta chef, dessert specialist) with a smart dispatcher who sends each order to the right specialist. Only the relevant chef works on each dish.</p>

<h4 id="how-moe-works-in-practice">How MoE Works in Practice</h4>
<p>Instead of one giant feedforward network, we have many smaller expert networks (say 64 FFNs). A router (small gating network) decides which subset of experts (often 1 or 2) each token should use. Thus, for each token: Only ~1‚Äì2 experts are activated
The others are inactive (no compute for them).</p>
<h4 id="why-moe-provides-massive-savings">Why MoE Provides Massive Savings</h4>
<p><strong>Computational Savings</strong>:</p>
<ul>
  <li><strong>Before</strong>: Use 100% of the giant network for every input</li>
  <li><strong>After</strong>: Use only 10-20% of total network capacity</li>
  <li><strong>Result</strong>: 5-10√ó faster inference with same quality</li>
</ul>

<p><strong>Specialization Benefits</strong>:</p>
<ul>
  <li><strong>Expert 1</strong>: Math and science</li>
  <li><strong>Expert 2</strong>: Creative writing</li>
  <li><strong>Expert 3</strong>: Code and programming</li>
  <li><strong>Expert 4</strong>: Languages and translation</li>
</ul>

<p><strong>Scale Without Pain</strong>:</p>
<ul>
  <li><strong>Traditional</strong>: 2√ó bigger model = 2√ó more computation</li>
  <li><strong>MoE</strong>: 2√ó more experts ‚âà same computation (since only 1-2 active)</li>
</ul>

<p>Since feed forward networks use 57% of computation in large models, MoE can reduce this to 6-12%, eliminating the primary bottleneck.</p>

<h3 id="part-6-key-insights-and-practical-implications"><strong>Part 6: Key Insights and Practical Implications</strong></h3>

<h4 id="for-model-developers"><strong>For Model Developers</strong></h4>

<p><strong>Small Models (&lt; 1B parameters)</strong>:</p>
<ul>
  <li>Output projection optimization matters most</li>
  <li>Vocabulary efficiency and embedding techniques provide biggest gains</li>
  <li>Feed forward optimization secondary</li>
</ul>

<p><strong>Large Models (&gt; 10B parameters)</strong>:</p>
<ul>
  <li>Feed forward networks are the primary target (MoE, quantization, pruning)</li>
  <li>Attention projection optimizations become important</li>
  <li>Output projection becomes negligible</li>
</ul>

<p><strong>Long Context Models</strong>:</p>
<ul>
  <li>Attention computation becomes dominant bottleneck</li>
  <li>Memory optimization equally critical as computation optimization</li>
  <li>Linear attention mechanisms essential</li>
</ul>

<h4 id="for-infrastructure-teams"><strong>For Infrastructure Teams</strong></h4>

<p><strong>Hardware Requirements Scale Predictably</strong>:</p>

<p>Computational Scaling Patterns:</p>

<ul>
  <li>Feed forward: Scales as O(d_model¬≤ √ó layers)
    <ul>
      <li>GPT-2 Small ‚Üí XL: d_model grows 2.1x, layers grow 4x ‚Üí FF computation grows ~17x</li>
      <li>Dominates short-sequence/context window workloads (57% of computation)</li>
      <li>Requires high tensor core utilization for matrix multiplication</li>
    </ul>
  </li>
  <li>Attention: Scales as O(sequence_length¬≤ √ó layers)
    <ul>
      <li>1K ‚Üí 16K context window: sequence_length grows 16x ‚Üí attention grows 256x</li>
      <li>Becomes dominant for long sequences (62% computation at 16K context)</li>
      <li>Requires specialized attention kernels and memory optimization</li>
    </ul>
  </li>
</ul>

<p>Memory Scaling Reality:</p>
<ul>
  <li>Model weights: Static ~6 GB for GPT-2 XL (predictable)</li>
  <li>Activations: Variable based on sequence length
    <ul>
      <li>Short sequences (1K): ~0.5 GB activations</li>
      <li>Long sequences (16K): ~57 GB activations (100x more!)</li>
    </ul>
  </li>
  <li>Attention matrices: The memory killer at long sequences
    <ul>
      <li>1K context: 0.2 GB attention matrices</li>
      <li>16K context: 51.5 GB attention matrices (256x increase)</li>
    </ul>
  </li>
</ul>

<p><em>Activations</em> are the intermediate values computed during the forward pass that must be stored in memory for:</p>
<ul>
  <li>Computing the next layer‚Äôs input</li>
  <li>Backpropagation (during training)</li>
  <li>Gradient computation</li>
</ul>

<p><strong>Optimization Priorities</strong>:</p>
<ul>
  <li><strong>Short sequences</strong>: Focus on feed forward efficiency</li>
  <li><strong>Long sequences</strong>: Focus on attention efficiency</li>
  <li><strong>Both</strong>: Memory bandwidth becomes critical</li>
</ul>

<h3 id="conclusion"><strong>Conclusion</strong></h3>

<p>Large language models may seem like black boxes, but their computational patterns follow clear mathematical principles:</p>

<ol>
  <li><strong>Feed forward networks dominate</strong> computation in most scenarios (57% for large models)</li>
  <li><strong>Model scaling</strong> predictably shifts optimization priorities from vocabulary to feed forward efficiency</li>
  <li><strong>Long context</strong> fundamentally changes the game, making attention the primary bottleneck</li>
  <li><strong>Memory requirements</strong> often exceed computational requirements for optimization</li>
</ol>

<p>Understanding these patterns isn‚Äôt just academic‚Äîit directly informs optimization strategies, hardware requirements, and research directions. As we push toward even larger models and longer contexts, these computational realities will increasingly determine what‚Äôs possible and what‚Äôs practical in the world of large language models.</p>]]></content><author><name>[&quot;Han Yu&quot;]</name></author><category term="cs336" /><summary type="html"><![CDATA[Understanding where the computation really goes in transformer language models*]]></summary></entry><entry><title type="html">Study Notes: Stanford CS336 Language Modeling from Scratch [6]</title><link href="http://localhost:4000/cs336/2025/09/17/cs336-transformer-architecture-overview.html" rel="alternate" type="text/html" title="Study Notes: Stanford CS336 Language Modeling from Scratch [6]" /><published>2025-09-17T00:00:00-07:00</published><updated>2025-09-17T00:00:00-07:00</updated><id>http://localhost:4000/cs336/2025/09/17/cs336-transformer-architecture-overview</id><content type="html" xml:base="http://localhost:4000/cs336/2025/09/17/cs336-transformer-architecture-overview.html"><![CDATA[<h2 id="an-overview-of-popular-transformer-architectures">An Overview of Popular Transformer Architectures</h2>

<p>While working on the Transformer LM assignments, I realized it would be helpful to also step back and look at some of the most popular Transformer architectures. Here are my notes and takeaways.</p>

<h3 id="table-of-contents">Table of Contents</h3>
<ol>
  <li><a href="#introduction">Introduction</a></li>
  <li><a href="#architecture-overview">Architecture Overview</a></li>
  <li><a href="#encoder-decoder-transformers">Encoder-Decoder Transformers</a></li>
  <li><a href="#decoder-only-transformers">Decoder-Only Transformers</a></li>
  <li><a href="#encoder-only-transformers">Encoder-Only Transformers</a></li>
  <li><a href="#comparison-summary">Comparison Summary</a></li>
  <li><a href="#modern-trends-and-applications">Modern Trends and Applications</a></li>
</ol>

<h3 id="introduction">Introduction</h3>

<p>Transformer architectures have revolutionized natural language processing and machine learning. Since the original ‚ÄúAttention is All You Need‚Äù paper in 2017, three main architectural variants have emerged, each optimized for different types of tasks:</p>

<ul>
  <li><strong>Encoder-Decoder</strong>: Sequence-to-sequence transformations</li>
  <li><strong>Decoder-Only</strong>: Autoregressive text generation</li>
  <li><strong>Encoder-Only</strong>: Text understanding and classification</li>
</ul>

<p>This note provides an overview of how each architecture works, their training methodologies, evaluation approaches, and practical applications.</p>

<hr />

<h3 id="architecture-overview">Architecture Overview</h3>

<h4 id="core-components">Core Components</h4>

<p>All transformer architectures share fundamental building blocks:</p>

<ul>
  <li><strong>Self-Attention Mechanism</strong>: Allows tokens to attend to other tokens</li>
  <li><strong>Feed-Forward Networks</strong>: Position-wise processing layers</li>
  <li><strong>Layer Normalization</strong>: Stabilizes training</li>
  <li><strong>Residual Connections</strong>: Enables deep architectures</li>
  <li><strong>Positional Encodings</strong>: Provides sequence position information</li>
</ul>

<h4 id="multi-head-self-attention-deep-dive">Multi-Head Self-Attention Deep Dive</h4>

<p>The multi-head self-attention mechanism is the core innovation of transformers. Here‚Äôs how it works in detail:</p>

<p><img src="/assets/picture/2025-09-17-cs336-transformer-architecture-overview/encoder-multi-head-self-attention.png" alt="Multi-Head Self-Attention Mechanism" width="1080" /></p>

<p><strong>Key Steps:</strong></p>
<ol>
  <li><strong>Linear Projections</strong>: Input embeddings are transformed into Query (Q), Key (K), and Value (V) matrices</li>
  <li><strong>Head Splitting</strong>: Q, K, V matrices are reshaped and split into multiple attention heads</li>
  <li><strong>Parallel Attention</strong>: Each head computes attention independently using scaled dot-product attention</li>
  <li><strong>Concatenation</strong>: All head outputs are concatenated back together</li>
  <li><strong>Final Projection</strong>: A final linear layer projects the concatenated result back to the model dimension</li>
</ol>

<p>This parallel processing allows the model to attend to different types of relationships simultaneously - some heads might focus on syntactic relationships while others capture semantic connections.</p>

<h4 id="key-differences">Key Differences</h4>

<p>The main distinction lies in the <strong>attention patterns</strong>:</p>

<table>
  <thead>
    <tr>
      <th>Architecture</th>
      <th>Attention Pattern</th>
      <th>Primary Use Case</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Encoder-Decoder</td>
      <td>Bidirectional (encoder) + Causal (decoder)</td>
      <td>Sequence-to-sequence tasks</td>
    </tr>
    <tr>
      <td>Decoder-Only</td>
      <td>Causal only</td>
      <td>Autoregressive generation</td>
    </tr>
    <tr>
      <td>Encoder-Only</td>
      <td>Bidirectional only</td>
      <td>Understanding and classification</td>
    </tr>
  </tbody>
</table>

<hr />

<h3 id="encoder-decoder-transformers">Encoder-Decoder Transformers</h3>

<h4 id="architecture-design">Architecture Design</h4>

<p>The encoder-decoder architecture consists of two separate stacks connected through cross-attention:</p>

<p><img src="/assets/picture/2025-09-17-cs336-transformer-architecture-overview/encoder-decoder-transformer-architecture.png" alt="Encoder-Decoder Transformer Architecture" width="1080" /></p>

<p><strong>Key Components:</strong></p>
<ul>
  <li><strong>Encoder</strong>: Uses bidirectional self-attention to process input sequence with full context</li>
  <li><strong>Decoder</strong>: Uses causal self-attention + cross-attention to generate output sequence</li>
  <li><strong>Cross-Attention</strong>: Allows decoder to attend to encoder representations at each layer</li>
  <li><strong>Layer-by-Layer Processing</strong>: Each decoder layer receives information from the corresponding encoder layer</li>
</ul>

<p><strong>Key Features:</strong></p>
<ul>
  <li><strong>üîÑ Bidirectional Encoder</strong>: Full context understanding for source sequence</li>
  <li><strong>üîó Cross-Attention</strong>: Decoder attends to encoder representations</li>
  <li><strong>üìù Sequence-to-Sequence</strong>: Perfect for translation, summarization, and question answering</li>
</ul>

<p>This architecture excels at tasks requiring structured input-output transformations where the model needs to understand the entire input before generating the output.</p>

<h4 id="training-methodology">Training Methodology</h4>

<p><strong>Objective</strong>: Learn to map input sequences to output sequences</p>

<p><strong>Training Process:</strong></p>
<ol>
  <li><strong>Teacher Forcing</strong>: Use ground truth target tokens as decoder input</li>
  <li><strong>Parallel Training</strong>: All target positions trained simultaneously</li>
  <li><strong>Cross-Entropy Loss</strong>: Computed over target vocabulary</li>
</ol>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Training pseudocode
</span><span class="k">def</span> <span class="nf">train_encoder_decoder</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">dataloader</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">dataloader</span><span class="p">:</span>
        <span class="n">src_tokens</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="s">'source'</span><span class="p">]</span>      <span class="c1"># Input sequence
</span>        <span class="n">tgt_tokens</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="s">'target'</span><span class="p">]</span>      <span class="c1"># Target sequence
</span>        
        <span class="c1"># Teacher forcing setup
</span>        <span class="n">tgt_input</span> <span class="o">=</span> <span class="n">tgt_tokens</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>       <span class="c1"># Decoder input
</span>        <span class="n">tgt_output</span> <span class="o">=</span> <span class="n">tgt_tokens</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>       <span class="c1"># Expected output
</span>        
        <span class="c1"># Forward pass
</span>        <span class="n">logits</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">src_tokens</span><span class="p">,</span> <span class="n">tgt_input</span><span class="p">)</span>
        
        <span class="c1"># Compute loss
</span>        <span class="n">loss</span> <span class="o">=</span> <span class="n">cross_entropy</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">tgt_output</span><span class="p">)</span>
        
        <span class="c1"># Backpropagation
</span>        <span class="n">loss</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="p">.</span><span class="n">step</span><span class="p">()</span>
</code></pre></div></div>

<p><strong>Training Data Requirements:</strong></p>
<ul>
  <li><strong>Parallel Corpora</strong>: Paired input-output sequences</li>
  <li><strong>Domain-Specific</strong>: Task-dependent datasets</li>
  <li><strong>Quality</strong>: High-quality alignments crucial for performance</li>
</ul>

<h4 id="evaluation-methods">Evaluation Methods</h4>

<p><strong>Generation-Based Evaluation:</strong></p>

<ol>
  <li><strong>Automatic Metrics</strong>:
    <ul>
      <li><strong>BLEU</strong>: N-gram overlap for translation</li>
      <li><strong>ROUGE</strong>: Recall-oriented for summarization</li>
      <li><strong>METEOR</strong>: Semantic similarity measures</li>
      <li><strong>BERTScore</strong>: Contextual embeddings comparison</li>
    </ul>
  </li>
  <li><strong>Human Evaluation</strong>:
    <ul>
      <li><strong>Fluency</strong>: How natural the output sounds</li>
      <li><strong>Adequacy</strong>: How well meaning is preserved</li>
      <li><strong>Faithfulness</strong>: Accuracy to source content</li>
    </ul>
  </li>
</ol>

<p><strong>Task-Specific Benchmarks:</strong></p>
<ul>
  <li><strong>Translation</strong>: WMT datasets, OPUS corpora</li>
  <li><strong>Summarization</strong>: CNN/DailyMail, XSum</li>
  <li><strong>Question Answering</strong>: SQuAD variants</li>
</ul>

<h4 id="use-cases-and-applications">Use Cases and Applications</h4>

<p><strong>Primary Applications:</strong></p>
<ul>
  <li><strong>Machine Translation</strong>: Language pair transformations</li>
  <li><strong>Text Summarization</strong>: Document to summary conversion</li>
  <li><strong>Dialogue Systems</strong>: Context-aware response generation</li>
  <li><strong>Code Translation</strong>: Between programming languages</li>
  <li><strong>Data-to-Text</strong>: Structured data to natural language</li>
</ul>

<p><strong>Examples:</strong></p>
<ul>
  <li>Google Translate (earlier versions)</li>
  <li>T5 (Text-to-Text Transfer Transformer)</li>
  <li>BART (Bidirectional and Auto-Regressive Transformers)</li>
  <li>mT5 (Multilingual T5)</li>
</ul>

<hr />

<h3 id="decoder-only-transformers">Decoder-Only Transformers</h3>

<h4 id="architecture-design-1">Architecture Design</h4>

<p>Decoder-only models use a single stack with causal attention:</p>

<p><img src="/assets/picture/2025-09-17-cs336-transformer-architecture-overview/decoder-only-transformer-lm.png" alt="Decoder-Only Transformer Architecture" width="880" /></p>

<p><strong>Key Characteristics:</strong></p>
<ul>
  <li><strong>Causal Masking</strong>: Prevents attention to future tokens during training and inference</li>
  <li><strong>Autoregressive Generation</strong>: Produces one token at a time during generation</li>
  <li><strong>Unified Architecture</strong>: Same model architecture handles various tasks through different prompting strategies</li>
  <li><strong>Scalability</strong>: Architecture scales well to very large model sizes (billions of parameters)</li>
</ul>

<p><strong>Key Features:</strong></p>
<ul>
  <li><strong>üîí Causal Masking</strong>: Can only attend to previous tokens</li>
  <li><strong>üîÑ Autoregressive</strong>: Generates tokens one at a time</li>
  <li><strong>üí¨ Text Generation</strong>: Chat, completion, and code generation</li>
</ul>

<p>This architecture has become the foundation for modern large language models like GPT, excelling at open-ended text generation and few-shot learning through prompting.</p>

<h4 id="training-methodology-1">Training Methodology</h4>

<p><strong>Objective</strong>: Learn to predict the next token given previous context</p>

<p><strong>Training Process:</strong></p>
<ol>
  <li><strong>Next Token Prediction</strong>: Core training objective</li>
  <li><strong>Causal Masking</strong>: Maintains autoregressive property during training</li>
  <li><strong>Large-Scale Data</strong>: Trained on massive text corpora</li>
</ol>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Training pseudocode
</span><span class="k">def</span> <span class="nf">train_decoder_only</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">dataloader</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">dataloader</span><span class="p">:</span>
        <span class="c1"># Sequence: "The cat sat on the mat"
</span>        <span class="n">input_tokens</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="s">'tokens'</span><span class="p">][:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>   <span class="c1"># "The cat sat on the"
</span>        <span class="n">target_tokens</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="s">'tokens'</span><span class="p">][</span><span class="mi">1</span><span class="p">:]</span>   <span class="c1"># "cat sat on the mat"
</span>        
        <span class="c1"># Forward pass with causal masking
</span>        <span class="n">logits</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">input_tokens</span><span class="p">)</span>
        
        <span class="c1"># Next token prediction loss
</span>        <span class="n">loss</span> <span class="o">=</span> <span class="n">cross_entropy</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">target_tokens</span><span class="p">)</span>
        
        <span class="n">loss</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="p">.</span><span class="n">step</span><span class="p">()</span>
</code></pre></div></div>

<p><strong>Multi-Stage Training:</strong></p>

<ol>
  <li><strong>Pre-training</strong>:
    <ul>
      <li><strong>Data</strong>: Large-scale web text, books, articles</li>
      <li><strong>Objective</strong>: Next token prediction</li>
      <li><strong>Scale</strong>: Billions to trillions of tokens</li>
    </ul>
  </li>
  <li><strong>Instruction Fine-tuning</strong>:
    <ul>
      <li><strong>Data</strong>: Human-written instruction-response pairs</li>
      <li><strong>Objective</strong>: Follow instructions accurately</li>
      <li><strong>Benefits</strong>: Improved task performance</li>
    </ul>
  </li>
  <li><strong>Reinforcement Learning from Human Feedback (RLHF)</strong>:
    <ul>
      <li><strong>Data</strong>: Human preference comparisons</li>
      <li><strong>Objective</strong>: Align with human values</li>
      <li><strong>Benefits</strong>: Safer, more helpful responses</li>
    </ul>
  </li>
</ol>

<h4 id="evaluation-methods-1">Evaluation Methods</h4>

<p><strong>Multiple Evaluation Paradigms:</strong></p>

<ol>
  <li><strong>Perplexity Measurement</strong>:
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">compute_perplexity</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">test_data</span><span class="p">):</span>
 <span class="n">total_loss</span> <span class="o">=</span> <span class="mi">0</span>
 <span class="n">total_tokens</span> <span class="o">=</span> <span class="mi">0</span>
    
 <span class="k">for</span> <span class="n">sequence</span> <span class="ow">in</span> <span class="n">test_data</span><span class="p">:</span>
     <span class="n">loss</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">compute_loss</span><span class="p">(</span><span class="n">sequence</span><span class="p">)</span>
     <span class="n">total_loss</span> <span class="o">+=</span> <span class="n">loss</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">sequence</span><span class="p">)</span>
     <span class="n">total_tokens</span> <span class="o">+=</span> <span class="nb">len</span><span class="p">(</span><span class="n">sequence</span><span class="p">)</span>
    
 <span class="k">return</span> <span class="n">exp</span><span class="p">(</span><span class="n">total_loss</span> <span class="o">/</span> <span class="n">total_tokens</span><span class="p">)</span>
</code></pre></div>    </div>
  </li>
  <li><strong>Generation Quality</strong>:
    <ul>
      <li><strong>Human Evaluation</strong>: Coherence, relevance, helpfulness</li>
      <li><strong>Automatic Metrics</strong>: Diversity, repetition, toxicity</li>
      <li><strong>Task-Specific</strong>: BLEU for translation, ROUGE for summarization</li>
    </ul>
  </li>
  <li><strong>Benchmark Evaluation</strong>:
    <ul>
      <li><strong>GLUE/SuperGLUE</strong>: General language understanding</li>
      <li><strong>MMLU</strong>: Massive multitask language understanding</li>
      <li><strong>HumanEval</strong>: Code generation capabilities</li>
      <li><strong>HellaSwag</strong>: Commonsense reasoning</li>
    </ul>
  </li>
</ol>

<h4 id="use-cases-and-applications-1">Use Cases and Applications</h4>

<p><strong>Primary Applications:</strong></p>
<ul>
  <li><strong>Text Generation</strong>: Creative writing, content creation</li>
  <li><strong>Conversational AI</strong>: Chatbots, virtual assistants</li>
  <li><strong>Code Generation</strong>: Programming assistance</li>
  <li><strong>Question Answering</strong>: Information retrieval and reasoning</li>
  <li><strong>Few-Shot Learning</strong>: Task adaptation through prompting</li>
</ul>

<p><strong>Examples:</strong></p>
<ul>
  <li>GPT family (GPT-2, GPT-3, GPT-4)</li>
  <li>LLaMA (Large Language Model Meta AI)</li>
  <li>PaLM (Pathways Language Model)</li>
  <li>Claude (Anthropic‚Äôs assistant)</li>
  <li>ChatGPT and GPT-4</li>
</ul>

<hr />

<h3 id="encoder-only-transformers">Encoder-Only Transformers</h3>

<h4 id="architecture-design-2">Architecture Design</h4>

<p>Encoder-only models use bidirectional attention for understanding:</p>

<p><img src="/assets/picture/2025-09-17-cs336-transformer-architecture-overview/encoder-only-transformer-lm.png" alt="Encoder-Only Transformer Architecture" width="880" /></p>

<p><strong>Key Characteristics:</strong></p>
<ul>
  <li><strong>Bidirectional Context</strong>: Can attend to all positions in the sequence simultaneously</li>
  <li><strong>Rich Representations</strong>: Deep contextual understanding from both left and right context</li>
  <li><strong>Task Adaptation</strong>: Requires fine-tuning for downstream tasks but excels at understanding</li>
  <li><strong>Special Tokens</strong>: Uses [CLS] and [SEP] tokens for sequence classification and separation</li>
</ul>

<p><strong>Key Features:</strong></p>
<ul>
  <li><strong>üîÑ Bidirectional Attention</strong>: Full context understanding from both directions</li>
  <li><strong>üß† Understanding Tasks</strong>: Classification, extraction, comprehension</li>
  <li><strong>üìö Pre-training + Fine-tuning</strong>: Masked language modeling then task-specific training</li>
</ul>

<p>This architecture excels at tasks requiring deep understanding of text, where the model benefits from seeing the entire context before making predictions. The bidirectional nature makes it particularly powerful for classification and extraction tasks.</p>

<h4 id="training-methodology-2">Training Methodology</h4>

<p><strong>Pre-training Objective</strong>: Masked Language Modeling (MLM)</p>

<p><strong>Training Process:</strong></p>
<ol>
  <li><strong>Token Masking</strong>: Randomly mask 15% of input tokens</li>
  <li><strong>Bidirectional Processing</strong>: Full context available for predictions</li>
  <li><strong>Mask Prediction</strong>: Reconstruct original tokens</li>
</ol>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Training pseudocode
</span><span class="k">def</span> <span class="nf">train_encoder_only</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">dataloader</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">dataloader</span><span class="p">:</span>
        <span class="c1"># Original: "The cat sat on the mat"
</span>        <span class="c1"># Masked:   "The [MASK] sat on the [MASK]"
</span>        
        <span class="n">masked_tokens</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="s">'masked_input'</span><span class="p">]</span>
        <span class="n">original_tokens</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="s">'original_input'</span><span class="p">]</span>
        <span class="n">mask_positions</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="s">'mask_positions'</span><span class="p">]</span>
        
        <span class="c1"># Bidirectional encoding
</span>        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">masked_tokens</span><span class="p">)</span>
        
        <span class="c1"># Predict only at masked positions
</span>        <span class="n">masked_predictions</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="p">[</span><span class="n">mask_positions</span><span class="p">]</span>
        <span class="n">masked_targets</span> <span class="o">=</span> <span class="n">original_tokens</span><span class="p">[</span><span class="n">mask_positions</span><span class="p">]</span>
        
        <span class="n">loss</span> <span class="o">=</span> <span class="n">cross_entropy</span><span class="p">(</span><span class="n">masked_predictions</span><span class="p">,</span> <span class="n">masked_targets</span><span class="p">)</span>
        
        <span class="n">loss</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="p">.</span><span class="n">step</span><span class="p">()</span>
</code></pre></div></div>

<p><strong>Masking Strategy:</strong></p>
<ul>
  <li><strong>80%</strong>: Replace with [MASK] token</li>
  <li><strong>10%</strong>: Replace with random token</li>
  <li><strong>10%</strong>: Keep original token</li>
</ul>

<p><strong>Fine-tuning for Downstream Tasks:</strong></p>

<p>After pre-training, models are fine-tuned for specific applications:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">finetune_classification</span><span class="p">(</span><span class="n">pretrained_model</span><span class="p">,</span> <span class="n">task_data</span><span class="p">):</span>
    <span class="c1"># Add task-specific classification head
</span>    <span class="n">classifier</span> <span class="o">=</span> <span class="n">Linear</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">)</span>
    
    <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">task_data</span><span class="p">:</span>
        <span class="n">inputs</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="s">'text'</span><span class="p">],</span> <span class="n">batch</span><span class="p">[</span><span class="s">'labels'</span><span class="p">]</span>
        
        <span class="c1"># Get contextual representations
</span>        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">pretrained_model</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
        
        <span class="c1"># Use [CLS] token for classification
</span>        <span class="n">cls_representation</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span>
        
        <span class="c1"># Classification prediction
</span>        <span class="n">logits</span> <span class="o">=</span> <span class="n">classifier</span><span class="p">(</span><span class="n">cls_representation</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">cross_entropy</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
        
        <span class="n">loss</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="p">.</span><span class="n">step</span><span class="p">()</span>
</code></pre></div></div>

<h4 id="evaluation-methods-2">Evaluation Methods</h4>

<p><strong>Task-Specific Evaluation:</strong></p>

<ol>
  <li><strong>Classification Tasks</strong>:
    <ul>
      <li><strong>Accuracy</strong>: Percentage of correct predictions</li>
      <li><strong>F1-Score</strong>: Harmonic mean of precision and recall</li>
      <li><strong>Matthews Correlation</strong>: Balanced measure for imbalanced data</li>
    </ul>
  </li>
  <li><strong>Token-Level Tasks</strong>:
    <ul>
      <li><strong>Named Entity Recognition</strong>: Entity-level F1</li>
      <li><strong>Part-of-Speech Tagging</strong>: Token-level accuracy</li>
      <li><strong>Dependency Parsing</strong>: Unlabeled/labeled attachment scores</li>
    </ul>
  </li>
  <li><strong>Span-Based Tasks</strong>:
    <ul>
      <li><strong>Question Answering</strong>: Exact match and F1 scores</li>
      <li><strong>Reading Comprehension</strong>: Answer extraction accuracy</li>
    </ul>
  </li>
</ol>

<p><strong>Benchmark Suites:</strong></p>
<ul>
  <li><strong>GLUE</strong>: General Language Understanding Evaluation</li>
  <li><strong>SuperGLUE</strong>: More challenging language understanding tasks</li>
  <li><strong>SentEval</strong>: Sentence representation evaluation</li>
</ul>

<h4 id="use-cases-and-applications-2">Use Cases and Applications</h4>

<p><strong>Primary Applications:</strong></p>
<ul>
  <li><strong>Text Classification</strong>: Sentiment analysis, topic classification</li>
  <li><strong>Named Entity Recognition</strong>: Information extraction</li>
  <li><strong>Question Answering</strong>: Extractive QA systems</li>
  <li><strong>Semantic Similarity</strong>: Text matching and retrieval</li>
  <li><strong>Language Understanding</strong>: Intent classification, slot filling</li>
</ul>

<p><strong>Examples:</strong></p>
<ul>
  <li>BERT (Bidirectional Encoder Representations from Transformers)</li>
  <li>RoBERTa (Robustly Optimized BERT Pretraining Approach)</li>
  <li>DeBERTa (Decoding-enhanced BERT with Disentangled Attention)</li>
  <li>ELECTRA (Efficiently Learning an Encoder that Classifies Token Replacements Accurately)</li>
</ul>

<hr />

<h3 id="comparison-summary">Comparison Summary</h3>

<h4 id="architecture-comparison">Architecture Comparison</h4>

<p>The three transformer architectures shown in the diagrams above have distinct characteristics that make them suitable for different tasks:</p>

<table>
  <thead>
    <tr>
      <th>Aspect</th>
      <th>Encoder-Decoder</th>
      <th>Decoder-Only</th>
      <th>Encoder-Only</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Attention Pattern</strong></td>
      <td>Bidirectional + Causal</td>
      <td>Causal Only</td>
      <td>Bidirectional Only</td>
    </tr>
    <tr>
      <td><strong>Primary Strength</strong></td>
      <td>Seq2seq transformation</td>
      <td>Text generation</td>
      <td>Text understanding</td>
    </tr>
    <tr>
      <td><strong>Training Data</strong></td>
      <td>Parallel sequences</td>
      <td>Raw text</td>
      <td>Raw text + labels</td>
    </tr>
    <tr>
      <td><strong>Evaluation Focus</strong></td>
      <td>Generation quality</td>
      <td>Perplexity + tasks</td>
      <td>Task performance</td>
    </tr>
    <tr>
      <td><strong>Inference</strong></td>
      <td>Autoregressive</td>
      <td>Autoregressive</td>
      <td>Single forward pass</td>
    </tr>
    <tr>
      <td><strong>Architecture Complexity</strong></td>
      <td>Most complex (2 stacks)</td>
      <td>Simple (1 stack)</td>
      <td>Simple (1 stack)</td>
    </tr>
    <tr>
      <td><strong>Cross-Attention</strong></td>
      <td>‚úÖ Required</td>
      <td>‚ùå None</td>
      <td>‚ùå None</td>
    </tr>
  </tbody>
</table>

<h4 id="training-requirements">Training Requirements</h4>

<table>
  <thead>
    <tr>
      <th>Requirement</th>
      <th>Encoder-Decoder</th>
      <th>Decoder-Only</th>
      <th>Encoder-Only</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Data Quantity</strong></td>
      <td>Moderate (paired data)</td>
      <td>Large (raw text)</td>
      <td>Moderate (raw + labeled)</td>
    </tr>
    <tr>
      <td><strong>Data Quality</strong></td>
      <td>High (alignment crucial)</td>
      <td>Variable (web-scale)</td>
      <td>High (clean text)</td>
    </tr>
    <tr>
      <td><strong>Compute Cost</strong></td>
      <td>Moderate</td>
      <td>Very High</td>
      <td>Moderate</td>
    </tr>
    <tr>
      <td><strong>Training Time</strong></td>
      <td>Days to weeks</td>
      <td>Weeks to months</td>
      <td>Days to weeks</td>
    </tr>
  </tbody>
</table>

<h4 id="use-case-suitability">Use Case Suitability</h4>

<table>
  <thead>
    <tr>
      <th>Task Type</th>
      <th>Best Architecture</th>
      <th>Rationale</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Translation</strong></td>
      <td>Encoder-Decoder</td>
      <td>Structured input-output mapping with cross-attention</td>
    </tr>
    <tr>
      <td><strong>Text Generation</strong></td>
      <td>Decoder-Only</td>
      <td>Autoregressive nature with causal masking</td>
    </tr>
    <tr>
      <td><strong>Classification</strong></td>
      <td>Encoder-Only</td>
      <td>Bidirectional understanding with task-specific heads</td>
    </tr>
    <tr>
      <td><strong>Summarization</strong></td>
      <td>Encoder-Decoder / Decoder-Only</td>
      <td>Both work well - encoder-decoder for extractive, decoder-only for abstractive</td>
    </tr>
    <tr>
      <td><strong>Question Answering</strong></td>
      <td>All three</td>
      <td>Encoder-only for extractive, decoder-only for generative, encoder-decoder for complex reasoning</td>
    </tr>
    <tr>
      <td><strong>Dialogue</strong></td>
      <td>Decoder-Only</td>
      <td>Generative conversation with context understanding</td>
    </tr>
    <tr>
      <td><strong>Code Generation</strong></td>
      <td>Decoder-Only</td>
      <td>Sequential token generation with programming syntax</td>
    </tr>
    <tr>
      <td><strong>Sentiment Analysis</strong></td>
      <td>Encoder-Only</td>
      <td>Classification task with bidirectional context</td>
    </tr>
    <tr>
      <td><strong>Named Entity Recognition</strong></td>
      <td>Encoder-Only</td>
      <td>Token-level classification with full context</td>
    </tr>
  </tbody>
</table>

<h4 id="architecture-selection-guide">Architecture Selection Guide</h4>

<p><strong>Choose Encoder-Decoder when:</strong></p>
<ul>
  <li>You have paired input-output data (parallel corpora)</li>
  <li>Tasks require understanding input completely before generating output</li>
  <li>You need structured transformations (translation, summarization)</li>
  <li>Cross-attention between source and target is beneficial</li>
</ul>

<p><strong>Choose Decoder-Only when:</strong></p>
<ul>
  <li>You want a unified model for multiple tasks</li>
  <li>Open-ended text generation is the primary goal</li>
  <li>You have large amounts of raw text data</li>
  <li>You want to leverage in-context learning and prompting</li>
</ul>

<p><strong>Choose Encoder-Only when:</strong></p>
<ul>
  <li>Understanding and classification are the primary goals</li>
  <li>You don‚Äôt need to generate long sequences</li>
  <li>You have labeled data for fine-tuning</li>
  <li>Bidirectional context improves performance significantly</li>
</ul>

<p>This is just a quick note üìù ‚Äî to dive into the details, you‚Äôd probably need to read some relevant papers üìö, but I hope it still shared something useful ‚ú®</p>]]></content><author><name>[&quot;Han Yu&quot;]</name></author><category term="cs336" /><summary type="html"><![CDATA[An Overview of Popular Transformer Architectures]]></summary></entry><entry><title type="html">Study Notes: Stanford CS336 Language Modeling from Scratch [5]</title><link href="http://localhost:4000/cs336/2025/09/13/cs336-build-a-transformer-language-model.html" rel="alternate" type="text/html" title="Study Notes: Stanford CS336 Language Modeling from Scratch [5]" /><published>2025-09-13T00:00:00-07:00</published><updated>2025-09-13T00:00:00-07:00</updated><id>http://localhost:4000/cs336/2025/09/13/cs336-build-a-transformer-language-model</id><content type="html" xml:base="http://localhost:4000/cs336/2025/09/13/cs336-build-a-transformer-language-model.html"><![CDATA[<h2 id="building-a-transformer-language-model-a-complete-guide">Building a Transformer Language Model: A Complete Guide</h2>

<p>Ever wondered how ChatGPT and other language models actually work under the hood? Let‚Äôs build one from scratch and understand every component.This guide explains how to build a Transformer language model from scratch using PyTorch. We‚Äôll cover each building block and show how they work together to create a complete language model.</p>

<h3 id="table-of-contents">Table of Contents</h3>
<ol>
  <li><a href="#overview">Overview</a></li>
  <li><a href="#basic-building-blocks">Basic Building Blocks</a></li>
  <li><a href="#core-components">Core Components</a></li>
  <li><a href="#advanced-components">Advanced Components</a></li>
  <li><a href="#putting-it-all-together">Putting It All Together</a></li>
  <li><a href="#the-complete-transformer">The Complete Transformer</a></li>
</ol>

<h3 id="overview">Overview</h3>

<p>A Transformer language model is essentially a sophisticated pattern recognition system that learns to predict the next word in a sequence. Think of it as an incredibly advanced autocomplete that understands context, grammar, and meaning.
By the end of this guide, you‚Äôll understand:</p>

<ul>
  <li>How words become numbers a neural network can process</li>
  <li>How attention mechanisms let models ‚Äúfocus‚Äù on relevant information</li>
  <li>How all the pieces fit together to create a complete language model</li>
</ul>

<p>Before diving into details, let‚Äôs understand the overall architecture:</p>
<blockquote>
  <p><strong>Input:</strong> ‚ÄúThe cat sat on the‚Äù</p>

  <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>          ‚¨áÔ∏è
</code></pre></div>  </div>

  <p><strong>Step 1:</strong> Convert words to numbers (embeddings)</p>

  <p><strong>Step 2:</strong> Process through multiple transformer blocks:</p>
  <ul>
    <li>Attention: ‚ÄúWhat should I focus on?‚Äù</li>
    <li>Feed-forward: ‚ÄúWhat patterns do I see?‚Äù</li>
  </ul>

  <p><strong>Step 3:</strong> Predict next word probabilities</p>

  <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>          ‚¨áÔ∏è
</code></pre></div>  </div>

  <p><strong>Output:</strong> ‚Äúmat‚Äù (85%), ‚Äúchair‚Äù (10%), ‚Äúfloor‚Äù (5%)</p>
</blockquote>

<p>Now let‚Äôs build each component step by step.</p>
<h3 id="basic-building-blocks">Basic Building Blocks</h3>

<h4 id="1-linear-layer-cs336_basicslinearpy">1. Linear Layer (<code class="language-plaintext highlighter-rouge">cs336_basics/linear.py</code>)</h4>

<p><strong>What it does:</strong> A linear layer is the most basic building block of neural networks. It takes input numbers and transforms them using a mathematical operation: <code class="language-plaintext highlighter-rouge">output = input √ó weight</code>.</p>

<p><strong>Why we need it:</strong> Linear layers let the model learn patterns by adjusting their weights during training.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Example: Transform 64 dimensions to 128 dimensions
</span><span class="n">linear</span> <span class="o">=</span> <span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">128</span><span class="p">)</span>
<span class="c1"># If input is shape (batch_size, 64), output will be (batch_size, 128)
</span></code></pre></div></div>

<p><strong>Implementation:</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">math</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>

<span class="k">class</span> <span class="nc">Linear</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="s">"""
    A linear transformation module that inherits from torch.nn.Module.
    Performs y = xW^T without bias.
    """</span>
    
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_features</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">out_features</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">in_features</span> <span class="o">=</span> <span class="n">in_features</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">out_features</span> <span class="o">=</span> <span class="n">out_features</span>
        
        <span class="c1"># Create weight parameter W (not W^T) for memory ordering reasons
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">W</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">empty</span><span class="p">(</span><span class="n">out_features</span><span class="p">,</span> <span class="n">in_features</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">))</span>
        
        <span class="bp">self</span><span class="p">.</span><span class="n">reset_parameters</span><span class="p">()</span>
    
    <span class="k">def</span> <span class="nf">reset_parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="s">"""
        Initialize weights using truncated normal distribution.
        Linear weights: N(Œº=0, œÉ¬≤=2/(d_in + d_out)) truncated at [-3œÉ, 3œÉ]
        """</span>
        <span class="n">std</span> <span class="o">=</span> <span class="n">math</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mf">2.0</span> <span class="o">/</span> <span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">in_features</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">out_features</span><span class="p">))</span>
        <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">init</span><span class="p">.</span><span class="n">trunc_normal_</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">W</span><span class="p">,</span> <span class="n">mean</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">std</span><span class="o">=</span><span class="n">std</span><span class="p">,</span> <span class="n">a</span><span class="o">=-</span><span class="mi">3</span><span class="o">*</span><span class="n">std</span><span class="p">,</span> <span class="n">b</span><span class="o">=</span><span class="mi">3</span><span class="o">*</span><span class="n">std</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="s">"""Apply the linear transformation to the input."""</span>
        <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">W</span><span class="p">.</span><span class="n">T</span><span class="p">)</span>
</code></pre></div></div>

<p><strong>Key features:</strong></p>
<ul>
  <li>No bias term (simpler than standard PyTorch Linear)</li>
  <li>Weights initialized using truncated normal distribution</li>
  <li>Used throughout the Transformer for projections and transformations</li>
</ul>

<h4 id="2-embedding-layer-cs336_basicsembeddingpy">2. Embedding Layer (<code class="language-plaintext highlighter-rouge">cs336_basics/embedding.py</code>)</h4>

<p><strong>What it does:</strong> Converts discrete tokens (like word IDs) into dense vector representations that neural networks can work with.</p>

<p><strong>Think of it like this:</strong> If words were people, embeddings would be detailed personality profiles. Similar words get similar profiles.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Example: Convert word IDs to 512-dimensional vectors
</span><span class="n">embedding</span> <span class="o">=</span> <span class="n">Embedding</span><span class="p">(</span><span class="n">num_embeddings</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="o">=</span><span class="mi">512</span><span class="p">)</span>
<span class="c1"># Input: [5, 23, 156] (word IDs)
# Output: 3 vectors, each with 512 numbers
</span></code></pre></div></div>

<p><strong>Implementation:</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>

<span class="k">class</span> <span class="nc">Embedding</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="s">"""
    An embedding module that inherits from torch.nn.Module.
    Performs embedding lookup by indexing into an embedding matrix.
    """</span>
    
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_embeddings</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">num_embeddings</span> <span class="o">=</span> <span class="n">num_embeddings</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">embedding_dim</span> <span class="o">=</span> <span class="n">embedding_dim</span>
        
        <span class="c1"># Create embedding matrix parameter with shape (vocab_size, d_model)
</span>        <span class="c1"># Store with d_model as the final dimension
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">embeddings</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">empty</span><span class="p">(</span><span class="n">num_embeddings</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">,</span> 
                                                  <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">))</span>
        
        <span class="bp">self</span><span class="p">.</span><span class="n">reset_parameters</span><span class="p">()</span>
    
    <span class="k">def</span> <span class="nf">reset_parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="s">"""
        Initialize embedding weights using truncated normal distribution.
        Embedding: N(Œº=0, œÉ¬≤=1) truncated at [-3, 3]
        """</span>
        <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">init</span><span class="p">.</span><span class="n">trunc_normal_</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">embeddings</span><span class="p">,</span> <span class="n">mean</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">std</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">a</span><span class="o">=-</span><span class="mf">3.0</span><span class="p">,</span> <span class="n">b</span><span class="o">=</span><span class="mf">3.0</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">token_ids</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="s">"""
        Lookup the embedding vectors for the given token IDs.
        
        Args:
            token_ids: Token IDs with shape (batch_size, sequence_length)
            
        Returns:
            Embedding vectors with shape (batch_size, sequence_length, embedding_dim)
        """</span>
        <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">embeddings</span><span class="p">[</span><span class="n">token_ids</span><span class="p">]</span>
</code></pre></div></div>

<p><strong>Key features:</strong></p>
<ul>
  <li>Maps discrete tokens to continuous vectors</li>
  <li>Similar words learn to have similar embeddings</li>
  <li>The embedding matrix is learned during training</li>
</ul>

<h4 id="3-softmax-function-cs336_basicssoftmaxpy">3. Softmax Function (<code class="language-plaintext highlighter-rouge">cs336_basics/softmax.py</code>)</h4>

<p><strong>What it does:</strong> Converts a list of numbers into probabilities that sum to 1.</p>

<p><strong>Why we need it:</strong> At the end of the model, we need probabilities for each possible next word.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Example: Convert logits to probabilities
</span><span class="n">logits</span> <span class="o">=</span> <span class="p">[</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">]</span>  <span class="c1"># Raw scores
</span><span class="n">probabilities</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span>  <span class="c1"># [0.659, 0.242, 0.099] - sums to 1.0
</span></code></pre></div></div>

<p><strong>Implementation:</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch</span>

<span class="k">def</span> <span class="nf">softmax</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="s">"""
    Apply the softmax operation to a tensor along the specified dimension.
    
    Uses the numerical stability trick of subtracting the maximum value
    from all elements before applying exponential.
    
    Args:
        x: torch.Tensor - Input tensor
        dim: int - Dimension along which to apply softmax
        
    Returns:
        torch.Tensor - Output tensor with same shape as input, with normalized
                      probability distribution along the specified dimension
    """</span>
    <span class="c1"># Subtract maximum for numerical stability
</span>    <span class="c1"># keepdim=True ensures the shape is preserved for broadcasting
</span>    <span class="n">max_vals</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nb">max</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">dim</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">x_stable</span> <span class="o">=</span> <span class="n">x</span> <span class="o">-</span> <span class="n">max_vals</span>
    
    <span class="c1"># Apply exponential
</span>    <span class="n">exp_vals</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x_stable</span><span class="p">)</span>
    
    <span class="c1"># Compute sum along the specified dimension
</span>    <span class="n">sum_exp</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">exp_vals</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">dim</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    
    <span class="c1"># Normalize to get probabilities
</span>    <span class="k">return</span> <span class="n">exp_vals</span> <span class="o">/</span> <span class="n">sum_exp</span>
</code></pre></div></div>

<p><strong>Key features:</strong></p>
<ul>
  <li>Uses numerical stability trick (subtracts max value)</li>
  <li>Higher input values become higher probabilities</li>
  <li>All outputs sum to exactly 1.0</li>
</ul>

<h3 id="core-components">Core Components</h3>

<h4 id="4-rmsnorm-cs336_basicsrmsnormpy">4. RMSNorm (<code class="language-plaintext highlighter-rouge">cs336_basics/rmsnorm.py</code>)</h4>

<p><strong>What it does:</strong> Normalizes the inputs to keep the model stable during training. It‚Äôs like adjusting the volume on different audio channels to keep them balanced.</p>

<p><strong>Formula:</strong> <code class="language-plaintext highlighter-rouge">RMSNorm(x) = x / RMS(x) * learnable_scale</code></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Example: Normalize 512-dimensional vectors
</span><span class="n">rmsnorm</span> <span class="o">=</span> <span class="n">RMSNorm</span><span class="p">(</span><span class="n">d_model</span><span class="o">=</span><span class="mi">512</span><span class="p">)</span>
<span class="c1"># Keeps all dimensions on a similar scale
</span></code></pre></div></div>

<p><strong>Implementation:</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>

<span class="k">class</span> <span class="nc">RMSNorm</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="s">"""
    RMSNorm (Root Mean Square Layer Normalization) module.
    
    Rescales each activation a_i as: RMSNorm(a_i) = a_i/RMS(a) * g_i
    where RMS(a) = sqrt(1/d_model * ‚àëa^2_i + Œµ)
    """</span>
    
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">eps</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-5</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">d_model</span> <span class="o">=</span> <span class="n">d_model</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">eps</span> <span class="o">=</span> <span class="n">eps</span>
        
        <span class="c1"># Learnable gain parameter g_i
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">g</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">ones</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">))</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="c1"># Store original dtype
</span>        <span class="n">in_dtype</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">dtype</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
        
        <span class="c1"># Compute RMS: sqrt(1/d_model * ‚àëa^2_i + Œµ)
</span>        <span class="c1"># Mean of squares over the last dimension (d_model)
</span>        <span class="n">mean_square</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">x</span> <span class="o">**</span> <span class="mi">2</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="n">rms</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">mean_square</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">eps</span><span class="p">)</span>
        
        <span class="c1"># Apply RMSNorm: a_i/RMS(a) * g_i
</span>        <span class="n">result</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span> <span class="o">/</span> <span class="n">rms</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="p">.</span><span class="n">g</span>
        
        <span class="c1"># Return in original dtype
</span>        <span class="k">return</span> <span class="n">result</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="n">in_dtype</span><span class="p">)</span>
</code></pre></div></div>

<p><strong>Why it‚Äôs important:</strong></p>
<ul>
  <li>Prevents values from becoming too large or too small</li>
  <li>Helps the model train faster and more stably</li>
  <li>Applied before major operations in the Transformer</li>
</ul>

<h4 id="5-attention-mechanism-cs336_basicsattentionpy">5. Attention Mechanism (<code class="language-plaintext highlighter-rouge">cs336_basics/attention.py</code>)</h4>

<p><strong>What it does:</strong> Allows the model to focus on different parts of the input when making predictions. Like highlighting important words when reading.</p>

<p><strong>Key concepts:</strong></p>
<ul>
  <li><strong>Queries (Q):</strong> ‚ÄúWhat am I looking for?‚Äù</li>
  <li><strong>Keys (K):</strong> ‚ÄúWhat information is available?‚Äù</li>
  <li><strong>Values (V):</strong> ‚ÄúWhat is the actual information?‚Äù</li>
  <li><strong>Dimension Requirements</strong>
    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  # Q: [batch, seq_len_q, d_k]  - Query dimension
  # K: [batch, seq_len_k, d_k]  - Key dimension  
  # V: [batch, seq_len_v, d_v]  - Value dimension

  # Requirements:
  # 1. Q and K MUST have same d_k (for dot product)
  # 2. K and V MUST have same seq_len (they describe the same items)
  # 3. V can have different d_v (output dimension can differ)
</code></pre></div>    </div>
  </li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Attention formula: Attention(Q,K,V) = softmax(Q√óK^T / ‚àöd_k) √ó V
</span><span class="n">attention_output</span> <span class="o">=</span> <span class="n">scaled_dot_product_attention</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">V</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span>
</code></pre></div></div>

<p><strong>Implementation:</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">math</span>
<span class="kn">from</span> <span class="nn">.softmax</span> <span class="kn">import</span> <span class="n">softmax</span>

<span class="k">def</span> <span class="nf">scaled_dot_product_attention</span><span class="p">(</span><span class="n">Q</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">K</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">V</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">mask</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="bp">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="s">"""
    Scaled dot-product attention implementation.
    
    Computes: Attention(Q, K, V) = softmax(Q @ K^T / sqrt(d_k)) @ V
    """</span>
    <span class="c1"># Get dimensions
</span>    <span class="n">d_k</span> <span class="o">=</span> <span class="n">Q</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    
    <span class="c1"># Compute scaled dot-product: Q @ K^T / sqrt(d_k)
</span>    <span class="n">scores</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="p">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span> <span class="o">/</span> <span class="n">math</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">d_k</span><span class="p">)</span>
    
    <span class="c1"># Apply mask if provided
</span>    <span class="k">if</span> <span class="n">mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
        <span class="c1"># Where mask is False, set scores to negative infinity
</span>        <span class="c1"># This will make softmax output 0 for those positions
</span>        <span class="n">scores</span> <span class="o">=</span> <span class="n">scores</span><span class="p">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="o">~</span><span class="n">mask</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="s">'-inf'</span><span class="p">))</span>
    
    <span class="c1"># Apply softmax along the last dimension (over keys)
</span>    <span class="n">attention_weights</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    
    <span class="c1"># Handle the case where entire rows are masked (all -inf)
</span>    <span class="k">if</span> <span class="n">mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
        <span class="c1"># If a row is entirely masked, attention_weights will have NaN
</span>        <span class="c1"># Replace NaN with 0
</span>        <span class="n">attention_weights</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">where</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">attention_weights</span><span class="p">),</span> 
                                      <span class="n">torch</span><span class="p">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">attention_weights</span><span class="p">),</span> 
                                      <span class="n">attention_weights</span><span class="p">)</span>
    
    <span class="c1"># Apply attention to values
</span>    <span class="n">output</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">attention_weights</span><span class="p">,</span> <span class="n">V</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">output</span>
</code></pre></div></div>

<p><strong>Attention Mechanism Flow:</strong></p>

<p><img src="/assets/picture/2025-09-13-cs336-build-a-transformer-language-model/attention_flow.png" alt="Attention Mechanism Flow" /></p>

<p><strong>Features:</strong></p>
<ul>
  <li><strong>Causal masking:</strong> Prevents looking at future words (essential for language modeling)</li>
  <li><strong>Scaling:</strong> Divides by ‚àöd_k for numerical stability</li>
  <li><strong>Flexible dimensions:</strong> Works with any number of batch dimensions</li>
</ul>

<h4 id="6-rotary-position-embedding-rope-cs336_basicsropepy">6. Rotary Position Embedding (RoPE) (<code class="language-plaintext highlighter-rouge">cs336_basics/rope.py</code>)</h4>

<p><strong>What it does:</strong> Tells the model where each word is positioned in the sequence by rotating the attention vectors based on position.</p>

<p><strong>Why it‚Äôs needed:</strong> Without position information, ‚Äúcat sat on mat‚Äù and ‚Äúmat sat on cat‚Äù would look identical to the model.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># RoPE rotates query and key vectors based on their position
</span><span class="n">rope</span> <span class="o">=</span> <span class="n">RotaryPositionalEmbedding</span><span class="p">(</span><span class="n">theta</span><span class="o">=</span><span class="mf">10000.0</span><span class="p">,</span> <span class="n">d_k</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">max_seq_len</span><span class="o">=</span><span class="mi">512</span><span class="p">)</span>
<span class="n">rotated_queries</span> <span class="o">=</span> <span class="n">rope</span><span class="p">(</span><span class="n">queries</span><span class="p">,</span> <span class="n">token_positions</span><span class="p">)</span>
</code></pre></div></div>

<p><strong>Implementation:</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>

<span class="k">class</span> <span class="nc">RotaryPositionalEmbedding</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="s">"""
    Rotary Positional Embedding (RoPE) module.
    
    Applies rotary positional embeddings to input tensors by rotating pairs of dimensions
    based on their position in the sequence.
    """</span>
    
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">d_k</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">max_seq_len</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">theta</span> <span class="o">=</span> <span class="n">theta</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">d_k</span> <span class="o">=</span> <span class="n">d_k</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">max_seq_len</span> <span class="o">=</span> <span class="n">max_seq_len</span>
        
        <span class="c1"># Precompute the frequency values for each dimension pair
</span>        <span class="k">assert</span> <span class="n">d_k</span> <span class="o">%</span> <span class="mi">2</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="s">"d_k must be even for RoPE"</span>
        
        <span class="c1"># Create frequency values: theta^(-2i/d_k) for i = 0, 1, ..., d_k/2 - 1
</span>        <span class="n">dim_indices</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">d_k</span> <span class="o">//</span> <span class="mi">2</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
        <span class="n">freqs</span> <span class="o">=</span> <span class="n">theta</span> <span class="o">**</span> <span class="p">(</span><span class="o">-</span><span class="mf">2.0</span> <span class="o">*</span> <span class="n">dim_indices</span> <span class="o">/</span> <span class="n">d_k</span><span class="p">)</span>
        
        <span class="c1"># Create position indices for the maximum sequence length
</span>        <span class="n">positions</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="n">max_seq_len</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
        
        <span class="c1"># Compute the angles: position * frequency for each position and frequency
</span>        <span class="n">angles</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">outer</span><span class="p">(</span><span class="n">positions</span><span class="p">,</span> <span class="n">freqs</span><span class="p">)</span>
        
        <span class="c1"># Precompute cos and sin values
</span>        <span class="c1"># We need to repeat each value twice to match the pairing structure
</span>        <span class="n">cos_vals</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">repeat_interleave</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">cos</span><span class="p">(</span><span class="n">angles</span><span class="p">),</span> <span class="mi">2</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">sin_vals</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">repeat_interleave</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">sin</span><span class="p">(</span><span class="n">angles</span><span class="p">),</span> <span class="mi">2</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        
        <span class="c1"># Register as buffers so they move with the module
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s">'cos_cached'</span><span class="p">,</span> <span class="n">cos_vals</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s">'sin_cached'</span><span class="p">,</span> <span class="n">sin_vals</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">_rotate_half</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="s">"""
        Rotate the last dimension of x by swapping and negating pairs of elements.
        For RoPE, we rotate pairs of dimensions: (x1, x2) -&gt; (-x2, x1)
        """</span>
        <span class="c1"># Split into two halves and swap with negation
</span>        <span class="n">x1</span> <span class="o">=</span> <span class="n">x</span><span class="p">[...,</span> <span class="p">::</span><span class="mi">2</span><span class="p">]</span>  <span class="c1"># Even indices (0, 2, 4, ...)
</span>        <span class="n">x2</span> <span class="o">=</span> <span class="n">x</span><span class="p">[...,</span> <span class="mi">1</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span>  <span class="c1"># Odd indices (1, 3, 5, ...)
</span>        
        <span class="c1"># Interleave -x2 and x1
</span>        <span class="n">rotated</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">stack</span><span class="p">((</span><span class="o">-</span><span class="n">x2</span><span class="p">,</span> <span class="n">x1</span><span class="p">),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">rotated</span><span class="p">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">start_dim</span><span class="o">=-</span><span class="mi">2</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">token_positions</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="c1"># Extract cos and sin values for the given positions
</span>        <span class="n">cos_vals</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">cos_cached</span><span class="p">[</span><span class="n">token_positions</span><span class="p">]</span>  <span class="c1"># (..., seq_len, d_k)
</span>        <span class="n">sin_vals</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">sin_cached</span><span class="p">[</span><span class="n">token_positions</span><span class="p">]</span>  <span class="c1"># (..., seq_len, d_k)
</span>        
        <span class="c1"># Apply RoPE: x * cos + rotate_half(x) * sin
</span>        <span class="n">rotated_x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">_rotate_half</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span> <span class="o">*</span> <span class="n">cos_vals</span> <span class="o">+</span> <span class="n">rotated_x</span> <span class="o">*</span> <span class="n">sin_vals</span>
</code></pre></div></div>

<p><strong>Key features:</strong></p>
<ul>
  <li>Encodes position directly into attention mechanism</li>
  <li>Works well with different sequence lengths</li>
  <li>Applied to queries and keys, but not values, because positional information is used for attention computation, not for the content being retrieved</li>
</ul>

<h3 id="advanced-components">Advanced Components</h3>

<h4 id="7-swiglu-feed-forward-network-cs336_basicsswiglupy">7. SwiGLU Feed-Forward Network (<code class="language-plaintext highlighter-rouge">cs336_basics/swiglu.py</code>)</h4>

<p><strong>What it does:</strong> A sophisticated feed-forward network that processes information after attention. It‚Äôs like a specialized filter that enhances certain patterns.</p>

<p><strong>Components:</strong></p>
<ul>
  <li><strong>SiLU activation:</strong> <code class="language-plaintext highlighter-rouge">SiLU(x) = x √ó sigmoid(x)</code> - smoother than ReLU</li>
  <li><strong>Gated Linear Unit:</strong> Combines two transformations with element-wise multiplication</li>
  <li><strong>Three linear layers:</strong> W1, W2, W3</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># SwiGLU formula: W2(SiLU(W1√óx) ‚äô W3√óx)
# ‚äô means element-wise multiplication
</span><span class="n">swiglu</span> <span class="o">=</span> <span class="n">SwiGLU</span><span class="p">(</span><span class="n">d_model</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">d_ff</span><span class="o">=</span><span class="mi">1365</span><span class="p">)</span>  <span class="c1"># d_ff ‚âà 8/3 √ó d_model
</span></code></pre></div></div>

<p><strong>Implementation:</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">from</span> <span class="nn">.linear</span> <span class="kn">import</span> <span class="n">Linear</span>

<span class="k">def</span> <span class="nf">silu</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="s">"""
    SiLU (Swish) activation function: SiLU(x) = x * sigmoid(x) = x / (1 + e^(-x))
    """</span>
    <span class="k">return</span> <span class="n">x</span> <span class="o">*</span> <span class="n">torch</span><span class="p">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="k">class</span> <span class="nc">SwiGLU</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="s">"""
    SwiGLU: SiLU-based Gated Linear Unit

    FFN(x) = SwiGLU(x, W1, W2, W3) = W2(SiLU(W1x) ‚äô W3x)
    where ‚äô represents element-wise multiplication
    """</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">d_model</span> <span class="o">=</span> <span class="n">d_model</span>

        <span class="c1"># Calculate d_ff if not provided: 8/3 * d_model, rounded to nearest multiple of 64
</span>        <span class="k">if</span> <span class="n">d_ff</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">d_ff</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="mi">8</span><span class="o">/</span><span class="mi">3</span> <span class="o">*</span> <span class="n">d_model</span><span class="p">)</span>
            <span class="c1"># Round to nearest multiple of 64
</span>            <span class="n">d_ff</span> <span class="o">=</span> <span class="p">((</span><span class="n">d_ff</span> <span class="o">+</span> <span class="mi">31</span><span class="p">)</span> <span class="o">//</span> <span class="mi">64</span><span class="p">)</span> <span class="o">*</span> <span class="mi">64</span>

        <span class="bp">self</span><span class="p">.</span><span class="n">d_ff</span> <span class="o">=</span> <span class="n">d_ff</span>

        <span class="c1"># Three linear transformations: W1, W2, W3
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">W1</span> <span class="o">=</span> <span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">W2</span> <span class="o">=</span> <span class="n">Linear</span><span class="p">(</span><span class="n">d_ff</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">W3</span> <span class="o">=</span> <span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="c1"># SwiGLU(x, W1, W2, W3) = W2(SiLU(W1x) ‚äô W3x)
</span>        <span class="n">w1_output</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">W1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># (..., d_ff)
</span>        <span class="n">w3_output</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">W3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># (..., d_ff)
</span>
        <span class="c1"># Apply SiLU to W1 output and element-wise multiply with W3 output
</span>        <span class="n">gated</span> <span class="o">=</span> <span class="n">silu</span><span class="p">(</span><span class="n">w1_output</span><span class="p">)</span> <span class="o">*</span> <span class="n">w3_output</span>  <span class="c1"># (..., d_ff)
</span>
        <span class="c1"># Final linear transformation
</span>        <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">W2</span><span class="p">(</span><span class="n">gated</span><span class="p">)</span>  <span class="c1"># (..., d_model)
</span></code></pre></div></div>

<p><strong>SwiGLU Architecture:</strong></p>

<p><img src="/assets/picture/2025-09-13-cs336-build-a-transformer-language-model//swiglu_architecture.png" alt="SwiGLU Architecture" /></p>

<p><strong>Why SwiGLU:</strong></p>
<ul>
  <li>More expressive than simple feed-forward networks</li>
  <li>Gating mechanism helps control information flow</li>
  <li>Proven to work better in practice for language models</li>
</ul>

<h4 id="8-multi-head-self-attention-cs336_basicsmultihead_attentionpy">8. Multi-Head Self-Attention (<code class="language-plaintext highlighter-rouge">cs336_basics/multihead_attention.py</code>)</h4>

<p><strong>What it does:</strong> Implements the complete multi-head attention mechanism with causal masking and RoPE support. This is where the model learns to focus on relevant parts of the input sequence.</p>

<p><strong>Key features:</strong></p>
<ul>
  <li><strong>Multiple attention heads:</strong> Each head can focus on different types of relationships</li>
  <li><strong>Causal masking:</strong> Prevents looking at future tokens</li>
  <li><strong>RoPE integration:</strong> Position encoding directly in the attention mechanism</li>
  <li><strong>Parallel computation:</strong> All heads computed simultaneously</li>
</ul>

<p><strong>Why Multiple Heads?</strong></p>

<p>The motivation is <strong>representation power</strong> and <strong>diversity of attention patterns</strong>.</p>

<ol>
  <li><strong>Different subspaces of information</strong>
    <ul>
      <li>Each head learns its own set of projection matrices (<code class="language-plaintext highlighter-rouge">W_q</code>, <code class="language-plaintext highlighter-rouge">W_k</code>, <code class="language-plaintext highlighter-rouge">W_v</code>).</li>
      <li>Each head looks at the input through a <em>different lens</em>, projecting embeddings into different subspaces.</li>
      <li>One head might focus on syntactic relations, another on semantics, another on positional information.</li>
    </ul>
  </li>
  <li><strong>Richer attention patterns</strong>
    <ul>
      <li>Multiple heads can attend to <strong>different tokens simultaneously</strong>.</li>
      <li>Example: in translation, one head might track word order, another align nouns, another focus on verbs.</li>
    </ul>
  </li>
  <li><strong>Stability and expressiveness</strong>
    <ul>
      <li>A single attention head is essentially a weighted average ‚Äî too simple.</li>
      <li>Multiple heads prevent the model from collapsing into one dominant pattern and encourage <strong>diverse contextualization</strong>.</li>
    </ul>
  </li>
</ol>

<p><strong>How to Select the Number of Heads?</strong></p>

<p>There‚Äôs no universal formula, but here are <strong>practical guidelines</strong>:</p>

<ol>
  <li><strong>Divisibility with model dimension</strong>
    <ul>
      <li>Embedding dimension <code class="language-plaintext highlighter-rouge">d_model</code> must be divisible by the number of heads <code class="language-plaintext highlighter-rouge">h</code>.</li>
      <li>Each head gets a sub-dimension <code class="language-plaintext highlighter-rouge">d_k = d_model / h</code>.</li>
      <li>Example: <code class="language-plaintext highlighter-rouge">d_model = 512</code>, common choices are <code class="language-plaintext highlighter-rouge">h = 8</code> (<code class="language-plaintext highlighter-rouge">d_k = 64</code>) or <code class="language-plaintext highlighter-rouge">h = 16</code> (<code class="language-plaintext highlighter-rouge">d_k = 32</code>).</li>
    </ul>
  </li>
  <li><strong>Balance between capacity and efficiency</strong>
    <ul>
      <li>Too few heads ‚Üí each head has a large <code class="language-plaintext highlighter-rouge">d_k</code> ‚Üí less diversity, harder to capture multiple relations.</li>
      <li>Too many heads ‚Üí each head has a tiny <code class="language-plaintext highlighter-rouge">d_k</code> ‚Üí may lose expressive power, and overhead grows.</li>
    </ul>
  </li>
  <li><strong>Empirical rules from practice</strong>
    <ul>
      <li><strong>Original Transformer (Vaswani et al.)</strong>:
        <ul>
          <li><code class="language-plaintext highlighter-rouge">d_model = 512</code>, <code class="language-plaintext highlighter-rouge">h = 8</code> ‚Üí <code class="language-plaintext highlighter-rouge">d_k = 64</code>.</li>
        </ul>
      </li>
      <li><strong>BERT-base</strong>:
        <ul>
          <li><code class="language-plaintext highlighter-rouge">d_model = 768</code>, <code class="language-plaintext highlighter-rouge">h = 12</code> ‚Üí <code class="language-plaintext highlighter-rouge">d_k = 64</code>.</li>
        </ul>
      </li>
      <li><strong>BERT-large / GPT-3 style models</strong>:
        <ul>
          <li><code class="language-plaintext highlighter-rouge">d_model = 1024‚Äì12288</code>, <code class="language-plaintext highlighter-rouge">h = 16‚Äì96</code>, often keeping <code class="language-plaintext highlighter-rouge">d_k ‚âà 64</code>.</li>
        </ul>
      </li>
      <li>In practice, many architectures fix <strong><code class="language-plaintext highlighter-rouge">d_k ‚âà 64</code> per head</strong> and scale <code class="language-plaintext highlighter-rouge">h</code> with model size.</li>
    </ul>
  </li>
  <li><strong>Scaling law intuition</strong>
    <ul>
      <li>Larger models tend to use more heads.</li>
      <li>But going below <code class="language-plaintext highlighter-rouge">d_k &lt; 32</code> per head often hurts performance ‚Äî each head needs enough dimensions to be useful.</li>
    </ul>
  </li>
</ol>

<p><strong>Intuition</strong></p>

<ul>
  <li><strong>One head = one spotlight.</strong> It can only focus on <em>one kind</em> of relationship at a time.</li>
  <li><strong>Multiple heads = multiple spotlights.</strong> Each head looks at different aspects, and their outputs are concatenated and mixed to form a richer representation.</li>
</ul>

<p><strong>Implementation:</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">from</span> <span class="nn">.linear</span> <span class="kn">import</span> <span class="n">Linear</span>
<span class="kn">from</span> <span class="nn">.attention</span> <span class="kn">import</span> <span class="n">scaled_dot_product_attention</span>
<span class="kn">from</span> <span class="nn">.rope</span> <span class="kn">import</span> <span class="n">RotaryPositionalEmbedding</span>

<span class="k">class</span> <span class="nc">MultiHeadSelfAttention</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="s">"""
    Causal Multi-Head Self-Attention module.

    Implements the multi-head self-attention mechanism with causal masking
    to prevent attending to future tokens.
    """</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">d_model</span> <span class="o">=</span> <span class="n">d_model</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">num_heads</span>

        <span class="c1"># Following Vaswani et al., d_k = d_v = d_model / num_heads
</span>        <span class="k">assert</span> <span class="n">d_model</span> <span class="o">%</span> <span class="n">num_heads</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="s">"d_model must be divisible by num_heads"</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">d_k</span> <span class="o">=</span> <span class="n">d_model</span> <span class="o">//</span> <span class="n">num_heads</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">d_v</span> <span class="o">=</span> <span class="n">d_model</span> <span class="o">//</span> <span class="n">num_heads</span>

        <span class="c1"># Linear projections for Q, K, V
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">W_q</span> <span class="o">=</span> <span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">W_k</span> <span class="o">=</span> <span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">W_v</span> <span class="o">=</span> <span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>

        <span class="c1"># Output projection
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">W_o</span> <span class="o">=</span> <span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">rope</span><span class="p">:</span> <span class="n">RotaryPositionalEmbedding</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span> <span class="n">token_positions</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="bp">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">d_model</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span>

        <span class="c1"># Apply linear projections
</span>        <span class="n">Q</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">W_q</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># (batch_size, seq_len, d_model)
</span>        <span class="n">K</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">W_k</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># (batch_size, seq_len, d_model)
</span>        <span class="n">V</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">W_v</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># (batch_size, seq_len, d_model)
</span>
        <span class="c1"># Reshape to multi-head format
</span>        <span class="n">Q</span> <span class="o">=</span> <span class="n">Q</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">d_k</span><span class="p">)</span>
        <span class="n">K</span> <span class="o">=</span> <span class="n">K</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">d_k</span><span class="p">)</span>
        <span class="n">V</span> <span class="o">=</span> <span class="n">V</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">d_v</span><span class="p">)</span>

        <span class="c1"># Transpose to (batch_size, num_heads, seq_len, d_k/d_v)
</span>        <span class="n">Q</span> <span class="o">=</span> <span class="n">Q</span><span class="p">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">K</span> <span class="o">=</span> <span class="n">K</span><span class="p">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">V</span> <span class="o">=</span> <span class="n">V</span><span class="p">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

        <span class="c1"># Apply RoPE if provided (only to Q and K, not V)
</span>        <span class="k">if</span> <span class="n">rope</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span> <span class="ow">and</span> <span class="n">token_positions</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="c1"># Reshape for RoPE
</span>            <span class="n">Q_rope</span> <span class="o">=</span> <span class="n">Q</span><span class="p">.</span><span class="n">contiguous</span><span class="p">().</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span> <span class="o">*</span> <span class="bp">self</span><span class="p">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">d_k</span><span class="p">)</span>
            <span class="n">K_rope</span> <span class="o">=</span> <span class="n">K</span><span class="p">.</span><span class="n">contiguous</span><span class="p">().</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span> <span class="o">*</span> <span class="bp">self</span><span class="p">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">d_k</span><span class="p">)</span>

            <span class="c1"># Expand token_positions to match
</span>            <span class="n">token_positions_expanded</span> <span class="o">=</span> <span class="n">token_positions</span><span class="p">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">).</span><span class="n">expand</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">).</span><span class="n">contiguous</span><span class="p">().</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span> <span class="o">*</span> <span class="bp">self</span><span class="p">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">)</span>

            <span class="c1"># Apply RoPE
</span>            <span class="n">Q_rope</span> <span class="o">=</span> <span class="n">rope</span><span class="p">(</span><span class="n">Q_rope</span><span class="p">,</span> <span class="n">token_positions_expanded</span><span class="p">)</span>
            <span class="n">K_rope</span> <span class="o">=</span> <span class="n">rope</span><span class="p">(</span><span class="n">K_rope</span><span class="p">,</span> <span class="n">token_positions_expanded</span><span class="p">)</span>

            <span class="c1"># Reshape back
</span>            <span class="n">Q</span> <span class="o">=</span> <span class="n">Q_rope</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">d_k</span><span class="p">)</span>
            <span class="n">K</span> <span class="o">=</span> <span class="n">K_rope</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">d_k</span><span class="p">)</span>

        <span class="c1"># Create causal mask: lower triangular matrix
</span>        <span class="n">causal_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">triu</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">ones</span><span class="p">(</span><span class="n">seq_len</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">bool</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">x</span><span class="p">.</span><span class="n">device</span><span class="p">),</span> <span class="n">diagonal</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">causal_mask</span> <span class="o">=</span> <span class="o">~</span><span class="n">causal_mask</span>  <span class="c1"># Invert: True means attend, False means don't attend
</span>
        <span class="c1"># Apply scaled dot-product attention
</span>        <span class="n">attn_output</span> <span class="o">=</span> <span class="n">scaled_dot_product_attention</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">V</span><span class="p">,</span> <span class="n">causal_mask</span><span class="p">)</span>

        <span class="c1"># Transpose back and reshape
</span>        <span class="n">attn_output</span> <span class="o">=</span> <span class="n">attn_output</span><span class="p">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">attn_output</span> <span class="o">=</span> <span class="n">attn_output</span><span class="p">.</span><span class="n">contiguous</span><span class="p">().</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>

        <span class="c1"># Apply output projection
</span>        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">W_o</span><span class="p">(</span><span class="n">attn_output</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">output</span>
</code></pre></div></div>

<p><strong>Multi-Head Attention Flow:</strong></p>

<p><img src="/assets/picture/2025-09-13-cs336-build-a-transformer-language-model/multi_head_attention_flow.png" alt="Multi-Head Attention Flow" /></p>

<h3 id="putting-it-all-together">Putting It All Together</h3>

<h4 id="9-transformer-block-cs336_basicstransformer_blockpy">9. Transformer Block (<code class="language-plaintext highlighter-rouge">cs336_basics/transformer_block.py</code>)</h4>

<p><strong>What it does:</strong> Combines attention and feed-forward processing with residual connections and normalization. This is the core building block of the Transformer.</p>

<p><strong>Architecture (Pre-Norm):</strong></p>

<p><img src="/assets/picture/2025-09-13-cs336-build-a-transformer-language-model/transformer_block.png" alt="Transformer Block Architecture" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Each transformer block does:
# 1. x = x + attention(norm(x))
# 2. x = x + feedforward(norm(x))
</span><span class="n">transformer_block</span> <span class="o">=</span> <span class="n">TransformerBlock</span><span class="p">(</span><span class="n">d_model</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">num_heads</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">d_ff</span><span class="o">=</span><span class="mi">1365</span><span class="p">)</span>
</code></pre></div></div>

<p><strong>Key concepts:</strong></p>
<ul>
  <li><strong>Residual connections:</strong> Add input to output (<code class="language-plaintext highlighter-rouge">x + f(x)</code>) - helps training deep networks</li>
  <li><strong>Pre-normalization:</strong> Apply normalization before operations, not after</li>
  <li><strong>Multi-head attention:</strong> Run multiple attention operations in parallel</li>
</ul>

<p><strong>Implementation:</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">from</span> <span class="nn">.rmsnorm</span> <span class="kn">import</span> <span class="n">RMSNorm</span>
<span class="kn">from</span> <span class="nn">.multihead_attention</span> <span class="kn">import</span> <span class="n">MultiHeadSelfAttention</span>
<span class="kn">from</span> <span class="nn">.swiglu</span> <span class="kn">import</span> <span class="n">SwiGLU</span>
<span class="kn">from</span> <span class="nn">.rope</span> <span class="kn">import</span> <span class="n">RotaryPositionalEmbedding</span>

<span class="k">class</span> <span class="nc">TransformerBlock</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="s">"""
    A pre-norm Transformer block as shown in Figure 2.

    Architecture (from bottom to top):
    1. Input tensor
    2. Norm -&gt; Causal Multi-Head Self-Attention w/ RoPE -&gt; Add (residual)
    3. Norm -&gt; Position-Wise Feed-Forward -&gt; Add (residual)
    4. Output tensor
    """</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">d_model</span> <span class="o">=</span> <span class="n">d_model</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">num_heads</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">d_ff</span> <span class="o">=</span> <span class="n">d_ff</span>

        <span class="c1"># Layer normalization for the two sublayers (RMSNorm)
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">ln1</span> <span class="o">=</span> <span class="n">RMSNorm</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>  <span class="c1"># Before attention
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">ln2</span> <span class="o">=</span> <span class="n">RMSNorm</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>  <span class="c1"># Before feed-forward
</span>
        <span class="c1"># Causal Multi-Head Self-Attention w/ RoPE
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">attn</span> <span class="o">=</span> <span class="n">MultiHeadSelfAttention</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>

        <span class="c1"># Position-Wise Feed-Forward (using SwiGLU)
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">ffn</span> <span class="o">=</span> <span class="n">SwiGLU</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">rope</span><span class="p">:</span> <span class="n">RotaryPositionalEmbedding</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span> <span class="n">token_positions</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="bp">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="c1"># First sublayer: Norm -&gt; Causal Multi-Head Self-Attention w/ RoPE -&gt; Add
</span>        <span class="n">norm1_output</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">ln1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">attn_output</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">attn</span><span class="p">(</span><span class="n">norm1_output</span><span class="p">,</span> <span class="n">rope</span><span class="o">=</span><span class="n">rope</span><span class="p">,</span> <span class="n">token_positions</span><span class="o">=</span><span class="n">token_positions</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">attn_output</span>  <span class="c1"># Residual connection
</span>
        <span class="c1"># Second sublayer: Norm -&gt; Position-Wise Feed-Forward -&gt; Add
</span>        <span class="n">norm2_output</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">ln2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">ffn_output</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">ffn</span><span class="p">(</span><span class="n">norm2_output</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">ffn_output</span>  <span class="c1"># Residual connection
</span>
        <span class="k">return</span> <span class="n">x</span>
</code></pre></div></div>

<h3 id="the-complete-transformer">The Complete Transformer</h3>

<h4 id="10-transformer-language-model-cs336_basicstransformer_lmpy">10. Transformer Language Model (<code class="language-plaintext highlighter-rouge">cs336_basics/transformer_lm.py</code>)</h4>

<p><strong>What it does:</strong> Combines everything into a complete language model that can predict the next word in a sequence.</p>

<p><strong>Full Architecture:</strong></p>

<p><img src="/assets/picture/2025-09-13-cs336-build-a-transformer-language-model/full_architecture.png" alt="Complete Transformer Language Model Architecture" /></p>

<p><strong>Architecture Flow:</strong></p>
<ol>
  <li><strong>Token IDs</strong> ‚Üí Input integers representing words/subwords</li>
  <li><strong>Token Embeddings</strong> ‚Üí Convert IDs to dense vectors</li>
  <li><strong>Transformer Blocks</strong> (repeated num_layers times):
    <ul>
      <li>RMSNorm ‚Üí Multi-Head Attention (with RoPE) ‚Üí Residual connection</li>
      <li>RMSNorm ‚Üí SwiGLU Feed-Forward ‚Üí Residual connection</li>
    </ul>
  </li>
  <li><strong>Final RMSNorm</strong> ‚Üí Normalize before output</li>
  <li><strong>Linear Head</strong> ‚Üí Project to vocabulary size</li>
  <li><strong>Softmax</strong> ‚Üí Convert to probabilities (optional)</li>
</ol>

<p><strong>How it works:</strong></p>
<ol>
  <li><strong>Input Processing:</strong> Convert word IDs to embeddings</li>
  <li><strong>Pattern Recognition:</strong> Each Transformer block learns different patterns</li>
  <li><strong>Output Generation:</strong> Final layer predicts next word probabilities</li>
</ol>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Complete model
</span><span class="n">model</span> <span class="o">=</span> <span class="n">TransformerLM</span><span class="p">(</span>
    <span class="n">vocab_size</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span>      <span class="c1"># Number of possible words
</span>    <span class="n">context_length</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span>    <span class="c1"># Maximum sequence length
</span>    <span class="n">d_model</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span>           <span class="c1"># Model dimension
</span>    <span class="n">num_layers</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span>          <span class="c1"># Number of transformer blocks
</span>    <span class="n">num_heads</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>           <span class="c1"># Number of attention heads
</span>    <span class="n">d_ff</span><span class="o">=</span><span class="mi">1365</span>             <span class="c1"># Feed-forward dimension
</span><span class="p">)</span>

<span class="c1"># Usage
</span><span class="n">logits</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">input_ids</span><span class="p">)</span>  <span class="c1"># Get next-word predictions
</span></code></pre></div></div>

<p><strong>Complete Implementation:</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">from</span> <span class="nn">.embedding</span> <span class="kn">import</span> <span class="n">Embedding</span>
<span class="kn">from</span> <span class="nn">.transformer_block</span> <span class="kn">import</span> <span class="n">TransformerBlock</span>
<span class="kn">from</span> <span class="nn">.rmsnorm</span> <span class="kn">import</span> <span class="n">RMSNorm</span>
<span class="kn">from</span> <span class="nn">.linear</span> <span class="kn">import</span> <span class="n">Linear</span>
<span class="kn">from</span> <span class="nn">.rope</span> <span class="kn">import</span> <span class="n">RotaryPositionalEmbedding</span>
<span class="kn">from</span> <span class="nn">.softmax</span> <span class="kn">import</span> <span class="n">softmax</span>

<span class="k">class</span> <span class="nc">TransformerLM</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="s">"""
    A Transformer language model as described in Figure 1.

    Architecture (from bottom to top):
    1. Inputs (token IDs)
    2. Token Embedding
    3. Multiple Transformer Blocks (num_layers)
    4. Norm (RMSNorm)
    5. Linear (Output Embedding)
    6. Softmax
    7. Output Probabilities
    """</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">vocab_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">context_length</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">d_model</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">num_layers</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">num_heads</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">d_ff</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">rope_theta</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">10000.0</span><span class="p">,</span>
        <span class="n">device</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
        <span class="n">dtype</span><span class="o">=</span><span class="bp">None</span>
    <span class="p">):</span>
        <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">vocab_size</span> <span class="o">=</span> <span class="n">vocab_size</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">context_length</span> <span class="o">=</span> <span class="n">context_length</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">d_model</span> <span class="o">=</span> <span class="n">d_model</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">num_layers</span> <span class="o">=</span> <span class="n">num_layers</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">num_heads</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">d_ff</span> <span class="o">=</span> <span class="n">d_ff</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">rope_theta</span> <span class="o">=</span> <span class="n">rope_theta</span>

        <span class="c1"># Token Embedding
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">token_embeddings</span> <span class="o">=</span> <span class="n">Embedding</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>

        <span class="c1"># Transformer Blocks
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">layers</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">ModuleList</span><span class="p">([</span>
            <span class="n">TransformerBlock</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_layers</span><span class="p">)</span>
        <span class="p">])</span>

        <span class="c1"># Final layer norm
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">ln_f</span> <span class="o">=</span> <span class="n">RMSNorm</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>

        <span class="c1"># Output projection (Linear - Output Embedding)
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">lm_head</span> <span class="o">=</span> <span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>

        <span class="c1"># RoPE module
</span>        <span class="n">d_k</span> <span class="o">=</span> <span class="n">d_model</span> <span class="o">//</span> <span class="n">num_heads</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">rope</span> <span class="o">=</span> <span class="n">RotaryPositionalEmbedding</span><span class="p">(</span>
            <span class="n">theta</span><span class="o">=</span><span class="n">rope_theta</span><span class="p">,</span>
            <span class="n">d_k</span><span class="o">=</span><span class="n">d_k</span><span class="p">,</span>
            <span class="n">max_seq_len</span><span class="o">=</span><span class="n">context_length</span><span class="p">,</span>
            <span class="n">device</span><span class="o">=</span><span class="n">device</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_ids</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">apply_softmax</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span> <span class="o">=</span> <span class="n">input_ids</span><span class="p">.</span><span class="n">shape</span>

        <span class="c1"># Generate token positions for RoPE
</span>        <span class="n">token_positions</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="n">seq_len</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">input_ids</span><span class="p">.</span><span class="n">device</span><span class="p">).</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">).</span><span class="n">expand</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># Token Embedding
</span>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">token_embeddings</span><span class="p">(</span><span class="n">input_ids</span><span class="p">)</span>  <span class="c1"># (batch_size, seq_len, d_model)
</span>
        <span class="c1"># Apply Transformer blocks
</span>        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">layers</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">rope</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">rope</span><span class="p">,</span> <span class="n">token_positions</span><span class="o">=</span><span class="n">token_positions</span><span class="p">)</span>

        <span class="c1"># Final layer norm
</span>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">ln_f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># (batch_size, seq_len, d_model)
</span>
        <span class="c1"># Output projection
</span>        <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">lm_head</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># (batch_size, seq_len, vocab_size)
</span>
        <span class="c1"># Apply softmax if requested
</span>        <span class="k">if</span> <span class="n">apply_softmax</span><span class="p">:</span>
            <span class="c1"># Apply softmax over the vocabulary dimension (last dimension)
</span>            <span class="n">output_probs</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">output_probs</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">logits</span>
</code></pre></div></div>

<h3 id="how-everything-works-together">How Everything Works Together</h3>

<ol>
  <li><strong>Token Embeddings</strong> convert words to vectors the model can process</li>
  <li><strong>RMSNorm</strong> keeps values stable before major operations</li>
  <li><strong>Attention</strong> lets the model look at relevant previous words</li>
  <li><strong>RoPE</strong> tells attention where each word is positioned</li>
  <li><strong>SwiGLU</strong> processes information after attention</li>
  <li><strong>Linear layers</strong> transform dimensions throughout the model</li>
  <li><strong>Softmax</strong> converts final outputs to word probabilities</li>
  <li><strong>Transformer blocks</strong> stack these operations to learn complex patterns</li>
</ol>

<p>The beauty of this architecture is its simplicity and effectiveness. With these building blocks, we can now understand the core technology behind ChatGPT, GPT-4, and other large language models.</p>]]></content><author><name>[&quot;Han Yu&quot;]</name></author><category term="cs336" /><summary type="html"><![CDATA[Building a Transformer Language Model: A Complete Guide]]></summary></entry><entry><title type="html">Study Notes: Stanford CS336 Language Modeling from Scratch [4]</title><link href="http://localhost:4000/cs336/2025/08/10/cs336-gpt2-regex-for-pretokenization-explaind.html" rel="alternate" type="text/html" title="Study Notes: Stanford CS336 Language Modeling from Scratch [4]" /><published>2025-08-10T00:00:00-07:00</published><updated>2025-08-10T00:00:00-07:00</updated><id>http://localhost:4000/cs336/2025/08/10/cs336-gpt2-regex-for-pretokenization-explaind</id><content type="html" xml:base="http://localhost:4000/cs336/2025/08/10/cs336-gpt2-regex-for-pretokenization-explaind.html"><![CDATA[<h2 id="demystifying-gpt-2s-pre-tokenization-how-one-regex-pattern-handles-the-worlds-languages">Demystifying GPT-2‚Äôs Pre-Tokenization: How One Regex Pattern Handles the World‚Äôs Languages</h2>

<p>While working on <strong>Assignment 1</strong> of <em>Stanford‚Äôs CS336: Language Modeling from Scratch</em>, I came across a deceptively simple ‚Äî yet remarkably powerful ‚Äî regex pattern used in the pre-tokenization stage of the BPE algorithm.</p>

<p>I thought it would be worthwhile to share my notes and walk through how this single pattern can handle text from <strong>multiple languages, scripts, and symbol sets</strong> with precision.</p>

<p>You can find my full BPE assignment implementation here:</p>
<ul>
  <li><strong>BPE training algorithm:</strong> <a href="https://github.com/bearbearyu1223/assignment1-basics/blob/main/cs336_basics/bpe.py">bpe.py</a></li>
  <li><strong>Tokenizer class:</strong> <a href="https://github.com/bearbearyu1223/assignment1-basics/blob/main/cs336_basics/tokenizer.py">tokenizer.py</a></li>
</ul>

<hr />

<h3 id="-how-to-run-the-bpe-training-process">üîß How to Run the BPE Training Process</h3>

<ol>
  <li><strong>Clone the repository</strong>
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>git clone https://github.com/bearbearyu1223/assignment1-basics.git
<span class="nb">cd </span>assignment1-basics
</code></pre></div>    </div>
  </li>
  <li>
    <p><strong>Set up the local development environment</strong><br />
Follow the instructions in the <a href="https://github.com/bearbearyu1223/assignment1-basics/blob/main/developer_guide.md">developer_guide.md</a>.</p>
  </li>
  <li><strong>Run BPE training</strong>
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>uv run cs336_basics/train_bpe_example.py
</code></pre></div>    </div>
    <p>This will train a BPE tokenizer using the <code class="language-plaintext highlighter-rouge">TinyStoriesV2-GPT4-train.txt</code> dataset, with 10,000 vocabulary size and with special token <code class="language-plaintext highlighter-rouge">"&lt;|endoftext|&gt;"</code>.</p>
  </li>
</ol>

<hr />

<h3 id="-how-to-test-the-tokenizer">üß™ How to Test the Tokenizer</h3>

<p>Run:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>uv run pytest tests/test_train_bpe.py
</code></pre></div></div>

<p>This will validate the tokenizer‚Äôs functionality and ensure the pre-tokenization regex behaves as expected.</p>

<h3 id="-the-gpt-2-split-pattern">üìú The GPT-2 Split Pattern</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">regex</span>

<span class="n">GPT2_SPLIT_PATTERN</span> <span class="o">=</span> <span class="sa">r</span><span class="s">"""'(?:[sdmt]|ll|ve|re)| ?\p{L}+| ?\p{N}+| ?[^\s\p{L}\p{N}]+|\s+(?!\S)|\s+"""</span>
</code></pre></div></div>

<p>This single (but mighty) regex is responsible for splitting text into meaningful segments ‚Äî <strong>words, numbers, punctuation, symbols, whitespace</strong> ‚Äî in a way that is consistent across languages and scripts.</p>

<hr />

<h3 id="-pattern-breakdown">üîç Pattern Breakdown</h3>

<h4 id="1-contractions">1. Contractions</h4>
<pre><code class="language-regex">'(?:[sdmt]|ll|ve|re)
</code></pre>
<p>Matches <strong>common English contractions</strong> starting with an apostrophe:<br />
<code class="language-plaintext highlighter-rouge">'s, 'd, 'm, 't, 'll, 've, 're</code></p>

<p><strong>Examples:</strong></p>
<ul>
  <li><code class="language-plaintext highlighter-rouge">"don't"</code> ‚Üí <code class="language-plaintext highlighter-rouge">["don", "'t"]</code></li>
  <li><code class="language-plaintext highlighter-rouge">"we're"</code> ‚Üí <code class="language-plaintext highlighter-rouge">["we", "'re"]</code></li>
</ul>

<hr />

<h4 id="2-letters-any-language">2. Letters (Any Language)</h4>
<pre><code class="language-regex"> ?\p{L}+
</code></pre>
<p>Matches <strong>letters</strong> from <strong>any Unicode language</strong> (with optional leading space).</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">\p{L}</code> = Unicode ‚ÄúLetter‚Äù category</li>
  <li>Covers: English, Chinese, Arabic, accented characters, and more.</li>
</ul>

<p><strong>Examples:</strong></p>
<ul>
  <li><code class="language-plaintext highlighter-rouge">"hello world"</code> ‚Üí <code class="language-plaintext highlighter-rouge">["hello", " world"]</code></li>
  <li><code class="language-plaintext highlighter-rouge">"caf√© Âåó‰∫¨"</code> ‚Üí <code class="language-plaintext highlighter-rouge">["caf√©", " Âåó‰∫¨"]</code></li>
</ul>

<hr />

<h4 id="3-numbers-any-script">3. Numbers (Any Script)</h4>
<pre><code class="language-regex"> ?\p{N}+
</code></pre>
<p>Matches <strong>numbers</strong> from any writing system (with optional leading space).</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">\p{N}</code> = Unicode ‚ÄúNumber‚Äù category</li>
  <li>Covers: Arabic numerals (<code class="language-plaintext highlighter-rouge">0‚Äì9</code>), Roman numerals (<code class="language-plaintext highlighter-rouge">‚Ö†, ‚Ö°, ‚Ö¢</code>), Arabic-Indic (<code class="language-plaintext highlighter-rouge">Ÿ†Ÿ°Ÿ¢</code>), etc.</li>
</ul>

<p><strong>Examples:</strong></p>
<ul>
  <li><code class="language-plaintext highlighter-rouge">"I have 5 items"</code> ‚Üí <code class="language-plaintext highlighter-rouge">["I", " have", " 5", " items"]</code></li>
  <li><code class="language-plaintext highlighter-rouge">"‚Ö¢ winners"</code> ‚Üí <code class="language-plaintext highlighter-rouge">["‚Ö¢", " winners"]</code></li>
</ul>

<hr />

<h4 id="4-punctuation--symbols">4. Punctuation / Symbols</h4>
<pre><code class="language-regex"> ?[^\s\p{L}\p{N}]+
</code></pre>
<p>Matches <strong>punctuation or symbols</strong> (with optional leading space).</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">[^\s\p{L}\p{N}]</code> = NOT whitespace, NOT letters, NOT numbers</li>
  <li>Captures: <code class="language-plaintext highlighter-rouge">!@#$%^&amp;*()_+-=[]{}|;:'",./&lt;&gt;?</code></li>
</ul>

<p><strong>Examples:</strong></p>
<ul>
  <li><code class="language-plaintext highlighter-rouge">"Wow!!!"</code> ‚Üí <code class="language-plaintext highlighter-rouge">["Wow", "!!!"]</code></li>
  <li><code class="language-plaintext highlighter-rouge">" $100"</code> ‚Üí <code class="language-plaintext highlighter-rouge">[" $", "100"]</code></li>
</ul>

<hr />

<h4 id="5-trailing-whitespace">5. Trailing Whitespace</h4>
<pre><code class="language-regex">\s+(?!\S)
</code></pre>
<p>Matches <strong>whitespace at the end</strong> of text or before more whitespace.<br />
This ensures trailing spaces are preserved as tokens.</p>

<hr />

<h4 id="6-general-whitespace">6. General Whitespace</h4>
<pre><code class="language-regex">\s+
</code></pre>
<p>Matches <strong>any remaining whitespace</strong>.</p>

<hr />

<h3 id="-testing-the-pattern">üõ† Testing the Pattern</h3>

<p>Here‚Äôs a helper function to test how this regex splits different inputs.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">test_regex</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">description</span><span class="o">=</span><span class="s">""</span><span class="p">):</span>
    <span class="s">"""Test the regex pattern and display results clearly"""</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"</span><span class="se">\n</span><span class="si">{</span><span class="s">'='</span><span class="o">*</span><span class="mi">60</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"TEST: </span><span class="si">{</span><span class="n">description</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"INPUT: '</span><span class="si">{</span><span class="n">text</span><span class="si">}</span><span class="s">'"</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"</span><span class="si">{</span><span class="s">'='</span><span class="o">*</span><span class="mi">60</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>

    <span class="n">matches</span> <span class="o">=</span> <span class="n">regex</span><span class="p">.</span><span class="n">findall</span><span class="p">(</span><span class="n">GPT2_SPLIT_PATTERN</span><span class="p">,</span> <span class="n">text</span><span class="p">)</span>

    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"TOKENS (</span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">matches</span><span class="p">)</span><span class="si">}</span><span class="s">):"</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">token</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">matches</span><span class="p">,</span> <span class="mi">1</span><span class="p">):</span>
        <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"  </span><span class="si">{</span><span class="n">i</span><span class="si">:</span><span class="mi">2</span><span class="n">d</span><span class="si">}</span><span class="s">: </span><span class="si">{</span><span class="nb">repr</span><span class="p">(</span><span class="n">token</span><span class="p">)</span><span class="si">}</span><span class="s">"</span><span class="p">)</span> 

    <span class="n">reconstructed</span> <span class="o">=</span> <span class="s">''</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">matches</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"</span><span class="se">\n</span><span class="s">RECONSTRUCTION CHECK: </span><span class="si">{</span><span class="s">'‚úì PASS'</span> <span class="k">if</span> <span class="n">reconstructed</span> <span class="o">==</span> <span class="n">text</span> <span class="k">else</span> <span class="s">'‚úó FAIL'</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">matches</span>
</code></pre></div></div>

<hr />

<h3 id="-real-world-test-cases">üß™ Real-World Test Cases</h3>

<p>Below are <strong>diverse examples</strong> ‚Äî from contractions to Unicode scripts, punctuation to code.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">test_cases</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">(</span><span class="s">"I can't believe it's working!"</span><span class="p">,</span> <span class="s">"Basic contractions"</span><span class="p">),</span>
    <span class="p">(</span><span class="s">"You're right, they'll see we've done it."</span><span class="p">,</span> <span class="s">"Multiple contractions"</span><span class="p">),</span>
    <span class="p">(</span><span class="s">"Hello ‰∏ñÁïå! Caf√© fran√ßais üåç"</span><span class="p">,</span> <span class="s">"Unicode letters and emoji"</span><span class="p">),</span>
    <span class="p">(</span><span class="s">"I have 5 cats, Ÿß dogs, and ‚Ö¢ birds."</span><span class="p">,</span> <span class="s">"Various number systems"</span><span class="p">),</span>
    <span class="p">(</span><span class="s">"Wait... What?!? $100.50 (seriously)!!!"</span><span class="p">,</span> <span class="s">"Complex punctuation"</span><span class="p">),</span>
    <span class="p">(</span><span class="s">"  Multiple   spaces   everywhere  "</span><span class="p">,</span> <span class="s">"Multiple spaces"</span><span class="p">),</span>
    <span class="p">(</span><span class="s">"She's got $1,000.50 in caf√© ‚Ññ Ÿß... Amazing!!! üöÄ"</span><span class="p">,</span> <span class="s">"Complex mixed text"</span><span class="p">),</span>
    <span class="p">(</span><span class="s">"'s'd'm't'll've're"</span><span class="p">,</span> <span class="s">"Contraction edge cases"</span><span class="p">),</span>
    <span class="p">(</span><span class="s">"!@#$%^&amp;*()_+-=[]{}|;:"</span><span class="p">,.</span><span class="o">/&lt;&gt;</span><span class="err">?</span><span class="s">", "</span><span class="n">Pure</span> <span class="n">punctuation</span><span class="s">"),
    ("</span>   \<span class="n">t</span>\<span class="n">n</span>  <span class="s">", "</span><span class="n">Pure</span> <span class="n">whitespace</span><span class="s">"),
    ("", "</span><span class="n">Empty</span> <span class="n">string</span><span class="s">"),
    ("</span><span class="n">a</span> <span class="mi">1</span> <span class="err">!</span> <span class="s">'", "Single characters"),
    ("ÊàëÊúâ3Âè™Áå´ÔºåÂæàÂèØÁà±ÔºÅ", "Chinese with numbers"),
    ("ŸÖÿ±ÿ≠ÿ®ÿß ÿ®ÿßŸÑÿπÿßŸÑŸÖ Ÿ°Ÿ¢Ÿ£", "Arabic text with numbers"),
    ("def hello_world(): return '</span><span class="n">Hello</span><span class="p">,</span> <span class="n">World</span><span class="err">!</span><span class="s">'", "Code-like text"),
    ("Visit https://example.com or email test@domain.co.uk", "URLs and emails"),
]

for text, description in test_cases:
    test_regex(text, description)
</span></code></pre></div></div>

<p>Running these cases produces token lists that <strong>perfectly reconstruct the original text</strong>, see the test results below:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    ============================================================
    TEST: Basic contractions
    INPUT: 'I can't believe it's working!'
    ============================================================
    TOKENS (8):
       1: 'I'
       2: ' can'
       3: "'t"
       4: ' believe'
       5: ' it'
       6: "'s"
       7: ' working'
       8: '!'
    
    RECONSTRUCTION CHECK: ‚úì PASS
    
    ============================================================
    TEST: Multiple contractions
    INPUT: 'You're right, they'll see we've done it.'
    ============================================================
    TOKENS (12):
       1: 'You'
       2: "'re"
       3: ' right'
       4: ','
       5: ' they'
       6: "'ll"
       7: ' see'
       8: ' we'
       9: "'ve"
      10: ' done'
      11: ' it'
      12: '.'
    
    RECONSTRUCTION CHECK: ‚úì PASS
    
    ============================================================
    TEST: Unicode letters and emoji
    INPUT: 'Hello ‰∏ñÁïå! Caf√© fran√ßais üåç'
    ============================================================
    TOKENS (6):
       1: 'Hello'
       2: ' ‰∏ñÁïå'
       3: '!'
       4: ' Caf√©'
       5: ' fran√ßais'
       6: ' üåç'
    
    RECONSTRUCTION CHECK: ‚úì PASS
    
    ============================================================
    TEST: Various number systems
    INPUT: 'I have 5 cats, Ÿß dogs, and ‚Ö¢ birds.'
    ============================================================
    TOKENS (12):
       1: 'I'
       2: ' have'
       3: ' 5'
       4: ' cats'
       5: ','
       6: ' Ÿß'
       7: ' dogs'
       8: ','
       9: ' and'
      10: ' ‚Ö¢'
      11: ' birds'
      12: '.'
    
    RECONSTRUCTION CHECK: ‚úì PASS
    
    ============================================================
    TEST: Complex punctuation
    INPUT: 'Wait... What?!? $100.50 (seriously)!!!'
    ============================================================
    TOKENS (11):
       1: 'Wait'
       2: '...'
       3: ' What'
       4: '?!?'
       5: ' $'
       6: '100'
       7: '.'
       8: '50'
       9: ' ('
      10: 'seriously'
      11: ')!!!'
    
    RECONSTRUCTION CHECK: ‚úì PASS
    
    ============================================================
    TEST: Multiple spaces
    INPUT: '  Multiple   spaces   everywhere  '
    ============================================================
    TOKENS (7):
       1: ' '
       2: ' Multiple'
       3: '  '
       4: ' spaces'
       5: '  '
       6: ' everywhere'
       7: '  '
    
    RECONSTRUCTION CHECK: ‚úì PASS
    
    ============================================================
    TEST: Complex mixed text
    INPUT: 'She's got $1,000.50 in caf√© ‚Ññ Ÿß... Amazing!!! üöÄ'
    ============================================================
    TOKENS (17):
       1: 'She'
       2: "'s"
       3: ' got'
       4: ' $'
       5: '1'
       6: ','
       7: '000'
       8: '.'
       9: '50'
      10: ' in'
      11: ' caf√©'
      12: ' ‚Ññ'
      13: ' Ÿß'
      14: '...'
      15: ' Amazing'
      16: '!!!'
      17: ' üöÄ'
    
    RECONSTRUCTION CHECK: ‚úì PASS
    
    ============================================================
    TEST: Contraction edge cases
    INPUT: ''s'd'm't'll've're'
    ============================================================
    TOKENS (7):
       1: "'s"
       2: "'d"
       3: "'m"
       4: "'t"
       5: "'ll"
       6: "'ve"
       7: "'re"
    
    RECONSTRUCTION CHECK: ‚úì PASS
    
    ============================================================
    TEST: Pure punctuation
    INPUT: '!@#$%^&amp;*()_+-=[]{}|;:",./&lt;&gt;?'
    ============================================================
    TOKENS (1):
       1: '!@#$%^&amp;*()_+-=[]{}|;:",./&lt;&gt;?'
    
    RECONSTRUCTION CHECK: ‚úì PASS
    
    ============================================================
    TEST: Pure whitespace
    INPUT: '   	
      '
    ============================================================
    TOKENS (1):
       1: '   \t\n  '
    
    RECONSTRUCTION CHECK: ‚úì PASS
    
    ============================================================
    TEST: Empty string
    INPUT: ''
    ============================================================
    TOKENS (0):
    
    RECONSTRUCTION CHECK: ‚úì PASS
    
    ============================================================
    TEST: Single characters
    INPUT: 'a 1 ! ''
    ============================================================
    TOKENS (4):
       1: 'a'
       2: ' 1'
       3: ' !'
       4: " '"
    
    RECONSTRUCTION CHECK: ‚úì PASS
    
    ============================================================
    TEST: Chinese with numbers
    INPUT: 'ÊàëÊúâ3Âè™Áå´ÔºåÂæàÂèØÁà±ÔºÅ'
    ============================================================
    TOKENS (6):
       1: 'ÊàëÊúâ'
       2: '3'
       3: 'Âè™Áå´'
       4: 'Ôºå'
       5: 'ÂæàÂèØÁà±'
       6: 'ÔºÅ'
    
    RECONSTRUCTION CHECK: ‚úì PASS
    
    ============================================================
    TEST: Arabic text with numbers
    INPUT: 'ŸÖÿ±ÿ≠ÿ®ÿß ÿ®ÿßŸÑÿπÿßŸÑŸÖ Ÿ°Ÿ¢Ÿ£'
    ============================================================
    TOKENS (3):
       1: 'ŸÖÿ±ÿ≠ÿ®ÿß'
       2: ' ÿ®ÿßŸÑÿπÿßŸÑŸÖ'
       3: ' Ÿ°Ÿ¢Ÿ£'
    
    RECONSTRUCTION CHECK: ‚úì PASS
    
    ============================================================
    TEST: Code-like text
    INPUT: 'def hello_world(): return 'Hello, World!''
    ============================================================
    TOKENS (11):
       1: 'def'
       2: ' hello'
       3: '_'
       4: 'world'
       5: '():'
       6: ' return'
       7: " '"
       8: 'Hello'
       9: ','
      10: ' World'
      11: "!'"
    
    RECONSTRUCTION CHECK: ‚úì PASS
    
    ============================================================
    TEST: URLs and emails
    INPUT: 'Visit https://example.com or email test@domain.co.uk'
    ============================================================
    TOKENS (15):
       1: 'Visit'
       2: ' https'
       3: '://'
       4: 'example'
       5: '.'
       6: 'com'
       7: ' or'
       8: ' email'
       9: ' test'
      10: '@'
      11: 'domain'
      12: '.'
      13: 'co'
      14: '.'
      15: 'uk'
    
    RECONSTRUCTION CHECK: ‚úì PASS
</code></pre></div></div>

<hr />

<h3 id="-why-this-matters">üí° Why This Matters</h3>

<p>BPE pre-tokenization <strong>sets the stage</strong> for the tokenizer‚Äôs merge rules to apply. A well-designed split pattern ensures:</p>

<ul>
  <li><strong>Language independence</strong> ‚Äî Works with English, Arabic, Chinese, emoji, etc.</li>
  <li><strong>Symbol awareness</strong> ‚Äî Keeps punctuation and symbols intact.</li>
  <li><strong>Whitespace fidelity</strong> ‚Äî Preserves exact spacing for reversible tokenization.</li>
  <li><strong>Downstream accuracy</strong> ‚Äî Reduces surprises during model training or inference.</li>
</ul>

<hr />

<h3 id="-takeaways">üöÄ Takeaways</h3>

<ul>
  <li>This regex is a <strong>core building block</strong> of GPT-2‚Äôs tokenization process.</li>
  <li>It‚Äôs <strong>language-agnostic</strong>, <strong>Unicode-friendly</strong>, and <strong>precise</strong> in splitting.</li>
  <li>Understanding it helps when <strong>building custom tokenizers</strong> or adapting GPT-2 BPE to new domains.</li>
</ul>

<p>If you‚Äôre working with <strong>LLMs, tokenization, or multilingual NLP</strong>, knowing the details behind this pattern will help you <strong>debug</strong>, <strong>customize</strong>, and <strong>optimize</strong> your preprocessing pipeline.</p>]]></content><author><name>[&quot;Han Yu&quot;]</name></author><category term="cs336" /><summary type="html"><![CDATA[Demystifying GPT-2‚Äôs Pre-Tokenization: How One Regex Pattern Handles the World‚Äôs Languages]]></summary></entry><entry><title type="html">Study Notes: Stanford CS336 Language Modeling from Scratch [3]</title><link href="http://localhost:4000/cs336/2025/07/26/cs336-note-train-bpe-tinystories.html" rel="alternate" type="text/html" title="Study Notes: Stanford CS336 Language Modeling from Scratch [3]" /><published>2025-07-26T00:00:00-07:00</published><updated>2025-07-26T00:00:00-07:00</updated><id>http://localhost:4000/cs336/2025/07/26/cs336-note-train-bpe-tinystories</id><content type="html" xml:base="http://localhost:4000/cs336/2025/07/26/cs336-note-train-bpe-tinystories.html"><![CDATA[<h1 id="building-a-bpe-tokenizer-from-scratch-train-the-tokenizer-using-tinystories-dataset">Building a BPE Tokenizer from Scratch: Train the Tokenizer using TinyStories Dataset</h1>

<p>Ever wondered how modern language models like GPT break down text into tokens? In this note, I will share how to build a Byte Pair Encoding (BPE) tokenizer from scratch and train it on the <a href="https://arxiv.org/abs/2305.07759">TinyStories Dataset</a>. We will see how BPE achieves impressive compression ratios.</p>

<h2 id="what-is-bpe-tokenization">What is BPE Tokenization?</h2>

<p>Byte Pair Encoding (BPE) is a compression algorithm that‚Äôs become the backbone of modern tokenization. Here‚Äôs how it works:</p>

<ol>
  <li><strong>Start with bytes</strong>: Every character becomes its byte representation (0-255)</li>
  <li><strong>Find frequent pairs</strong>: Look for the most common pair of adjacent tokens</li>
  <li><strong>Merge and repeat</strong>: Replace the most frequent pair with a new token, then repeat</li>
</ol>

<h3 id="a-simple-example">A Simple Example</h3>

<p>Let‚Äôs say we have the word ‚Äúhello‚Äù appearing many times in our text:</p>
<ul>
  <li>Initially: <code class="language-plaintext highlighter-rouge">h-e-l-l-o</code> (5 tokens)</li>
  <li>If ‚Äúl-l‚Äù is the most frequent pair, merge it: <code class="language-plaintext highlighter-rouge">h-e-ll-o</code> (4 tokens)</li>
  <li>If ‚Äúe-ll‚Äù becomes frequent, merge it: <code class="language-plaintext highlighter-rouge">h-ell-o</code> (3 tokens)</li>
</ul>

<p>This process creates a vocabulary that efficiently represents common patterns in your text. Check out <a href="https://bearbearyu1223.github.io/cs336/2025/07/22/cs336-note-simple-bpe.html">my previous post</a> for a brief introduction.</p>

<h2 id="the-tinystories-dataset">The TinyStories Dataset</h2>

<p>We‚Äôll train our tokenizer on <a href="https://arxiv.org/abs/2305.07759">TinyStories</a>, a fascinating dataset of short stories written using only words that 3-4 year olds typically understand. These stories were generated by GPT-3.5 and GPT-4, making them perfect for experimenting with tokenization.</p>

<h3 id="downloading-the-data">Downloading the Data</h3>

<p>First, let‚Äôs download the TinyStories froom Huggingface:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="err">!</span><span class="n">mkdir</span> <span class="o">-</span><span class="n">p</span> <span class="n">data</span>
<span class="err">!</span><span class="n">cd</span> <span class="n">data</span>

<span class="err">!</span><span class="n">wget</span> <span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">huggingface</span><span class="p">.</span><span class="n">co</span><span class="o">/</span><span class="n">datasets</span><span class="o">/</span><span class="n">roneneldan</span><span class="o">/</span><span class="n">TinyStories</span><span class="o">/</span><span class="n">resolve</span><span class="o">/</span><span class="n">main</span><span class="o">/</span><span class="n">TinyStoriesV2</span><span class="o">-</span><span class="n">GPT4</span><span class="o">-</span><span class="n">train</span><span class="p">.</span><span class="n">txt</span>
<span class="err">!</span><span class="n">wget</span> <span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">huggingface</span><span class="p">.</span><span class="n">co</span><span class="o">/</span><span class="n">datasets</span><span class="o">/</span><span class="n">roneneldan</span><span class="o">/</span><span class="n">TinyStories</span><span class="o">/</span><span class="n">resolve</span><span class="o">/</span><span class="n">main</span><span class="o">/</span><span class="n">TinyStoriesV2</span><span class="o">-</span><span class="n">GPT4</span><span class="o">-</span><span class="n">valid</span><span class="p">.</span><span class="n">txt</span>

<span class="err">!</span><span class="n">wget</span> <span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">huggingface</span><span class="p">.</span><span class="n">co</span><span class="o">/</span><span class="n">datasets</span><span class="o">/</span><span class="n">stanford</span><span class="o">-</span><span class="n">cs336</span><span class="o">/</span><span class="n">owt</span><span class="o">-</span><span class="n">sample</span><span class="o">/</span><span class="n">resolve</span><span class="o">/</span><span class="n">main</span><span class="o">/</span><span class="n">owt_train</span><span class="p">.</span><span class="n">txt</span><span class="p">.</span><span class="n">gz</span>
<span class="err">!</span><span class="n">gunzip</span> <span class="n">owt_train</span><span class="p">.</span><span class="n">txt</span><span class="p">.</span><span class="n">gz</span>
<span class="err">!</span><span class="n">wget</span> <span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">huggingface</span><span class="p">.</span><span class="n">co</span><span class="o">/</span><span class="n">datasets</span><span class="o">/</span><span class="n">stanford</span><span class="o">-</span><span class="n">cs336</span><span class="o">/</span><span class="n">owt</span><span class="o">-</span><span class="n">sample</span><span class="o">/</span><span class="n">resolve</span><span class="o">/</span><span class="n">main</span><span class="o">/</span><span class="n">owt_valid</span><span class="p">.</span><span class="n">txt</span><span class="p">.</span><span class="n">gz</span>
<span class="err">!</span><span class="n">gunzip</span> <span class="n">owt_valid</span><span class="p">.</span><span class="n">txt</span><span class="p">.</span><span class="n">gz</span>

<span class="err">!</span><span class="n">cd</span> <span class="p">..</span>
</code></pre></div></div>

<h2 id="challenge-parallelizing-pre-tokenization">Challenge: Parallelizing Pre-tokenization</h2>

<p>The TinyStories dataset is big (over 2GB), which presents a challenge for tokenizer training. We need to:</p>
<ol>
  <li>Process the file in parallel for speed</li>
  <li>Ensure we don‚Äôt split tokens incorrectly at chunk boundaries</li>
</ol>

<h3 id="solution-smart-chunking-with-special-tokens">Solution: Smart Chunking with Special Tokens</h3>

<p>Our solution uses special tokens (like <code class="language-plaintext highlighter-rouge">&lt;|endoftext|&gt;</code>) as natural boundaries for splitting the file.</p>

<p><strong>Simple Example</strong>: Let‚Äôs say we have a text file containing: ‚ÄúHello<code class="language-plaintext highlighter-rouge">&lt;SPLIT&gt;</code>World<code class="language-plaintext highlighter-rouge">&lt;SPLIT&gt;</code>How<code class="language-plaintext highlighter-rouge">&lt;SPLIT&gt;</code>Are<code class="language-plaintext highlighter-rouge">&lt;SPLIT&gt;</code>You‚Äù, special split token is <code class="language-plaintext highlighter-rouge">&lt;SPLIT&gt;</code>, and we want to divide the text into 3 chunks.</p>

<p>Here‚Äôs one implementation for intelligent file chunking as shared in the <a href="https://github.com/stanford-cs336/assignment1-basics/blob/main/cs336_basics/pretokenization_example.py">cs336 lecture notes</a>:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">BinaryIO</span>

<span class="k">def</span> <span class="nf">find_chunk_boundaries</span><span class="p">(</span> <span class="nb">file</span><span class="p">:</span> <span class="n">BinaryIO</span><span class="p">,</span>
      <span class="n">desired_num_chunks</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
      <span class="n">split_special_token</span><span class="p">:</span> <span class="nb">bytes</span><span class="p">)</span><span class="o">-&gt;</span><span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">]:</span>
  <span class="s">"""
  Chunk the file into parts that can be counted independently.
  May return fewer chunks if the boundaries end up overlapping.
  """</span>
  <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">split_special_token</span><span class="p">,</span> <span class="nb">bytes</span><span class="p">),(</span>
      <span class="s">"Must represent special token as a bytestring"</span>
  <span class="p">)</span>

  <span class="c1"># Get total file size in bytes
</span>  <span class="nb">file</span><span class="p">.</span><span class="n">seek</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">os</span><span class="p">.</span><span class="n">SEEK_END</span><span class="p">)</span>
  <span class="n">file_size</span> <span class="o">=</span> <span class="nb">file</span><span class="p">.</span><span class="n">tell</span><span class="p">()</span>
  <span class="nb">file</span><span class="p">.</span><span class="n">seek</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

  <span class="n">chunk_size</span> <span class="o">=</span> <span class="n">file_size</span> <span class="o">//</span> <span class="n">desired_num_chunks</span>

  <span class="c1"># Initial guesses for chunk boundary locations, uniformly spaced
</span>  <span class="c1"># Chunks start on previous index, don't include last index
</span>  <span class="n">chunk_boundaries</span> <span class="o">=</span> <span class="p">[</span><span class="n">i</span> <span class="o">*</span> <span class="n">chunk_size</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">desired_num_chunks</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)]</span>
  <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Initial guess of the chunk boundaries: </span><span class="si">{</span><span class="n">chunk_boundaries</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
  <span class="n">chunk_boundaries</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">file_size</span>

  <span class="n">mini_chunk_size</span> <span class="o">=</span> <span class="mi">4096</span>  <span class="c1"># Read ahead by 4k bytes at a time
</span>
  <span class="k">for</span> <span class="n">bi</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">chunk_boundaries</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>
      <span class="n">initial_position</span> <span class="o">=</span> <span class="n">chunk_boundaries</span><span class="p">[</span><span class="n">bi</span><span class="p">]</span>
      <span class="nb">file</span><span class="p">.</span><span class="n">seek</span><span class="p">(</span><span class="n">initial_position</span><span class="p">)</span>  <span class="c1"># Start at boundary guess
</span>      <span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
          <span class="n">mini_chunk</span> <span class="o">=</span> <span class="nb">file</span><span class="p">.</span><span class="n">read</span><span class="p">(</span><span class="n">mini_chunk_size</span><span class="p">)</span>  <span class="c1"># Read a mini chunk
</span>
          <span class="c1"># If EOF, this boundary should be at the end of the file
</span>          <span class="k">if</span> <span class="n">mini_chunk</span> <span class="o">==</span> <span class="sa">b</span><span class="s">""</span><span class="p">:</span>
              <span class="n">chunk_boundaries</span><span class="p">[</span><span class="n">bi</span><span class="p">]</span> <span class="o">=</span> <span class="n">file_size</span>
              <span class="k">break</span>

          <span class="c1"># Find the special token in the mini chunk
</span>          <span class="n">found_at</span> <span class="o">=</span> <span class="n">mini_chunk</span><span class="p">.</span><span class="n">find</span><span class="p">(</span><span class="n">split_special_token</span><span class="p">)</span>
          <span class="k">if</span> <span class="n">found_at</span> <span class="o">!=</span> <span class="o">-</span><span class="mi">1</span><span class="p">:</span>
              <span class="n">chunk_boundaries</span><span class="p">[</span><span class="n">bi</span><span class="p">]</span> <span class="o">=</span> <span class="n">initial_position</span> <span class="o">+</span> <span class="n">found_at</span>
              <span class="k">break</span>
          <span class="n">initial_position</span> <span class="o">+=</span> <span class="n">mini_chunk_size</span>

  <span class="c1"># Make sure all boundaries are unique, but might be fewer than desired_num_chunks
</span>  <span class="k">return</span> <span class="nb">sorted</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">chunk_boundaries</span><span class="p">))</span>
</code></pre></div></div>

<h3 id="testing-our-chunking-algorithm">Testing Our Chunking Algorithm</h3>

<p>Let‚Äôs see how this works with a concrete example:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">io</span>
<span class="k">def</span> <span class="nf">demonstrate_chunk_boundaries</span><span class="p">():</span>
    <span class="s">"""Demonstrate how to use find_chunk_boundaries with a practical example."""</span>

    <span class="c1"># Create sample data - our example text
</span>    <span class="n">sample_text</span> <span class="o">=</span> <span class="s">"Hello&lt;SPLIT&gt;World&lt;SPLIT&gt;How&lt;SPLIT&gt;Are&lt;SPLIT&gt;You"</span>
    <span class="n">sample_bytes</span> <span class="o">=</span> <span class="n">sample_text</span><span class="p">.</span><span class="n">encode</span><span class="p">(</span><span class="s">'utf-8'</span><span class="p">)</span>

    <span class="k">print</span><span class="p">(</span><span class="s">"=== Original Data ==="</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Text: </span><span class="si">{</span><span class="n">sample_text</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Bytes: </span><span class="si">{</span><span class="n">sample_bytes</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Total size: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">sample_bytes</span><span class="p">)</span><span class="si">}</span><span class="s"> bytes"</span><span class="p">)</span>
    <span class="k">print</span><span class="p">()</span>

    <span class="c1"># Create a file-like object from our sample data
</span>    <span class="n">file_obj</span> <span class="o">=</span> <span class="n">io</span><span class="p">.</span><span class="n">BytesIO</span><span class="p">(</span><span class="n">sample_bytes</span><span class="p">)</span>

    <span class="c1"># Define our split token
</span>    <span class="n">split_token</span> <span class="o">=</span> <span class="sa">b</span><span class="s">"&lt;SPLIT&gt;"</span>
    <span class="n">desired_chunks</span> <span class="o">=</span> <span class="mi">3</span>

    <span class="k">print</span><span class="p">(</span><span class="s">"=== Finding Chunk Boundaries ==="</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Desired number of chunks: </span><span class="si">{</span><span class="n">desired_chunks</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Split token: </span><span class="si">{</span><span class="n">split_token</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
    <span class="k">print</span><span class="p">()</span>

    <span class="c1"># Find the chunk boundaries
</span>    <span class="n">boundaries</span> <span class="o">=</span> <span class="n">find_chunk_boundaries</span><span class="p">(</span><span class="n">file_obj</span><span class="p">,</span> <span class="n">desired_chunks</span><span class="p">,</span> <span class="n">split_token</span><span class="p">)</span>

    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Final boundaries: </span><span class="si">{</span><span class="n">boundaries</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Number of chunks created: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">boundaries</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
    <span class="k">print</span><span class="p">()</span>

    <span class="c1"># Demonstrate how to use the boundaries to read chunks
</span>    <span class="k">print</span><span class="p">(</span><span class="s">"=== Reading Chunks ==="</span><span class="p">)</span>
    <span class="n">file_obj</span><span class="p">.</span><span class="n">seek</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># Reset file pointer
</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">boundaries</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>
        <span class="n">start_pos</span> <span class="o">=</span> <span class="n">boundaries</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
        <span class="n">end_pos</span> <span class="o">=</span> <span class="n">boundaries</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span>
        <span class="n">chunk_size</span> <span class="o">=</span> <span class="n">end_pos</span> <span class="o">-</span> <span class="n">start_pos</span>

        <span class="c1"># Read the chunk
</span>        <span class="n">file_obj</span><span class="p">.</span><span class="n">seek</span><span class="p">(</span><span class="n">start_pos</span><span class="p">)</span>
        <span class="n">chunk_data</span> <span class="o">=</span> <span class="n">file_obj</span><span class="p">.</span><span class="n">read</span><span class="p">(</span><span class="n">chunk_size</span><span class="p">)</span>
        <span class="n">chunk_text</span> <span class="o">=</span> <span class="n">chunk_data</span><span class="p">.</span><span class="n">decode</span><span class="p">(</span><span class="s">'utf-8'</span><span class="p">)</span>

        <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Chunk </span><span class="si">{</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="si">}</span><span class="s">:"</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"  Position: bytes </span><span class="si">{</span><span class="n">start_pos</span><span class="si">}</span><span class="s">-</span><span class="si">{</span><span class="n">end_pos</span><span class="o">-</span><span class="mi">1</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"  Size: </span><span class="si">{</span><span class="n">chunk_size</span><span class="si">}</span><span class="s"> bytes"</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"  Content: '</span><span class="si">{</span><span class="n">chunk_text</span><span class="si">}</span><span class="s">'"</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"  Raw bytes: </span><span class="si">{</span><span class="n">chunk_data</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
        <span class="k">print</span><span class="p">()</span>
</code></pre></div></div>

<p>Running this demonstration:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">demonstrate_chunk_boundaries</span><span class="p">()</span>
</code></pre></div></div>

<p><strong>Output:</strong></p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>=== Original Data ===
Text: Hello&lt;SPLIT&gt;World&lt;SPLIT&gt;How&lt;SPLIT&gt;Are&lt;SPLIT&gt;You
Bytes: b'Hello&lt;SPLIT&gt;World&lt;SPLIT&gt;How&lt;SPLIT&gt;Are&lt;SPLIT&gt;You'
Total size: 47 bytes

=== Finding Chunk Boundaries ===
Desired number of chunks: 3
Split token: b'&lt;SPLIT&gt;'

Initial guess of the chunk boundaries: [0, 15, 30, 45]
Final boundaries: [0, 17, 37, 47]
Number of chunks created: 3

=== Reading Chunks ===
Chunk 1:
  Position: bytes 0-16
  Size: 17 bytes
  Content: 'Hello&lt;SPLIT&gt;World'
  Raw bytes: b'Hello&lt;SPLIT&gt;World'

Chunk 2:
  Position: bytes 17-36
  Size: 20 bytes
  Content: '&lt;SPLIT&gt;How&lt;SPLIT&gt;Are'
  Raw bytes: b'&lt;SPLIT&gt;How&lt;SPLIT&gt;Are'

Chunk 3:
  Position: bytes 37-46
  Size: 10 bytes
  Content: '&lt;SPLIT&gt;You'
  Raw bytes: b'&lt;SPLIT&gt;You'
</code></pre></div></div>

<p>Notice how the algorithm automatically adjusted the boundaries to align with <code class="language-plaintext highlighter-rouge">&lt;SPLIT&gt;</code> tokens, ensuring clean chunk separation.</p>

<h2 id="bpe-training-implementation">BPE Training Implementation</h2>

<p>Now implement the core BPE training algorithm. The implementation shared here handles parallel processing, special tokens, and efficient pair counting.</p>

<h3 id="core-training-function">Core Training Function</h3>

<p>Here‚Äôs is my complete BPE training implementation:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">re</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">multiprocessing</span> <span class="k">as</span> <span class="n">mp</span>
<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">defaultdict</span><span class="p">,</span> <span class="n">Counter</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">List</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">,</span> <span class="n">BinaryIO</span>

<span class="c1"># Simplified GPT-2-style regex pattern for pre-tokenization (using standard re module)
</span><span class="n">GPT2_SPLIT_PATTERN</span> <span class="o">=</span> <span class="sa">r</span><span class="s">"""'(?:[sdmt]|ll|ve|re)| ?[a-zA-Z√Ä-√ø]+| ?[0-9]+| ?[^\s\w]+|\s+(?!\S)|\s+"""</span>

<span class="k">def</span> <span class="nf">process_chunk</span><span class="p">(</span><span class="n">args</span><span class="p">):</span>
    <span class="s">"""Process a chunk of the file and return word counts."""</span>
    <span class="n">start</span><span class="p">,</span> <span class="n">end</span><span class="p">,</span> <span class="n">input_path</span><span class="p">,</span> <span class="n">special_tokens</span> <span class="o">=</span> <span class="n">args</span>
    <span class="n">word_counts</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>

    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">input_path</span><span class="p">,</span> <span class="s">'rb'</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="n">f</span><span class="p">.</span><span class="n">seek</span><span class="p">(</span><span class="n">start</span><span class="p">)</span>
        <span class="n">chunk</span> <span class="o">=</span> <span class="n">f</span><span class="p">.</span><span class="n">read</span><span class="p">(</span><span class="n">end</span> <span class="o">-</span> <span class="n">start</span><span class="p">).</span><span class="n">decode</span><span class="p">(</span><span class="s">'utf-8'</span><span class="p">,</span> <span class="n">errors</span><span class="o">=</span><span class="s">'ignore'</span><span class="p">)</span>

        <span class="c1"># Split on special tokens to prevent merging across boundaries
</span>        <span class="k">if</span> <span class="n">special_tokens</span><span class="p">:</span>
            <span class="n">pattern</span> <span class="o">=</span> <span class="s">'|'</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">re</span><span class="p">.</span><span class="n">escape</span><span class="p">(</span><span class="n">token</span><span class="p">)</span> <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">special_tokens</span><span class="p">)</span>
            <span class="n">text_segments</span> <span class="o">=</span> <span class="n">re</span><span class="p">.</span><span class="n">split</span><span class="p">(</span><span class="sa">f</span><span class="s">'(</span><span class="si">{</span><span class="n">pattern</span><span class="si">}</span><span class="s">)'</span><span class="p">,</span> <span class="n">chunk</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">text_segments</span> <span class="o">=</span> <span class="p">[</span><span class="n">chunk</span><span class="p">]</span>

        <span class="k">for</span> <span class="n">segment</span> <span class="ow">in</span> <span class="n">text_segments</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">segment</span> <span class="ow">in</span> <span class="n">special_tokens</span><span class="p">:</span>
                <span class="k">continue</span>  <span class="c1"># Skip special tokens during counting
</span>
            <span class="c1"># Apply GPT-2 regex pattern
</span>            <span class="k">for</span> <span class="n">match</span> <span class="ow">in</span> <span class="n">re</span><span class="p">.</span><span class="n">finditer</span><span class="p">(</span><span class="n">GPT2_SPLIT_PATTERN</span><span class="p">,</span> <span class="n">segment</span><span class="p">):</span>
                <span class="n">token_text</span> <span class="o">=</span> <span class="n">match</span><span class="p">.</span><span class="n">group</span><span class="p">()</span>
                <span class="n">token_bytes</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">token_text</span><span class="p">.</span><span class="n">encode</span><span class="p">(</span><span class="s">'utf-8'</span><span class="p">))</span>
                <span class="n">word_counts</span><span class="p">[</span><span class="n">token_bytes</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>

    <span class="k">return</span> <span class="n">word_counts</span>


<span class="k">def</span> <span class="nf">train_bpe_tokenizer</span><span class="p">(</span><span class="n">input_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">special_tokens</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="n">verbose</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="nb">dict</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">bytes</span><span class="p">],</span> <span class="nb">list</span><span class="p">[</span><span class="nb">tuple</span><span class="p">[</span><span class="nb">bytes</span><span class="p">,</span> <span class="nb">bytes</span><span class="p">]]]:</span>
    <span class="s">"""
    Train a byte-level Byte Pair Encoding (BPE) tokenizer from a text file.

    Args:
        input_path: Path to the input text file containing training data
        vocab_size: Maximum size of the final vocabulary (includes initial bytes + special tokens + merges)
        special_tokens: List of special token strings to include in vocabulary
        verbose: Whether to print training progress information

    Returns:
        vocab: Complete tokenizer vocabulary mapping token IDs to byte sequences
        merges: Ordered list of BPE merge operations performed during training
    """</span>
    <span class="kn">import</span> <span class="nn">time</span>

    <span class="c1"># Initialize vocabulary with bytes 0-255
</span>    <span class="n">vocab</span> <span class="o">=</span> <span class="p">{</span><span class="n">i</span><span class="p">:</span> <span class="nb">bytes</span><span class="p">([</span><span class="n">i</span><span class="p">])</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">256</span><span class="p">)}</span>
    <span class="n">next_id</span> <span class="o">=</span> <span class="mi">256</span>

    <span class="c1"># Add special tokens to vocabulary
</span>    <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">special_tokens</span><span class="p">:</span>
        <span class="n">token_bytes</span> <span class="o">=</span> <span class="n">token</span><span class="p">.</span><span class="n">encode</span><span class="p">(</span><span class="s">'utf-8'</span><span class="p">)</span>
        <span class="n">vocab</span><span class="p">[</span><span class="n">next_id</span><span class="p">]</span> <span class="o">=</span> <span class="n">token_bytes</span>
        <span class="n">next_id</span> <span class="o">+=</span> <span class="mi">1</span>

    <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
        <span class="k">print</span><span class="p">(</span><span class="s">"Step 1: Setting up parallel processing..."</span><span class="p">)</span>

    <span class="c1"># Get chunk boundaries for multiprocessing
</span>    <span class="n">num_processes</span> <span class="o">=</span> <span class="n">mp</span><span class="p">.</span><span class="n">cpu_count</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
        <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Using </span><span class="si">{</span><span class="n">num_processes</span><span class="si">}</span><span class="s"> processes for parallel tokenization"</span><span class="p">)</span>

    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">input_path</span><span class="p">,</span> <span class="s">'rb'</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">special_tokens</span><span class="p">:</span>
            <span class="c1"># Use first special token for chunking boundaries
</span>            <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
                <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Finding chunk boundaries aligned with special token: </span><span class="si">{</span><span class="n">special_tokens</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
            <span class="n">boundaries</span> <span class="o">=</span> <span class="n">find_chunk_boundaries</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">num_processes</span><span class="p">,</span> <span class="n">special_tokens</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">encode</span><span class="p">(</span><span class="s">'utf-8'</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Use simple chunking without special token alignment
</span>            <span class="n">f</span><span class="p">.</span><span class="n">seek</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">os</span><span class="p">.</span><span class="n">SEEK_END</span><span class="p">)</span>
            <span class="n">file_size</span> <span class="o">=</span> <span class="n">f</span><span class="p">.</span><span class="n">tell</span><span class="p">()</span>
            <span class="n">chunk_size</span> <span class="o">=</span> <span class="n">file_size</span> <span class="o">//</span> <span class="n">num_processes</span>
            <span class="n">boundaries</span> <span class="o">=</span> <span class="p">[</span><span class="n">i</span> <span class="o">*</span> <span class="n">chunk_size</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_processes</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)]</span>
            <span class="n">boundaries</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">file_size</span>
            <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
                <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"File size: </span><span class="si">{</span><span class="n">file_size</span><span class="si">:</span><span class="p">,</span><span class="si">}</span><span class="s"> bytes, chunk size: </span><span class="si">{</span><span class="n">chunk_size</span><span class="si">:</span><span class="p">,</span><span class="si">}</span><span class="s"> bytes"</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
        <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Created </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">boundaries</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="si">}</span><span class="s"> chunks for processing"</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="s">"</span><span class="se">\n</span><span class="s">Step 2: Pre-tokenizing text corpus..."</span><span class="p">)</span>

    <span class="c1"># Process chunks in parallel
</span>    <span class="n">chunk_args</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">start</span><span class="p">,</span> <span class="n">end</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">boundaries</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">boundaries</span><span class="p">[</span><span class="mi">1</span><span class="p">:]):</span>
        <span class="n">chunk_args</span><span class="p">.</span><span class="n">append</span><span class="p">((</span><span class="n">start</span><span class="p">,</span> <span class="n">end</span><span class="p">,</span> <span class="n">input_path</span><span class="p">,</span> <span class="n">special_tokens</span><span class="p">))</span>

    <span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="n">time</span><span class="p">()</span>
    <span class="k">with</span> <span class="n">mp</span><span class="p">.</span><span class="n">Pool</span><span class="p">(</span><span class="n">processes</span><span class="o">=</span><span class="n">num_processes</span><span class="p">)</span> <span class="k">as</span> <span class="n">pool</span><span class="p">:</span>
        <span class="n">chunk_results</span> <span class="o">=</span> <span class="n">pool</span><span class="p">.</span><span class="nb">map</span><span class="p">(</span><span class="n">process_chunk</span><span class="p">,</span> <span class="n">chunk_args</span><span class="p">)</span>
    <span class="n">tokenization_time</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start_time</span>

    <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
        <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Pre-tokenization completed in </span><span class="si">{</span><span class="n">tokenization_time</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s"> seconds"</span><span class="p">)</span>

    <span class="c1"># Merge results from all chunks
</span>    <span class="n">word_counts</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
    <span class="n">total_tokens</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">chunk_result</span> <span class="ow">in</span> <span class="n">chunk_results</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">count</span> <span class="ow">in</span> <span class="n">chunk_result</span><span class="p">.</span><span class="n">items</span><span class="p">():</span>
            <span class="n">word_counts</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="o">+=</span> <span class="n">count</span>
            <span class="n">total_tokens</span> <span class="o">+=</span> <span class="n">count</span>

    <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
        <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Found </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">word_counts</span><span class="p">)</span><span class="si">:</span><span class="p">,</span><span class="si">}</span><span class="s"> unique word types"</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Total token count: </span><span class="si">{</span><span class="n">total_tokens</span><span class="si">:</span><span class="p">,</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Most common words:"</span><span class="p">)</span>
        <span class="n">sorted_words</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">word_counts</span><span class="p">.</span><span class="n">items</span><span class="p">(),</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">reverse</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">word_bytes</span><span class="p">,</span> <span class="n">count</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">sorted_words</span><span class="p">[:</span><span class="mi">5</span><span class="p">]):</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="n">word_str</span> <span class="o">=</span> <span class="nb">bytes</span><span class="p">(</span><span class="n">word_bytes</span><span class="p">).</span><span class="n">decode</span><span class="p">(</span><span class="s">'utf-8'</span><span class="p">,</span> <span class="n">errors</span><span class="o">=</span><span class="s">'replace'</span><span class="p">)</span>
                <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"  </span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s">. '</span><span class="si">{</span><span class="n">word_str</span><span class="si">}</span><span class="s">' -&gt; </span><span class="si">{</span><span class="n">count</span><span class="si">:</span><span class="p">,</span><span class="si">}</span><span class="s"> times"</span><span class="p">)</span>
            <span class="k">except</span><span class="p">:</span>
                <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"  </span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s">. </span><span class="si">{</span><span class="n">word_bytes</span><span class="si">}</span><span class="s"> -&gt; </span><span class="si">{</span><span class="n">count</span><span class="si">:</span><span class="p">,</span><span class="si">}</span><span class="s"> times"</span><span class="p">)</span>

    <span class="c1"># Convert to working format for BPE (list of byte values)
</span>    <span class="n">word_freq</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">for</span> <span class="n">word_bytes</span><span class="p">,</span> <span class="n">freq</span> <span class="ow">in</span> <span class="n">word_counts</span><span class="p">.</span><span class="n">items</span><span class="p">():</span>
        <span class="n">word_tokens</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">word_bytes</span><span class="p">)</span>  <span class="c1"># Convert to list of ints
</span>        <span class="n">word_freq</span><span class="p">[</span><span class="nb">tuple</span><span class="p">(</span><span class="n">word_tokens</span><span class="p">)]</span> <span class="o">=</span> <span class="n">freq</span>

    <span class="n">merges</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">pair_index</span> <span class="o">=</span> <span class="p">{}</span>  <span class="c1"># Efficient indexing for pair counting
</span>
    <span class="k">def</span> <span class="nf">update_pair_index</span><span class="p">(</span><span class="n">word_freq</span><span class="p">,</span> <span class="n">pair_index</span><span class="p">):</span>
        <span class="s">"""Update the pair index for efficient counting."""</span>
        <span class="n">pair_index</span><span class="p">.</span><span class="n">clear</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">freq</span> <span class="ow">in</span> <span class="n">word_freq</span><span class="p">.</span><span class="n">items</span><span class="p">():</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">word</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>
                <span class="n">pair</span> <span class="o">=</span> <span class="p">(</span><span class="n">word</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">word</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">])</span>
                <span class="k">if</span> <span class="n">pair</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">pair_index</span><span class="p">:</span>
                    <span class="n">pair_index</span><span class="p">[</span><span class="n">pair</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
                <span class="n">pair_index</span><span class="p">[</span><span class="n">pair</span><span class="p">].</span><span class="n">append</span><span class="p">((</span><span class="n">word</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">freq</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">count_pairs</span><span class="p">(</span><span class="n">pair_index</span><span class="p">):</span>
        <span class="s">"""Count pair frequencies efficiently using the index."""</span>
        <span class="n">pair_counts</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">pair</span><span class="p">,</span> <span class="n">occurrences</span> <span class="ow">in</span> <span class="n">pair_index</span><span class="p">.</span><span class="n">items</span><span class="p">():</span>
            <span class="n">total_count</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">freq</span> <span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">freq</span> <span class="ow">in</span> <span class="n">occurrences</span><span class="p">)</span>
            <span class="n">pair_counts</span><span class="p">[</span><span class="n">pair</span><span class="p">]</span> <span class="o">=</span> <span class="n">total_count</span>
        <span class="k">return</span> <span class="n">pair_counts</span>

    <span class="c1"># BPE training loop
</span>    <span class="n">target_merges</span> <span class="o">=</span> <span class="n">vocab_size</span> <span class="o">-</span> <span class="nb">len</span><span class="p">(</span><span class="n">vocab</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
        <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"</span><span class="se">\n</span><span class="s">Step 3: Training BPE with </span><span class="si">{</span><span class="n">target_merges</span><span class="si">:</span><span class="p">,</span><span class="si">}</span><span class="s"> merges..."</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Initial vocabulary size: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">vocab</span><span class="p">)</span><span class="si">}</span><span class="s"> (256 bytes + </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">special_tokens</span><span class="p">)</span><span class="si">}</span><span class="s"> special tokens)"</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="s">"="</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>

    <span class="n">bpe_start_time</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="n">time</span><span class="p">()</span>

    <span class="k">for</span> <span class="n">merge_num</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">target_merges</span><span class="p">):</span>
        <span class="n">merge_step_start</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="n">time</span><span class="p">()</span>

        <span class="c1"># Update pair index
</span>        <span class="n">update_pair_index</span><span class="p">(</span><span class="n">word_freq</span><span class="p">,</span> <span class="n">pair_index</span><span class="p">)</span>

        <span class="c1"># Count pairs efficiently
</span>        <span class="n">pair_counts</span> <span class="o">=</span> <span class="n">count_pairs</span><span class="p">(</span><span class="n">pair_index</span><span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">pair_counts</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
                <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"No more pairs to merge at step </span><span class="si">{</span><span class="n">merge_num</span> <span class="o">+</span> <span class="mi">1</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
            <span class="k">break</span>

        <span class="c1"># Find most frequent pair (with lexicographic tiebreaking)
</span>        <span class="n">best_pair</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">pair_counts</span><span class="p">.</span><span class="n">items</span><span class="p">(),</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]))[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">best_count</span> <span class="o">=</span> <span class="n">pair_counts</span><span class="p">[</span><span class="n">best_pair</span><span class="p">]</span>

        <span class="c1"># Create new token for merge
</span>        <span class="n">new_token_id</span> <span class="o">=</span> <span class="n">next_id</span>
        <span class="n">next_id</span> <span class="o">+=</span> <span class="mi">1</span>

        <span class="c1"># Get the byte sequences for the tokens being merged
</span>        <span class="n">left_bytes</span> <span class="o">=</span> <span class="n">vocab</span><span class="p">[</span><span class="n">best_pair</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span>
        <span class="n">right_bytes</span> <span class="o">=</span> <span class="n">vocab</span><span class="p">[</span><span class="n">best_pair</span><span class="p">[</span><span class="mi">1</span><span class="p">]]</span>

        <span class="c1"># Record merge as byte sequences
</span>        <span class="n">merges</span><span class="p">.</span><span class="n">append</span><span class="p">((</span><span class="n">left_bytes</span><span class="p">,</span> <span class="n">right_bytes</span><span class="p">))</span>

        <span class="c1"># Update vocabulary - merge the two byte sequences
</span>        <span class="n">vocab</span><span class="p">[</span><span class="n">new_token_id</span><span class="p">]</span> <span class="o">=</span> <span class="n">left_bytes</span> <span class="o">+</span> <span class="n">right_bytes</span>

        <span class="c1"># Update word frequencies by applying merge
</span>        <span class="n">new_word_freq</span> <span class="o">=</span> <span class="p">{}</span>

        <span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">freq</span> <span class="ow">in</span> <span class="n">word_freq</span><span class="p">.</span><span class="n">items</span><span class="p">():</span>
            <span class="n">new_word</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="k">while</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">word</span><span class="p">):</span>
                <span class="k">if</span> <span class="p">(</span><span class="n">i</span> <span class="o">&lt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">word</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span> <span class="ow">and</span>
                    <span class="n">word</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">==</span> <span class="n">best_pair</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="ow">and</span>
                    <span class="n">word</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="n">best_pair</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
                    <span class="n">new_word</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">new_token_id</span><span class="p">)</span>
                    <span class="n">i</span> <span class="o">+=</span> <span class="mi">2</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">new_word</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">word</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
                    <span class="n">i</span> <span class="o">+=</span> <span class="mi">1</span>

            <span class="n">new_word_tuple</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">new_word</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">new_word_tuple</span> <span class="ow">in</span> <span class="n">new_word_freq</span><span class="p">:</span>
                <span class="n">new_word_freq</span><span class="p">[</span><span class="n">new_word_tuple</span><span class="p">]</span> <span class="o">+=</span> <span class="n">freq</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">new_word_freq</span><span class="p">[</span><span class="n">new_word_tuple</span><span class="p">]</span> <span class="o">=</span> <span class="n">freq</span>

        <span class="n">word_freq</span> <span class="o">=</span> <span class="n">new_word_freq</span>
        <span class="n">merge_step_time</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">merge_step_start</span>

        <span class="c1"># Progress logging
</span>        <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
            <span class="k">if</span> <span class="p">(</span><span class="n">merge_num</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">merge_num</span> <span class="o">&lt;</span> <span class="mi">10</span> <span class="ow">or</span> <span class="p">(</span><span class="n">merge_num</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="mi">1000</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">try</span><span class="p">:</span>
                    <span class="n">left_str</span> <span class="o">=</span> <span class="n">left_bytes</span><span class="p">.</span><span class="n">decode</span><span class="p">(</span><span class="s">'utf-8'</span><span class="p">,</span> <span class="n">errors</span><span class="o">=</span><span class="s">'replace'</span><span class="p">)</span>
                    <span class="n">right_str</span> <span class="o">=</span> <span class="n">right_bytes</span><span class="p">.</span><span class="n">decode</span><span class="p">(</span><span class="s">'utf-8'</span><span class="p">,</span> <span class="n">errors</span><span class="o">=</span><span class="s">'replace'</span><span class="p">)</span>
                    <span class="n">merged_str</span> <span class="o">=</span> <span class="p">(</span><span class="n">left_bytes</span> <span class="o">+</span> <span class="n">right_bytes</span><span class="p">).</span><span class="n">decode</span><span class="p">(</span><span class="s">'utf-8'</span><span class="p">,</span> <span class="n">errors</span><span class="o">=</span><span class="s">'replace'</span><span class="p">)</span>
                    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Merge </span><span class="si">{</span><span class="n">merge_num</span> <span class="o">+</span> <span class="mi">1</span><span class="si">:</span><span class="mi">4</span><span class="n">d</span><span class="si">}</span><span class="s">/</span><span class="si">{</span><span class="n">target_merges</span><span class="si">}</span><span class="s">: "</span>
                          <span class="sa">f</span><span class="s">"'</span><span class="si">{</span><span class="n">left_str</span><span class="si">}</span><span class="s">' + '</span><span class="si">{</span><span class="n">right_str</span><span class="si">}</span><span class="s">' -&gt; '</span><span class="si">{</span><span class="n">merged_str</span><span class="si">}</span><span class="s">' "</span>
                          <span class="sa">f</span><span class="s">"(freq: </span><span class="si">{</span><span class="n">best_count</span><span class="si">:</span><span class="p">,</span><span class="si">}</span><span class="s">, time: </span><span class="si">{</span><span class="n">merge_step_time</span><span class="si">:</span><span class="p">.</span><span class="mi">3</span><span class="n">f</span><span class="si">}</span><span class="s">s)"</span><span class="p">)</span>
                <span class="k">except</span><span class="p">:</span>
                    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Merge </span><span class="si">{</span><span class="n">merge_num</span> <span class="o">+</span> <span class="mi">1</span><span class="si">:</span><span class="mi">4</span><span class="n">d</span><span class="si">}</span><span class="s">/</span><span class="si">{</span><span class="n">target_merges</span><span class="si">}</span><span class="s">: "</span>
                          <span class="sa">f</span><span class="s">"</span><span class="si">{</span><span class="n">left_bytes</span><span class="si">}</span><span class="s"> + </span><span class="si">{</span><span class="n">right_bytes</span><span class="si">}</span><span class="s"> -&gt; </span><span class="si">{</span><span class="n">left_bytes</span> <span class="o">+</span> <span class="n">right_bytes</span><span class="si">}</span><span class="s"> "</span>
                          <span class="sa">f</span><span class="s">"(freq: </span><span class="si">{</span><span class="n">best_count</span><span class="si">:</span><span class="p">,</span><span class="si">}</span><span class="s">, time: </span><span class="si">{</span><span class="n">merge_step_time</span><span class="si">:</span><span class="p">.</span><span class="mi">3</span><span class="n">f</span><span class="si">}</span><span class="s">s)"</span><span class="p">)</span>

    <span class="n">bpe_time</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">bpe_start_time</span>

    <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
        <span class="k">print</span><span class="p">(</span><span class="s">"="</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"BPE training completed in </span><span class="si">{</span><span class="n">bpe_time</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s"> seconds"</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Final vocabulary size: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">vocab</span><span class="p">)</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Total merges performed: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">merges</span><span class="p">)</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>

        <span class="c1"># Show compression statistics
</span>        <span class="k">if</span> <span class="n">word_counts</span><span class="p">:</span>
            <span class="n">original_tokens</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="nb">bytes</span><span class="p">(</span><span class="n">word_bytes</span><span class="p">))</span> <span class="k">for</span> <span class="n">word_bytes</span><span class="p">,</span> <span class="n">count</span> <span class="ow">in</span> <span class="n">word_counts</span><span class="p">.</span><span class="n">items</span><span class="p">()</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">count</span><span class="p">))</span>
            <span class="n">compressed_tokens</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">word</span><span class="p">)</span> <span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">count</span> <span class="ow">in</span> <span class="n">word_freq</span><span class="p">.</span><span class="n">items</span><span class="p">()</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">count</span><span class="p">))</span>
            <span class="n">compression_ratio</span> <span class="o">=</span> <span class="n">original_tokens</span> <span class="o">/</span> <span class="n">compressed_tokens</span> <span class="k">if</span> <span class="n">compressed_tokens</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="mf">1.0</span>
            <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Compression ratio: </span><span class="si">{</span><span class="n">compression_ratio</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s">x (from </span><span class="si">{</span><span class="n">original_tokens</span><span class="si">:</span><span class="p">,</span><span class="si">}</span><span class="s"> to </span><span class="si">{</span><span class="n">compressed_tokens</span><span class="si">:</span><span class="p">,</span><span class="si">}</span><span class="s"> tokens)"</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">vocab</span><span class="p">,</span> <span class="n">merges</span>


<span class="k">def</span> <span class="nf">save_tokenizer</span><span class="p">(</span><span class="n">vocab</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">bytes</span><span class="p">],</span> <span class="n">merges</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">tuple</span><span class="p">[</span><span class="nb">bytes</span><span class="p">,</span> <span class="nb">bytes</span><span class="p">]],</span>
                  <span class="n">vocab_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">merges_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>
    <span class="s">"""Save vocabulary and merges to disk files."""</span>
    <span class="kn">import</span> <span class="nn">json</span>
    <span class="kn">import</span> <span class="nn">pickle</span>

    <span class="c1"># Save vocabulary
</span>    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">vocab_path</span><span class="p">,</span> <span class="s">'wb'</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="n">pickle</span><span class="p">.</span><span class="n">dump</span><span class="p">(</span><span class="n">vocab</span><span class="p">,</span> <span class="n">f</span><span class="p">)</span>

    <span class="c1"># Save merges
</span>    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">merges_path</span><span class="p">,</span> <span class="s">'wb'</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="n">pickle</span><span class="p">.</span><span class="n">dump</span><span class="p">(</span><span class="n">merges</span><span class="p">,</span> <span class="n">f</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">load_tokenizer</span><span class="p">(</span><span class="n">vocab_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">merges_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="nb">dict</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">bytes</span><span class="p">],</span> <span class="nb">list</span><span class="p">[</span><span class="nb">tuple</span><span class="p">[</span><span class="nb">bytes</span><span class="p">,</span> <span class="nb">bytes</span><span class="p">]]]:</span>
    <span class="s">"""Load vocabulary and merges from disk files."""</span>
    <span class="kn">import</span> <span class="nn">pickle</span>

    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">vocab_path</span><span class="p">,</span> <span class="s">'rb'</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="n">vocab</span> <span class="o">=</span> <span class="n">pickle</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>

    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">merges_path</span><span class="p">,</span> <span class="s">'rb'</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="n">merges</span> <span class="o">=</span> <span class="n">pickle</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">vocab</span><span class="p">,</span> <span class="n">merges</span>
</code></pre></div></div>

<h2 id="training-on-tinystories-dataset">Training on TinyStories Dataset</h2>

<p>Now let‚Äôs use our implementation to train a tokenizer on the TinyStories dataset. Here is one training function to demonstrate all the steps:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">time</span>
<span class="kn">import</span> <span class="nn">os</span>

<span class="k">def</span> <span class="nf">train_bpe_tokentizer_via_dataset</span><span class="p">(</span><span class="n">input_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"="</span> <span class="o">*</span> <span class="mi">80</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"BPE TOKENIZER TRAINING ON TINYSTORIES DATASET"</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"="</span> <span class="o">*</span> <span class="mi">80</span><span class="p">)</span>

    <span class="c1"># Configuration
</span>    <span class="n">vocab_size</span> <span class="o">=</span> <span class="mi">10000</span>
    <span class="n">special_tokens</span> <span class="o">=</span> <span class="p">[</span><span class="s">"&lt;|endoftext|&gt;"</span><span class="p">]</span>

    <span class="c1"># Check if input file exists
</span>    <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="n">exists</span><span class="p">(</span><span class="n">input_path</span><span class="p">):</span>
        <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Error: Input file '</span><span class="si">{</span><span class="n">input_path</span><span class="si">}</span><span class="s">' not found!"</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="s">"Please ensure the TinyStories dataset is in the data/ directory."</span><span class="p">)</span>
        <span class="k">return</span>

    <span class="c1"># Display configuration
</span>    <span class="n">file_size</span> <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="n">getsize</span><span class="p">(</span><span class="n">input_path</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Configuration:"</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"  Input file: </span><span class="si">{</span><span class="n">input_path</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"  File size: </span><span class="si">{</span><span class="n">file_size</span><span class="si">:</span><span class="p">,</span><span class="si">}</span><span class="s"> bytes (</span><span class="si">{</span><span class="n">file_size</span> <span class="o">/</span> <span class="mi">1024</span> <span class="o">/</span> <span class="mi">1024</span><span class="si">:</span><span class="p">.</span><span class="mi">1</span><span class="n">f</span><span class="si">}</span><span class="s"> MB)"</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"  Target vocabulary size: </span><span class="si">{</span><span class="n">vocab_size</span><span class="si">:</span><span class="p">,</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"  Special tokens: </span><span class="si">{</span><span class="n">special_tokens</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"  Verbose logging: Enabled"</span><span class="p">)</span>
    <span class="k">print</span><span class="p">()</span>

    <span class="c1"># Train the tokenizer with verbose output
</span>    <span class="n">overall_start_time</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="n">time</span><span class="p">()</span>
    <span class="n">vocab</span><span class="p">,</span> <span class="n">merges</span> <span class="o">=</span> <span class="n">train_bpe_tokenizer</span><span class="p">(</span>
        <span class="n">input_path</span><span class="o">=</span><span class="n">input_path</span><span class="p">,</span>
        <span class="n">vocab_size</span><span class="o">=</span><span class="n">vocab_size</span><span class="p">,</span>
        <span class="n">special_tokens</span><span class="o">=</span><span class="n">special_tokens</span><span class="p">,</span>
        <span class="n">verbose</span><span class="o">=</span><span class="bp">True</span>  <span class="c1"># Enable detailed logging
</span>    <span class="p">)</span>
    <span class="n">overall_end_time</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="n">time</span><span class="p">()</span>

    <span class="k">print</span><span class="p">(</span><span class="s">"</span><span class="se">\n</span><span class="s">"</span> <span class="o">+</span> <span class="s">"="</span> <span class="o">*</span> <span class="mi">80</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"TRAINING SUMMARY"</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"="</span> <span class="o">*</span> <span class="mi">80</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Total training time: </span><span class="si">{</span><span class="n">overall_end_time</span> <span class="o">-</span> <span class="n">overall_start_time</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s"> seconds"</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Final vocabulary size: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">vocab</span><span class="p">)</span><span class="si">:</span><span class="p">,</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Number of merges performed: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">merges</span><span class="p">)</span><span class="si">:</span><span class="p">,</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Actual vocab size vs target: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">vocab</span><span class="p">)</span><span class="si">}</span><span class="s"> / </span><span class="si">{</span><span class="n">vocab_size</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>

    <span class="c1"># Save the tokenizer
</span>    <span class="n">vocab_path</span> <span class="o">=</span> <span class="s">"tinystories_vocab.pkl"</span>
    <span class="n">merges_path</span> <span class="o">=</span> <span class="s">"tinystories_merges.pkl"</span>

    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"</span><span class="se">\n</span><span class="s">Saving tokenizer to disk..."</span><span class="p">)</span>
    <span class="n">save_tokenizer</span><span class="p">(</span><span class="n">vocab</span><span class="p">,</span> <span class="n">merges</span><span class="p">,</span> <span class="n">vocab_path</span><span class="p">,</span> <span class="n">merges_path</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"  ‚úì Vocabulary saved to: </span><span class="si">{</span><span class="n">vocab_path</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"  ‚úì Merges saved to: </span><span class="si">{</span><span class="n">merges_path</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>

    <span class="c1"># Detailed vocabulary analysis
</span>    <span class="k">print</span><span class="p">(</span><span class="s">"</span><span class="se">\n</span><span class="s">"</span> <span class="o">+</span> <span class="s">"="</span> <span class="o">*</span> <span class="mi">80</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"VOCABULARY ANALYSIS"</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"="</span> <span class="o">*</span> <span class="mi">80</span><span class="p">)</span>

    <span class="c1"># Count different types of tokens
</span>    <span class="n">byte_tokens</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="mi">1</span> <span class="k">for</span> <span class="n">token_id</span> <span class="ow">in</span> <span class="n">vocab</span><span class="p">.</span><span class="n">keys</span><span class="p">()</span> <span class="k">if</span> <span class="n">token_id</span> <span class="o">&lt;</span> <span class="mi">256</span><span class="p">)</span>
    <span class="n">special_token_count</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">special_tokens</span><span class="p">)</span>
    <span class="n">merged_tokens</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">vocab</span><span class="p">)</span> <span class="o">-</span> <span class="n">byte_tokens</span> <span class="o">-</span> <span class="n">special_token_count</span>

    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Token type breakdown:"</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"  Byte tokens (0-255): </span><span class="si">{</span><span class="n">byte_tokens</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"  Special tokens: </span><span class="si">{</span><span class="n">special_token_count</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"  Merged tokens: </span><span class="si">{</span><span class="n">merged_tokens</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"  Total: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">vocab</span><span class="p">)</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>

    <span class="c1"># Show some vocabulary examples
</span>    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"</span><span class="se">\n</span><span class="s">Byte tokens (first 10):"</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">vocab</span><span class="p">:</span>
            <span class="n">char</span> <span class="o">=</span> <span class="n">vocab</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="n">decode</span><span class="p">(</span><span class="s">'utf-8'</span><span class="p">,</span> <span class="n">errors</span><span class="o">=</span><span class="s">'replace'</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">char</span><span class="p">.</span><span class="n">isprintable</span><span class="p">()</span> <span class="ow">and</span> <span class="n">char</span> <span class="o">!=</span> <span class="s">' '</span><span class="p">:</span>
                <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"  Token </span><span class="si">{</span><span class="n">i</span><span class="si">:</span><span class="mi">3</span><span class="n">d</span><span class="si">}</span><span class="s">: </span><span class="si">{</span><span class="n">vocab</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="s"> -&gt; '</span><span class="si">{</span><span class="n">char</span><span class="si">}</span><span class="s">'"</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"  Token </span><span class="si">{</span><span class="n">i</span><span class="si">:</span><span class="mi">3</span><span class="n">d</span><span class="si">}</span><span class="s">: </span><span class="si">{</span><span class="n">vocab</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="s"> -&gt; </span><span class="si">{</span><span class="nb">repr</span><span class="p">(</span><span class="n">char</span><span class="p">)</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>

    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"</span><span class="se">\n</span><span class="s">Special tokens:"</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">token_str</span> <span class="ow">in</span> <span class="n">special_tokens</span><span class="p">:</span>
        <span class="n">token_bytes</span> <span class="o">=</span> <span class="n">token_str</span><span class="p">.</span><span class="n">encode</span><span class="p">(</span><span class="s">'utf-8'</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">token_id</span><span class="p">,</span> <span class="n">vocab_bytes</span> <span class="ow">in</span> <span class="n">vocab</span><span class="p">.</span><span class="n">items</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">vocab_bytes</span> <span class="o">==</span> <span class="n">token_bytes</span><span class="p">:</span>
                <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"  Token </span><span class="si">{</span><span class="n">token_id</span><span class="si">:</span><span class="mi">3</span><span class="n">d</span><span class="si">}</span><span class="s">: </span><span class="si">{</span><span class="n">vocab_bytes</span><span class="si">}</span><span class="s"> -&gt; '</span><span class="si">{</span><span class="n">token_str</span><span class="si">}</span><span class="s">'"</span><span class="p">)</span>
                <span class="k">break</span>

    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"</span><span class="se">\n</span><span class="s">Most recently merged tokens (last 10):"</span><span class="p">)</span>
    <span class="n">merged_token_ids</span> <span class="o">=</span> <span class="p">[</span><span class="n">tid</span> <span class="k">for</span> <span class="n">tid</span> <span class="ow">in</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">vocab</span><span class="p">.</span><span class="n">keys</span><span class="p">())</span> <span class="k">if</span> <span class="n">tid</span> <span class="o">&gt;=</span> <span class="mi">256</span> <span class="o">+</span> <span class="nb">len</span><span class="p">(</span><span class="n">special_tokens</span><span class="p">)]</span>
    <span class="k">for</span> <span class="n">token_id</span> <span class="ow">in</span> <span class="n">merged_token_ids</span><span class="p">[</span><span class="o">-</span><span class="mi">10</span><span class="p">:]:</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">decoded</span> <span class="o">=</span> <span class="n">vocab</span><span class="p">[</span><span class="n">token_id</span><span class="p">].</span><span class="n">decode</span><span class="p">(</span><span class="s">'utf-8'</span><span class="p">,</span> <span class="n">errors</span><span class="o">=</span><span class="s">'replace'</span><span class="p">)</span>
            <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"  Token </span><span class="si">{</span><span class="n">token_id</span><span class="si">:</span><span class="mi">4</span><span class="n">d</span><span class="si">}</span><span class="s">: </span><span class="si">{</span><span class="n">vocab</span><span class="p">[</span><span class="n">token_id</span><span class="p">]</span><span class="si">}</span><span class="s"> -&gt; '</span><span class="si">{</span><span class="n">decoded</span><span class="si">}</span><span class="s">'"</span><span class="p">)</span>
        <span class="k">except</span><span class="p">:</span>
            <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"  Token </span><span class="si">{</span><span class="n">token_id</span><span class="si">:</span><span class="mi">4</span><span class="n">d</span><span class="si">}</span><span class="s">: </span><span class="si">{</span><span class="n">vocab</span><span class="p">[</span><span class="n">token_id</span><span class="p">]</span><span class="si">}</span><span class="s"> -&gt; (non-UTF8)"</span><span class="p">)</span>

    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"</span><span class="se">\n</span><span class="s">First 10 merge operations:"</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">left</span><span class="p">,</span> <span class="n">right</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">merges</span><span class="p">[:</span><span class="mi">10</span><span class="p">]):</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">left_str</span> <span class="o">=</span> <span class="n">left</span><span class="p">.</span><span class="n">decode</span><span class="p">(</span><span class="s">'utf-8'</span><span class="p">,</span> <span class="n">errors</span><span class="o">=</span><span class="s">'replace'</span><span class="p">)</span>
            <span class="n">right_str</span> <span class="o">=</span> <span class="n">right</span><span class="p">.</span><span class="n">decode</span><span class="p">(</span><span class="s">'utf-8'</span><span class="p">,</span> <span class="n">errors</span><span class="o">=</span><span class="s">'replace'</span><span class="p">)</span>
            <span class="n">merged_str</span> <span class="o">=</span> <span class="p">(</span><span class="n">left</span> <span class="o">+</span> <span class="n">right</span><span class="p">).</span><span class="n">decode</span><span class="p">(</span><span class="s">'utf-8'</span><span class="p">,</span> <span class="n">errors</span><span class="o">=</span><span class="s">'replace'</span><span class="p">)</span>
            <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"  Merge </span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">:</span><span class="mi">2</span><span class="n">d</span><span class="si">}</span><span class="s">: '</span><span class="si">{</span><span class="n">left_str</span><span class="si">}</span><span class="s">' + '</span><span class="si">{</span><span class="n">right_str</span><span class="si">}</span><span class="s">' -&gt; '</span><span class="si">{</span><span class="n">merged_str</span><span class="si">}</span><span class="s">'"</span><span class="p">)</span>
        <span class="k">except</span><span class="p">:</span>
            <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"  Merge </span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">:</span><span class="mi">2</span><span class="n">d</span><span class="si">}</span><span class="s">: </span><span class="si">{</span><span class="n">left</span><span class="si">}</span><span class="s"> + </span><span class="si">{</span><span class="n">right</span><span class="si">}</span><span class="s"> -&gt; (binary)"</span><span class="p">)</span>

    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"</span><span class="se">\n</span><span class="s">Last 10 merge operations:"</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">left</span><span class="p">,</span> <span class="n">right</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">merges</span><span class="p">[</span><span class="o">-</span><span class="mi">10</span><span class="p">:],</span> <span class="nb">len</span><span class="p">(</span><span class="n">merges</span><span class="p">)</span> <span class="o">-</span> <span class="mi">9</span><span class="p">):</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">left_str</span> <span class="o">=</span> <span class="n">left</span><span class="p">.</span><span class="n">decode</span><span class="p">(</span><span class="s">'utf-8'</span><span class="p">,</span> <span class="n">errors</span><span class="o">=</span><span class="s">'replace'</span><span class="p">)</span>
            <span class="n">right_str</span> <span class="o">=</span> <span class="n">right</span><span class="p">.</span><span class="n">decode</span><span class="p">(</span><span class="s">'utf-8'</span><span class="p">,</span> <span class="n">errors</span><span class="o">=</span><span class="s">'replace'</span><span class="p">)</span>
            <span class="n">merged_str</span> <span class="o">=</span> <span class="p">(</span><span class="n">left</span> <span class="o">+</span> <span class="n">right</span><span class="p">).</span><span class="n">decode</span><span class="p">(</span><span class="s">'utf-8'</span><span class="p">,</span> <span class="n">errors</span><span class="o">=</span><span class="s">'replace'</span><span class="p">)</span>
            <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"  Merge </span><span class="si">{</span><span class="n">i</span><span class="si">:</span><span class="mi">2</span><span class="n">d</span><span class="si">}</span><span class="s">: '</span><span class="si">{</span><span class="n">left_str</span><span class="si">}</span><span class="s">' + '</span><span class="si">{</span><span class="n">right_str</span><span class="si">}</span><span class="s">' -&gt; '</span><span class="si">{</span><span class="n">merged_str</span><span class="si">}</span><span class="s">'"</span><span class="p">)</span>
        <span class="k">except</span><span class="p">:</span>
            <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"  Merge </span><span class="si">{</span><span class="n">i</span><span class="si">:</span><span class="mi">2</span><span class="n">d</span><span class="si">}</span><span class="s">: </span><span class="si">{</span><span class="n">left</span><span class="si">}</span><span class="s"> + </span><span class="si">{</span><span class="n">right</span><span class="si">}</span><span class="s"> -&gt; (binary)"</span><span class="p">)</span>

    <span class="c1"># Show file sizes
</span>    <span class="n">vocab_size_bytes</span> <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="n">getsize</span><span class="p">(</span><span class="n">vocab_path</span><span class="p">)</span>
    <span class="n">merges_size_bytes</span> <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="n">getsize</span><span class="p">(</span><span class="n">merges_path</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"</span><span class="se">\n</span><span class="s">Output file sizes:"</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"  Vocabulary file: </span><span class="si">{</span><span class="n">vocab_size_bytes</span><span class="si">:</span><span class="p">,</span><span class="si">}</span><span class="s"> bytes (</span><span class="si">{</span><span class="n">vocab_size_bytes</span> <span class="o">/</span> <span class="mi">1024</span><span class="si">:</span><span class="p">.</span><span class="mi">1</span><span class="n">f</span><span class="si">}</span><span class="s"> KB)"</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"  Merges file: </span><span class="si">{</span><span class="n">merges_size_bytes</span><span class="si">:</span><span class="p">,</span><span class="si">}</span><span class="s"> bytes (</span><span class="si">{</span><span class="n">merges_size_bytes</span> <span class="o">/</span> <span class="mi">1024</span><span class="si">:</span><span class="p">.</span><span class="mi">1</span><span class="n">f</span><span class="si">}</span><span class="s"> KB)"</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"  Total: </span><span class="si">{</span><span class="n">vocab_size_bytes</span> <span class="o">+</span> <span class="n">merges_size_bytes</span><span class="si">:</span><span class="p">,</span><span class="si">}</span><span class="s"> bytes (</span><span class="si">{</span><span class="p">(</span><span class="n">vocab_size_bytes</span> <span class="o">+</span> <span class="n">merges_size_bytes</span><span class="p">)</span> <span class="o">/</span> <span class="mi">1024</span><span class="si">:</span><span class="p">.</span><span class="mi">1</span><span class="n">f</span><span class="si">}</span><span class="s"> KB)"</span><span class="p">)</span>

    <span class="k">print</span><span class="p">(</span><span class="s">"</span><span class="se">\n</span><span class="s">"</span> <span class="o">+</span> <span class="s">"="</span> <span class="o">*</span> <span class="mi">80</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"TRAINING COMPLETED SUCCESSFULLY!"</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"="</span> <span class="o">*</span> <span class="mi">80</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"You can now use the trained tokenizer for encoding/decoding text."</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Load with: vocab, merges = load_tokenizer('</span><span class="si">{</span><span class="n">vocab_path</span><span class="si">}</span><span class="s">', '</span><span class="si">{</span><span class="n">merges_path</span><span class="si">}</span><span class="s">')"</span><span class="p">)</span>
</code></pre></div></div>

<p>To run the training, one can try for example:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">train_bpe_tokentizer_via_dataset</span><span class="p">(</span><span class="n">input_path</span><span class="o">=</span><span class="s">"data/TinyStoriesV2-GPT4-train.txt"</span><span class="p">)</span>
</code></pre></div></div>

<p>And it will output the following info from the training process:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>================================================================================
BPE TOKENIZER TRAINING ON TINYSTORIES DATASET
================================================================================
Configuration:
  Input file: /content/TinyStoriesV2-GPT4-train.txt
  File size: 2,227,753,162 bytes (2124.6 MB)
  Target vocabulary size: 10,000
  Special tokens: ['&lt;|endoftext|&gt;']
  Verbose logging: Enabled

Step 1: Setting up parallel processing...
Using 12 processes for parallel tokenization
Finding chunk boundaries aligned with special token: &lt;|endoftext|&gt;
Initial guess of the chunk boundaries: [0, 185646096, 371292192, 556938288, 742584384, 928230480, 1113876576, 1299522672, 1485168768, 1670814864, 1856460960, 2042107056, 2227753152]
Created 12 chunks for processing

Step 2: Pre-tokenizing text corpus...
Pre-tokenization completed in 66.52 seconds
Found 59,904 unique word types
Total token count: 536,592,162
Most common words:
  1. '.' -&gt; 41,764,519 times
  2. ',' -&gt; 23,284,331 times
  3. ' the' -&gt; 20,828,576 times
  4. ' and' -&gt; 19,475,966 times
  5. ' a' -&gt; 15,063,529 times

Step 3: Training BPE with 9,743 merges...
Initial vocabulary size: 257 (256 bytes + 1 special tokens)
============================================================
Merge    1/9743: ' ' + 't' -&gt; ' t' (freq: 63,482,199, time: 0.273s)
Merge    2/9743: 'h' + 'e' -&gt; 'he' (freq: 63,341,860, time: 0.318s)
Merge    3/9743: ' ' + 'a' -&gt; ' a' (freq: 47,465,635, time: 0.340s)
Merge    4/9743: ' ' + 's' -&gt; ' s' (freq: 32,362,158, time: 0.340s)
Merge    5/9743: ' ' + 'w' -&gt; ' w' (freq: 31,485,643, time: 0.327s)
Merge    6/9743: 'n' + 'd' -&gt; 'nd' (freq: 28,922,386, time: 0.332s)
Merge    7/9743: ' t' + 'he' -&gt; ' the' (freq: 28,915,024, time: 0.320s)
Merge    8/9743: 'e' + 'd' -&gt; 'ed' (freq: 24,836,456, time: 0.317s)
Merge    9/9743: ' ' + 'b' -&gt; ' b' (freq: 22,147,488, time: 0.326s)
Merge   10/9743: ' t' + 'o' -&gt; ' to' (freq: 20,892,273, time: 0.322s)
Merge  100/9743: ' ha' + 'pp' -&gt; ' happ' (freq: 3,147,884, time: 0.251s)
Merge  200/9743: ' s' + 'e' -&gt; ' se' (freq: 1,410,130, time: 0.343s)
Merge  300/9743: ' s' + 'omet' -&gt; ' somet' (freq: 790,510, time: 0.245s)
Merge  400/9743: ' g' + 'ot' -&gt; ' got' (freq: 524,776, time: 0.338s)
Merge  500/9743: ' e' + 'ach' -&gt; ' each' (freq: 369,637, time: 0.321s)
Merge  600/9743: 'l' + 'f' -&gt; 'lf' (freq: 279,566, time: 0.230s)
Merge  700/9743: ' wal' + 'k' -&gt; ' walk' (freq: 221,114, time: 0.237s)
Merge  800/9743: ' do' + 'll' -&gt; ' doll' (freq: 177,602, time: 0.324s)
Merge  900/9743: ' ' + 'G' -&gt; ' G' (freq: 147,699, time: 0.214s)
Merge 1000/9743: 'ec' + 't' -&gt; 'ect' (freq: 127,288, time: 0.233s)
Merge 1100/9743: ' l' + 'ight' -&gt; ' light' (freq: 108,006, time: 0.208s)
Merge 1200/9743: ' d' + 'in' -&gt; ' din' (freq: 92,211, time: 0.225s)
Merge 1300/9743: ' picture' + 's' -&gt; ' pictures' (freq: 80,416, time: 0.318s)
Merge 1400/9743: 'itt' + 'en' -&gt; 'itten' (freq: 68,466, time: 0.235s)
Merge 1500/9743: 'A' + 'my' -&gt; 'Amy' (freq: 59,829, time: 0.306s)
Merge 1600/9743: ' tal' + 'king' -&gt; ' talking' (freq: 53,781, time: 0.330s)
Merge 1700/9743: 'b' + 'all' -&gt; 'ball' (freq: 48,005, time: 0.309s)
Merge 1800/9743: ' k' + 'iss' -&gt; ' kiss' (freq: 43,477, time: 0.318s)
...
Merge 8000/9743: ' mom' + 'mies' -&gt; ' mommies' (freq: 879, time: 0.205s)
Merge 8100/9743: ' cryst' + 'als' -&gt; ' crystals' (freq: 840, time: 0.299s)
Merge 8200/9743: ' playd' + 'ate' -&gt; ' playdate' (freq: 809, time: 0.283s)
Merge 8300/9743: ' support' + 'ing' -&gt; ' supporting' (freq: 778, time: 0.200s)
Merge 8400/9743: ' activ' + 'ity' -&gt; ' activity' (freq: 747, time: 0.300s)
Merge 8500/9743: 'L' + 'izzy' -&gt; 'Lizzy' (freq: 716, time: 0.284s)
Merge 8600/9743: 'er' + 'ing' -&gt; 'ering' (freq: 691, time: 0.311s)
Merge 8700/9743: ' tid' + 'ied' -&gt; ' tidied' (freq: 660, time: 0.308s)
Merge 8800/9743: 'f' + 'lowers' -&gt; 'flowers' (freq: 633, time: 0.295s)
Merge 8900/9743: ' Gra' + 'nd' -&gt; ' Grand' (freq: 609, time: 0.299s)
Merge 9000/9743: ' frustr' + 'ation' -&gt; ' frustration' (freq: 584, time: 0.301s)
Merge 9100/9743: 'amil' + 'iar' -&gt; 'amiliar' (freq: 561, time: 0.205s)
Merge 9200/9743: ' P' + 'retty' -&gt; ' Pretty' (freq: 542, time: 0.310s)
Merge 9300/9743: ' sal' + 'on' -&gt; ' salon' (freq: 521, time: 0.292s)
Merge 9400/9743: ' p' + 'ounced' -&gt; ' pounced' (freq: 502, time: 0.196s)
Merge 9500/9743: ' pops' + 'ic' -&gt; ' popsic' (freq: 485, time: 0.185s)
Merge 9600/9743: ' pain' + 'ful' -&gt; ' painful' (freq: 469, time: 0.298s)
Merge 9700/9743: 'solut' + 'ely' -&gt; 'solutely' (freq: 454, time: 0.308s)
============================================================
BPE training completed in 2731.72 seconds
Final vocabulary size: 10000
Total merges performed: 9743
Compression ratio: 4.07x (from 2,192,422,648 to 538,511,097 tokens)

================================================================================
TRAINING SUMMARY
================================================================================
Total training time: 2898.45 seconds
Final vocabulary size: 10,000
Number of merges performed: 9,743
Actual vocab size vs target: 10000 / 10000

Saving tokenizer to disk...
  ‚úì Vocabulary saved to: tinystories_vocab.pkl
  ‚úì Merges saved to: tinystories_merges.pkl

================================================================================
VOCABULARY ANALYSIS
================================================================================
Token type breakdown:
  Byte tokens (0-255): 256
  Special tokens: 1
  Merged tokens: 9743
  Total: 10000

Byte tokens (first 10):
  Token   0: b'\x00' -&gt; '\x00'
  Token   1: b'\x01' -&gt; '\x01'
  Token   2: b'\x02' -&gt; '\x02'
  Token   3: b'\x03' -&gt; '\x03'
  Token   4: b'\x04' -&gt; '\x04'
  Token   5: b'\x05' -&gt; '\x05'
  Token   6: b'\x06' -&gt; '\x06'
  Token   7: b'\x07' -&gt; '\x07'
  Token   8: b'\x08' -&gt; '\x08'
  Token   9: b'\t' -&gt; '\t'

Special tokens:
  Token 256: b'&lt;|endoftext|&gt;' -&gt; '&lt;|endoftext|&gt;'

Most recently merged tokens (last 10):
  Token 9990: b' improving' -&gt; ' improving'
  Token 9991: b' nicest' -&gt; ' nicest'
  Token 9992: b' whiskers' -&gt; ' whiskers'
  Token 9993: b' booth' -&gt; ' booth'
  Token 9994: b' Land' -&gt; ' Land'
  Token 9995: b'Rocky' -&gt; 'Rocky'
  Token 9996: b' meadows' -&gt; ' meadows'
  Token 9997: b' Starry' -&gt; ' Starry'
  Token 9998: b' imaginary' -&gt; ' imaginary'
  Token 9999: b' bold' -&gt; ' bold'

First 10 merge operations:
  Merge  1: ' ' + 't' -&gt; ' t'
  Merge  2: 'h' + 'e' -&gt; 'he'
  Merge  3: ' ' + 'a' -&gt; ' a'
  Merge  4: ' ' + 's' -&gt; ' s'
  Merge  5: ' ' + 'w' -&gt; ' w'
  Merge  6: 'n' + 'd' -&gt; 'nd'
  Merge  7: ' t' + 'he' -&gt; ' the'
  Merge  8: 'e' + 'd' -&gt; 'ed'
  Merge  9: ' ' + 'b' -&gt; ' b'
  Merge 10: ' t' + 'o' -&gt; ' to'

Last 10 merge operations:
  Merge 9734: ' impro' + 'ving' -&gt; ' improving'
  Merge 9735: ' nice' + 'st' -&gt; ' nicest'
  Merge 9736: ' wh' + 'iskers' -&gt; ' whiskers'
  Merge 9737: ' bo' + 'oth' -&gt; ' booth'
  Merge 9738: ' L' + 'and' -&gt; ' Land'
  Merge 9739: 'Rock' + 'y' -&gt; 'Rocky'
  Merge 9740: ' meadow' + 's' -&gt; ' meadows'
  Merge 9741: ' St' + 'arry' -&gt; ' Starry'
  Merge 9742: ' imag' + 'inary' -&gt; ' imaginary'
  Merge 9743: ' bo' + 'ld' -&gt; ' bold'

Output file sizes:
  Vocabulary file: 117,701 bytes (114.9 KB)
  Merges file: 109,714 bytes (107.1 KB)
  Total: 227,415 bytes (222.1 KB)

================================================================================
TRAINING COMPLETED SUCCESSFULLY!
================================================================================
You can now use the trained tokenizer for encoding/decoding text.
Load with: vocab, merges = load_tokenizer('tinystories_vocab.pkl', 'tinystories_merges.pkl')
</code></pre></div></div>

<h2 id="using-the-trained-tokenizer">Using the Trained Tokenizer</h2>

<p>Once we have a trained tokenizer, we need a class to encode and decode text. Here‚Äôs one complete implementation:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">SimpleBPETokenizer</span><span class="p">:</span>
    <span class="s">"""Simple BPE tokenizer for encoding/decoding text."""</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">vocab</span><span class="p">,</span> <span class="n">merges</span><span class="p">,</span> <span class="n">special_tokens</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">vocab</span> <span class="o">=</span> <span class="n">vocab</span>  <span class="c1"># {token_id: bytes}
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">merges</span> <span class="o">=</span> <span class="n">merges</span>  <span class="c1"># [(left_bytes, right_bytes), ...]
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">special_tokens</span> <span class="o">=</span> <span class="n">special_tokens</span> <span class="ow">or</span> <span class="p">[</span><span class="s">"&lt;|endoftext|&gt;"</span><span class="p">]</span>

        <span class="c1"># Create reverse mapping for decoding
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">id_to_bytes</span> <span class="o">=</span> <span class="n">vocab</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">bytes_to_id</span> <span class="o">=</span> <span class="p">{</span><span class="n">v</span><span class="p">:</span> <span class="n">k</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">vocab</span><span class="p">.</span><span class="n">items</span><span class="p">()}</span>

        <span class="c1"># GPT-2 style regex pattern
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">pattern</span> <span class="o">=</span> <span class="sa">r</span><span class="s">"""'(?:[sdmt]|ll|ve|re)| ?[a-zA-Z√Ä-√ø]+| ?[0-9]+| ?[^\s\w]+|\s+(?!\S)|\s+"""</span>

        <span class="c1"># Build merge rules for faster encoding
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">merge_rules</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">left_bytes</span><span class="p">,</span> <span class="n">right_bytes</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">merges</span><span class="p">):</span>
            <span class="c1"># Find what tokens these bytes correspond to
</span>            <span class="n">left_id</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">bytes_to_id</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="n">left_bytes</span><span class="p">)</span>
            <span class="n">right_id</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">bytes_to_id</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="n">right_bytes</span><span class="p">)</span>
            <span class="n">merged_bytes</span> <span class="o">=</span> <span class="n">left_bytes</span> <span class="o">+</span> <span class="n">right_bytes</span>
            <span class="n">merged_id</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">bytes_to_id</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="n">merged_bytes</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">left_id</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span> <span class="ow">and</span> <span class="n">right_id</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span> <span class="ow">and</span> <span class="n">merged_id</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
                <span class="bp">self</span><span class="p">.</span><span class="n">merge_rules</span><span class="p">[(</span><span class="n">left_id</span><span class="p">,</span> <span class="n">right_id</span><span class="p">)]</span> <span class="o">=</span> <span class="n">merged_id</span>

    <span class="k">def</span> <span class="nf">encode</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">text</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">]:</span>
        <span class="s">"""Encode text to token IDs."""</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">text</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">[]</span>

        <span class="c1"># Handle special tokens
</span>        <span class="n">token_ids</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">remaining_text</span> <span class="o">=</span> <span class="n">text</span>

        <span class="c1"># Split on special tokens first
</span>        <span class="k">for</span> <span class="n">special_token</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">special_tokens</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">special_token</span> <span class="ow">in</span> <span class="n">remaining_text</span><span class="p">:</span>
                <span class="n">parts</span> <span class="o">=</span> <span class="n">remaining_text</span><span class="p">.</span><span class="n">split</span><span class="p">(</span><span class="n">special_token</span><span class="p">)</span>
                <span class="n">new_parts</span> <span class="o">=</span> <span class="p">[]</span>
                <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">part</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">parts</span><span class="p">):</span>
                    <span class="k">if</span> <span class="n">i</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                        <span class="c1"># Add special token
</span>                        <span class="n">special_bytes</span> <span class="o">=</span> <span class="n">special_token</span><span class="p">.</span><span class="n">encode</span><span class="p">(</span><span class="s">'utf-8'</span><span class="p">)</span>
                        <span class="n">special_id</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">bytes_to_id</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="n">special_bytes</span><span class="p">)</span>
                        <span class="k">if</span> <span class="n">special_id</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
                            <span class="n">token_ids</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">special_id</span><span class="p">)</span>
                    <span class="k">if</span> <span class="n">part</span><span class="p">:</span>
                        <span class="n">new_parts</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">part</span><span class="p">)</span>
                <span class="n">remaining_text</span> <span class="o">=</span> <span class="s">''</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">new_parts</span><span class="p">)</span>

        <span class="c1"># Apply regex tokenization
</span>        <span class="k">for</span> <span class="n">match</span> <span class="ow">in</span> <span class="n">re</span><span class="p">.</span><span class="n">finditer</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">pattern</span><span class="p">,</span> <span class="n">remaining_text</span><span class="p">):</span>
            <span class="n">word</span> <span class="o">=</span> <span class="n">match</span><span class="p">.</span><span class="n">group</span><span class="p">()</span>
            <span class="n">word_tokens</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">_encode_word</span><span class="p">(</span><span class="n">word</span><span class="p">)</span>
            <span class="n">token_ids</span><span class="p">.</span><span class="n">extend</span><span class="p">(</span><span class="n">word_tokens</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">token_ids</span>

    <span class="k">def</span> <span class="nf">_encode_word</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">word</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">]:</span>
        <span class="s">"""Encode a single word using BPE merges."""</span>
        <span class="c1"># Start with individual bytes
</span>        <span class="n">word_bytes</span> <span class="o">=</span> <span class="n">word</span><span class="p">.</span><span class="n">encode</span><span class="p">(</span><span class="s">'utf-8'</span><span class="p">)</span>
        <span class="n">tokens</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="c1"># Convert each byte to its token ID
</span>        <span class="k">for</span> <span class="n">byte_val</span> <span class="ow">in</span> <span class="n">word_bytes</span><span class="p">:</span>
            <span class="n">tokens</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">byte_val</span><span class="p">)</span>  <span class="c1"># Byte token IDs are 0-255
</span>
        <span class="c1"># Apply BPE merges
</span>        <span class="k">while</span> <span class="nb">len</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="c1"># Find the best merge to apply
</span>            <span class="n">best_merge</span> <span class="o">=</span> <span class="bp">None</span>
            <span class="n">best_pos</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
            <span class="n">best_merge_priority</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="s">'inf'</span><span class="p">)</span>

            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>
                <span class="n">pair</span> <span class="o">=</span> <span class="p">(</span><span class="n">tokens</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">tokens</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">])</span>
                <span class="k">if</span> <span class="n">pair</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">merge_rules</span><span class="p">:</span>
                    <span class="c1"># Find merge priority (earlier merges have higher priority)
</span>                    <span class="n">merged_bytes</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">id_to_bytes</span><span class="p">[</span><span class="n">tokens</span><span class="p">[</span><span class="n">i</span><span class="p">]]</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">id_to_bytes</span><span class="p">[</span><span class="n">tokens</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]]</span>
                    <span class="k">for</span> <span class="n">j</span><span class="p">,</span> <span class="p">(</span><span class="n">left_bytes</span><span class="p">,</span> <span class="n">right_bytes</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">merges</span><span class="p">):</span>
                        <span class="k">if</span> <span class="n">left_bytes</span> <span class="o">+</span> <span class="n">right_bytes</span> <span class="o">==</span> <span class="n">merged_bytes</span><span class="p">:</span>
                            <span class="k">if</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="n">best_merge_priority</span><span class="p">:</span>
                                <span class="n">best_merge</span> <span class="o">=</span> <span class="n">pair</span>
                                <span class="n">best_pos</span> <span class="o">=</span> <span class="n">i</span>
                                <span class="n">best_merge_priority</span> <span class="o">=</span> <span class="n">j</span>
                            <span class="k">break</span>

            <span class="k">if</span> <span class="n">best_merge</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
                <span class="k">break</span>

            <span class="c1"># Apply the best merge
</span>            <span class="n">new_tokens</span> <span class="o">=</span> <span class="n">tokens</span><span class="p">[:</span><span class="n">best_pos</span><span class="p">]</span>
            <span class="n">new_tokens</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">merge_rules</span><span class="p">[</span><span class="n">best_merge</span><span class="p">])</span>
            <span class="n">new_tokens</span><span class="p">.</span><span class="n">extend</span><span class="p">(</span><span class="n">tokens</span><span class="p">[</span><span class="n">best_pos</span> <span class="o">+</span> <span class="mi">2</span><span class="p">:])</span>
            <span class="n">tokens</span> <span class="o">=</span> <span class="n">new_tokens</span>

        <span class="k">return</span> <span class="n">tokens</span>

    <span class="k">def</span> <span class="nf">decode</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">token_ids</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
        <span class="s">"""Decode token IDs back to text."""</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">token_ids</span><span class="p">:</span>
            <span class="k">return</span> <span class="s">""</span>

        <span class="c1"># Convert token IDs to bytes
</span>        <span class="n">byte_sequences</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">token_id</span> <span class="ow">in</span> <span class="n">token_ids</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">token_id</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">id_to_bytes</span><span class="p">:</span>
                <span class="n">byte_sequences</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">id_to_bytes</span><span class="p">[</span><span class="n">token_id</span><span class="p">])</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># Handle unknown tokens
</span>                <span class="n">byte_sequences</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="sa">b</span><span class="s">'&lt;UNK&gt;'</span><span class="p">)</span>

        <span class="c1"># Concatenate all bytes and decode
</span>        <span class="n">all_bytes</span> <span class="o">=</span> <span class="sa">b</span><span class="s">''</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">byte_sequences</span><span class="p">)</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">all_bytes</span><span class="p">.</span><span class="n">decode</span><span class="p">(</span><span class="s">'utf-8'</span><span class="p">,</span> <span class="n">errors</span><span class="o">=</span><span class="s">'replace'</span><span class="p">)</span>
        <span class="k">except</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">all_bytes</span><span class="p">.</span><span class="n">decode</span><span class="p">(</span><span class="s">'utf-8'</span><span class="p">,</span> <span class="n">errors</span><span class="o">=</span><span class="s">'ignore'</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">tokenize_with_details</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">text</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>
        <span class="s">"""Tokenize text and show detailed breakdown."""</span>
        <span class="n">token_ids</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">encode</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>

        <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Original text: '</span><span class="si">{</span><span class="n">text</span><span class="si">}</span><span class="s">'"</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Length: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">text</span><span class="p">)</span><span class="si">}</span><span class="s"> characters"</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"UTF-8 bytes: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">text</span><span class="p">.</span><span class="n">encode</span><span class="p">(</span><span class="s">'utf-8'</span><span class="p">))</span><span class="si">}</span><span class="s"> bytes"</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Token count: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">token_ids</span><span class="p">)</span><span class="si">}</span><span class="s"> tokens"</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Compression ratio: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">text</span><span class="p">.</span><span class="n">encode</span><span class="p">(</span><span class="s">'utf-8'</span><span class="p">))</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">token_ids</span><span class="p">)</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s">x"</span><span class="p">)</span>
        <span class="k">print</span><span class="p">()</span>

        <span class="k">print</span><span class="p">(</span><span class="s">"Token breakdown:"</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">token_id</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">token_ids</span><span class="p">):</span>
            <span class="n">token_bytes</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">id_to_bytes</span><span class="p">[</span><span class="n">token_id</span><span class="p">]</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="n">token_str</span> <span class="o">=</span> <span class="n">token_bytes</span><span class="p">.</span><span class="n">decode</span><span class="p">(</span><span class="s">'utf-8'</span><span class="p">,</span> <span class="n">errors</span><span class="o">=</span><span class="s">'replace'</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">token_str</span><span class="p">.</span><span class="n">isprintable</span><span class="p">():</span>
                    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"  </span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">:</span><span class="mi">2</span><span class="n">d</span><span class="si">}</span><span class="s">. Token </span><span class="si">{</span><span class="n">token_id</span><span class="si">:</span><span class="mi">4</span><span class="n">d</span><span class="si">}</span><span class="s">: '</span><span class="si">{</span><span class="n">token_str</span><span class="si">}</span><span class="s">' (</span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">token_bytes</span><span class="p">)</span><span class="si">}</span><span class="s"> bytes)"</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"  </span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">:</span><span class="mi">2</span><span class="n">d</span><span class="si">}</span><span class="s">. Token </span><span class="si">{</span><span class="n">token_id</span><span class="si">:</span><span class="mi">4</span><span class="n">d</span><span class="si">}</span><span class="s">: </span><span class="si">{</span><span class="nb">repr</span><span class="p">(</span><span class="n">token_str</span><span class="p">)</span><span class="si">}</span><span class="s"> (</span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">token_bytes</span><span class="p">)</span><span class="si">}</span><span class="s"> bytes)"</span><span class="p">)</span>
            <span class="k">except</span><span class="p">:</span>
                <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"  </span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">:</span><span class="mi">2</span><span class="n">d</span><span class="si">}</span><span class="s">. Token </span><span class="si">{</span><span class="n">token_id</span><span class="si">:</span><span class="mi">4</span><span class="n">d</span><span class="si">}</span><span class="s">: </span><span class="si">{</span><span class="n">token_bytes</span><span class="si">}</span><span class="s"> (binary)"</span><span class="p">)</span>

        <span class="c1"># Verify round-trip
</span>        <span class="n">decoded</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">decode</span><span class="p">(</span><span class="n">token_ids</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"</span><span class="se">\n</span><span class="s">Decoded text: '</span><span class="si">{</span><span class="n">decoded</span><span class="si">}</span><span class="s">'"</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Round-trip successful: </span><span class="si">{</span><span class="n">text</span> <span class="o">==</span> <span class="n">decoded</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">token_ids</span>
</code></pre></div></div>

<p>Let us compose some simple test cases below:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">test_bpe_tokenizer</span><span class="p">():</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"="</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"BPE TOKENIZER SAMPLE TESTS"</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"="</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>

    <span class="c1"># Load the trained tokenizer
</span>    <span class="k">try</span><span class="p">:</span>
        <span class="n">vocab</span><span class="p">,</span> <span class="n">merges</span> <span class="o">=</span> <span class="n">load_tokenizer</span><span class="p">(</span><span class="s">'tinystories_vocab.pkl'</span><span class="p">,</span> <span class="s">'tinystories_merges.pkl'</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"‚úì Loaded tokenizer with </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">vocab</span><span class="p">)</span><span class="si">}</span><span class="s"> vocab entries and </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">merges</span><span class="p">)</span><span class="si">}</span><span class="s"> merges"</span><span class="p">)</span>
    <span class="k">except</span> <span class="nb">FileNotFoundError</span><span class="p">:</span>
        <span class="k">print</span><span class="p">(</span><span class="s">"Error: Tokenizer files not found!"</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="s">"Please run the training script first to create 'tinystories_vocab.pkl' and 'tinystories_merges.pkl'"</span><span class="p">)</span>
        <span class="k">return</span>

    <span class="c1"># Create tokenizer instance
</span>    <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">SimpleBPETokenizer</span><span class="p">(</span><span class="n">vocab</span><span class="p">,</span> <span class="n">merges</span><span class="p">)</span>
    <span class="k">print</span><span class="p">()</span>

    <span class="c1"># Example 1: Simple sentence
</span>    <span class="k">print</span><span class="p">(</span><span class="s">"EXAMPLE 1: Simple sentence"</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"-"</span> <span class="o">*</span> <span class="mi">30</span><span class="p">)</span>
    <span class="n">text1</span> <span class="o">=</span> <span class="s">"Hello world! How are you today?"</span>
    <span class="n">tokenizer</span><span class="p">.</span><span class="n">tokenize_with_details</span><span class="p">(</span><span class="n">text1</span><span class="p">)</span>
    <span class="k">print</span><span class="p">()</span>

    <span class="c1"># Example 2: Text with special token
</span>    <span class="k">print</span><span class="p">(</span><span class="s">"EXAMPLE 2: Text with special token"</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"-"</span> <span class="o">*</span> <span class="mi">30</span><span class="p">)</span>
    <span class="n">text2</span> <span class="o">=</span> <span class="s">"Once upon a time&lt;|endoftext|&gt;The end."</span>
    <span class="n">tokenizer</span><span class="p">.</span><span class="n">tokenize_with_details</span><span class="p">(</span><span class="n">text2</span><span class="p">)</span>
    <span class="k">print</span><span class="p">()</span>

    <span class="c1"># Example 3: Repeated words (should compress well)
</span>    <span class="k">print</span><span class="p">(</span><span class="s">"EXAMPLE 3: Repeated words"</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"-"</span> <span class="o">*</span> <span class="mi">30</span><span class="p">)</span>
    <span class="n">text3</span> <span class="o">=</span> <span class="s">"the the the cat cat sat sat on on the the mat mat"</span>
    <span class="n">tokenizer</span><span class="p">.</span><span class="n">tokenize_with_details</span><span class="p">(</span><span class="n">text3</span><span class="p">)</span>
    <span class="k">print</span><span class="p">()</span>

    <span class="c1"># Example 4: Numbers and punctuation
</span>    <span class="k">print</span><span class="p">(</span><span class="s">"EXAMPLE 4: Numbers and punctuation"</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"-"</span> <span class="o">*</span> <span class="mi">30</span><span class="p">)</span>
    <span class="n">text4</span> <span class="o">=</span> <span class="s">"I have 123 apples, 456 oranges, and 789 bananas!"</span>
    <span class="n">tokenizer</span><span class="p">.</span><span class="n">tokenize_with_details</span><span class="p">(</span><span class="n">text4</span><span class="p">)</span>
    <span class="k">print</span><span class="p">()</span>

    <span class="c1"># Example 5: Just encoding/decoding
</span>    <span class="k">print</span><span class="p">(</span><span class="s">"EXAMPLE 5: Simple encode/decode"</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"-"</span> <span class="o">*</span> <span class="mi">30</span><span class="p">)</span>
    <span class="n">text5</span> <span class="o">=</span> <span class="s">"This is a test."</span>
    <span class="n">token_ids</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">.</span><span class="n">encode</span><span class="p">(</span><span class="n">text5</span><span class="p">)</span>
    <span class="n">decoded_text</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">.</span><span class="n">decode</span><span class="p">(</span><span class="n">token_ids</span><span class="p">)</span>

    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Original: '</span><span class="si">{</span><span class="n">text5</span><span class="si">}</span><span class="s">'"</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Token IDs: </span><span class="si">{</span><span class="n">token_ids</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Decoded: '</span><span class="si">{</span><span class="n">decoded_text</span><span class="si">}</span><span class="s">'"</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Match: </span><span class="si">{</span><span class="n">text5</span> <span class="o">==</span> <span class="n">decoded_text</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
    <span class="k">print</span><span class="p">()</span>

    <span class="c1"># Show some vocabulary statistics
</span>    <span class="k">print</span><span class="p">(</span><span class="s">"VOCABULARY STATISTICS"</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"-"</span> <span class="o">*</span> <span class="mi">30</span><span class="p">)</span>
    <span class="n">byte_tokens</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="mi">1</span> <span class="k">for</span> <span class="n">tid</span> <span class="ow">in</span> <span class="n">vocab</span><span class="p">.</span><span class="n">keys</span><span class="p">()</span> <span class="k">if</span> <span class="n">tid</span> <span class="o">&lt;</span> <span class="mi">256</span><span class="p">)</span>
    <span class="n">special_tokens</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="mi">1</span> <span class="k">for</span> <span class="n">tid</span><span class="p">,</span> <span class="n">token_bytes</span> <span class="ow">in</span> <span class="n">vocab</span><span class="p">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="sa">b</span><span class="s">'&lt;|'</span> <span class="ow">in</span> <span class="n">token_bytes</span><span class="p">)</span>
    <span class="n">merged_tokens</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">vocab</span><span class="p">)</span> <span class="o">-</span> <span class="n">byte_tokens</span> <span class="o">-</span> <span class="n">special_tokens</span>

    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Byte tokens (0-255): </span><span class="si">{</span><span class="n">byte_tokens</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Special tokens: </span><span class="si">{</span><span class="n">special_tokens</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Merged tokens: </span><span class="si">{</span><span class="n">merged_tokens</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Total vocabulary: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">vocab</span><span class="p">)</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>

    <span class="c1"># Show some example merged tokens
</span>    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"</span><span class="se">\n</span><span class="s">Sample merged tokens:"</span><span class="p">)</span>
    <span class="n">merged_token_ids</span> <span class="o">=</span> <span class="p">[</span><span class="n">tid</span> <span class="k">for</span> <span class="n">tid</span> <span class="ow">in</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">vocab</span><span class="p">.</span><span class="n">keys</span><span class="p">())</span> <span class="k">if</span> <span class="n">tid</span> <span class="o">&gt;=</span> <span class="mi">257</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">token_id</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">merged_token_ids</span><span class="p">[:</span><span class="mi">10</span><span class="p">]):</span>
        <span class="n">token_bytes</span> <span class="o">=</span> <span class="n">vocab</span><span class="p">[</span><span class="n">token_id</span><span class="p">]</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">decoded</span> <span class="o">=</span> <span class="n">token_bytes</span><span class="p">.</span><span class="n">decode</span><span class="p">(</span><span class="s">'utf-8'</span><span class="p">,</span> <span class="n">errors</span><span class="o">=</span><span class="s">'replace'</span><span class="p">)</span>
            <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"  Token </span><span class="si">{</span><span class="n">token_id</span><span class="si">}</span><span class="s">: '</span><span class="si">{</span><span class="n">decoded</span><span class="si">}</span><span class="s">' (</span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">token_bytes</span><span class="p">)</span><span class="si">}</span><span class="s"> bytes)"</span><span class="p">)</span>
        <span class="k">except</span><span class="p">:</span>
            <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"  Token </span><span class="si">{</span><span class="n">token_id</span><span class="si">}</span><span class="s">: </span><span class="si">{</span><span class="n">token_bytes</span><span class="si">}</span><span class="s"> (binary)"</span><span class="p">)</span>

    <span class="k">print</span><span class="p">(</span><span class="s">"</span><span class="se">\n</span><span class="s">"</span> <span class="o">+</span> <span class="s">"="</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"All examples completed successfully!"</span><span class="p">)</span>
</code></pre></div></div>

<h1 id="bpe-tokenizer-sample-tests">BPE Tokenizer Sample Tests</h1>

<p>Now run our complete test suite:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">test_bpe_tokenizer</span><span class="p">()</span>
</code></pre></div></div>
<p>Based on the training output from the TinyStories dataset, here are the testing results:</p>

<p>‚úì Loaded tokenizer with 10000 vocab entries and 9743 merges</p>

<h2 id="example-1-simple-sentence">Example 1: Simple sentence</h2>

<p><strong>Original text:</strong> ‚ÄòHello world! How are you today?‚Äô<br />
<strong>Length:</strong> 31 characters<br />
<strong>UTF-8 bytes:</strong> 31 bytes<br />
<strong>Token count:</strong> 8 tokens<br />
<strong>Compression ratio:</strong> 3.88x</p>

<h3 id="token-breakdown">Token breakdown:</h3>
<ol>
  <li>Token 1183: ‚ÄòHello‚Äô (5 bytes)</li>
  <li>Token 1569: ‚Äò world‚Äô (6 bytes)</li>
  <li>Token 33: ‚Äò!‚Äô (1 bytes)</li>
  <li>Token 2683: ‚Äò How‚Äô (4 bytes)</li>
  <li>Token 483: ‚Äò are‚Äô (4 bytes)</li>
  <li>Token 349: ‚Äò you‚Äô (4 bytes)</li>
  <li>Token 1709: ‚Äò today‚Äô (6 bytes)</li>
  <li>Token 63: ‚Äò?‚Äô (1 bytes)</li>
</ol>

<p><strong>Decoded text:</strong> ‚ÄòHello world! How are you today?‚Äô<br />
<strong>Round-trip successful:</strong> True</p>

<h2 id="example-2-text-with-special-token">Example 2: Text with special token</h2>

<p><strong>Original text:</strong> ‚ÄòOnce upon a time&lt;|endoftext|&gt;The end.‚Äô<br />
<strong>Length:</strong> 37 characters<br />
<strong>UTF-8 bytes:</strong> 37 bytes<br />
<strong>Token count:</strong> 8 tokens<br />
<strong>Compression ratio:</strong> 4.62x</p>

<h3 id="token-breakdown-1">Token breakdown:</h3>
<ol>
  <li>Token 256: ‚Äò<code class="language-plaintext highlighter-rouge">&lt;|endoftext|&gt;</code>‚Äô (13 bytes)</li>
  <li>Token 430: ‚ÄòOnce‚Äô (4 bytes)</li>
  <li>Token 439: ‚Äò upon‚Äô (5 bytes)</li>
  <li>Token 259: ‚Äò a‚Äô (2 bytes)</li>
  <li>Token 398: ‚Äò time‚Äô (5 bytes)</li>
  <li>Token 410: ‚ÄòThe‚Äô (3 bytes)</li>
  <li>Token 870: ‚Äò end‚Äô (4 bytes)</li>
  <li>Token 46: ‚Äò.‚Äô (1 bytes)</li>
</ol>

<p><strong>Decoded text:</strong> ‚Äò&lt;|endoftext|&gt;Once upon a timeThe end.‚Äô<br />
<strong>Round-trip successful:</strong> False</p>

<h2 id="example-3-repeated-words">Example 3: Repeated words</h2>

<p><strong>Original text:</strong> ‚Äòthe the the cat cat sat sat on on the the mat mat‚Äô<br />
<strong>Length:</strong> 49 characters<br />
<strong>UTF-8 bytes:</strong> 49 bytes<br />
<strong>Token count:</strong> 13 tokens<br />
<strong>Compression ratio:</strong> 3.77x</p>

<h3 id="token-breakdown-2">Token breakdown:</h3>
<ol>
  <li>Token 7199: ‚Äòthe‚Äô (3 bytes)</li>
  <li>Token 263: ‚Äò the‚Äô (4 bytes)</li>
  <li>Token 263: ‚Äò the‚Äô (4 bytes)</li>
  <li>Token 459: ‚Äò cat‚Äô (4 bytes)</li>
  <li>Token 459: ‚Äò cat‚Äô (4 bytes)</li>
  <li>Token 1091: ‚Äò sat‚Äô (4 bytes)</li>
  <li>Token 1091: ‚Äò sat‚Äô (4 bytes)</li>
  <li>Token 354: ‚Äò on‚Äô (3 bytes)</li>
  <li>Token 354: ‚Äò on‚Äô (3 bytes)</li>
  <li>Token 263: ‚Äò the‚Äô (4 bytes)</li>
  <li>Token 263: ‚Äò the‚Äô (4 bytes)</li>
  <li>Token 1492: ‚Äò mat‚Äô (4 bytes)</li>
  <li>Token 1492: ‚Äò mat‚Äô (4 bytes)</li>
</ol>

<p><strong>Decoded text:</strong> ‚Äòthe the the cat cat sat sat on on the the mat mat‚Äô<br />
<strong>Round-trip successful:</strong> True</p>

<h2 id="example-4-numbers-and-punctuation">Example 4: Numbers and punctuation</h2>

<p><strong>Original text:</strong> ‚ÄòI have 123 apples, 456 oranges, and 789 bananas!‚Äô<br />
<strong>Length:</strong> 48 characters<br />
<strong>UTF-8 bytes:</strong> 48 bytes<br />
<strong>Token count:</strong> 19 tokens<br />
<strong>Compression ratio:</strong> 2.53x</p>

<h3 id="token-breakdown-3">Token breakdown:</h3>
<ol>
  <li>Token 73: ‚ÄòI‚Äô (1 bytes)</li>
  <li>Token 499: ‚Äò have‚Äô (5 bytes)</li>
  <li>Token 6314: ‚Äò 1‚Äô (2 bytes)</li>
  <li>Token 50: ‚Äò2‚Äô (1 bytes)</li>
  <li>Token 51: ‚Äò3‚Äô (1 bytes)</li>
  <li>Token 1836: ‚Äò apples‚Äô (7 bytes)</li>
  <li>Token 44: ‚Äò,‚Äô (1 bytes)</li>
  <li>Token 9079: ‚Äò 4‚Äô (2 bytes)</li>
  <li>Token 53: ‚Äò5‚Äô (1 bytes)</li>
  <li>Token 54: ‚Äò6‚Äô (1 bytes)</li>
  <li>Token 5193: ‚Äò oranges‚Äô (8 bytes)</li>
  <li>Token 44: ‚Äò,‚Äô (1 bytes)</li>
  <li>Token 267: ‚Äò and‚Äô (4 bytes)</li>
  <li>Token 32: ‚Äò ‚Äò (1 bytes)</li>
  <li>Token 55: ‚Äò7‚Äô (1 bytes)</li>
  <li>Token 56: ‚Äò8‚Äô (1 bytes)</li>
  <li>Token 57: ‚Äò9‚Äô (1 bytes)</li>
  <li>Token 3898: ‚Äò bananas‚Äô (8 bytes)</li>
  <li>Token 33: ‚Äò!‚Äô (1 bytes)</li>
</ol>

<p><strong>Decoded text:</strong> ‚ÄòI have 123 apples, 456 oranges, and 789 bananas!‚Äô<br />
<strong>Round-trip successful:</strong> True</p>

<h2 id="example-5-simple-encodedecode">Example 5: Simple encode/decode</h2>

<p><strong>Original:</strong> ‚ÄòThis is a test.‚Äô<br />
<strong>Token IDs:</strong> [1531, 431, 259, 2569, 46]<br />
<strong>Decoded:</strong> ‚ÄòThis is a test.‚Äô<br />
<strong>Match:</strong> True</p>

<h2 id="vocabulary-statistics">Vocabulary Statistics</h2>

<p><strong>Byte tokens (0-255):</strong> 256<br />
<strong>Special tokens:</strong> 1<br />
<strong>Merged tokens:</strong> 9743<br />
<strong>Total vocabulary:</strong> 10000</p>

<h3 id="sample-merged-tokens">Sample merged tokens:</h3>
<ul>
  <li>Token 257: ‚Äò t‚Äô (2 bytes)</li>
  <li>Token 258: ‚Äòhe‚Äô (2 bytes)</li>
  <li>Token 259: ‚Äò a‚Äô (2 bytes)</li>
  <li>Token 260: ‚Äò s‚Äô (2 bytes)</li>
  <li>Token 261: ‚Äò w‚Äô (2 bytes)</li>
  <li>Token 262: ‚Äònd‚Äô (2 bytes)</li>
  <li>Token 263: ‚Äò the‚Äô (4 bytes)</li>
  <li>Token 264: ‚Äòed‚Äô (2 bytes)</li>
  <li>Token 265: ‚Äò b‚Äô (2 bytes)</li>
  <li>Token 266: ‚Äò to‚Äô (3 bytes)</li>
</ul>

<hr />
<p>All examples completed successfully!</p>]]></content><author><name>[&quot;Han Yu&quot;]</name></author><category term="cs336" /><summary type="html"><![CDATA[Building a BPE Tokenizer from Scratch: Train the Tokenizer using TinyStories Dataset]]></summary></entry><entry><title type="html">Study Notes: Stanford CS336 Language Modeling from Scratch [2]</title><link href="http://localhost:4000/cs336/2025/07/22/cs336-note-simple-bpe.html" rel="alternate" type="text/html" title="Study Notes: Stanford CS336 Language Modeling from Scratch [2]" /><published>2025-07-22T00:00:00-07:00</published><updated>2025-07-22T00:00:00-07:00</updated><id>http://localhost:4000/cs336/2025/07/22/cs336-note-simple-bpe</id><content type="html" xml:base="http://localhost:4000/cs336/2025/07/22/cs336-note-simple-bpe.html"><![CDATA[<h1 id="byte-pair-encoding-bpe-tokenizer-in-a-nutshell">Byte Pair Encoding (BPE) Tokenizer in a Nutshell</h1>
<h2 id="key-terms">Key Terms</h2>

<table>
  <thead>
    <tr>
      <th style="text-align: left">Concept</th>
      <th style="text-align: left">Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">Unicode</td>
      <td style="text-align: left">System that assigns every character a unique codepoint (e.g., ‚ÄòA‚Äô ‚Üí 65)</td>
    </tr>
    <tr>
      <td style="text-align: left">UTF-8</td>
      <td style="text-align: left">A way to encode those codepoints into 1-4 bytes</td>
    </tr>
    <tr>
      <td style="text-align: left">Byte</td>
      <td style="text-align: left">8 bits; one byte can hold values from 0 to 255</td>
    </tr>
    <tr>
      <td style="text-align: left">Tokenization</td>
      <td style="text-align: left">Breaking text corpus input into manageable pieces (tokens) for a model</td>
    </tr>
  </tbody>
</table>

<p>Let us take the following string as a simple example to illustrate the concept.</p>

<h2 id="example-encoding-a">Example: Encoding ‚ÄòAüòä‚Äô</h2>

<h3 id="step-1-get-the-unicode-codepoints">Step 1: Get the Unicode Codepoints</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">text</span> <span class="o">=</span> <span class="s">'Aüòä'</span>
<span class="n">codepoints</span> <span class="o">=</span> <span class="p">[</span><span class="nb">ord</span><span class="p">(</span><span class="s">'A'</span><span class="p">),</span> <span class="nb">ord</span><span class="p">(</span><span class="s">'üòä'</span><span class="p">)]</span>
<span class="k">print</span><span class="p">(</span><span class="n">codepoints</span><span class="p">)</span>  <span class="c1"># [65, 128522]
</span></code></pre></div></div>

<p>Output:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[65, 128522]
</code></pre></div></div>

<h3 id="step-2-utf-8-encoding-turn-codepoints-into-bytes">Step 2: UTF-8 Encoding (Turn Codepoints into Bytes)</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">utf8_bytes</span> <span class="o">=</span> <span class="n">text</span><span class="p">.</span><span class="n">encode</span><span class="p">(</span><span class="s">"utf-8"</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="nb">tuple</span><span class="p">(</span><span class="n">utf8_bytes</span><span class="p">))</span>  <span class="c1"># (65, 240, 159, 152, 138)
</span></code></pre></div></div>

<p>Output:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(65, 240, 159, 152, 138)
</code></pre></div></div>

<p>Here‚Äôs what happened:</p>
<ul>
  <li>‚ÄòA‚Äô is encoded using one byte: 65</li>
  <li>‚Äòüòä‚Äô is encoded using four bytes: [240, 159, 152, 138]</li>
  <li>‚ÄòAüòä‚Äô is encoded as the sequence [65, 240, 159, 152, 138]</li>
</ul>

<h2 id="why-using-utf-8-for-encoding-is-helpful">Why Using UTF-8 for Encoding is Helpful</h2>

<p>Instead of dealing with hundreds of thousands of possible codepoints (Unicode has more than 150,000 codepoints) or millions of words/subwords in vocabulary, we can model text using sequences of bytes. Each byte can be represented by an integer from 0 to 255, so we only need a vocabulary of size 256 to model input text. This approach is simple and complete‚Äîany character in any language can be represented as bytes, eliminating out-of-vocabulary token concerns.</p>

<h2 id="tokenization-spectrum">Tokenization Spectrum</h2>

<table>
  <thead>
    <tr>
      <th style="text-align: left">Tokenization Level</th>
      <th style="text-align: left">Example Tokens</th>
      <th style="text-align: left">Pros</th>
      <th style="text-align: left">Cons</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left"><strong>Word-level</strong></td>
      <td style="text-align: left"><code class="language-plaintext highlighter-rouge">["unbelievable"]</code></td>
      <td style="text-align: left">Human-readable, efficient</td>
      <td style="text-align: left">OOV (out-of-vocabulary) issues</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>Subword-level</strong> (BPE)</td>
      <td style="text-align: left"><code class="language-plaintext highlighter-rouge">["un", "believ", "able"]</code></td>
      <td style="text-align: left">Handles rare words, compact</td>
      <td style="text-align: left">Requires training</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>Byte-level</strong></td>
      <td style="text-align: left"><code class="language-plaintext highlighter-rouge">[117, 110, 98, 101, ...]</code> (bytes)</td>
      <td style="text-align: left">No OOV, simple</td>
      <td style="text-align: left">Longer sequences, less semantic meaning</td>
    </tr>
  </tbody>
</table>

<h2 id="why-subword-tokenization-is-the-middle-ground">Why Subword Tokenization is the Middle Ground</h2>

<p><strong>Subword tokenization</strong> with <strong>Byte Pair Encoding (BPE)</strong> provides a balance between the other approaches:</p>

<ul>
  <li><strong>Word-level tokenization</strong> struggles with rare or unseen words (e.g., ‚Äúunbelievable‚Äù might be unknown even if ‚Äúbelieve‚Äù is known)</li>
  <li><strong>Byte-level tokenization</strong> avoids unknown token issues but creates long, inefficient sequences</li>
  <li><strong>Subword tokenization</strong> (BPE):
    <ol>
      <li>Breaks rare words into familiar pieces (subwords)</li>
      <li>Retains compactness for common words</li>
      <li>Is learnable from corpus statistics</li>
    </ol>
  </li>
</ul>

<h2 id="byte-pair-encoding-bpe-algorithm-overview">Byte Pair Encoding (BPE) Algorithm Overview</h2>

<p>BPE starts from characters and iteratively <strong>merges the most frequent adjacent pairs</strong> into longer tokens.</p>

<h3 id="example-training-corpus">Example Training Corpus</h3>

<p>Consider this toy training corpus:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>"low"     (5 times)  
"lowest"  (2 times)  
"newest"  (6 times)  
"wider"   (3 times)
</code></pre></div></div>

<p>We want to learn a compact subword vocabulary that reuses frequent patterns like ‚Äúlow‚Äù and ‚Äúest‚Äù.</p>

<h3 id="step-by-step-bpe-process">Step-by-Step BPE Process</h3>

<h4 id="step-0-preprocess-as-characters">Step 0: Preprocess as Characters</h4>
<p>Each word is broken into characters with an end-of-word marker <code class="language-plaintext highlighter-rouge">&lt;/w&gt;</code>:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>"l o w &lt;/w&gt;"        (5)
"l o w e s t &lt;/w&gt;"  (2)
"n e w e s t &lt;/w&gt;"  (6)
"w i d e r &lt;/w&gt;"    (3)
</code></pre></div></div>

<h4 id="step-1-count-adjacent-pairs">Step 1: Count Adjacent Pairs</h4>
<p>Compute most frequent adjacent pairs across all words:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>('e', 's') appears 8 times
('s', 't') appears 8 times
('l', 'o') appears 7 times
('o', 'w') appears 7 times
</code></pre></div></div>

<h4 id="step-2-merge-e--s--es">Step 2: Merge ‚Äòe‚Äô + ‚Äòs‚Äô ‚Üí ‚Äòes‚Äô</h4>
<p>Update vocabulary:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>"l o w &lt;/w&gt;"          (5)
"l o w es t &lt;/w&gt;"     (2)
"n e w es t &lt;/w&gt;"     (6)
"w i d e r &lt;/w&gt;"      (3)
</code></pre></div></div>

<h4 id="step-3-merge-es--t--est">Step 3: Merge ‚Äòes‚Äô + ‚Äòt‚Äô ‚Üí ‚Äòest‚Äô</h4>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>"l o w &lt;/w&gt;"         (5)
"l o w est &lt;/w&gt;"     (2)
"n e w est &lt;/w&gt;"     (6)
"w i d e r &lt;/w&gt;"     (3)
</code></pre></div></div>

<h4 id="step-4-merge-l--o--lo-then-lo--w--low">Step 4: Merge ‚Äòl‚Äô + ‚Äòo‚Äô ‚Üí ‚Äòlo‚Äô, then ‚Äòlo‚Äô + ‚Äòw‚Äô ‚Üí ‚Äòlow‚Äô</h4>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>"low &lt;/w&gt;"          (5)
"low est &lt;/w&gt;"      (2)
"n e w est &lt;/w&gt;"    (6)
"w i d e r &lt;/w&gt;"    (3)
</code></pre></div></div>

<h4 id="continue-merging">Continue merging‚Ä¶</h4>
<p>Eventually we learn useful building blocks like ‚Äòlow‚Äô, ‚Äòest‚Äô, and ‚Äònew‚Äô. After training, ‚Äúnewest‚Äù would tokenize to <code class="language-plaintext highlighter-rouge">['new', 'est', '&lt;/w&gt;']</code>.</p>

<h2 id="bpe-implementation">BPE Implementation</h2>

<p>Below is a complete implementation demonstrating the BPE algorithm on the corpus:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>"low low low low low lower lower widest widest widest newest newest newest newest newest newest"
</code></pre></div></div>

<h3 id="key-components">Key Components</h3>

<ol>
  <li><strong>Initialization</strong>: Creates vocabulary with <code class="language-plaintext highlighter-rouge">&lt;|endoftext|&gt;</code> special token and all 256 byte values</li>
  <li><strong>Pre-tokenization</strong>: Splits text on whitespace and converts words to byte tuples</li>
  <li><strong>Pair Frequency Counting</strong>: Counts all adjacent byte pairs across the corpus</li>
  <li><strong>Merging</strong>: Merges the most frequent pair (lexicographically largest in case of ties)</li>
  <li><strong>Tokenization</strong>: Uses learned merges to tokenize new words</li>
</ol>

<h3 id="how-it-works">How It Works</h3>

<ol>
  <li><strong>Pre-tokenization</strong>: Converts <code class="language-plaintext highlighter-rouge">"low low low..."</code> into <code class="language-plaintext highlighter-rouge">{(l,o,w): 5, (l,o,w,e,r): 2, ...}</code></li>
  <li><strong>Merge Selection</strong>: Finds most frequent pairs like <code class="language-plaintext highlighter-rouge">('s','t')</code> and <code class="language-plaintext highlighter-rouge">('e','s')</code>, chooses lexicographically larger <code class="language-plaintext highlighter-rouge">('s','t')</code></li>
  <li><strong>Iterative Merging</strong>: Continues merging until desired number of merges is reached</li>
  <li><strong>Tokenization</strong>: Applies learned merges in order to tokenize new words</li>
</ol>

<h3 id="expected-output">Expected Output</h3>

<p>With 6 merges, the algorithm produces:</p>
<ul>
  <li><strong>Merges</strong>: <code class="language-plaintext highlighter-rouge">['s t', 'e st', 'o w', 'l ow', 'w est', 'n e']</code></li>
  <li><strong>Final vocabulary</strong>: <code class="language-plaintext highlighter-rouge">&lt;|endoftext|&gt;</code>, 256 byte chars, <code class="language-plaintext highlighter-rouge">st</code>, <code class="language-plaintext highlighter-rouge">est</code>, <code class="language-plaintext highlighter-rouge">ow</code>, <code class="language-plaintext highlighter-rouge">low</code>, <code class="language-plaintext highlighter-rouge">west</code>, <code class="language-plaintext highlighter-rouge">ne</code></li>
  <li><strong>‚Äúnewest‚Äù tokenizes as</strong>: <code class="language-plaintext highlighter-rouge">['ne', 'west']</code></li>
</ul>

<p>Below is one implementation for Algorithm 1 of <a href="https://arxiv.org/abs/1508.07909">Sennrich et al. [2016]</a>.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">defaultdict</span><span class="p">,</span> <span class="n">Counter</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">List</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">,</span> <span class="n">Set</span><span class="p">,</span> <span class="n">Union</span>

<span class="k">class</span> <span class="nc">BPEEncoder</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># Initialize vocabulary with special token and 256 byte values
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">vocab</span> <span class="o">=</span> <span class="p">{</span><span class="s">"&lt;|endoftext|&gt;"</span><span class="p">:</span> <span class="mi">0</span><span class="p">}</span>
        <span class="c1"># Add all possible byte values (0-255) to vocabulary
</span>        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">256</span><span class="p">):</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">vocab</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span>

        <span class="bp">self</span><span class="p">.</span><span class="n">merges</span> <span class="o">=</span> <span class="p">[]</span>  <span class="c1"># List of (token1, token2) pairs that were merged
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">merge_tokens</span> <span class="o">=</span> <span class="p">{}</span>  <span class="c1"># Maps (token1, token2) -&gt; new_token_id
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">token_names</span> <span class="o">=</span> <span class="p">{}</span>  <span class="c1"># Maps token_id -&gt; readable name
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">next_token_id</span> <span class="o">=</span> <span class="mi">257</span>

    <span class="k">def</span> <span class="nf">pre_tokenize</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">text</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="p">...],</span> <span class="nb">int</span><span class="p">]:</span>
        <span class="s">"""
        Pre-tokenize text by splitting on whitespace and convert to byte tuples.
        Returns frequency count of each word as tuple of byte integers.
        For example, converts "low low low..." into {(l,o,w): 5, (l,o,w,e,r): 2, ...}
        """</span>
        <span class="n">words</span> <span class="o">=</span> <span class="n">text</span><span class="p">.</span><span class="n">split</span><span class="p">()</span>
        <span class="n">word_freq</span> <span class="o">=</span> <span class="n">Counter</span><span class="p">(</span><span class="n">words</span><span class="p">)</span>

        <span class="c1"># Convert to tuple of byte integers
</span>        <span class="n">byte_word_freq</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">freq</span> <span class="ow">in</span> <span class="n">word_freq</span><span class="p">.</span><span class="n">items</span><span class="p">():</span>
            <span class="n">byte_tuple</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">word</span><span class="p">.</span><span class="n">encode</span><span class="p">(</span><span class="s">'utf-8'</span><span class="p">))</span>
            <span class="n">byte_word_freq</span><span class="p">[</span><span class="n">byte_tuple</span><span class="p">]</span> <span class="o">=</span> <span class="n">freq</span>

        <span class="k">return</span> <span class="n">byte_word_freq</span>

    <span class="k">def</span> <span class="nf">get_pair_frequencies</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">word_freq</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">str</span><span class="p">],</span> <span class="p">...],</span> <span class="nb">int</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">str</span><span class="p">],</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">str</span><span class="p">]],</span> <span class="nb">int</span><span class="p">]:</span>
        <span class="s">"""
        Count frequency of all adjacent token pairs across all words.
        """</span>
        <span class="n">pair_freq</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">freq</span> <span class="ow">in</span> <span class="n">word_freq</span><span class="p">.</span><span class="n">items</span><span class="p">():</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">word</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>
                <span class="n">pair</span> <span class="o">=</span> <span class="p">(</span><span class="n">word</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">word</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">])</span>
                <span class="n">pair_freq</span><span class="p">[</span><span class="n">pair</span><span class="p">]</span> <span class="o">+=</span> <span class="n">freq</span>

        <span class="k">return</span> <span class="nb">dict</span><span class="p">(</span><span class="n">pair_freq</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">merge_pair</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">word_freq</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">str</span><span class="p">],</span> <span class="p">...],</span> <span class="nb">int</span><span class="p">],</span>
                   <span class="n">pair_to_merge</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">str</span><span class="p">],</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">str</span><span class="p">]],</span>
                   <span class="n">new_token</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">str</span><span class="p">],</span> <span class="p">...],</span> <span class="nb">int</span><span class="p">]:</span>
        <span class="s">"""
        Merge the specified pair in all words where it appears.
        """</span>
        <span class="n">new_word_freq</span> <span class="o">=</span> <span class="p">{}</span>

        <span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">freq</span> <span class="ow">in</span> <span class="n">word_freq</span><span class="p">.</span><span class="n">items</span><span class="p">():</span>
            <span class="n">new_word</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="k">while</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">word</span><span class="p">):</span>
                <span class="c1"># Check if current position matches the pair to merge
</span>                <span class="k">if</span> <span class="p">(</span><span class="n">i</span> <span class="o">&lt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">word</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span> <span class="ow">and</span>
                    <span class="n">word</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">==</span> <span class="n">pair_to_merge</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="ow">and</span>
                    <span class="n">word</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="n">pair_to_merge</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
                    <span class="n">new_word</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">new_token</span><span class="p">)</span>
                    <span class="n">i</span> <span class="o">+=</span> <span class="mi">2</span>  <span class="c1"># Skip both tokens of the pair
</span>                <span class="k">else</span><span class="p">:</span>
                    <span class="n">new_word</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">word</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
                    <span class="n">i</span> <span class="o">+=</span> <span class="mi">1</span>

            <span class="n">new_word_freq</span><span class="p">[</span><span class="nb">tuple</span><span class="p">(</span><span class="n">new_word</span><span class="p">)]</span> <span class="o">=</span> <span class="n">freq</span>

        <span class="k">return</span> <span class="n">new_word_freq</span>

    <span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">text</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">num_merges</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
        <span class="s">"""
        Train BPE on the given text for specified number of merges.
        Returns list of merge operations performed.
        """</span>
        <span class="c1"># Pre-tokenize text
</span>        <span class="n">word_freq</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">pre_tokenize</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Initial word frequencies: </span><span class="si">{</span><span class="bp">self</span><span class="p">.</span><span class="n">_format_word_freq</span><span class="p">(</span><span class="n">word_freq</span><span class="p">)</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>

        <span class="n">merges_performed</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="k">for</span> <span class="n">merge_step</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_merges</span><span class="p">):</span>
            <span class="c1"># Get pair frequencies
</span>            <span class="n">pair_freq</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">get_pair_frequencies</span><span class="p">(</span><span class="n">word_freq</span><span class="p">)</span>

            <span class="k">if</span> <span class="ow">not</span> <span class="n">pair_freq</span><span class="p">:</span>
                <span class="k">break</span>

            <span class="c1"># Find most frequent pair (lexicographically largest in case of tie)
</span>            <span class="n">max_freq</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">pair_freq</span><span class="p">.</span><span class="n">values</span><span class="p">())</span>
            <span class="n">most_frequent_pairs</span> <span class="o">=</span> <span class="p">[</span><span class="n">pair</span> <span class="k">for</span> <span class="n">pair</span><span class="p">,</span> <span class="n">freq</span> <span class="ow">in</span> <span class="n">pair_freq</span><span class="p">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="n">freq</span> <span class="o">==</span> <span class="n">max_freq</span><span class="p">]</span>

            <span class="c1"># Sort pairs lexicographically - convert to comparable format
</span>            <span class="k">def</span> <span class="nf">pair_sort_key</span><span class="p">(</span><span class="n">pair</span><span class="p">):</span>
                <span class="k">def</span> <span class="nf">token_to_str</span><span class="p">(</span><span class="n">token</span><span class="p">):</span>
                    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">token</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
                        <span class="k">return</span> <span class="nb">chr</span><span class="p">(</span><span class="n">token</span><span class="p">)</span>
                    <span class="k">return</span> <span class="nb">str</span><span class="p">(</span><span class="n">token</span><span class="p">)</span>
                <span class="k">return</span> <span class="p">(</span><span class="n">token_to_str</span><span class="p">(</span><span class="n">pair</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="n">token_to_str</span><span class="p">(</span><span class="n">pair</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>

            <span class="c1"># Take lexicographically largest (max)
</span>            <span class="n">best_pair</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">most_frequent_pairs</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="n">pair_sort_key</span><span class="p">)</span>

            <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"</span><span class="se">\n</span><span class="s">Merge </span><span class="si">{</span><span class="n">merge_step</span> <span class="o">+</span> <span class="mi">1</span><span class="si">}</span><span class="s">:"</span><span class="p">)</span>
            <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Pair frequencies: </span><span class="si">{</span><span class="bp">self</span><span class="p">.</span><span class="n">_format_pair_freq</span><span class="p">(</span><span class="n">pair_freq</span><span class="p">)</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
            <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Most frequent pair: </span><span class="si">{</span><span class="bp">self</span><span class="p">.</span><span class="n">_format_pair</span><span class="p">(</span><span class="n">best_pair</span><span class="p">)</span><span class="si">}</span><span class="s"> (freq: </span><span class="si">{</span><span class="n">max_freq</span><span class="si">}</span><span class="s">)"</span><span class="p">)</span>

            <span class="c1"># Create new token name
</span>            <span class="n">new_token</span> <span class="o">=</span> <span class="sa">f</span><span class="s">"merge_</span><span class="si">{</span><span class="bp">self</span><span class="p">.</span><span class="n">next_token_id</span><span class="si">}</span><span class="s">"</span>

            <span class="c1"># Perform the merge
</span>            <span class="n">word_freq</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">merge_pair</span><span class="p">(</span><span class="n">word_freq</span><span class="p">,</span> <span class="n">best_pair</span><span class="p">,</span> <span class="n">new_token</span><span class="p">)</span>

            <span class="c1"># Record the merge
</span>            <span class="n">token1_name</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">_token_to_str</span><span class="p">(</span><span class="n">best_pair</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
            <span class="n">token2_name</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">_token_to_str</span><span class="p">(</span><span class="n">best_pair</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
            <span class="n">merge_str</span> <span class="o">=</span> <span class="sa">f</span><span class="s">"</span><span class="si">{</span><span class="n">token1_name</span><span class="si">}</span><span class="s"> </span><span class="si">{</span><span class="n">token2_name</span><span class="si">}</span><span class="s">"</span>
            <span class="n">merges_performed</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">merge_str</span><span class="p">)</span>

            <span class="c1"># Store merge information
</span>            <span class="bp">self</span><span class="p">.</span><span class="n">merges</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">best_pair</span><span class="p">)</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">merge_tokens</span><span class="p">[</span><span class="n">best_pair</span><span class="p">]</span> <span class="o">=</span> <span class="n">new_token</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">vocab</span><span class="p">[</span><span class="n">new_token</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">next_token_id</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">token_names</span><span class="p">[</span><span class="n">new_token</span><span class="p">]</span> <span class="o">=</span> <span class="n">merge_str</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">next_token_id</span> <span class="o">+=</span> <span class="mi">1</span>

            <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"After merge: </span><span class="si">{</span><span class="bp">self</span><span class="p">.</span><span class="n">_format_word_freq</span><span class="p">(</span><span class="n">word_freq</span><span class="p">)</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">merges_performed</span>

    <span class="k">def</span> <span class="nf">tokenize</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">word</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
        <span class="s">"""
        Tokenize a word using the learned BPE merges.
        """</span>
        <span class="c1"># Start with individual bytes as integers
</span>        <span class="n">tokens</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">word</span><span class="p">.</span><span class="n">encode</span><span class="p">(</span><span class="s">'utf-8'</span><span class="p">))</span>

        <span class="c1"># Apply merges in order
</span>        <span class="k">for</span> <span class="n">merge_pair</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">merges</span><span class="p">:</span>
            <span class="n">new_tokens</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="k">while</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">tokens</span><span class="p">):</span>
                <span class="k">if</span> <span class="p">(</span><span class="n">i</span> <span class="o">&lt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span> <span class="ow">and</span>
                    <span class="n">tokens</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">==</span> <span class="n">merge_pair</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="ow">and</span>
                    <span class="n">tokens</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="n">merge_pair</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
                    <span class="c1"># Replace with the merged token name
</span>                    <span class="n">merged_token</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">merge_tokens</span><span class="p">[</span><span class="n">merge_pair</span><span class="p">]</span>
                    <span class="n">new_tokens</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">merged_token</span><span class="p">)</span>
                    <span class="n">i</span> <span class="o">+=</span> <span class="mi">2</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">new_tokens</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">tokens</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
                    <span class="n">i</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="n">tokens</span> <span class="o">=</span> <span class="n">new_tokens</span>

        <span class="c1"># Convert to readable format
</span>        <span class="n">result</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">tokens</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">token</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
                <span class="n">result</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="nb">chr</span><span class="p">(</span><span class="n">token</span><span class="p">))</span>
            <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">token</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span> <span class="ow">and</span> <span class="n">token</span><span class="p">.</span><span class="n">startswith</span><span class="p">(</span><span class="s">'merge_'</span><span class="p">):</span>
                <span class="c1"># Convert back to the original characters this merge represents
</span>                <span class="n">result</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">token_names</span><span class="p">[</span><span class="n">token</span><span class="p">])</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">result</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">token</span><span class="p">))</span>

        <span class="k">return</span> <span class="n">result</span>

    <span class="k">def</span> <span class="nf">_format_word_freq</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">word_freq</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">str</span><span class="p">],</span> <span class="p">...],</span> <span class="nb">int</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
        <span class="s">"""Format word frequency dictionary for readable output."""</span>
        <span class="n">formatted</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">for</span> <span class="n">word_tuple</span><span class="p">,</span> <span class="n">freq</span> <span class="ow">in</span> <span class="n">word_freq</span><span class="p">.</span><span class="n">items</span><span class="p">():</span>
            <span class="n">tokens</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="p">.</span><span class="n">_token_to_str</span><span class="p">(</span><span class="n">token</span><span class="p">)</span> <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">word_tuple</span><span class="p">]</span>
            <span class="n">word_str</span> <span class="o">=</span> <span class="s">'('</span> <span class="o">+</span> <span class="s">','</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span> <span class="o">+</span> <span class="s">')'</span>
            <span class="n">formatted</span><span class="p">[</span><span class="n">word_str</span><span class="p">]</span> <span class="o">=</span> <span class="n">freq</span>
        <span class="k">return</span> <span class="nb">str</span><span class="p">(</span><span class="n">formatted</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_format_pair_freq</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pair_freq</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">str</span><span class="p">],</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">str</span><span class="p">]],</span> <span class="nb">int</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
        <span class="s">"""Format pair frequency dictionary for readable output."""</span>
        <span class="n">formatted</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">for</span> <span class="n">pair</span><span class="p">,</span> <span class="n">freq</span> <span class="ow">in</span> <span class="n">pair_freq</span><span class="p">.</span><span class="n">items</span><span class="p">():</span>
            <span class="n">first</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">_token_to_str</span><span class="p">(</span><span class="n">pair</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
            <span class="n">second</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">_token_to_str</span><span class="p">(</span><span class="n">pair</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
            <span class="n">pair_str</span> <span class="o">=</span> <span class="n">first</span> <span class="o">+</span> <span class="n">second</span>
            <span class="n">formatted</span><span class="p">[</span><span class="n">pair_str</span><span class="p">]</span> <span class="o">=</span> <span class="n">freq</span>
        <span class="k">return</span> <span class="nb">str</span><span class="p">(</span><span class="n">formatted</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_format_pair</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pair</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">str</span><span class="p">],</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">str</span><span class="p">]])</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
        <span class="s">"""Format a pair for readable output."""</span>
        <span class="n">first</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">_token_to_str</span><span class="p">(</span><span class="n">pair</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="n">second</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">_token_to_str</span><span class="p">(</span><span class="n">pair</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
        <span class="k">return</span> <span class="sa">f</span><span class="s">"(</span><span class="si">{</span><span class="n">first</span><span class="si">}</span><span class="s">, </span><span class="si">{</span><span class="n">second</span><span class="si">}</span><span class="s">)"</span>

    <span class="k">def</span> <span class="nf">_token_to_str</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">token</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">str</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
        <span class="s">"""Convert a token to readable string."""</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">token</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
            <span class="k">return</span> <span class="nb">chr</span><span class="p">(</span><span class="n">token</span><span class="p">)</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">token</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span> <span class="ow">and</span> <span class="n">token</span><span class="p">.</span><span class="n">startswith</span><span class="p">(</span><span class="s">'merge_'</span><span class="p">):</span>
            <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">token_names</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="n">token</span><span class="p">,</span> <span class="n">token</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="nb">str</span><span class="p">(</span><span class="n">token</span><span class="p">)</span>

<span class="c1"># Example usage
</span><span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="s">"__main__"</span><span class="p">:</span>
    <span class="c1"># Initialize BPE encoder
</span>    <span class="n">bpe</span> <span class="o">=</span> <span class="n">BPEEncoder</span><span class="p">()</span>

    <span class="c1"># Example corpus
</span>    <span class="n">corpus</span> <span class="o">=</span> <span class="s">"low low low low low lower lower widest widest widest newest newest newest newest newest newest"</span>

    <span class="k">print</span><span class="p">(</span><span class="s">"BPE Training on Corpus:"</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Corpus: </span><span class="si">{</span><span class="n">corpus</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"="</span> <span class="o">*</span> <span class="mi">50</span><span class="p">)</span>

    <span class="c1"># Train with 6 merges
</span>    <span class="n">merges</span> <span class="o">=</span> <span class="n">bpe</span><span class="p">.</span><span class="n">train</span><span class="p">(</span><span class="n">corpus</span><span class="p">,</span> <span class="n">num_merges</span><span class="o">=</span><span class="mi">6</span><span class="p">)</span>

    <span class="k">print</span><span class="p">(</span><span class="s">"</span><span class="se">\n</span><span class="s">"</span> <span class="o">+</span> <span class="s">"="</span> <span class="o">*</span> <span class="mi">50</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"Training Complete!"</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Merges performed: </span><span class="si">{</span><span class="n">merges</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>

    <span class="c1"># Test tokenization
</span>    <span class="k">print</span><span class="p">(</span><span class="s">"</span><span class="se">\n</span><span class="s">"</span> <span class="o">+</span> <span class="s">"="</span> <span class="o">*</span> <span class="mi">50</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"Tokenization Examples:"</span><span class="p">)</span>
    <span class="n">test_words</span> <span class="o">=</span> <span class="p">[</span><span class="s">"newest"</span><span class="p">,</span> <span class="s">"lower"</span><span class="p">,</span> <span class="s">"widest"</span><span class="p">,</span> <span class="s">"low"</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">test_words</span><span class="p">:</span>
        <span class="n">tokens</span> <span class="o">=</span> <span class="n">bpe</span><span class="p">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">word</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"'</span><span class="si">{</span><span class="n">word</span><span class="si">}</span><span class="s">' -&gt; </span><span class="si">{</span><span class="n">tokens</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>

    <span class="c1"># Show final vocabulary (subset)
</span>    <span class="k">print</span><span class="p">(</span><span class="s">"</span><span class="se">\n</span><span class="s">"</span> <span class="o">+</span> <span class="s">"="</span> <span class="o">*</span> <span class="mi">50</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"New Vocabulary (merged tokens only):"</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">token</span><span class="p">,</span> <span class="n">token_id</span> <span class="ow">in</span> <span class="n">bpe</span><span class="p">.</span><span class="n">vocab</span><span class="p">.</span><span class="n">items</span><span class="p">():</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">token</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span> <span class="ow">and</span> <span class="n">token</span><span class="p">.</span><span class="n">startswith</span><span class="p">(</span><span class="s">'merge_'</span><span class="p">):</span>
            <span class="n">description</span> <span class="o">=</span> <span class="n">bpe</span><span class="p">.</span><span class="n">token_names</span><span class="p">[</span><span class="n">token</span><span class="p">]</span>
            <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Token ID </span><span class="si">{</span><span class="n">token_id</span><span class="si">}</span><span class="s">: '</span><span class="si">{</span><span class="n">description</span><span class="si">}</span><span class="s">'"</span><span class="p">)</span>
</code></pre></div></div>

<h2 id="sample-output">Sample Output</h2>

<p>When you run this code, you‚Äôll see output like:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    BPE Training on Corpus:
    Corpus: low low low low low lower lower widest widest widest newest newest newest newest newest newest
    ==================================================
    Initial word frequencies: {'(l,o,w)': 5, '(l,o,w,e,r)': 2, '(w,i,d,e,s,t)': 3, '(n,e,w,e,s,t)': 6}
    
    Merge 1:
    Pair frequencies: {'lo': 7, 'ow': 7, 'we': 8, 'er': 2, 'wi': 3, 'id': 3, 'de': 3, 'es': 9, 'st': 9, 'ne': 6, 'ew': 6}
    Most frequent pair: (s, t) (freq: 9)
    After merge: {'(l,o,w)': 5, '(l,o,w,e,r)': 2, '(w,i,d,e,s t)': 3, '(n,e,w,e,s t)': 6}
    
    Merge 2:
    Pair frequencies: {'lo': 7, 'ow': 7, 'we': 8, 'er': 2, 'wi': 3, 'id': 3, 'de': 3, 'es t': 9, 'ne': 6, 'ew': 6}
    Most frequent pair: (e, s t) (freq: 9)
    After merge: {'(l,o,w)': 5, '(l,o,w,e,r)': 2, '(w,i,d,e s t)': 3, '(n,e,w,e s t)': 6}
    
    Merge 3:
    Pair frequencies: {'lo': 7, 'ow': 7, 'we': 2, 'er': 2, 'wi': 3, 'id': 3, 'de s t': 3, 'ne': 6, 'ew': 6, 'we s t': 6}
    Most frequent pair: (o, w) (freq: 7)
    After merge: {'(l,o w)': 5, '(l,o w,e,r)': 2, '(w,i,d,e s t)': 3, '(n,e,w,e s t)': 6}
    
    Merge 4:
    Pair frequencies: {'lo w': 7, 'o we': 2, 'er': 2, 'wi': 3, 'id': 3, 'de s t': 3, 'ne': 6, 'ew': 6, 'we s t': 6}
    Most frequent pair: (l, o w) (freq: 7)
    After merge: {'(l o w)': 5, '(l o w,e,r)': 2, '(w,i,d,e s t)': 3, '(n,e,w,e s t)': 6}
    
    Merge 5:
    Pair frequencies: {'l o we': 2, 'er': 2, 'wi': 3, 'id': 3, 'de s t': 3, 'ne': 6, 'ew': 6, 'we s t': 6}
    Most frequent pair: (w, e s t) (freq: 6)
    After merge: {'(l o w)': 5, '(l o w,e,r)': 2, '(w,i,d,e s t)': 3, '(n,e,w e s t)': 6}
    
    Merge 6:
    Pair frequencies: {'l o we': 2, 'er': 2, 'wi': 3, 'id': 3, 'de s t': 3, 'ne': 6, 'ew e s t': 6}
    Most frequent pair: (n, e) (freq: 6)
    After merge: {'(l o w)': 5, '(l o w,e,r)': 2, '(w,i,d,e s t)': 3, '(n e,w e s t)': 6}
    
    ==================================================
    Training Complete!
    Merges performed: ['s t', 'e s t', 'o w', 'l o w', 'w e s t', 'n e']
    
    ==================================================
    Tokenization Examples:
    'newest' -&gt; ['n e', 'w e s t']
    'lower' -&gt; ['l o w', 'e', 'r']
    'widest' -&gt; ['w', 'i', 'd', 'e s t']
    'low' -&gt; ['l o w']
    
    ==================================================
    New Vocabulary (merged tokens only):
    Token ID 257: 's t'
    Token ID 258: 'e s t'
    Token ID 259: 'o w'
    Token ID 260: 'l o w'
    Token ID 261: 'w e s t'
    Token ID 262: 'n e'
</code></pre></div></div>

<p>This implementation demonstrates how BPE learns to represent text efficiently by identifying and merging frequently occurring character patterns, creating a vocabulary that balances between the simplicity of byte-level tokenization and the efficiency of word-level tokenization.</p>]]></content><author><name>[&quot;Han Yu&quot;]</name></author><category term="cs336" /><summary type="html"><![CDATA[Byte Pair Encoding (BPE) Tokenizer in a Nutshell Key Terms]]></summary></entry><entry><title type="html">Study Notes: Stanford CS336 Language Modeling from Scratch [1]</title><link href="http://localhost:4000/cs336/2025/07/20/cs336-note-get-started.html" rel="alternate" type="text/html" title="Study Notes: Stanford CS336 Language Modeling from Scratch [1]" /><published>2025-07-20T00:00:00-07:00</published><updated>2025-07-20T00:00:00-07:00</updated><id>http://localhost:4000/cs336/2025/07/20/cs336-note-get-started</id><content type="html" xml:base="http://localhost:4000/cs336/2025/07/20/cs336-note-get-started.html"><![CDATA[<h2 id="introduction">Introduction</h2>

<p>It‚Äôs been a while since my last blog post about my generative AI learning journey (January 1, 2024). My day job has demanded significant time and energy, but I don‚Äôt want that to derail my passion and curiosity for deep-diving into this field.</p>

<p>This week, I had a realization: instead of endlessly reflecting, thinking, or planning, it‚Äôs time to act. Even with just one hour of focused study after work each day, I can make meaningful progress by year-end.</p>

<p>I‚Äôve decided to tackle Stanford‚Äôs <a href="https://stanford-cs336.github.io/spring2025/"><strong>CS336: Language Modeling from Scratch</strong></a> at my own pace. I‚Äôm grateful that Stanford makes their <a href="https://www.youtube.com/watch?v=SQ3fZ1sAqXI&amp;list=PLoROMvodv4rOY23Y0BoGoBGgQ1zmU_MT_&amp;ab_channel=StanfordOnline"><strong>Lecture Videos</strong></a>, <a href="https://github.com/stanford-cs336"><strong>Lecture Notes &amp; Assignments</strong></a> freely available online‚Äîa perfect fit for someone like me who can‚Äôt commit to fixed class schedules but still craves structured learning.</p>

<p>To give back to the community, I‚Äôll document and share my learning notes as I progress. I‚Äôm curious to see how much I can accomplish by dedicating just an hour a day (or ~10 hours per week) through the end of the year. Can I complete all the lectures and assignments? I know I‚Äôll move slowly, but I‚Äôm excited to test my consistency and see what‚Äôs possible.</p>

<p>This first post covers <strong>setting up the local development environment</strong>‚Äîa small but necessary step to begin engaging with the lecture materials.</p>

<h2 id="setting-up-the-local-dev-environment">Setting Up the Local Dev Environment</h2>

<h3 id="prerequisites">Prerequisites</h3>

<p>Make sure you have Git, Python 3.11, and Node.js installed on your system. You‚Äôll also need <a href="https://github.com/astral-sh/uv"><code class="language-plaintext highlighter-rouge">uv</code></a>, a fast Python package manager.</p>

<h3 id="step-1-clone-the-cs336-repository">Step 1: Clone the CS336 Repository</h3>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>git clone https://github.com/stanford-cs336/spring2025-lectures
<span class="nb">cd </span>spring2025-lectures
</code></pre></div></div>

<h3 id="step-2-set-up-python-virtual-environment-with-uv">Step 2: Set Up Python Virtual Environment with UV</h3>

<p>Install <code class="language-plaintext highlighter-rouge">uv</code> (if not already installed):</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>curl <span class="nt">-LsSf</span> https://astral.sh/uv/install.sh | sh
</code></pre></div></div>

<p>Create a virtual environment using Python 3.11:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>uv venv <span class="nt">--python</span><span class="o">=</span>3.11
</code></pre></div></div>

<p>Activate the environment:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># macOS/Linux</span>
<span class="nb">source</span> .venv/bin/activate
</code></pre></div></div>

<h3 id="step-3-install-python-dependencies">Step 3: Install Python Dependencies</h3>

<blockquote>
  <p>üí° Note: I‚Äôm using a MacBook Pro (M4). The <code class="language-plaintext highlighter-rouge">triton</code> package doesn‚Äôt support Apple Silicon, so I excluded it during the initial setup.<br />
More info: <a href="https://github.com/triton-lang/triton">Triton GitHub</a></p>
</blockquote>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">grep</span> <span class="nt">-v</span> <span class="s2">"triton"</span> requirements.txt <span class="o">&gt;</span> requirements_no_triton.txt
uv pip <span class="nb">install</span> <span class="nt">-r</span> requirements_no_triton.txt
</code></pre></div></div>

<h3 id="step-4-generate-executable-lecture-content">Step 4: Generate Executable Lecture Content</h3>

<p>Compile a lecture:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python execute.py <span class="nt">-m</span> lecture_01
</code></pre></div></div>

<p>This will generate a trace file at:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>var/traces/lecture_01.json
</code></pre></div></div>

<h3 id="step-5-build-the-local-react-web-app-to-view-lectures">Step 5: Build the Local React Web App to View Lectures</h3>

<p>Install Node.js if needed:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>brew <span class="nb">install </span>node
</code></pre></div></div>

<p>Then build and serve the trace viewer:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">cd </span>trace-viewer
npm <span class="nb">install
</span>npm run dev
</code></pre></div></div>

<p>Open the viewer in your browser:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>http://localhost:&lt;PORT&gt;?trace=var/traces/lecture_01.json
</code></pre></div></div>

<p>It should be something like <img src="/assets/picture/2025_07_20_cs336_note_get_started/cs336_lecture_view.png" alt="this" />.</p>

<p>Then enjoy going through the lecture notes!</p>]]></content><author><name>[&quot;Han Yu&quot;]</name></author><category term="cs336" /><summary type="html"><![CDATA[Introduction]]></summary></entry><entry><title type="html">Supercharge Message Summarization Experience: Parameter-Efficient Fine-Tuning and LLM Assisted Evaluation</title><link href="http://localhost:4000/parameter-efficient-fine-tuning/2024/01/01/PEFT-summarization.html" rel="alternate" type="text/html" title="Supercharge Message Summarization Experience: Parameter-Efficient Fine-Tuning and LLM Assisted Evaluation" /><published>2024-01-01T00:00:00-08:00</published><updated>2024-01-01T00:00:00-08:00</updated><id>http://localhost:4000/parameter-efficient-fine-tuning/2024/01/01/PEFT-summarization</id><content type="html" xml:base="http://localhost:4000/parameter-efficient-fine-tuning/2024/01/01/PEFT-summarization.html"><![CDATA[<center>
<img src="/assets/picture/2024_01_01_PEFT_summarization/cover.png" width="900" height="500" />
</center>
<h2 id="introduction">Introduction</h2>
<p>In today‚Äôs business landscape, we are surrounded by a wealth of opportunities to utilize advanced technology powered by AI. Think of large language models(LLMs) as versatile tools in our toolkit: we can <em>customize</em> them for a variety of specific downstream tasks, a process known as <em>fine-tuning</em>. However, a challenge arises in that each fine-tuned model typically maintains the same parameter size as the original. Therefore, managing multiple fine-tuned models requires careful consideration of factors such as accuracy performance, memory management, inference latency, and disk utilization.</p>

<p>Parameter-Efficient Fine-Tuning (PEFT) methods provide an efficient and streamlined approach for adapting pre-trained LLMs, commonly referred to as <em>base models</em>, to a range of specific downstream tasks. These tasks encompass diverse applications, including but not limited to text summarization, question answering, image generation, and text-to-speech synthesis. In contrast to traditional full fine-tuning, which consumes substantial computational resources, PEFT prioritizes the optimization of a significantly smaller parameter subset referred to as ‚Äúadapters‚Äù. These adapters work in tandem with the base model, achieving competitive performance while imposing lower computational and storage demands.</p>

<p>I‚Äôve shared a <a href="https://github.com/bearbearyu1223/llm-fine-tuning-playground/blob/main/finetune_falcon_7b_conversation_summarization.ipynb">Colab notebook</a> demonstrating a resource-efficient PEFT process using <a href="https://arxiv.org/abs/2305.14314">QLoRA</a> and <a href="https://github.com/huggingface/peft">HuggingFace PEFT libraries</a> to fine tune <a href="https://huggingface.co/vilsonrodrigues/falcon-7b-sharded">Falcon-7B-sharded model</a> on <a href="https://huggingface.co/datasets/samsum">SamSum dataset</a> for summarizing ‚Äúmessage-like‚Äù conversations. It achieves reasonable summarization performance after training for only 5 epochs on an A100 compute instance with a single GPU. Additionally, I‚Äôve employed <code class="language-plaintext highlighter-rouge">GPT-3.5-turbo</code> to assess generated summaries, showcasing a potentially automated evaluation method by formalizing evaluation guidelines into a prompt template. This approach stands in contrast to traditional automated evaluation metrics like ROUGE or BERTScore, which rely on reference summaries.</p>

<p>Furthermore, I will also share some insights and lessons I‚Äôve gained throughout this process, with a particular focus on considerations when leveraging LLMs to develop product experiences related to summarization.</p>

<p>I hope you‚Äôll discover this article both informative and intriguing, igniting your creativity as you explore the development of your unique product experiences and strategies through the use of fine-tuned foundation models.</p>

<p>Enjoy the read, and let your innovation flourish. Happy new year!</p>

<h2 id="fine-tuning-with-model-quantization-and-lora">Fine-Tuning with Model Quantization and LoRA</h2>
<p>Base models such as Claude, T5, Falcon, and Llama2 excel at predicting tokens in sequences, but they <em>struggle with generating responses that align with instructions</em>. Fine-tuning techniques, such as <strong>Supervised Fine-Tuning (SFT)</strong> and <strong>Reinforcement Learning from Human Feedback (RLHF)</strong>, can be employed to bridge these gaps. In this sample project, we‚Äôll explore the application of SFT to Falcon-7B, a 7-billion-parameter causal decoder model trained by TII on 1,500-billion tokens from RefinedWeb with curated corpora, for conversation summarization tasks.</p>

<h3 id="install-and-import-the-required-libraries">Install and Import the Required Libraries</h3>
<p>To get started, one can create a virtual environment and install all the required libraries needed for this sample project. In Colab, this can be done by running a cell containing the following scripts:</p>
<pre><code class="language-Shell">!pip install huggingface_hub==0.19.4
!pip install -q -U trl accelerate git+https://github.com/huggingface/peft.git
!pip install transformers==4.36.0
!pip install datasets==2.15.0 Tokenizers==0.15.0
!pip install -q bitsandbytes wandb
!pip install py7zr
</code></pre>
<p>then the installed libraries can be imported and be used during runtime via:</p>
<pre><code class="language-Python">import torch
import numpy as np
from huggingface_hub import notebook_login
from datasets import load_dataset, concatenate_datasets
from transformers import AutoModelForCausalLM, AutoTokenizer,BitsAndBytesConfig, AutoTokenizer, GenerationConfig
from peft import LoraConfig, get_peft_model, PeftConfig, PeftModel, prepare_model_for_kbit_training, TaskType
from transformers import TrainingArguments
from trl import SFTTrainer, DataCollatorForCompletionOnlyLM
</code></pre>
<h3 id="prepare-the-dataset-for-fine-tuning">Prepare the Dataset for Fine-Tuning</h3>
<p>You can load the <a href="https://huggingface.co/datasets/samsum">SamSum dataset</a> directly using the [Hugging Face Datasets libraries](https://huggingface.co/docs/datasets/index via Python code:</p>

<pre><code class="language-Python">dataset_name = "samsum"
dataset = load_dataset(dataset_name)

train_dataset = dataset['train']
eval_dataset = dataset['validation']
test_dataset = dataset['test']
dataset
</code></pre>
<p>The dataset contains a total of 14,732 samples for training, 818 samples for validation, and 818 samples for testing. A sample of the dataset is displayed below:
<img src="/assets/picture/2024_01_01_PEFT_summarization/dataset.png" alt="sample_data" /></p>

<p>To format the original training dataset into prompts for instruction fine-tuning, you can use the following helper function. For more details, refer to the detailed reference <a href="https://huggingface.co/docs/trl/sft_trainer#format-your-input-prompts">here</a>).</p>

<pre><code class="language-Python">def formatting_prompts_func(example):
    output_texts = []
    for i in range(len(example['id'])):
        instruction = "Summarize this Dialogue."
        input = example['dialogue'][i]
        output = example['summary'][i]
        prompt="### Instruction:\n{instruction}\n\n### Dialogue:\n{input}\n\n### Summary:\n{output}".format(instruction=instruction, input=input, output=output)
        output_texts.append(prompt)
    return output_texts
</code></pre>
<h3 id="set-up-the-configuration-for-fine-tuning">Set up the Configuration for Fine-Tuning</h3>
<p>To reduce VRAM usage during training, you will fine-tune <a href="https://huggingface.co/vilsonrodrigues/falcon-7b-sharded">a resharded version of Falcon-7B</a> in 4-bit precision using <a href="https://arxiv.org/abs/2305.14314">QLoRA</a>. You can use the following code snippet to load the base model and prepare it for the QLoRA experiment:</p>

<pre><code class="language-Python">model_name = "vilsonrodrigues/falcon-7b-sharded"

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_use_double_quant=True,
    bnb_4bit_compute_dtype=torch.float16,
)

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    quantization_config=bnb_config,
    device_map="auto",
    trust_remote_code=True,
)
model.config.use_cache = False
model.config.pretraining_tp = 1
</code></pre>
<p>Based on the QLoRA paper, we will taget all linear transformer block layers as target modules for fine-tuning (also see the discussions on reddit <a href="https://www.reddit.com/r/LocalLLaMA/comments/15sgg4m/what_modules_should_i_target_when_training_using/?rdt=53925">here</a>). You can leverage the following helper function to find these target modules:</p>

<pre><code class="language-Python">def find_target_modules(model):
    # Initialize a Set to Store Unique Layers
    unique_layers = set()

    # Iterate Over All Named Modules in the Model
    for name, module in model.named_modules():
        # Check if the Module Type Contains 'Linear4bit'
        if "Linear4bit" in str(type(module)):
            # Extract the Type of the Layer
            layer_type = name.split('.')[-1]

            # Add the Layer Type to the Set of Unique Layers
            unique_layers.add(layer_type)

    # Return the Set of Unique Layers Converted to a List
    return list(unique_layers)
target_modules = find_target_modules(model)
print(target_modules)
</code></pre>
<p>And in this case, the target modules for fine-tuning will be 
<code class="language-plaintext highlighter-rouge">['dense_4h_to_h', 'dense_h_to_4h', 'query_key_value', 'dense']</code>.</p>

<p>After loading and preparing the base model for QLoRA, you can configure the fine-tuning experiment using the following code:</p>
<pre><code class="language-Python">model = prepare_model_for_kbit_training(model)

lora_alpha = 32 
lora_dropout = 0.1 
lora_rank = 16

peft_config = LoraConfig(
    lora_alpha=lora_alpha,
    lora_dropout=lora_dropout,
    r=lora_rank,
    bias="none",
    task_type="CAUSAL_LM",
    target_modules=[
        "query_key_value",
        "dense",
        "dense_h_to_4h",
        "dense_4h_to_h",
    ]
)

peft_model = get_peft_model(model, peft_config)
peft_model.print_trainable_parameters()
</code></pre>
<p>This configuration will result in an <em>adapter model</em> with <em>32,636,928 trainable parameters</em>, which is only <em>0.47%</em> of the trainable parameters compared to the <em>6,954,357,632 parameters</em> of the base model.</p>
<h3 id="set-up-the-configuration-for-trainig">Set up the Configuration for Trainig</h3>
<p>Load the tokenizer from the pre-trained base model, both the base model, the LoRA config, and the tokenizer will be needed for the SFT trainer.</p>
<pre><code class="language-Python">tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)

if tokenizer.pad_token_id is None:
  tokenizer.pad_token = tokenizer.eos_token
  tokenizer.pad_token_id = tokenizer.eos_token_id
tokenizer.padding_side = "right"
</code></pre>
<p>Below is the configuration used for SFT</p>
<pre><code class="language-Python">output_dir = "falcon_7b_LoRA_r16_alpha32_epoch10_dialogue_summarization_v0.1"
per_device_train_batch_size = 32 #4
gradient_accumulation_steps = 4
gradient_checkpointing=False
optim = "paged_adamw_32bit"
save_steps = 20
logging_steps = 20
learning_rate = 2e-4
max_grad_norm = 0.1
warmup_ratio = 0.01
lr_scheduler_type = "cosine" #"constant"
num_train_epochs = 5
seed=42
max_seq_length = 512

training_arguments = TrainingArguments(
    output_dir=output_dir,
    per_device_train_batch_size=per_device_train_batch_size,
    gradient_accumulation_steps=gradient_accumulation_steps,
    gradient_checkpointing=gradient_checkpointing,
    optim=optim,
    save_steps=save_steps,
    logging_steps=logging_steps,
    learning_rate=learning_rate,
    fp16=True,
    max_grad_norm=max_grad_norm,
    num_train_epochs=num_train_epochs,
    warmup_ratio=warmup_ratio,
    group_by_length=True,
    lr_scheduler_type=lr_scheduler_type,
    push_to_hub=True,
    report_to="wandb"
)

trainer = SFTTrainer(
    model=peft_model,
    train_dataset=train_dataset,
    formatting_func=formatting_prompts_func,
    tokenizer=tokenizer,
    peft_config=peft_config,
    max_seq_length=max_seq_length,
    args=training_arguments,
)
</code></pre>
<p>You can initiate the fine-tuning experiment via</p>
<pre><code class="language-Python">trainer.train()
</code></pre>
<p>The entire training process took approximately 3 hours running on an A100 instance with a single GPU.</p>

<h3 id="model-inference-of-the-fined-tuned-model">Model Inference of the Fined-Tuned Model</h3>
<p>Upon completion of the training process, you can easily share the adapter model by uploading it to Hugging Face‚Äôs model repository using the following code:</p>
<pre><code class="language-Python">trainer.push_to_hub() 
</code></pre>
<p>This published adapter model can then be retrieved and used in conjunction with the base model for various summarization tasks, as demonstrated in the reference code snippet below.</p>
<pre><code class="language-Python">PEFT_MODEL = "bearbearyu1223/falcon_7b_LoRA_r16_alpha32_epoch10_dialogue_summarization_v0.1"
config = PeftConfig.from_pretrained(PEFT_MODEL)
peft_base_model = AutoModelForCausalLM.from_pretrained(
    config.base_model_name_or_path,
    return_dict=True,
    quantization_config=bnb_config,
    device_map="auto",
    trust_remote_code=True,
)

peft_model = PeftModel.from_pretrained(peft_base_model, PEFT_MODEL)

# Generate Summarization
def get_summary(dialogue, max_new_tokens=50, max_length=512, verbose=False):
  prompt= "### Instruction:\n{instruction}\n\n### Dialogue:\n{dialogue}\n\n### Summary:\n".format(instruction="Summarize the Dialogue below.", dialogue=dialogue)
  if verbose:
    print(prompt)

  peft_encoding = peft_tokenizer(prompt, truncation=True, return_tensors="pt").to(torch.device("cuda:0"))
  peft_outputs = peft_model.generate(input_ids=peft_encoding.input_ids, generation_config=GenerationConfig(max_length=max_length, do_sample=True,
                                                                                                         max_new_tokens=max_new_tokens,
                                                                                                         pad_token_id = peft_tokenizer.eos_token_id,
                                                                                                         eos_token_id = peft_tokenizer.eos_token_id,
                                                                                                         attention_mask = peft_encoding.attention_mask,
                                                                                                         temperature=0.1, top_k=1, repetition_penalty=30.0, num_return_sequences=1,))
  peft_text_output = peft_tokenizer.decode(peft_outputs[0], skip_special_tokens=True)

  sub = "### Summary:"
  raw_summary = peft_text_output.split(sub)[1]

  return raw_summary
</code></pre>
<p>See an example of a summary generated by the fine-tuned model in comparison to the reference summary crafted by a human below</p>
<pre><code class="language-Python">test_index=6
dialogue=test_dataset[test_index]['dialogue']
summary=test_dataset[test_index]['summary']
peft_output=get_summary(dialogue,verbose=True)

print("Human Summary:")
print(summary)
print("PEFT Summary:")
print(peft_output)
</code></pre>

<table>
  <thead>
    <tr>
      <th>Instruction</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Summarize the Dialogue below.</td>
    </tr>
  </tbody>
</table>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Max: Know any good sites to buy clothes from?
Payton: Sure :) &lt;file_other&gt; &lt;file_other&gt; &lt;file_other&gt; &lt;file_other&gt; &lt;file_other&gt; &lt;file_other&gt; &lt;file_other&gt;
Max: That's a lot of them!
Payton: Yeah, but they have different things so I usually buy things from 2 or 3 of them.
Max: I'll check them out. Thanks. 
Payton: No problem :)
Max: How about u?
Payton: What about me?
Max: Do u like shopping?
Payton: Yes and no.
Max: How come?
Payton: I like browsing, trying on, looking in the mirror and seeing how I look, but not always buying.
Max: Y not?
Payton: Isn't it obvious? ;)
Max: Sry ;)
Payton: If I bought everything I liked, I'd have nothing left to live on ;)
Max: Same here, but probably different category ;)
Payton: Lol
Max: So what do u usually buy?
Payton: Well, I have 2 things I must struggle to resist!
Max: Which are?
Payton: Clothes, ofc ;)
Max: Right. And the second one?
Payton: Books. I absolutely love reading!
Max: Gr8! What books do u read?
Payton: Everything I can get my hands on :)
Max: Srsly?
Payton: Yup :)
</code></pre></div></div>

<table>
  <thead>
    <tr>
      <th>Summary Type</th>
      <th>Summary Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Human</td>
      <td>Payton provides Max with websites selling clothes. Payton likes browsing and trying on the clothes but not necessarily buying them. Payton usually buys clothes and books as he loves reading.</td>
    </tr>
    <tr>
      <td>PEFT</td>
      <td>Payton sends Max some links with online shops where she buys her stuff. Payton likes both fashion items and literature. She reads all kinds of fiction.</td>
    </tr>
  </tbody>
</table>

<h2 id="evaluation-of-summarization-quality">Evaluation of Summarization Quality</h2>
<p>Traditional evaluation methods for summarization tasks rely on metrics like ROUGE and BLEU, which evaluate the generated summaries by comparing them to human-written reference summaries. These metrics assess aspects such as the overlap in n-grams and word sequences, offering a quantitative and also automated assessment of summary quality.</p>

<p>In cases where human reference summaries are unavailable, it becomes imperative to establish well-defined and consistent annotation guidelines for human annotators. Below is a list of criteria we will consider when formulating the annotation guidelines.</p>

<h3 id="metric-1-relevance">Metric 1: Relevance</h3>
<p><strong>Capturing the Essence:</strong> The LLM will assist annotators in evaluating the relevance of a summary. Annotators will evaluate the relevance of a summary on a scale of 1 to 5 (higher score indicates better quality), considering whether the summary effectively extracts important content from the source conversation, avoiding redundancies and excess information. With clear criteria and steps, annotators can confidently assign scores that reflect the summary‚Äôs ability to convey essential details.</p>

<h3 id="metric-2-coherence">Metric 2: Coherence</h3>
<p><strong>Creating Clarity:</strong> The LLM will assist annotators in evaluating the coherence of a summary. Annotators will rate summaries from 1 to 5 (higher score indicates better quality), focusing on the summary‚Äôs organization and logical flow. Clear guidelines enable annotators to determine how well the summary presents information in a structured and coherent manner.</p>

<h3 id="metric-3-consistency">Metric 3: Consistency</h3>
<p><strong>Factually Sound:</strong> The LLM will assist annotators in evaluating the consistency of a summary. Annotators will assess summaries for factual alignment with the source conversation, rating them from 1 to 5 (higher score indicates better quality). SummarizeMaster ensures that annotators identify and penalize summaries containing factual inaccuracies or hallucinated facts, enhancing the reliability of the evaluation process.</p>

<h3 id="metric-4-fluency">Metric 4: Fluency</h3>
<p><strong>Language Excellence:</strong> The LLM will assist annotators in evaluating the fluency of a summary. Fluency is a critical aspect of summary evaluation. Annotators will assess summaries for grammar, spelling, punctuation, word choice, and sentence structure, assigning scores from 1 to 5 (higher score indicates better quality).</p>

<p>We will transform these instructions into a prompt template for input to <code class="language-plaintext highlighter-rouge">GPT-3.5-turbo</code> to assess the quality of the summaries generated by our fine-tuned model (check out this <a href="https://github.com/bearbearyu1223/llm-fine-tuning-playground/blob/main/model_eval_finetune_falcon_7b_conversation_summarization.ipynb">Colab</a> to run the evaluation). This approach is primarily motivated by the goal of achieving consistency, standardization, and efficiency in the manual evaluation process, which can otherwise be labor-intensive.</p>

<h2 id="lessons-learned">Lessons Learned</h2>
<p>The automated evaluation results for the 818 test samples are shared <a href="https://github.com/bearbearyu1223/llm-fine-tuning-playground/blob/main/results/falcon_7b_LoRA_r16_dialogue_summarization_12_23_2023_results_eval.csv">here</a>. Simple statistical analysis reveals that the fine-tuned LLM demonstrated a reasonable level of performance on the test set when compared to the human-generated summaries (refer to the chart below)</p>
<center>
<img src="/assets/picture/2024_01_01_PEFT_summarization/eval_score.png" />
</center>
<!-- ![eval_score](/assets/picture/2024_01_01_PEFT_summarization/eval_score.png) -->

<p>However, is that truly the situation? Let‚Äôs examine a few instances together.</p>

<p>Example 1:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Yaz: Hi babes, you off out Friday, I need to get my freak on!
Tania: Why the hell not, college work is really doing my head in with the deadlines!
Yaz: Seen Sophie lately, her and lover boy are looking well serious!
Tania: Yeah, saw her in the canteen a couple of days ago, she's thinking of breaking up before Uni. Cutting ties etc
Yaz: Well, maybe not so serious after all then!
Tania: Best to go there single, in my opinion!
Yaz: Yeah, not got much choice in the matter. Maybe I'll meet my dream woman there!üòÄ
Tania: Why the hell not?! I can't wait to get out of here and up to Manchester, only 4 months and a bit, now!
Yaz: Christ, that came around quick, just the little matter of A Levels and getting into Exeter to deal with. I need such high grades for my course, medicine is a cut-throat world.
Tania: Hope not! If anyone is suited to becoming a Dr love, it's you, I am positive of it!
Yaz: Hope you're right, babes! Anyway, pub Friday at 8ish?
Tania: Hell, yes! See you!
</code></pre></div></div>

<table>
  <thead>
    <tr>
      <th>Summary - Human Baseline</th>
      <th>Summary - PEFT Baseline</th>
      <th>Relevance<br />Human</th>
      <th>Relevance<br />PEFT Model</th>
      <th>Coherence<br />Human</th>
      <th>Coherence<br />PEFT Model</th>
      <th>Consistency<br />Human</th>
      <th>Consistency<br />PEFT Model</th>
      <th>Fluency<br />Human</th>
      <th>Fluency<br />PEFT Model</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Yaz and Tania will go to a pub on Friday around 8. Sophie is thinking of breaking up with her boyfriend before Uni. Tania is going to Manchester in 4 months. Yaz wants to study medicine in Exeter, so she needs high grades.</td>
      <td>Sophie wants to break up because he boyfriend has been cheating on him. Tania needs good results from exams to be admitted to university. Yaz will see his friends at the bar tonight about 8 pm.</td>
      <td>4</td>
      <td>3</td>
      <td>4</td>
      <td>3</td>
      <td>4</td>
      <td>2</td>
      <td>4</td>
      <td>2</td>
    </tr>
  </tbody>
</table>

<p><strong>Learnings: The LLM-based evaluator is capable of performing a reasonably accurate evaluation.</strong> In this example, it appears that the summary generated by the fine-tuned model does not align with the factual content of the source conversation. The original source conversation does not mention that Sophie‚Äôs boyfriend is cheating. Furthermore, there is a factual inaccuracy in the summary, as it is Yaz, not Tania, who requires a high score for admission to Exeter for studying medicine. Additionally, there are some grammar errors that can be improved or corrected as suggested below (e.g., one can further prompt LLM and ask suggestions to improve the fluency of the summary):</p>

<ol>
  <li>
    <p>‚ÄúSophie wants to break up because he boyfriend has been cheating on him.‚Äù
Correction: ‚ÄúSophie wants to break up because her boyfriend has been cheating on her.‚Äù</p>
  </li>
  <li>
    <p>‚ÄúTania needs good results from exams to be admitted to university.‚Äù
Correction: ‚ÄúTania needs good exam results to be admitted to university.‚Äù</p>
  </li>
  <li>
    <p>‚ÄúYaz will see his friends at the bar tonight about 8 pm.‚Äù
Correction: ‚ÄúYaz will see his friends at the bar tonight at about 8 pm.‚Äù</p>
  </li>
</ol>

<p>These quality issues have been identified by the LLM-based evaluator, which rated both consistency and fluency as 2.</p>

<p>Example 2:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Petra: I need to sleep, I can't stand how sleepy I am
Andy: I know, and it's so boring today, nobody's working at the office
Ezgi: I am working! lazy pigs
Petra: I'm sleeping with my eyes open, kill me
Andy: ask the fat woman from HR
Petra: she would kill me on spot without batting an eye
Andy: she always repeats she has a black belt in karate 
Petra: it's hard to believe she can move, but let her have whatever belt she wants
Andy: LOL
Petra: sooooo sleepy
</code></pre></div></div>

<table>
  <thead>
    <tr>
      <th>Summary - Human Baseline</th>
      <th>Summary - PEFT Baseline</th>
      <th>Relevance<br />Human</th>
      <th>Relevance<br />PEFT Model</th>
      <th>Coherence<br />Human</th>
      <th>Coherence<br />PEFT Model</th>
      <th>Consistency<br />Human</th>
      <th>Consistency<br />PEFT Model</th>
      <th>Fluency<br />Human</th>
      <th>Fluency<br />PEFT Model</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Petra is very sleepy at work today, Andy finds the day boring, and Ezgi is working.</td>
      <td>It is difficult for Petra not to fall asleep because of being tired. Andy suggests that Petra should talk about this issue with the lady who works as human resources manager. She might be able to help Petra.</td>
      <td>4</td>
      <td>4</td>
      <td>4</td>
      <td>4</td>
      <td>4</td>
      <td>4</td>
      <td>4</td>
      <td>4</td>
    </tr>
  </tbody>
</table>

<p><strong>Learnings: LLM-based evaluators may not always accurately capture the intended context or nuances from the conversation.</strong> In this example, the human-generated summary outperforms the summary generated by the fine-tuned model. Interestingly, neither the fine-tuned model‚Äôs generated summary nor the LLM-based evaluator were able to accurately recognize the sarcasm conveyed in the original conversations, particularly with reference to the ‚ÄúHR lady‚Äù.</p>

<p>Example 3:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Finn: Hey
Zadie: Hi there! What's up?
Finn: All fine. You? 
Zadie: Not bad, thanks 
Finn: Look, I was thinking of going to this neighborhood called Elephant and Castle tomorrow, it's apparently full of Latin American stuff. Fancy joining? 
Zadie: Sure! But what's ""stuff""? üòÇ
Finn: lol So apparently it's a place were random people from ""Latin America"" (meaning fuck knows which countries) started running small businesses and restaurant, and a nice little community was formed
Zadie: Oh cool 
Finn: Then capitalism came and it's all going to be demolished soon, so it's like the last chance to go
Zadie: What a shame :( Yeah, I haven't had Latin American üòÇ food for ages so I'm totally up for it
Finn: Can't wait to taste this cuisine of unspecified Latino origin lol
Zadie: üòÇüòÇüòÇ
Finn: But we can specify time and place if and only if you wish 
Zadie: I might be tempted to lol I'd say early evening, 2-ish?
Finn: Yeah, that's fine by me. So most of the places we want to visit are in this Elephant and Castle shopping centre. Shall I see you at the main entrance, wherever that is 
Zadie: 2 o'clock at unspecified main entrance then? Sounds good to mw
Finn: Yer
Zadie: Cool, see you there! And thanks so much for remembering about me üíúüíúüíú
Finn: Thanks for saying yes to such an ill-defined plan lmao 
Zadie: Ha ha You know I love those
Finn: See you tomorrow then 
Zadie: Yep Call me if you get lost
Finn: I will I will ü§ô byeeee 
Zadie: Toodles
</code></pre></div></div>

<table>
  <thead>
    <tr>
      <th>Summary - Human Baseline</th>
      <th>Summary - PEFT Baseline</th>
      <th>Relevance<br />Human</th>
      <th>Relevance<br />PEFT Model</th>
      <th>Coherence<br />Human</th>
      <th>Coherence<br />PEFT Model</th>
      <th>Consistency<br />Human</th>
      <th>Consistency<br />PEFT Model</th>
      <th>Fluency<br />Human</th>
      <th>Fluency<br />PEFT Model</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Finn and Zadie are going to Elephant and Castle tomorrow at 2. They will meet at the main entrance.</td>
      <td>Tomorrow Finn wants Zadie join him on his trip around London. They meet at 2 pm near the elephant statue outside the Elephant and Castle Shopping Centre. The neighbourhood has been created by immigrants from South America.</td>
      <td>4</td>
      <td>4</td>
      <td>4</td>
      <td>4</td>
      <td>4</td>
      <td>4</td>
      <td>4</td>
      <td>4</td>
    </tr>
  </tbody>
</table>

<p><strong>Learnings: Both the fine-tuned LLM model and the LLM-based evaluator may occasionally treat context derived from their pre-learned world knowledge as factual information. This underscores the importance of exercising caution when incorporating inferred contextual information into the product experience.</strong> In this example, the summary generated by the fine-tuned model is considered to yield ‚Äúimpressive results‚Äù because it correctly identifies that ‚ÄúElephant and Castle‚Äù is a place of interest located in London, and it recognizes the presence of an elephant statue near the entrance to the shopping center, both of which are not directly mentioned in the original conversation. <em>Those additional context is derived from the base model‚Äôs world knowledge from training</em>.</p>

<p>However, it raises questions about whether we should deem this as a good summary when the LLM leverages its world knowledge to infer information beyond the direct content of the conversation. It‚Äôs important to acknowledge that this approach can be a <em>double-edged sword from a product experience perspective</em>. Some users may find it useful and intelligent as it demonstrates the LLM‚Äôs ability to provide context and additional information. Still, it can also potentially be problematic in many situations.</p>

<p>Here are some considerations:</p>

<ul>
  <li>
    <p><strong>Cultural Variations and Sensitivity</strong>: Inferring information based on world knowledge can be problematic when dealing with culturally sensitive topics or diverse audiences. Assumptions made by the model may not align with cultural variations or norms, potentially leading to misunderstandings or offense.</p>
  </li>
  <li>
    <p><strong>Privacy and Ethical Concerns</strong>: Inferences based on world knowledge can sometimes delve into personal or sensitive areas, raising ethical concerns.</p>
  </li>
  <li>
    <p><strong>Accuracy and Context</strong>: While the model‚Äôs inferences may be impressive, they may not always accurately capture the intended context or nuances since the world knowledge can be updated, and the model may be trained on outdated data. This can result in potential inaccuracies when applying such knowledge to generate summaries.</p>
  </li>
  <li>
    <p><strong>Users‚Äô Preferences and Control</strong>: Users‚Äô preferences for summaries may vary. Some may appreciate the additional context provided by model inference, while others may prefer more straightforward and direct summaries. It‚Äôs essential to provide users with transparency and control over how additional context is used to generate summaries. Users should have the option to enable or disable such contextual inferences to align with their preferences.</p>
  </li>
</ul>

<h2 id="my-top-3-takeaways-from-this-intriguing-project">My Top 3 Takeaways from this Intriguing Project!</h2>

<ol>
  <li>
    <p>Fine-Tuning LLM through adaptation has demonstrated itself as an exceptionally efficient and cost-effective method for developing LLM-powered product experiences. It allows companies, even those not creating their own foundation models, to harness this approach and benefit from the latest and most advanced outcomes driven by Generative AI.</p>
  </li>
  <li>
    <p>Leveraging the world knowledge acquired by the base model can indeed lead to ‚Äúimpressive results‚Äù for the fine-tuned model. However, it‚Äôs essential to bear in mind that this can be a double-edged sword!</p>
  </li>
  <li>
    <p>LLM can serve as a referee much like a human, evaluating generation results from another LLM or a fine-tuned model. Nevertheless, exercise caution when depending on an LLM-based evaluator to shape your product experience, as not all evaluations may be equally reliable!</p>
  </li>
</ol>

<p>I hope my insights resonate with your experience as well!</p>]]></content><author><name>[&quot;Han Yu&quot;]</name></author><category term="Parameter-Efficient-Fine-Tuning" /><summary type="html"><![CDATA[Introduction In today‚Äôs business landscape, we are surrounded by a wealth of opportunities to utilize advanced technology powered by AI. Think of large language models(LLMs) as versatile tools in our toolkit: we can customize them for a variety of specific downstream tasks, a process known as fine-tuning. However, a challenge arises in that each fine-tuned model typically maintains the same parameter size as the original. Therefore, managing multiple fine-tuned models requires careful consideration of factors such as accuracy performance, memory management, inference latency, and disk utilization.]]></summary></entry><entry><title type="html">Craft Your Own Kitchen Companion: Leveraging OpenAI Assistant, Image Generation, and Text-to-Speech for a Revolutionary Cooking Experience</title><link href="http://localhost:4000/chatbot/2023/11/21/assistant-api-playground.html" rel="alternate" type="text/html" title="Craft Your Own Kitchen Companion: Leveraging OpenAI Assistant, Image Generation, and Text-to-Speech for a Revolutionary Cooking Experience" /><published>2023-11-21T00:00:00-08:00</published><updated>2023-11-21T00:00:00-08:00</updated><id>http://localhost:4000/chatbot/2023/11/21/assistant-api-playground</id><content type="html" xml:base="http://localhost:4000/chatbot/2023/11/21/assistant-api-playground.html"><![CDATA[<h2 id="tldr">TL;DR</h2>
<p><img src="/assets/picture/2023_11_21_assistant_api_playground/cropped_image.png" alt="page_cover" /></p>

<p>OpenAI unveiled its <a href="https://platform.openai.com/docs/assistants/overview">Assistant API</a> during its very first developer conference-<a href="https://devday.openai.com/">OpenAI DevDay</a>. This marked a significant addition to the suite of tools and models available for building conversational AI, a domain in which I‚Äôve been actively engaged.</p>

<p>Although I have used other frameworks, tools and models to build customized Assistant in the past (check out my previous posts  <a href="https://bearbearyu1223.github.io/chatbot/2023/07/31/food-qna-on-server-llm.html">Food QnA Chatbot : Help Answer Food Related Questions from Your Own Cookbook</a> and <a href="https://bearbearyu1223.github.io/chatbot/2023/08/14/food-qna-on-device-llm.html">Build On-Device QnA with LangChain and Llama2</a>), intrigued by the new capabilities of the Assistant API, I decided to integrate it into a new project. This sample project is a refreshed demonstration of a simple Cooking Assistant, leveraging not only the Assistant API but also incorporating additional functionalities such as <a href="https://platform.openai.com/docs/guides/text-to-speech">Text-to-Speech</a> and <a href="https://platform.openai.com/docs/guides/images?context=node">Image Generation</a>, showcasing a holistic approach to building conversational AI systems with only a few lines of code.</p>

<p>Get inspired by the latest from Open AI DevDay with my project on <a href="https://github.com/bearbearyu1223/assistant_api_playground"><img src="https://img.shields.io/badge/github-%23121011.svg?style=for-the-badge&amp;logo=github&amp;logoColor=white" alt="GitHub" /></a>? Ready to unleash your creativity? Start building your own generative AI-powered experience today, join the forefront of AI innovation, and let us make AI for everyone!</p>

<h2 id="the-crafted-product-experience">The Crafted Product Experience</h2>

<ol>
  <li><strong>Direct Communication</strong>:
    <ul>
      <li>Purposefully address user (in this case, it will be ‚ÄúHan‚Äù) directly in all correspondences, ensuring each interaction is clear and personalized.</li>
    </ul>
  </li>
  <li><strong>Resource Utilization</strong>:
    <ul>
      <li>Utilize information solely from user‚Äôs provided cookbooks when responding to his/her inquiries, ensuring responses are relevant and accurate.</li>
    </ul>
  </li>
  <li><strong>Requests for Visual Representation</strong>:
    <ul>
      <li><strong>Initial Acknowledgment</strong>: When user requests a visual representation of a recipe, promptly reply with: ‚ÄúAbsolutely! A delightful visual preview of the recipe is coming up shortly. Please hold on!‚Äù</li>
      <li><strong>Action Step</strong>: Subsequently, craft a detailed and imaginative textual prompt to aid in visualizing the specified recipe.</li>
    </ul>
  </li>
  <li><strong>Recipe Responses</strong>:
    <ul>
      <li><strong>Introduction</strong>: Initiate responses to recipe queries with: ‚ÄúHere‚Äôs a recipe I discovered!‚Äù</li>
      <li><strong>Summary Style</strong>: Summarize the recipe succinctly using bullet points for easy comprehension.</li>
      <li><strong>Summary Length</strong>: Aim to keep the summary under 150 words to maintain brevity and focus.</li>
    </ul>
  </li>
  <li><strong>Handling Limitations and Concluding Respectfully</strong>:
    <ul>
      <li><strong>Addressing Unavailability</strong>: If user‚Äôs questions cannot be resolved with the cookbooks at hand, or if an appropriate recipe is not available, inform the user politely about these constraints.</li>
      <li><strong>Concluding Interactions</strong>: End conversations with respect and professionalism, ensuring a positive and courteous closure.</li>
    </ul>
  </li>
</ol>

<h2 id="set-up-the-development-environment">Set Up the Development Environment</h2>

<p>The provided sample project on <a href="https://github.com/bearbearyu1223/assistant_api_playground">GitHub</a> acts as a prototype to illustrate the product experience described previously. Follow these steps to set up your local development environment:</p>

<ol>
  <li><strong>Clone the Sample Project</strong>: Use the following command to clone the project from GitHub:
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>git clone https://github.com/bearbearyu1223/assistant_api_playground
</code></pre></div>    </div>
  </li>
  <li><strong>Create a Virtual Environment</strong>: Navigate to the cloned project directory and create a Python virtual environment named <code class="language-plaintext highlighter-rouge">cooking_assistant</code> by executing:
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python3 <span class="nt">-m</span> venv cooking_assistant
</code></pre></div>    </div>
  </li>
  <li><strong>Activate the Virtual Environment</strong>: Activate the newly created virtual environment with:
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">source </span>cooking_assistant/bin/activate
</code></pre></div>    </div>
  </li>
  <li><strong>Install Required Libraries</strong>: Inside the activated virtual environment, install the required libraries using the command:
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip3 <span class="nb">install</span> <span class="nt">-r</span> requirements.txt
</code></pre></div>    </div>
  </li>
  <li><strong>Set Up Environment Variables</strong>: Create a <code class="language-plaintext highlighter-rouge">.env</code> file in the project root directory. Add your OpenAI API key to the file as follows:
    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>OPENAI_API_KEY=&lt;Your API KEY&gt;
</code></pre></div>    </div>
  </li>
</ol>

<h2 id="run-the-application">Run the Application</h2>
<p>Simply add your own cookbook in PDF under the directory <code class="language-plaintext highlighter-rouge">cookbook</code>, or just play with the existing cookbook uploaded with my sample project. Engage with the project interactively by issuing food preparation and cooking queries. Initiate the application with the following command:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python main.py
</code></pre></div></div>
<p>To terminate the program, simply use <code class="language-plaintext highlighter-rouge">Ctrl + C</code>. Below is an example of a conversation generated by running the application.</p>

<table>
  <thead>
    <tr>
      <th>Turn ID</th>
      <th>User Query</th>
      <th>Assistant Response</th>
      <th>TTS Playback</th>
      <th>Notes on Tested User Query</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>üë©üèª‚Äçüç≥: Hi there, Thanksgiving is coming! I would like to have some ideas for cooking!</td>
      <td>üîÆ: Thanksgiving is a great occasion to prepare a feast. Here are some main dish ideas inspired by the contents of the ‚ÄúDeliciously Healthy Dinners‚Äù cookbook that could suit a Thanksgiving dinner: [Main dishes listed]</td>
      <td><video src="https://github.com/bearbearyu1223/assistant_api_playground/assets/7468124/06aaa9e8-9833-48a9-950e-95b8f9494c90" controls="control"></video></td>
      <td>This query is designed to evaluate the clarity of the Assistant‚Äôs defined role and the effectiveness of its execution, specifically in tasks such as generating creative ideas for holiday cooking.</td>
    </tr>
    <tr>
      <td>1</td>
      <td>üë©üèª‚Äçüç≥: How to make Cornbread-Crusted Turkey?</td>
      <td>üîÆ: Here‚Äôs a recipe I found for Cornbread-Crusted Turkey from your cookbook: [Recipe details]</td>
      <td><video src="https://github.com/bearbearyu1223/assistant_api_playground/assets/7468124/cb672f77-e66d-46cd-bcff-7a8abb1a834c" controls="control"></video></td>
      <td>This query seeks to assess the Assistant‚Äôs ability to accurately extract relevant information from the user‚Äôs cookbook and succinctly summarize it.</td>
    </tr>
    <tr>
      <td>2</td>
      <td>üë©üèª‚Äçüç≥: Thanks, what kind of salad will pair nicely with it?</td>
      <td>üîÆ: An Autumn Salad would pair nicely with Cornbread-Crusted Turkey. Here‚Äôs the recipe from your cookbook: [Recipe details]</td>
      <td><video src="https://github.com/bearbearyu1223/assistant_api_playground/assets/7468124/cc4b66e0-b158-419e-bd5d-98946fe21404" controls="control"></video></td>
      <td>This query is intended to assess the Assistant‚Äôs ability to provide useful responses to related follow-up questions</td>
    </tr>
    <tr>
      <td>3</td>
      <td>üë©üèª‚Äçüç≥: Can I see a visual of the autumn salad with the cornbread crusted turkey?</td>
      <td>üîÆ: Absolutely! Prepare for a delightful visual preview of the recipe, coming up shortly. Please hold on!</td>
      <td><video src="https://github.com/bearbearyu1223/assistant_api_playground/assets/7468124/1f1765bc-206f-4eed-afa6-d860965ad25c" controls="control"></video></td>
      <td>This query aims to evaluate the Assistant‚Äôs ability to create visualizations based on recipe information previously requested by the user.</td>
    </tr>
    <tr>
      <td>¬†</td>
      <td>¬†</td>
      <td>See the visual representation generated from the Assistant via DALL-E-3 based on the textual prompt generated from GPT4: Envision a warm, inviting plate centered with a golden-brown Cornbread-Crusted Turkey fillet, its crunchy exterior hinting at the tender, juicy meat within; Next to it lies a vibrant and colorful Autumn Salad, a bed of mixed lettuce greens punctuated by the bold reds of dried cranberries and the crisp green of thinly sliced Granny Smith apples; Scattered on top are earthy, chopped walnuts and the delicate shimmer of unsalted sunflower seeds, offering both texture and taste; The salad glistens lightly, dressed in a delicate sheen of low-fat raspberry vinaigrette, complementing the robust flavors of the turkey; The plate is a harmonious blend of the heartiness of Thanksgiving and the freshness of autumn, a celebration of both comfort and zest on the palate. <img src="/assets/picture/2023_11_21_assistant_api_playground/recipe_image.png" alt="recipe_image" /></td>
      <td>¬†</td>
      <td>¬†</td>
    </tr>
    <tr>
      <td>4</td>
      <td>üë©üèª‚Äçüç≥: Oh that is very nice. How many calories per serving, including the autumn salad and the cornbread crusted turkey?</td>
      <td>üîÆ: The combined total calories per serving for the Autumn Salad and the Cornbread-Crusted Turkey is 395 calories.</td>
      <td><video src="https://github.com/bearbearyu1223/assistant_api_playground/assets/7468124/b87b3eea-786b-4557-b216-66d3acc42d9e" controls="control"></video></td>
      <td>This query is designed to assess the Assistant‚Äôs proficiency in extracting information, applying logical reasoning, and performing basic mathematical calculations.</td>
    </tr>
    <tr>
      <td>5</td>
      <td>üë©üèª‚Äçüç≥: Can you give me a breakdown of how the total calories per serving is calculated?</td>
      <td>üîÆ: Certainly! Here‚Äôs the breakdown of how the total calories per serving is calculated for the combination of Autumn Salad and Cornbread-Crusted Turkey: [Calorie breakdown]</td>
      <td><video src="https://github.com/bearbearyu1223/assistant_api_playground/assets/7468124/03bad1bd-e575-4ba2-8958-460298c24993" controls="control"></video></td>
      <td>More evaluations on the Assistant‚Äôs logical reasoning and mathematical skills.</td>
    </tr>
    <tr>
      <td>6</td>
      <td>üë©üèª‚Äçüç≥: Nice. Can you help me create a grocery list for making the autumn salad and the cornbread crusted turkey?</td>
      <td>üîÆ: Absolutely, here‚Äôs your grocery list for making the Autumn Salad and the Cornbread-Crusted Turkey: [Grocery list details]</td>
      <td><video src="https://github.com/bearbearyu1223/assistant_api_playground/assets/7468124/9b6b07a1-4fc0-4608-9cdb-150b87a0f387" controls="control"></video></td>
      <td>This query aims to evaluate the Assistant‚Äôs ability to comprehend and summarize information based on its previous conversation history with the user.</td>
    </tr>
    <tr>
      <td>7</td>
      <td>üë©üèª‚Äçüç≥: Thank you! Happy Thanksgiving!</td>
      <td>üîÆ: You‚Äôre welcome! If you need any more help in the future, feel free to reach out. Have a wonderful Thanksgiving and enjoy your delicious meal!</td>
      <td><video src="https://github.com/bearbearyu1223/assistant_api_playground/assets/7468124/ed240241-2220-4d36-9516-60b5104e1db1" controls="control"></video></td>
      <td>Assess whether the Assistant can respond politely and meaningfully to the user‚Äôs casual conversation.</td>
    </tr>
  </tbody>
</table>]]></content><author><name>[&quot;Han Yu&quot;]</name></author><category term="chatbot" /><summary type="html"><![CDATA[TL;DR]]></summary></entry></feed>