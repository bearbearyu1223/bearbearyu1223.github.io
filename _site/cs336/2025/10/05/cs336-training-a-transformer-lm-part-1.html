<!DOCTYPE html>
<html lang="en"><head>
  <link rel="shortcut icon" type="image/png" href="/assets/favicon.png">
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Study Notes: Stanford CS336 Language Modeling from Scratch [8] | üçí Han‚Äôs Generative AI Quest</title>
<meta name="generator" content="Jekyll v4.3.2" />
<meta property="og:title" content="Study Notes: Stanford CS336 Language Modeling from Scratch [8]" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Planning LLM Training: Cross-Entropy Loss, Optimizers, Memory and Computational Cost, and other practical levers" />
<meta property="og:description" content="Planning LLM Training: Cross-Entropy Loss, Optimizers, Memory and Computational Cost, and other practical levers" />
<link rel="canonical" href="http://localhost:4000/cs336/2025/10/05/cs336-training-a-transformer-lm-part-1.html" />
<meta property="og:url" content="http://localhost:4000/cs336/2025/10/05/cs336-training-a-transformer-lm-part-1.html" />
<meta property="og:site_name" content="üçí Han‚Äôs Generative AI Quest" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2025-10-05T00:00:00-07:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Study Notes: Stanford CS336 Language Modeling from Scratch [8]" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2025-10-05T00:00:00-07:00","datePublished":"2025-10-05T00:00:00-07:00","description":"Planning LLM Training: Cross-Entropy Loss, Optimizers, Memory and Computational Cost, and other practical levers","headline":"Study Notes: Stanford CS336 Language Modeling from Scratch [8]","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/cs336/2025/10/05/cs336-training-a-transformer-lm-part-1.html"},"url":"http://localhost:4000/cs336/2025/10/05/cs336-training-a-transformer-lm-part-1.html"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="üçí Han&apos;s Generative AI Quest" />

<!-- MathJax Configuration -->
<script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$', '$$'], ['\\[', '\\]']],
      processEscapes: true,
      processEnvironments: true
    },
    options: {
      skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
    }
  };
</script>

<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/">üçí Han&#39;s Generative AI Quest</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Study Notes: Stanford CS336 Language Modeling from Scratch [8]</h1>
    <p class="post-meta"><time class="dt-published" datetime="2025-10-05T00:00:00-07:00" itemprop="datePublished">
        Oct 5, 2025
      </time>‚Ä¢ 
          <span itemprop="author" itemscope itemtype="http://schema.org/Person">
            <span class="p-author h-card" itemprop="name">Han Yu</span></span></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <h2 id="planning-llm-training-cross-entropy-loss-optimizers-memory-and-computational-cost-and-other-practical-levers">Planning LLM Training: Cross-Entropy Loss, Optimizers, Memory and Computational Cost, and other practical levers</h2>

<h3 id="table-of-contents">Table of Contents</h3>
<ol>
  <li><a href="#cross-entropy-loss">Cross-Entropy Loss: Measuring How Wrong We Are</a></li>
  <li><a href="#perplexity">Perplexity: A More Intuitive Metric</a></li>
  <li><a href="#sgd-optimizer">SGD Optimizer: Walking Downhill</a></li>
  <li><a href="#adamw">AdamW: The Smart Optimizer</a></li>
  <li><a href="#memory-requirements">Memory Requirements: Can It Fit?</a></li>
  <li><a href="#computational-cost">Computational Cost: How Long Will This Take?</a></li>
  <li><a href="#learning-rate-schedules">Learning Rate Schedules: Starting Fast, Ending Slow</a></li>
  <li><a href="#gradient-clipping">Gradient Clipping: The Safety Mechanism</a></li>
</ol>

<hr />

<h3 id="cross-entropy-loss">Cross-Entropy Loss: Measuring How Wrong We Are</h3>

<p><em>A language model is trying to <strong>predict the next word</strong> in a sequence. Noted that we‚Äôre training a supervised learning model, as we know what is the next correct word in the sequence from the training dataset, and we want to minimize the loss when the model did not predict the correct word.</em></p>

<h4 id="simple-example">Simple Example</h4>

<p>Imagine you have the sentence: ‚ÄúThe cat sat on the ___‚Äù</p>

<ul>
  <li>The model needs to predict what comes next</li>
  <li>Maybe the correct word is ‚Äúmat‚Äù</li>
  <li>The model gives probabilities for all possible words in its vocabulary</li>
</ul>

<h4 id="the-simple-version-of-the-math">The Simple Version of the Math</h4>

<p><strong>Step 1: The model outputs ‚Äúlogits‚Äù</strong> (raw scores for each possible word)</p>
<ul>
  <li>Think of logits as unnormalized scores</li>
  <li>Example: ‚Äúmat‚Äù gets score 5.2, ‚Äúdog‚Äù gets 1.3, ‚Äútable‚Äù gets 3.1, etc.</li>
</ul>

<p><strong>Step 2: Convert logits to probabilities using softmax</strong></p>

\[p(\text{word}) = \frac{e^{\text{score of that word}}}{\text{sum of } e^{\text{score}} \text{ for all words}}\]

<p>Example:</p>
<ul>
  <li>If ‚Äúmat‚Äù has score 5.2: probability = $\frac{e^{5.2}}{e^{5.2} + e^{1.3} + e^{3.1} + ‚Ä¶}$</li>
</ul>

<p><strong>Step 3: Calculate the loss</strong></p>

\[\text{loss} = -\log(p(\text{correct word}))\]

<p>Assume the correct word is ‚Äúmat‚Äù, and we want the model to assign high probability to the correct word and low probability to the incorrect word.</p>

<ul>
  <li>If the model gives ‚Äúmat‚Äù a probability of 0.8 ‚Üí loss = -log(0.8) ‚âà 0.22 (small loss, good!)</li>
  <li>If the model gives ‚Äúmat‚Äù a probability of 0.1 ‚Üí loss = -log(0.1) ‚âà 2.30 (big loss, bad!)</li>
  <li>If the model gives ‚Äúmat‚Äù a probability of 1.0 ‚Üí loss = -log(1.0) = 0 (no loss, perfect!)</li>
</ul>

<h4 id="the-full-version-of-the-math">The Full Version of the Math</h4>

<p>The complete formula averages this loss over:</p>
<ul>
  <li>All positions in a sequence (i = 1 to m)</li>
  <li>All sequences in the training dataset D (all x in D)</li>
</ul>

\[\ell(\theta; D) = \frac{1}{|D|m} \sum_{x \in D} \sum_{i=1}^{m} -\log p_\theta(x_{i+1} | x_{1:i})\]

<p><strong>Bottom line:</strong> Cross-entropy loss is small when the model assigns high probability to the correct next word, and large when it doesn‚Äôt. Training tries to minimize this loss!</p>

<hr />

<h3 id="perplexity">Perplexity: A More Intuitive Metric</h3>

<p><strong>Perplexity</strong> is an <strong>evaluation metric</strong> (not a loss function) that provides a more intuitive way to measure how good your language model is. It answers the question: <strong>‚ÄúOn average, how many words is the model confused between?‚Äù</strong>  While we <strong>train</strong> using cross-entropy loss, we <strong>report</strong> perplexity to humans because it‚Äôs easier to interpret.</p>

<h4 id="simple-analogy">Simple Analogy</h4>

<p>Imagine a multiple-choice test:</p>

<ul>
  <li><strong>Perplexity = 1</strong>: The model is 100% certain (like having only 1 choice)</li>
  <li><strong>Perplexity = 10</strong>: The model is as confused as if it had to guess among 10 equally likely options</li>
  <li><strong>Perplexity = 100</strong>: The model is as confused as if it had to guess among 100 equally likely options</li>
</ul>

<p><strong>Lower perplexity = Better model!</strong></p>

<h4 id="the-math">The Math</h4>

<p>For a sequence where we make m predictions with cross-entropy losses $\ell_1, \ell_2, ‚Ä¶, \ell_m$:</p>

\[\text{perplexity} = \exp\left(\frac{1}{m} \sum_{i=1}^{m} \ell_i\right)\]

<p>This is equivalent to:</p>

\[\text{perplexity} = \exp(\text{average cross-entropy loss})\]

<p><strong>Breaking it down:</strong></p>

<p><strong>Step 1:</strong> Calculate the average cross-entropy loss</p>
<ul>
  <li>Recall that for each position i: 
  \(\ell_i = -\log p(x_{i+1} | x_{1:i})\)</li>
  <li>Add up all the losses: $\ell_1 + \ell_2 + ‚Ä¶ + \ell_m$</li>
  <li>Divide by m (the number of token predictions in the sequence)</li>
  <li>Average loss = $\frac{1}{m} \sum_{i=1}^{m} \ell_i$</li>
</ul>

<p><strong>Step 2:</strong> Take the exponential</p>
<ul>
  <li>Apply $\exp()$ to the average loss</li>
  <li>This ‚Äúundoes‚Äù the log in the cross-entropy formula</li>
</ul>

<h4 id="why-exponential">Why Exponential?</h4>

<p>The exponential transformation converts the abstract loss value into an interpretable number:</p>

<p><strong>Mathematical intuition:</strong></p>
<ul>
  <li>Cross-entropy loss: $\ell = -\log p(\text{correct word})$</li>
  <li>Perplexity undoes the log: $\exp(\ell) = \exp(-\log p) = \frac{1}{p}$</li>
  <li>If average probability is 0.1, perplexity ‚âà 10 (confused among ~10 words)</li>
  <li>If average probability is 0.01, perplexity ‚âà 100 (confused among ~100 words)</li>
</ul>

<h4 id="concrete-example">Concrete Example</h4>

<p>Say your model predicts 3 words in a sequence:</p>
<ul>
  <li>Word 1: $\ell_1 = 0$, probability was 1.0 (perfect!)</li>
  <li>Word 2: $\ell_2 = 2.3$, probability was 0.1</li>
  <li>Word 3: $\ell_3 = 0.69$, probability was 0.5</li>
</ul>

<p><strong>Calculation:</strong></p>

<p>Average loss = $\frac{0 + 2.3 + 0.69}{3} = \frac{2.99}{3} \approx 1.0$</p>

<p>Perplexity = $\exp(1.0) \approx 2.72$</p>

<p><strong>Interpretation:</strong> On average, the model is as uncertain as if it had to choose uniformly among about <strong>2.72 equally likely words</strong> at each position.</p>

<h4 id="relationship-to-training">Relationship to Training</h4>

<ul>
  <li><strong>During training:</strong> We minimize cross-entropy loss</li>
  <li><strong>During evaluation:</strong> We report perplexity for interpretability</li>
  <li><strong>They‚Äôre equivalent:</strong> Lower cross-entropy ‚ü∫ Lower perplexity</li>
</ul>

<p>Since perplexity is just an exponential transformation of cross-entropy, optimizing one automatically optimizes the other. We use cross-entropy for training because it has better mathematical properties for gradient-based optimization.</p>

<h4 id="key-takeaway">Key Takeaway</h4>

<p><strong>Perplexity is a user-friendly version of cross-entropy loss:</strong></p>
<ul>
  <li>Lower perplexity = model is more confident and accurate</li>
  <li>Higher perplexity = model is confused and uncertain</li>
  <li>It‚Äôs <strong>not used for training</strong>, only for <strong>reporting results</strong> in a more interpretable way</li>
  <li>Cross-entropy and perplexity are mathematically equivalent‚Äîminimizing one minimizes the other</li>
</ul>

<hr />

<h3 id="sgd-optimizer">SGD Optimizer: Which direction to walk to go downhill</h3>

<p>SGD (Stochastic Gradient Descent) is an algorithm that <strong>adjusts your model‚Äôs parameters</strong> during the training process to make the loss smaller. Think of it as teaching the model to make better predictions.</p>
<h4 id="the-mountain-analogy">The Mountain Analogy</h4>

<p>Imagine you‚Äôre standing on a mountain in the fog (you can‚Äôt see far):</p>
<ul>
  <li>Your <strong>position</strong> = model parameters (Œ∏)</li>
  <li>Your <strong>altitude</strong> = loss (how bad the model is)</li>
  <li>Your <strong>goal</strong> = get to the bottom of the valley (minimize loss)</li>
</ul>

<p><strong>SGD tells you which direction to walk to go downhill!</strong></p>

<h4 id="how-sgd-works-step-by-step">How SGD Works (Step by Step)</h4>

<h5 id="step-1-start-randomly">Step 1: Start Randomly</h5>
<ul>
  <li>Œ∏‚ÇÄ = random starting position on the mountain</li>
  <li>You don‚Äôt know where the bottom is yet</li>
</ul>

<h5 id="step-2-look-around-calculate-gradient">Step 2: Look Around (Calculate Gradient)</h5>
<ul>
  <li>‚àáL(Œ∏‚Çú; B‚Çú) = ‚ÄúWhich direction is downhill?‚Äù</li>
  <li>The gradient tells you the steepest uphill direction</li>
  <li>So <strong>negative gradient</strong> points downhill!</li>
</ul>

<h5 id="step-3-take-a-step-downhill">Step 3: Take a Step Downhill</h5>

\[\theta_{t+1} = \theta_t - \alpha_t \nabla L(\theta_t; B_t)\]

<p>Let me break down each part:</p>
<ul>
  <li><strong>Œ∏‚Çú</strong> = where you are now</li>
  <li><strong>‚àáL(Œ∏‚Çú; B‚Çú)</strong> = direction of steepest uphill</li>
  <li><strong>-‚àáL(Œ∏‚Çú; B‚Çú)</strong> = direction of steepest downhill (flip the sign!)</li>
  <li><strong>Œ±‚Çú</strong> = learning rate (how big a step to take)</li>
  <li><strong>Œ∏‚Çú‚Çä‚ÇÅ</strong> = your new position</li>
</ul>

<h5 id="step-4-repeat">Step 4: Repeat!</h5>
<ul>
  <li>Keep taking steps downhill until you reach the valley (minimum loss)</li>
</ul>

<h4 id="key-concepts">Key Concepts</h4>

<h5 id="learning-rate-Œ±‚Çú">Learning Rate (Œ±‚Çú)</h5>
<ul>
  <li><strong>Too large</strong>: You take huge steps and might overshoot the valley</li>
  <li><strong>Too small</strong>: You take tiny steps and it takes forever</li>
  <li><strong>Just right</strong>: You make steady progress</li>
</ul>

<p><strong>Example:</strong></p>
<ul>
  <li>If gradient says ‚Äúgo left by 10 units‚Äù and Œ± = 0.1</li>
  <li>You actually move right by: 10 √ó 0.1 = 1 unit</li>
</ul>

<h5 id="batch-b‚Çú">Batch (B‚Çú)</h5>
<ul>
  <li>Instead of using ALL your data to calculate the gradient (slow!), use a <strong>random small batch</strong></li>
  <li>This is the ‚Äústochastic‚Äù part - it‚Äôs random! Here ‚Äúrandom‚Äù means at each training step t, we randomly sample a subset of examples from the full training dataset D.</li>
  <li><strong>Batch size</strong> = how many examples you use each step (this is fixed during training)</li>
</ul>

<p><strong>Why random batches?</strong></p>
<ul>
  <li>Much faster! (calculating gradient on 1 million examples is slow)</li>
  <li>Still gives you a good enough direction</li>
  <li>Adds helpful randomness that can escape bad spots</li>
</ul>

<h4 id="simple-example-1">Simple Example</h4>

<p>Suppose your model has one parameter Œ∏ (to keep it simple):</p>

<p><strong>Initial:</strong> Œ∏‚ÇÄ = 5, loss = 100</p>

<p><strong>Step 1:</strong></p>
<ul>
  <li>Calculate gradient on a batch: ‚àáL = 20 (loss increases if we increase Œ∏)</li>
  <li>Learning rate: Œ± = 0.1</li>
  <li>Update: Œ∏‚ÇÅ = 5 - 0.1 √ó 20 = 5 - 2 = <strong>3</strong></li>
</ul>

<p><strong>Step 2:</strong></p>
<ul>
  <li>New gradient: ‚àáL = 10</li>
  <li>Update: Œ∏‚ÇÇ = 3 - 0.1 √ó 10 = 3 - 1 = <strong>2</strong></li>
</ul>

<p><strong>Step 3:</strong></p>
<ul>
  <li>New gradient: ‚àáL = 2</li>
  <li>Update: Œ∏‚ÇÉ = 2 - 0.1 √ó 2 = 2 - 0.2 = <strong>1.8</strong></li>
</ul>

<p>You keep going until the loss stops decreasing!</p>

<h4 id="key-takeaway-1">Key Takeaway</h4>

<p><strong>SGD is like walking downhill in small steps:</strong></p>
<ol>
  <li>Check which way is uphill (gradient)</li>
  <li>Take a step in the negative direction (size of the step determined by learning rate)</li>
  <li>Repeat until you reach the bottom (minimum loss)</li>
</ol>

<p>The ‚Äústochastic‚Äù part just means you randomly sample small batches of data instead of using entire dataset when calculate the gradient, making it much faster!</p>

<hr />

<h3 id="adamw">AdamW: The Smart Optimizer</h3>

<h4 id="whats-the-problem-with-sgd">What‚Äôs the Problem with SGD?</h4>

<p>Remember SGD takes the same size step (Œ±) for every parameter. But what if:</p>
<ul>
  <li>Some parameters need <strong>big updates</strong> (they‚Äôre far from optimal)</li>
  <li>Some parameters need <strong>tiny updates</strong> (they‚Äôre almost perfect)</li>
</ul>

<p><strong>AdamW is smarter</strong> via adapting the step size for each parameter individually.</p>

<h4 id="the-big-idea">The Big Idea</h4>

<p>AdamW keeps track of <strong>two pieces of memory</strong> for each parameter:</p>

<ol>
  <li><strong>m (first moment)</strong>: ‚ÄúWhich direction has this parameter been moving lately?‚Äù (like momentum)</li>
  <li><strong>v (second moment)</strong>: ‚ÄúHow much has this parameter been jumping around?‚Äù (like volatility)</li>
</ol>

<p>Then it uses this information to take smarter steps!</p>

<h4 id="how-adamw-works-step-by-step">How AdamW Works (Step by Step)</h4>

<h5 id="setup">Setup</h5>
<ul>
  <li><strong>m = 0</strong>: Start with no momentum</li>
  <li><strong>v = 0</strong>: Start with no volatility estimate</li>
  <li><strong>Œ≤‚ÇÅ = 0.9</strong>: How much to remember past directions (typically 90%)</li>
  <li><strong>Œ≤‚ÇÇ = 0.999</strong>: How much to remember past volatility (typically 99.9%)</li>
</ul>

<h5 id="each-training-step">Each Training Step</h5>

<p><strong>Step 1: Calculate gradient</strong> (same as SGD)</p>
<ul>
  <li>g = ‚àá‚Ñì(Œ∏; B‚Çú)</li>
  <li>‚ÄúWhich way should we move?‚Äù</li>
</ul>

<p><strong>Step 2: Update momentum (first moment)</strong></p>

\[m = \beta_1 \cdot m + (1-\beta_1) \cdot g\]

<p>Think of this as an <strong>exponential moving average</strong>:</p>
<ul>
  <li>Keep 90% of the old direction (Œ≤‚ÇÅm)</li>
  <li>Add 10% of the new direction ((1-Œ≤‚ÇÅ)g)</li>
  <li>This smooths out noisy gradients!</li>
</ul>

<p><strong>Step 3: Update volatility (second moment)</strong></p>

\[v = \beta_2 \cdot v + (1-\beta_2) \cdot g^2\]

<p>Same idea but for squared gradients:</p>
<ul>
  <li>Keep 99.9% of old volatility estimate</li>
  <li>Add 0.1% of new squared gradient</li>
  <li>This tracks how ‚Äújumpy‚Äù the parameter is</li>
</ul>

<p><strong>Step 4: Adjust learning rate</strong></p>

\[\alpha_t = \alpha \frac{\sqrt{1-\beta_2^t}}{1-\beta_1^t}\]

<p>This <strong>bias correction</strong> compensates for starting at m=0 and v=0 (they start biased toward zero!)</p>

<p><strong>Step 5: Update parameters (the smart part!)</strong></p>

\[\theta = \theta - \alpha_t \frac{m}{\sqrt{v} + \epsilon}\]

<p>This is where the magic happens:</p>
<ul>
  <li><strong>m</strong> tells us which direction to go</li>
  <li><strong>‚àöv</strong> tells us how confident we should be</li>
  <li>If v is <strong>large</strong> (parameter is jumpy) ‚Üí take <strong>smaller</strong> steps</li>
  <li>If v is <strong>small</strong> (parameter is stable) ‚Üí take <strong>larger</strong> steps</li>
</ul>

<p><strong>Step 6: Weight decay</strong></p>

\[\theta = \theta - \alpha\lambda\theta\]

<p>Pull parameters slightly toward zero to prevent them from getting too large (regularization)</p>

<h4 id="simple-example-2">Simple Example</h4>

<p>Imagine two parameters:</p>

<p><strong>Parameter A:</strong></p>
<ul>
  <li>Gradients: [5, 5.1, 4.9, 5, 5] (very stable!)</li>
  <li>v will be small ‚Üí AdamW takes <strong>bigger</strong> steps</li>
  <li>Makes sense! We‚Äôre confident about the direction</li>
</ul>

<p><strong>Parameter B:</strong></p>
<ul>
  <li>Gradients: [5, -4, 6, -3, 5] (super noisy!)</li>
  <li>v will be large ‚Üí AdamW takes <strong>smaller</strong> steps</li>
  <li>Makes sense! We‚Äôre uncertain, so be cautious</li>
</ul>

<h4 id="key-hyperparameters">Key Hyperparameters</h4>

<table>
  <thead>
    <tr>
      <th>Parameter</th>
      <th>Typical Value</th>
      <th>What it does</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Œ± (learning rate)</td>
      <td>0.001 or 0.0001</td>
      <td>Base step size</td>
    </tr>
    <tr>
      <td>Œ≤‚ÇÅ</td>
      <td>0.9</td>
      <td>How much momentum to keep</td>
    </tr>
    <tr>
      <td>Œ≤‚ÇÇ</td>
      <td>0.95-0.999</td>
      <td>How much volatility history to keep</td>
    </tr>
    <tr>
      <td>Œª (weight decay)</td>
      <td>0.01</td>
      <td>How much to pull toward zero</td>
    </tr>
    <tr>
      <td>Œµ</td>
      <td>10‚Åª‚Å∏</td>
      <td>Prevent division by zero</td>
    </tr>
  </tbody>
</table>

<h4 id="key-takeaway-2">Key Takeaway</h4>

<p><strong>AdamW is like a smart GPS:</strong></p>
<ul>
  <li><strong>SGD</strong>: ‚ÄúAlways drive 50 mph, no matter what‚Äù</li>
  <li><strong>AdamW</strong>: ‚ÄúDrive faster on smooth highways, slower on bumpy roads‚Äù</li>
</ul>

<p>It <strong>adapts the step size</strong> for each parameter based on:</p>
<ol>
  <li>Recent direction (momentum)</li>
  <li>Recent stability (volatility)</li>
</ol>

<p>This makes training <strong>faster and more stable</strong>, which is why all modern language models use it.</p>

<hr />

<h3 id="memory-requirements">Memory Requirements: Can It Fit?</h3>

<p>Let‚Äôs calculate how much memory we need to train a model like GPT-2 XL using AdamW with float32 precision.</p>

<h4 id="setup-1">Setup</h4>
<ul>
  <li>Data type: <strong>float32</strong> = <strong>4 bytes</strong> per number</li>
  <li>Batch size: <strong>B</strong></li>
  <li>Sequence length: <strong>L</strong> (context_length)</li>
  <li>Model dimension: <strong>d</strong> (d_model)</li>
  <li>Number of layers: <strong>N</strong> (num_layers)</li>
  <li>Number of heads: <strong>H</strong> (num_heads)</li>
  <li>Vocabulary size: <strong>V</strong> (vocab_size)</li>
  <li>Feed-forward dimension: <strong>d_ff = 4d</strong></li>
</ul>

<h3 id="memory-components">Memory Components</h3>

<p>Training a Transformer model requires four main types of memory. <strong>Parameters</strong> store the model‚Äôs learnable weights‚Äîthe numbers that define what the model knows. <strong>Gradients</strong> store the <em>direction</em> and <em>magnitude</em> of how each parameter should change during training, computed during backpropagation. <strong>Optimizer state</strong> keeps AdamW‚Äôs running statistics: <em>momentum</em> (which direction parameters have been moving) and <em>volatility</em> (how much parameters have been fluctuating), allowing the optimizer to make smarter, adaptive updates for each parameter. <strong>Activations</strong> store all the intermediate calculations from the forward pass‚Äîlike attention scores, normalized values, and layer outputs‚Äîwhich must be kept in memory so we can compute gradients during backpropagation. While parameters, gradients, and optimizer state have fixed size (Parameters, gradients, and optimizer state represent the model‚Äôs internal structure‚Äîthey exist regardless of what data you feed into the model), activations are the actual values flowing through the network for the particular batch therefore they scale dramatically with both batch size (B) and sequence length (L), particularly the attention scores which grow quadratically as O(BL¬≤). This is why memory, not computation, is often the bottleneck in training large language models‚Äîwith GPT-2 XL, even an 80GB GPU can only fit a batch size of 3.</p>

<h4 id="1-parameters-memory">1. Parameters Memory</h4>

<p>Let‚Äôs count the learnable parameters in each component of the Transformer.</p>

<h5 id="per-transformer-block">Per Transformer Block:</h5>

<p><strong>A. RMSNorm Layers (2 per block)</strong></p>

<p>Each RMSNorm layer has a learnable scale parameter for each dimension:</p>
<ul>
  <li>Pre-attention RMSNorm: <strong>d parameters</strong></li>
  <li>Pre-FFN RMSNorm: <strong>d parameters</strong></li>
  <li><strong>Subtotal: 2d parameters</strong></li>
</ul>

<p><strong>B. Multi-Head Self-Attention</strong></p>

<p>The attention mechanism consists of four projection matrices. Importantly, <strong>the number of heads H does not affect the parameter count</strong>‚Äîwe split the d dimensions across heads rather than expanding them.</p>

<ul>
  <li>Query projection W_Q: (d √ó d) ‚Üí <strong>d¬≤ parameters</strong></li>
  <li>Key projection W_K: (d √ó d) ‚Üí <strong>d¬≤ parameters</strong></li>
  <li>Value projection W_V: (d √ó d) ‚Üí <strong>d¬≤ parameters</strong></li>
  <li>Output projection W_O: (d √ó d) ‚Üí <strong>d¬≤ parameters</strong></li>
  <li><strong>Subtotal: 4d¬≤ parameters</strong></li>
</ul>

<p><em>Note: Modern architectures typically omit bias terms in these projections.</em></p>

<p><strong>C. Feed-Forward Network (FFN)</strong></p>

<p>The FFN expands to an intermediate dimension d_ff = 4d, then projects back:</p>
<ul>
  <li>First layer W‚ÇÅ: (d √ó 4d) ‚Üí <strong>4d¬≤ parameters</strong></li>
  <li>Activation (SiLU/GELU): <strong>0 parameters</strong> (no learnable weights)</li>
  <li>Second layer W‚ÇÇ: (4d √ó d) ‚Üí <strong>4d¬≤ parameters</strong></li>
  <li><strong>Subtotal: 8d¬≤ parameters</strong></li>
</ul>

<p><strong>Total per block: 2d + 4d¬≤ + 8d¬≤ = 12d¬≤ + 2d</strong></p>

<h5 id="all-n-transformer-blocks">All N Transformer Blocks:</h5>

<p><strong>N √ó (12d¬≤ + 2d) = 12Nd¬≤ + 2Nd</strong></p>

<h5 id="additional-components">Additional Components:</h5>

<p><strong>Token Embedding</strong></p>
<ul>
  <li>Maps each of V vocabulary tokens to a d-dimensional vector</li>
  <li>Shape: (V √ó d)</li>
  <li><strong>Parameters: Vd</strong></li>
</ul>

<p><strong>Final RMSNorm</strong></p>
<ul>
  <li>One scale parameter per dimension after the last transformer block</li>
  <li><strong>Parameters: d</strong></li>
</ul>

<p><strong>Output Projection</strong></p>
<ul>
  <li>In modern LLMs (GPT-2, LLaMA, etc.), the output projection <strong>shares weights</strong> with the token embedding (weight tying)</li>
  <li><strong>Additional parameters: 0</strong></li>
</ul>

<p><strong>Positional Embeddings</strong> (architecture-dependent)</p>
<ul>
  <li><strong>Modern models (LLaMA, GPT-3+):</strong> Use RoPE or ALiBi ‚Üí <strong>0 parameters</strong> ‚úì</li>
  <li><strong>Older models (GPT-2, BERT):</strong> Learned positional embeddings ‚Üí <strong>L_max √ó d parameters</strong></li>
</ul>

<p>For this calculation, we assume modern architecture with no learned positional embeddings.</p>

<h5 id="total-parameters">Total Parameters:</h5>

<p><strong>P = 12Nd¬≤ + 2Nd + Vd + d</strong></p>

<p>Which can be factored as:</p>

<p><strong>P = 12Nd¬≤ + d(2N + V + 1)</strong></p>

<p><strong>Important notes:</strong></p>
<ul>
  <li>The sequence length L does <strong>not</strong> affect parameter count (for modern architectures)</li>
  <li>The number of attention heads H does <strong>not</strong> affect parameter count</li>
  <li>The d¬≤ term dominates for large models (quadratic scaling with model dimension)</li>
  <li>The Vd term can be significant for large vocabularies</li>
</ul>

<h5 id="memory-requirement">Memory Requirement:</h5>

<p>Since each parameter is stored as <strong>float32</strong> (4 bytes):</p>

<p><strong>Parameters memory = 4P bytes</strong></p>

<p><strong>Example (GPT-2 XL):</strong></p>
<ul>
  <li>N = 48, d = 1,600, V = 50,257</li>
  <li>P = 12(48)(1,600¬≤) + 1,600(2√ó48 + 50,257 + 1)</li>
  <li>P ‚âà <strong>1,555,126,400 parameters</strong> (~1.56B)</li>
  <li><strong>Memory = 4 √ó 1.56B ‚âà 6.2 GB</strong></li>
</ul>

<h4 id="2-gradients-memory">2. Gradients Memory</h4>

<p>During backpropagation, we compute gradients for all parameters. AdamW requires storing these gradients to perform parameter updates.</p>

<p><strong>Gradients have the same shape as parameters:</strong></p>
<ul>
  <li>One gradient value per parameter</li>
  <li>Stored as float32 (4 bytes each)</li>
</ul>

<p><strong>Gradients memory = 4P bytes</strong></p>

<h4 id="3-optimizer-state-memory">3. Optimizer State Memory</h4>

<p>AdamW is a <strong>stateful optimizer</strong> that maintains running statistics for each parameter:</p>

<p><strong>First moment (m):</strong> Exponential moving average of gradients (momentum)</p>
<ul>
  <li>Shape: same as parameters</li>
  <li>Storage: 4 bytes per parameter (float32)</li>
  <li><strong>Memory: 4P bytes</strong></li>
</ul>

<p><strong>Second moment (v):</strong> Exponential moving average of squared gradients (volatility)</p>
<ul>
  <li>Shape: same as parameters</li>
  <li>Storage: 4 bytes per parameter (float32)</li>
  <li><strong>Memory: 4P bytes</strong></li>
</ul>

<p><strong>Total optimizer state memory = 4P + 4P = 8P bytes</strong></p>

<p><strong>Note:</strong> Unlike parameters and gradients which all models need, this 8P overhead is specific to Adam-family optimizers. Simpler optimizers like SGD only need gradient storage (4P), while more complex optimizers may require even more state.</p>

<h4 id="4-activations-memory">4. Activations Memory</h4>

<p>Activations are intermediate values computed during the forward pass that must be stored for backpropagation. This is where batch size (B) and sequence length (L) have major impact.</p>

<p><strong>Key factors:</strong></p>
<ul>
  <li>Activations scale with <strong>B</strong> (batch size) and <strong>L</strong> (sequence length)</li>
  <li>We need to store activations at multiple points for gradient computation</li>
  <li>This is typically the <strong>memory bottleneck</strong> for training</li>
</ul>

<h5 id="per-transformer-layer-activations">Per Transformer Layer Activations:</h5>

<p><strong>A. RMSNorm activations:</strong></p>
<ul>
  <li>Input to pre-attention norm: BLd</li>
  <li>Output of pre-attention norm: BLd</li>
  <li>Input to pre-FFN norm: BLd</li>
  <li><strong>Subtotal: ~3BLd</strong></li>
</ul>

<p><strong>B. Attention intermediate values:</strong></p>
<ul>
  <li>Q, K, V projections: 3 √ó BLd = <strong>3BLd</strong></li>
  <li>Attention scores (before softmax): B √ó H √ó L √ó L = <strong>BHL¬≤</strong></li>
  <li>Attention weights (after softmax): B √ó H √ó L √ó L = <strong>BHL¬≤</strong> (needed for softmax backward)</li>
  <li>Attention output: BLd</li>
</ul>

<p><strong>Subtotal: ~4BLd + 2BHL¬≤</strong></p>

<p><strong>C. Feed-Forward Network activations:</strong></p>
<ul>
  <li>W‚ÇÅ output (before activation): B √ó L √ó 4d = <strong>4BLd</strong></li>
  <li>SiLU/GELU output: B √ó L √ó 4d = <strong>4BLd</strong> (needed for activation backward)</li>
  <li>W‚ÇÇ output: BLd</li>
</ul>

<p><strong>Subtotal: ~9BLd</strong></p>

<p><strong>Total per layer: 3BLd + 4BLd + 2BHL¬≤ + 9BLd ‚âà 16BLd + 2BHL¬≤</strong></p>

<p><em>Note: The original formula‚Äôs 16BLd is an approximation; exact value depends on implementation details like whether certain intermediate values are recomputed vs. stored.</em></p>

<h5 id="all-n-layers">All N Layers:</h5>

<p><strong>N √ó (16BLd + 2BHL¬≤) ‚âà 16NBLd + 2NBHL¬≤</strong></p>

<h5 id="additional-activations-outside-layers">Additional Activations Outside Layers:</h5>

<p><strong>Token embeddings:</strong></p>
<ul>
  <li>Embedding lookup output: <strong>BLd</strong></li>
</ul>

<p><strong>Final RMSNorm:</strong></p>
<ul>
  <li>Negligible (included in layer activations)</li>
</ul>

<p><strong>Output layer (logits):</strong></p>
<ul>
  <li>Softmax probabilities: B √ó L √ó V = <strong>BLV</strong> (needed for cross-entropy backward)</li>
</ul>

<p><strong>Total additional: BLd + BLV</strong></p>

<h5 id="total-activation-count">Total Activation Count:</h5>

<p><strong>A = 16NBLd + 2NBHL¬≤ + BLd + BLV</strong></p>

<p>Simplified:
<strong>A ‚âà NBLd(16 + 1/N) + 2NBHL¬≤ + BLV</strong></p>

<p>For large N, this is dominated by: <strong>A ‚âà 16NBLd + 2NBHL¬≤ + BLV</strong></p>

<p><strong>Activations memory = 4A bytes</strong> (float32)</p>

<p><strong>Key observations:</strong></p>
<ul>
  <li><strong>O(BL) scaling:</strong> Most activations scale linearly with batch size and sequence length</li>
  <li><strong>O(BL¬≤) scaling:</strong> Attention scores create quadratic memory growth with sequence length</li>
  <li><strong>Bottleneck:</strong> For long sequences, the 2NBHL¬≤ term (attention scores) dominates</li>
</ul>

<h4 id="total-peak-memory">Total Peak Memory</h4>

<table>
  <thead>
    <tr>
      <th>Component</th>
      <th>Memory (bytes)</th>
      <th>Notes</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Parameters</strong></td>
      <td>4P</td>
      <td>Model weights</td>
    </tr>
    <tr>
      <td><strong>Gradients</strong></td>
      <td>4P</td>
      <td>‚àÇL/‚àÇŒ∏ for all parameters</td>
    </tr>
    <tr>
      <td><strong>Optimizer State (m, v)</strong></td>
      <td>8P</td>
      <td>AdamW momentum and variance</td>
    </tr>
    <tr>
      <td><strong>Activations</strong></td>
      <td>4A</td>
      <td>Intermediate values for backprop</td>
    </tr>
    <tr>
      <td><strong>TOTAL</strong></td>
      <td><strong>16P + 4A</strong></td>
      <td>Peak during training</td>
    </tr>
  </tbody>
</table>

<p><strong>Breakdown:</strong></p>
<ul>
  <li><strong>Fixed cost (16P):</strong> Independent of batch size and sequence length</li>
  <li><strong>Variable cost (4A):</strong> Scales with B, L, and L¬≤</li>
</ul>

<h4 id="gpt-2-xl-example">GPT-2 XL Example</h4>

<p><strong>Model specifications:</strong></p>
<ul>
  <li>vocab_size (V) = 50,257</li>
  <li>context_length (L) = 1,024</li>
  <li>num_layers (N) = 48</li>
  <li>d_model (d) = 1,600</li>
  <li>num_heads (H) = 25</li>
  <li>d_ff = 4 √ó d = 6,400</li>
</ul>

<h5 id="step-1-calculate-parameters-p">Step 1: Calculate Parameters (P)</h5>

<p><strong>P = 12Nd¬≤ + 2Nd + Vd + d</strong></p>

<p>P = 12(48)(1,600¬≤) + 2(48)(1,600) + 50,257(1,600) + 1,600</p>

<p><strong>P ‚âà 1,555,126,400 ‚âà 1.56 √ó 10‚Åπ parameters</strong></p>

<h5 id="step-2-calculate-fixed-memory-16p">Step 2: Calculate Fixed Memory (16P)</h5>

<p>Fixed memory = 16 √ó 1,555,126,400 bytes</p>

<p><strong>Fixed memory ‚âà 24.88 GB</strong></p>

<p>This includes parameters (4P), gradients (4P), and optimizer state (8P).</p>

<h5 id="step-3-calculate-activation-memory-per-batch-4a">Step 3: Calculate Activation Memory per Batch (4A)</h5>

<p><strong>A = 16NBLd + 2NBHL¬≤ + BLd + BLV</strong></p>

<p>For batch_size = B:</p>

<p>A = 16(48)(B)(1,024)(1,600) + 2(48)(B)(25)(1,024¬≤) + B(1,024)(1,600) + B(1,024)(50,257)</p>

<p><strong>Activation memory = 4A ‚âà 14.3 √ó B GB</strong></p>

<h5 id="step-4-total-memory-formula">Step 4: Total Memory Formula</h5>

<p><strong>Total Memory = 24.88 + 14.3 √ó B GB</strong></p>

<h5 id="step-5-maximum-batch-size-for-80gb-gpu">Step 5: Maximum Batch Size for 80GB GPU</h5>

<p>Solving: 24.88 + 14.3B ‚â§ 80</p>

<p><strong>B ‚â§ 3.85</strong></p>

<p><strong>Maximum batch_size = 3</strong> (must be integer)</p>

<p><strong>Key insight:</strong> So on a single A100 80 GB, GPT-2 XL in pure FP32 training fits a batch size of 3 without further memory optimization. This demonstrates why:</p>
<ol>
  <li>Large-scale training requires massive GPU clusters</li>
  <li>Techniques like gradient accumulation, mixed precision (float16/bfloat16), and activation checkpointing are essential</li>
  <li>Memory, not computation, is often the bottleneck</li>
</ol>

<p>The estimate above assumes na√Øve attention that stores full B√óH√óL√óL score and probability tensors, and a non-fused cross-entropy head. Modern implementations cut this drastically:</p>

<table>
  <thead>
    <tr>
      <th>Technique</th>
      <th>What it Removes</th>
      <th>Result (GPT-2 XL, FP32)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>FlashAttention</strong></td>
      <td>avoids L¬≤ attention matrices</td>
      <td>‚âà 5 GB per batch</td>
    </tr>
    <tr>
      <td><strong>Fused CE</strong></td>
      <td>streams logits ‚Üí softmax</td>
      <td>reduce 0.5‚Äì1 GB per batch</td>
    </tr>
    <tr>
      <td><strong>Activation checkpointing</strong></td>
      <td>recomputes during backward</td>
      <td>‚âà √ó 4‚Äì6 less</td>
    </tr>
    <tr>
      <td><strong>BF16 / FP16</strong></td>
      <td>halves memory per value</td>
      <td>‚âà √ó 2 less</td>
    </tr>
  </tbody>
</table>

<p>With FlashAttention and combined with other memory optimization techniques, batch_size = 12-16 is achievable for GPT-2 XL on an 80GB GPU with FP32, and this can be scaled to 32-40 with BF16 or 60-70 with activation checkpointing.</p>

<h3 id="computational-cost">Computational Cost: How Long Will This Take?</h3>

<h4 id="the-standard-formula">The Standard Formula</h4>

<p>For Transformer models, there‚Äôs a widely-used approximation:</p>

<p><strong>Training FLOPs per token ‚âà 6 √ó number of parameters</strong></p>

<p>This breaks down as:</p>
<ul>
  <li><strong>Forward pass:</strong> 2P FLOPs per token</li>
  <li><strong>Backward pass:</strong> 4P FLOPs per token (approximately 2√ó forward)</li>
  <li><strong>Total:</strong> 6P FLOPs per token</li>
</ul>

<p><strong>Why this approximation works:</strong></p>
<ul>
  <li>Dominated by matrix multiplications in attention and FFN layers</li>
  <li>For a matrix multiply of (m √ó k) @ (k √ó n), we perform 2mkn FLOPs</li>
  <li>The ‚Äú2‚Äù accounts for <em>multiply</em> and <em>add</em> operations</li>
  <li>Backward pass requires computing gradients for all weight matrices (roughly 2√ó forward)</li>
</ul>

<p><strong>What‚Äôs excluded:</strong></p>
<ul>
  <li>Optimizer computations (~11 FLOPs per parameter, negligible compared to 6P per token)</li>
  <li>Element-wise operations (LayerNorm, activations)</li>
  <li>Attention softmax</li>
</ul>

<p>These omissions are small compared to the matrix multiplications, making ‚Äú6P per token‚Äù a robust rule of thumb.</p>

<h4 id="gpt-2-xl-training-example">GPT-2 XL Training Example</h4>

<p><strong>Given:</strong></p>
<ul>
  <li>Parameters: P ‚âà 1.56 √ó 10‚Åπ</li>
  <li>Training steps: 400,000</li>
  <li><strong>Batch size: 1,024 tokens per step</strong> (total tokens processed)</li>
  <li>Hardware: Single NVIDIA A100 GPU (40GB or 80GB)</li>
  <li>Theoretical peak: 19.5 teraFLOP/s (FP32)</li>
  <li>MFU (Model FLOPs Utilization): 50%</li>
</ul>

<p><strong>Note on batch size:</strong> ‚Äú1,024 tokens‚Äù typically means the <strong>total number of tokens</strong> processed in one training step.</p>

<p><strong>Calculation:</strong></p>

<p><strong>Step 1: FLOPs per token</strong></p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>6 √ó 1.56 √ó 10‚Åπ = 9.36 √ó 10‚Åπ FLOPs per token
</code></pre></div></div>

<p><strong>Step 2: FLOPs per training step</strong></p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>9.36 √ó 10‚Åπ √ó 1,024 tokens = 9.585 √ó 10¬π¬≤ FLOPs per step
</code></pre></div></div>

<p><strong>Step 3: Total FLOPs for 400K steps</strong></p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>9.585 √ó 10¬π¬≤ √ó 400,000 = 3.834 √ó 10¬π‚Å∏ FLOPs
</code></pre></div></div>

<p><strong>Step 4: Effective throughput at 50% MFU</strong></p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Theoretical: 19.5 √ó 10¬π¬≤ FLOP/s
Effective: 19.5 √ó 10¬π¬≤ √ó 0.5 = 9.75 √ó 10¬π¬≤ FLOP/s
</code></pre></div></div>

<p><strong>Step 5: Training time</strong></p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Time = (3.834 √ó 10¬π‚Å∏) / (9.75 √ó 10¬π¬≤) = 393,231 seconds
       ‚âà 109.2 hours
       ‚âà 4.55 days
</code></pre></div></div>
<h4 id="key-insights">Key Insights</h4>

<p><strong>Why this matters:</strong></p>

<ol>
  <li><strong>Single GPU training is impractical for large models</strong>
    <ul>
      <li>Even ‚Äúmedium-sized‚Äù GPT-2 XL takes <strong>109 hours (~4.5 days)</strong> on a top-tier A100</li>
      <li>Larger models (GPT-3: 175B parameters) would take <strong>months</strong> on a single GPU</li>
      <li>GPT-3 would require: (175B/1.56B) √ó 109 hours ‚âà 12,200 hours ‚âà <strong>1.4 years</strong> on one A100!</li>
    </ul>
  </li>
  <li><strong>Parallelism is essential</strong>
    <ul>
      <li><strong>With 100 A100s:</strong> 109.2 / 100 ‚âà <strong>1.1 hours</strong> (assuming perfect scaling)</li>
      <li><strong>With 1,000 A100s:</strong> 109.2 / 1,000 ‚âà <strong>6.6 minutes</strong> (assuming perfect scaling)</li>
      <li><strong>Real-world scaling efficiency</strong> is typically 60-90% due to:
        <ul>
          <li>Communication overhead (gradient synchronization)</li>
          <li>Load imbalancing</li>
          <li>Pipeline bubbles</li>
          <li>Network bandwidth limitations</li>
        </ul>
      </li>
      <li><strong>Realistic with 100 A100s:</strong> 109.2 / (100 √ó 0.7) ‚âà <strong>1.6 hours</strong> (at 70% efficiency)</li>
    </ul>
  </li>
  <li><strong>Cost considerations</strong>
    <ul>
      <li><strong>A100 cloud cost:</strong> ~$2-4/hour (varies by provider: AWS, GCP, Azure)</li>
      <li><strong>Single A100 training:</strong> 109.2 hours √ó $2-4 = <strong>$218-437</strong></li>
      <li><strong>100 A100s (70% efficiency):</strong>
        <ul>
          <li>Time: ~1.6 hours</li>
          <li>Cost: 100 GPUs √ó 1.6 hours √ó $2-4 = <strong>$320-640</strong></li>
          <li><strong>Trade-off:</strong> Slightly higher cost, but <strong>68√ó faster!</strong></li>
        </ul>
      </li>
      <li><strong>Cost scales linearly with GPU count, but time scales sub-linearly</strong> (due to overhead)</li>
    </ul>
  </li>
  <li><strong>Memory vs. compute trade-off</strong>
    <ul>
      <li>We calculated <strong>batch_size = 3</strong> fits in 80GB memory (using FP32)</li>
      <li>Larger batches could improve training efficiency (better GPU utilization, more stable gradients)</li>
      <li><strong>Solutions to increase effective batch size:</strong>
        <ul>
          <li><strong>Gradient accumulation:</strong> Simulate larger batches by accumulating gradients over multiple forward/backward passes before updating
            <ul>
              <li>Example: Accumulate 32 micro-batches of size 3 ‚Üí effective batch size of 96</li>
            </ul>
          </li>
          <li><strong>Mixed precision (FP16/BF16):</strong> Reduce memory by 2√ó, allowing batch_size ‚âà 6-8</li>
          <li><strong>Gradient checkpointing:</strong> Trade compute for memory (recompute activations during backward pass)</li>
          <li><strong>Multi-GPU training:</strong> Distribute batch across GPUs (data parallelism)</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>Why GPT-3 scale requires massive clusters</strong>
    <ul>
      <li>GPT-3 (175B parameters): <strong>~112√ó larger</strong> than GPT-2 XL</li>
      <li>Single A100 would take: ~1.4 years</li>
      <li>With <strong>10,000 A100s</strong> (at 60% efficiency): ~12,200 / (10,000 √ó 0.6) ‚âà <strong>2 hours</strong></li>
      <li>This explains why frontier models require:
        <ul>
          <li>Tens of thousands of GPUs</li>
          <li>Custom datacenters</li>
          <li>Months of calendar time (even with massive parallelism)</li>
          <li>Millions of dollars in compute costs</li>
        </ul>
      </li>
    </ul>
  </li>
</ol>

<p><strong>Summary Table for training GPT-2 XL</strong></p>

<table>
  <thead>
    <tr>
      <th>Configuration</th>
      <th>Time</th>
      <th>Cost</th>
      <th>Notes</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>1 A100 (FP32)</strong></td>
      <td>109 hours</td>
      <td>$218-437</td>
      <td>Baseline</td>
    </tr>
    <tr>
      <td><strong>1 A100 (FP16)</strong></td>
      <td>~55 hours</td>
      <td>$110-220</td>
      <td>2√ó faster with mixed precision</td>
    </tr>
    <tr>
      <td><strong>100 A100s (perfect)</strong></td>
      <td>1.1 hours</td>
      <td>$220-440</td>
      <td>Theoretical best case</td>
    </tr>
    <tr>
      <td><strong>100 A100s (70% eff.)</strong></td>
      <td>1.6 hours</td>
      <td>$320-640</td>
      <td>Realistic with overhead</td>
    </tr>
    <tr>
      <td><strong>1,000 A100s (perfect)</strong></td>
      <td>6.6 minutes</td>
      <td>$220-440</td>
      <td>Theoretical best case</td>
    </tr>
    <tr>
      <td><strong>1,000 A100s (60% eff.)</strong></td>
      <td>11 minutes</td>
      <td>$367-733</td>
      <td>Realistic at scale</td>
    </tr>
  </tbody>
</table>

<p><strong>Key takeaway:</strong> Parallelism gives you speed, not cost savings. Using 100 GPUs costs about the same (or slightly more) but finishes <strong>68√ó faster</strong>, which matters for iteration speed and time-to-market!</p>

<hr />

<h3 id="learning-rate-schedules">Learning Rate Schedules: Starting Fast, Ending Slow</h3>

<h4 id="why-do-we-need-a-schedule">Why Do We Need a Schedule?</h4>

<p>Imagine you‚Äôre trying to find the lowest point in a valley while blindfolded:</p>

<ul>
  <li><strong>Beginning</strong>: You‚Äôre far from the goal ‚Üí take <strong>big steps</strong> to get there quickly</li>
  <li><strong>Middle</strong>: You‚Äôre getting close ‚Üí take <strong>medium steps</strong> to avoid overshooting</li>
  <li><strong>End</strong>: You‚Äôre very close ‚Üí take <strong>tiny steps</strong> to settle into the exact lowest point</li>
</ul>

<p>The learning rate schedule does exactly this for training!</p>

<h4 id="the-problem-with-fixed-learning-rate">The Problem with Fixed Learning Rate</h4>

<p><strong>Too high throughout training:</strong></p>
<ul>
  <li>Fast at first, but bounces around the minimum at the end</li>
  <li>Never settles into the best solution</li>
</ul>

<p><strong>Too low throughout training:</strong></p>
<ul>
  <li>Slow progress, takes forever to train</li>
  <li>Might get stuck in bad spots</li>
</ul>

<p><strong>Solution: Start high, gradually decrease!</strong></p>

<h4 id="cosine-annealing-schedule">Cosine Annealing Schedule</h4>

<p>The schedule has <strong>3 phases</strong>:</p>

<h5 id="phase-1-warm-up-t--t_w">Phase 1: Warm-up (t &lt; T_w)</h5>
<p><strong>‚ÄúEase into it‚Äù</strong></p>

\[\alpha_t = \frac{t}{T_w} \times \alpha_{max}\]

<ul>
  <li>Start from <strong>0</strong> and <strong>linearly increase</strong> to Œ±_max</li>
  <li>Example: If T_w = 1,000 and Œ±_max = 0.001:
    <ul>
      <li>Step 0: Œ± = 0</li>
      <li>Step 500: Œ± = 0.0005 (halfway)</li>
      <li>Step 1,000: Œ± = 0.001 (full speed!)</li>
    </ul>
  </li>
</ul>

<p><strong>Why warm-up?</strong></p>
<ul>
  <li>Prevents unstable updates at the very beginning</li>
  <li>Gives the model time to ‚Äúorient itself‚Äù</li>
  <li>Like warming up before exercise!</li>
</ul>

<h5 id="phase-2-cosine-annealing-t_w--t--t_c">Phase 2: Cosine Annealing (T_w ‚â§ t ‚â§ T_c)</h5>
<p><strong>‚ÄúSmooth slowdown‚Äù</strong></p>

\[\alpha_t = \alpha_{min} + \frac{1}{2}\left(1 + \cos\left(\frac{t - T_w}{T_c - T_w} \pi\right)\right)(\alpha_{max} - \alpha_{min})\]

<p>This creates a <strong>smooth curve</strong> from Œ±_max down to Œ±_min!</p>

<p><strong>Breaking it down:</strong></p>

<ol>
  <li><strong>Progress ratio</strong>: How far through annealing are we? (0 to 1)</li>
  <li><strong>Cosine curve</strong>: cos goes from 1 ‚Üí -1 as we progress</li>
  <li><strong>Scale to [0, 1]</strong>: Transform to go from 1 ‚Üí 0</li>
  <li><strong>Final value</strong>: Interpolate between Œ±_max and Œ±_min</li>
</ol>

<p><strong>The result: A smooth decrease from Œ±_max to Œ±_min</strong></p>

<h5 id="phase-3-post-annealing-t--t_c">Phase 3: Post-Annealing (t &gt; T_c)</h5>
<p><strong>‚ÄúMaintain minimum‚Äù</strong></p>

\[\alpha_t = \alpha_{min}\]

<ul>
  <li>Keep the learning rate at the minimum value</li>
  <li>Fine-tuning with tiny steps</li>
</ul>

<h4 id="visual-example">Visual Example</h4>

<p>Let‚Äôs say:</p>
<ul>
  <li>Œ±_max = 0.001</li>
  <li>Œ±_min = 0.0001</li>
  <li>T_w = 1,000 (warm-up ends)</li>
  <li>T_c = 10,000 (annealing ends)</li>
</ul>

<p><strong>Learning rate over time:</strong></p>

<table>
  <thead>
    <tr>
      <th>Step</th>
      <th>Phase</th>
      <th>Learning Rate</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>Warm-up</td>
      <td>0</td>
    </tr>
    <tr>
      <td>500</td>
      <td>Warm-up</td>
      <td>0.0005</td>
    </tr>
    <tr>
      <td>1,000</td>
      <td>Warm-up ‚Üí Annealing</td>
      <td>0.001</td>
    </tr>
    <tr>
      <td>3,000</td>
      <td>Annealing</td>
      <td>~0.00085</td>
    </tr>
    <tr>
      <td>5,500</td>
      <td>Annealing</td>
      <td>~0.00055</td>
    </tr>
    <tr>
      <td>8,000</td>
      <td>Annealing</td>
      <td>~0.00025</td>
    </tr>
    <tr>
      <td>10,000</td>
      <td>Annealing ‚Üí Post</td>
      <td>0.0001</td>
    </tr>
    <tr>
      <td>15,000</td>
      <td>Post-annealing</td>
      <td>0.0001</td>
    </tr>
  </tbody>
</table>

<h4 id="simple-math-example">Simple Math Example</h4>

<p>Let‚Äôs calculate learning rate at t = 5,500 (middle of annealing):</p>

<p><strong>Given:</strong></p>
<ul>
  <li>T_w = 1,000, T_c = 10,000</li>
  <li>Œ±_max = 0.001, Œ±_min = 0.0001</li>
</ul>

<p><strong>Step 1:</strong> Progress = (5,500 - 1,000) / (10,000 - 1,000) = 0.5</p>

<p><strong>Step 2:</strong> cos(0.5 √ó œÄ) = 0</p>

<p><strong>Step 3:</strong> ¬Ω(1 + 0) = 0.5</p>

<p><strong>Step 4:</strong> Œ± = 0.0001 + 0.5 √ó (0.001 - 0.0001) = <strong>0.00055</strong></p>

<p>Exactly halfway between min and max!</p>

<h4 id="key-takeaway-3">Key Takeaway</h4>

<p><strong>Learning rate schedule = adaptive step size:</strong></p>
<ol>
  <li><strong>Warm-up</strong>: Gradually increase from 0 ‚Üí big (safety at start)</li>
  <li><strong>Cosine annealing</strong>: Smoothly decrease from big ‚Üí small (careful landing)</li>
  <li><strong>Post-annealing</strong>: Stay small (fine-tuning)</li>
</ol>

<p>It‚Äôs like driving: accelerate leaving the driveway, cruise on the highway, then slow down smoothly as you approach your destination!</p>

<hr />

<h3 id="gradient-clipping">Gradient Clipping: The Safety Mechanism</h3>

<h4 id="the-problem-exploding-gradients">The Problem: Exploding Gradients</h4>

<p>Imagine you‚Äôre walking downhill with a GPS that tells you how steep the slope is:</p>

<ul>
  <li><strong>Normal case</strong>: ‚ÄúSlope is 5 degrees‚Äù ‚Üí take a reasonable step</li>
  <li><strong>Bad case</strong>: ‚ÄúSlope is 5,000 degrees!!!‚Äù ‚Üí you‚Äôd jump off a cliff!</li>
</ul>

<p>Sometimes during training, the model encounters weird examples that produce <strong>huge gradients</strong>. If you take a step proportional to these giant gradients, your model parameters can explode and training crashes!</p>

<h4 id="what-is-gradient-clipping">What is Gradient Clipping?</h4>

<p>Gradient clipping is like having a <strong>speed limiter</strong> on your updates:</p>

<p><strong>‚ÄúNo matter how steep the slope, I won‚Äôt step faster than X units‚Äù</strong></p>

<h4 id="how-it-works-step-by-step">How It Works (Step by Step)</h4>

<h5 id="step-1-calculate-the-gradient-norm">Step 1: Calculate the Gradient Norm</h5>

<p>After the backward pass, measure how ‚Äúbig‚Äù the gradients are overall:</p>

\[\|g\|_2 = \sqrt{g_1^2 + g_2^2 + g_3^2 + ... + g_n^2}\]

<p>This is the <strong>L2 norm</strong> (Euclidean distance) - just the length of the gradient vector.</p>

<p><strong>Example:</strong></p>
<ul>
  <li>If gradients are [3, 4]: norm = ‚àö(9 + 16) = <strong>5</strong></li>
  <li>If gradients are [30, 40]: norm = ‚àö(900 + 1,600) = <strong>50</strong></li>
</ul>

<h5 id="step-2-check-against-maximum">Step 2: Check Against Maximum</h5>

<p>Set a threshold <strong>M</strong> (e.g., M = 1.0):</p>

<p><strong>Is ‚à•g‚à•‚ÇÇ ‚â§ M?</strong></p>

<ul>
  <li><strong>YES</strong> ‚Üí Gradients are reasonable, use them as-is ‚úì</li>
  <li><strong>NO</strong> ‚Üí Gradients are too big, need to clip! ‚úÇÔ∏è</li>
</ul>

<h5 id="step-3-scale-down-if-needed">Step 3: Scale Down If Needed</h5>

<p>If the norm exceeds M, <strong>rescale</strong> the entire gradient vector:</p>

\[g_{\text{clipped}} = g \times \frac{M}{\|g\|_2 + \epsilon}\]

<p>Where Œµ ‚âà 10‚Åª‚Å∂ is for numerical stability.</p>

<p><strong>What this does:</strong></p>
<ul>
  <li>Keeps the <strong>direction</strong> the same</li>
  <li>Reduces the <strong>magnitude</strong> to exactly M</li>
</ul>

<h4 id="simple-example-3">Simple Example</h4>

<p><strong>Given:</strong></p>
<ul>
  <li>Gradient vector: g = [30, 40]</li>
  <li>Maximum norm: M = 1.0</li>
  <li>Œµ = 10‚Åª‚Å∂ (negligible)</li>
</ul>

<p><strong>Step 1: Calculate norm</strong></p>
<ul>
  <li>‚à•g‚à•‚ÇÇ = ‚àö(900 + 1,600) = <strong>50</strong></li>
</ul>

<p><strong>Step 2: Check threshold</strong></p>
<ul>
  <li>50 &gt; 1.0 ‚Üí <strong>Need to clip!</strong></li>
</ul>

<p><strong>Step 3: Scale down</strong></p>
<ul>
  <li>Scaling factor = 1.0 / 50 = <strong>0.02</strong></li>
  <li>g_clipped = [30, 40] √ó 0.02 = <strong>[0.6, 0.8]</strong></li>
</ul>

<p><strong>Verify new norm:</strong></p>
<ul>
  <li>‚à•g_clipped‚à•‚ÇÇ = ‚àö(0.36 + 0.64) = <strong>1.0</strong> ‚úì</li>
</ul>

<p><strong>Result:</strong> We‚Äôve scaled from norm 50 to norm 1.0, keeping the same direction!</p>

<h4 id="another-example-no-clipping">Another Example (No Clipping)</h4>

<p><strong>Given:</strong></p>
<ul>
  <li>Gradient: g = [0.3, 0.4]</li>
  <li>Maximum: M = 1.0</li>
</ul>

<p><strong>Norm:</strong> ‚àö(0.09 + 0.16) = <strong>0.5</strong></p>

<p><strong>Check:</strong> 0.5 ‚â§ 1.0 ‚Üí <strong>No clipping needed!</strong></p>

<p>Use gradient as-is: [0.3, 0.4]</p>

<h4 id="why-does-this-work">Why Does This Work?</h4>

<p><strong>Preserves Direction:</strong></p>
<ul>
  <li>We still move in the right direction (downhill)</li>
  <li>Just limit how far we jump</li>
</ul>

<p><strong>Prevents Instability:</strong></p>
<ul>
  <li>Giant gradients can make parameters explode</li>
  <li>Clipping ensures updates stay reasonable</li>
</ul>

<p><strong>Training Stability:</strong></p>
<ul>
  <li>Without clipping: loss might spike or become NaN</li>
  <li>With clipping: training stays smooth</li>
</ul>

<h4 id="pseudocode">Pseudocode</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># After backward pass
</span><span class="n">gradients</span> <span class="o">=</span> <span class="n">compute_gradients</span><span class="p">()</span>

<span class="c1"># Calculate L2 norm of all gradients
</span><span class="n">grad_norm</span> <span class="o">=</span> <span class="n">sqrt</span><span class="p">(</span><span class="nb">sum</span><span class="p">(</span><span class="n">g</span><span class="err">¬≤</span> <span class="k">for</span> <span class="nb">all</span> <span class="n">g</span> <span class="ow">in</span> <span class="n">gradients</span><span class="p">))</span>

<span class="c1"># Clip if needed
</span><span class="n">max_norm</span> <span class="o">=</span> <span class="mf">1.0</span>
<span class="k">if</span> <span class="n">grad_norm</span> <span class="o">&gt;</span> <span class="n">max_norm</span><span class="p">:</span>
    <span class="n">scaling_factor</span> <span class="o">=</span> <span class="n">max_norm</span> <span class="o">/</span> <span class="p">(</span><span class="n">grad_norm</span> <span class="o">+</span> <span class="mf">1e-6</span><span class="p">)</span>
    <span class="n">gradients</span> <span class="o">=</span> <span class="n">gradients</span> <span class="o">*</span> <span class="n">scaling_factor</span>

<span class="c1"># Use clipped gradients in optimizer
</span><span class="n">optimizer</span><span class="p">.</span><span class="n">step</span><span class="p">(</span><span class="n">gradients</span><span class="p">)</span>
</code></pre></div></div>

<h4 id="key-takeaway-4">Key Takeaway</h4>

<p><strong>Gradient clipping = speed limiter for training:</strong></p>

<ol>
  <li><strong>Measure</strong> how big the gradients are (L2 norm)</li>
  <li><strong>Check</strong> if they exceed the maximum allowed</li>
  <li><strong>Scale down</strong> if needed (keep direction, reduce magnitude)</li>
</ol>

<p><strong>Result:</strong> Training stays stable even when occasional batches produce huge gradients. It‚Äôs like having a safety governor on a car engine - you can still accelerate and steer normally, but it prevents dangerous speeds that could cause a crash.</p>

<hr />

<h3 id="conclusion">Conclusion</h3>

<p>Training large language models involves carefully balancing multiple components:</p>

<ol>
  <li><strong>Loss functions</strong> (cross-entropy) measure how bad the model did not predict the correct word</li>
  <li><strong>Metrics</strong> (perplexity) make model prediction results more interpretable</li>
  <li><strong>Optimizers</strong> (SGD ‚Üí AdamW) determine how we update model parameters</li>
  <li><strong>Memory management</strong> dictates what type of hardware we need</li>
  <li><strong>Computational budgets</strong> determine training time, make trade-off between cost and speed</li>
  <li><strong>Learning rate schedules</strong> help us converge smoothly</li>
  <li><strong>Safety mechanisms</strong> (gradient clipping) prevent training instability</li>
</ol>

<p>While the math can seem complex at first, the underlying intuitions are straightforward: we‚Äôre teaching a model to predict text by repeatedly showing it examples, measuring its mistakes, and adjusting its parameters to do better next time. The real challenge isn‚Äôt understanding any single component - it‚Äôs orchestrating all of them together efficiently at massive scale. That‚Äôs what makes training models like GPT-3 and GPT-4 such remarkable engineering achievements.</p>


  </div><a class="u-url" href="/cs336/2025/10/05/cs336-training-a-transformer-lm-part-1.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="http://localhost:4000/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>I chronicle my captivating journey through Generative AI, sharing insights,  breakthroughs, and learnings from my enthralling side projects in the field. 
</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"></ul>
</div>

  </div>

</footer>
</body>

</html>
