<!DOCTYPE html>
<html lang="en"><head>
  <link rel="shortcut icon" type="image/png" href="/assets/favicon.png">
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Study Notes: Stanford CS336 Language Modeling from Scratch [9] | üçí Han‚Äôs Generative AI Quest</title>
<meta name="generator" content="Jekyll v4.3.2" />
<meta property="og:title" content="Study Notes: Stanford CS336 Language Modeling from Scratch [9]" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Understanding Softmax, Log-Softmax, and Cross-Entropy: A Complete Implementation Guide This note explains how to implement Softmax, Log-Softmax, and Cross-Entropy from scratch in PyTorch, highlighting key mathematical tricks to ensure numerical stability. It shows why subtracting the maximum logit before exponentiation prevents overflow and underflow, and walks through essential PyTorch tensor operations‚Äîdim, keepdim, view, and reshape‚Äîthat are critical for implementing machine learning algorithms efficiently." />
<meta property="og:description" content="Understanding Softmax, Log-Softmax, and Cross-Entropy: A Complete Implementation Guide This note explains how to implement Softmax, Log-Softmax, and Cross-Entropy from scratch in PyTorch, highlighting key mathematical tricks to ensure numerical stability. It shows why subtracting the maximum logit before exponentiation prevents overflow and underflow, and walks through essential PyTorch tensor operations‚Äîdim, keepdim, view, and reshape‚Äîthat are critical for implementing machine learning algorithms efficiently." />
<link rel="canonical" href="http://localhost:4000/cs336/2025/10/19/cs336-implement-softmax-log_softmax-cross_entropy.html" />
<meta property="og:url" content="http://localhost:4000/cs336/2025/10/19/cs336-implement-softmax-log_softmax-cross_entropy.html" />
<meta property="og:site_name" content="üçí Han‚Äôs Generative AI Quest" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2025-10-19T00:00:00-07:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Study Notes: Stanford CS336 Language Modeling from Scratch [9]" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2025-10-19T00:00:00-07:00","datePublished":"2025-10-19T00:00:00-07:00","description":"Understanding Softmax, Log-Softmax, and Cross-Entropy: A Complete Implementation Guide This note explains how to implement Softmax, Log-Softmax, and Cross-Entropy from scratch in PyTorch, highlighting key mathematical tricks to ensure numerical stability. It shows why subtracting the maximum logit before exponentiation prevents overflow and underflow, and walks through essential PyTorch tensor operations‚Äîdim, keepdim, view, and reshape‚Äîthat are critical for implementing machine learning algorithms efficiently.","headline":"Study Notes: Stanford CS336 Language Modeling from Scratch [9]","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/cs336/2025/10/19/cs336-implement-softmax-log_softmax-cross_entropy.html"},"url":"http://localhost:4000/cs336/2025/10/19/cs336-implement-softmax-log_softmax-cross_entropy.html"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="üçí Han&apos;s Generative AI Quest" />

<!-- MathJax Configuration -->
<script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$', '$$'], ['\\[', '\\]']],
      processEscapes: true,
      processEnvironments: true
    },
    options: {
      skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
    }
  };
</script>

<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/">üçí Han&#39;s Generative AI Quest</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Study Notes: Stanford CS336 Language Modeling from Scratch [9]</h1>
    <p class="post-meta"><time class="dt-published" datetime="2025-10-19T00:00:00-07:00" itemprop="datePublished">
        Oct 19, 2025
      </time>‚Ä¢ 
          <span itemprop="author" itemscope itemtype="http://schema.org/Person">
            <span class="p-author h-card" itemprop="name">Han Yu</span></span></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <h2 id="understanding-softmax-log-softmax-and-cross-entropy-a-complete-implementation-guide">Understanding Softmax, Log-Softmax, and Cross-Entropy: A Complete Implementation Guide</h2>
<p>This note explains how to implement <code class="language-plaintext highlighter-rouge">Softmax</code>, <code class="language-plaintext highlighter-rouge">Log-Softmax</code>, and <code class="language-plaintext highlighter-rouge">Cross-Entropy</code> from scratch in PyTorch, highlighting key mathematical tricks to ensure numerical stability. It shows why subtracting the maximum logit before exponentiation prevents <strong>overflow</strong> and <strong>underflow</strong>, and walks through essential PyTorch tensor operations‚Äî<code class="language-plaintext highlighter-rouge">dim</code>, <code class="language-plaintext highlighter-rouge">keepdim</code>, <code class="language-plaintext highlighter-rouge">view</code>, and <code class="language-plaintext highlighter-rouge">reshape</code>‚Äîthat are critical for implementing machine learning algorithms efficiently.</p>

<h3 id="table-of-contents">Table of Contents</h3>
<ol>
  <li><a href="#numerical-stability-deep-dive">Numerical Stability Deep Dive</a></li>
  <li><a href="#pytorch-fundamentals">PyTorch Fundamentals: <code class="language-plaintext highlighter-rouge">dim</code>, <code class="language-plaintext highlighter-rouge">keepdim</code>, and <code class="language-plaintext highlighter-rouge">view</code></a></li>
  <li><a href="#the-implementation">The Implementation</a></li>
  <li><a href="#detailed-explanation">Detailed Explanation of the Implementation</a></li>
</ol>

<hr />

<h3 id="numerical-stability-deep-dive">Numerical Stability Deep Dive</h3>

<p>Before diving into the implementation, it‚Äôs crucial to understand why numerical stability matters. When implementing operations like softmax and cross-entropy, we must carefully handle potential <strong>overflow</strong> and <strong>underflow</strong> issues. Let‚Äôs explore these challenges and their solutions.</p>

<h4 id="why-we-subtract-the-maximum-preventing-overflow-and-underflow">Why We Subtract the Maximum: Preventing Overflow and Underflow</h4>

<p>When implementing softmax from scratch, you‚Äôll encounter a critical numerical stability trick: subtracting the maximum value before computing exponentials. Let‚Äôs explore why this matters with concrete examples.</p>

<h4 id="the-problem-exponential-overflow">The Problem: Exponential Overflow</h4>

<p>The softmax formula is:</p>

<p>$\text{softmax}(x_i) = \frac{e^{x_i}}{\sum_{j} e^{x_j}}$</p>

<h5 id="example-1-large-numbers-cause-overflow">Example 1: Large Numbers Cause Overflow</h5>

<p>Suppose you have logits: <code class="language-plaintext highlighter-rouge">[1000, 1001, 1002]</code></p>

<p><strong>Naive approach (without subtracting max):</strong></p>

<p>$e^{1000} \approx 2 \times 10^{434}$,
$e^{1001} \approx 5 \times 10^{434}$,
$e^{1002} \approx 1.4 \times 10^{435}$</p>

<p>These numbers are astronomically large and exceed what a computer can represent (approximately $10^{308}$ for 64-bit floats). Python/PyTorch returns <code class="language-plaintext highlighter-rouge">inf</code> (infinity), leading to:</p>

<p><strong>Result:</strong> <code class="language-plaintext highlighter-rouge">[inf, inf, inf]</code> ‚Üí Division gives <code class="language-plaintext highlighter-rouge">[nan, nan, nan]</code> ‚ùå</p>

<p><strong>Demonstration in PyTorch:</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch</span>

<span class="n">logits</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1000.0</span><span class="p">,</span> <span class="mf">1001.0</span><span class="p">,</span> <span class="mf">1002.0</span><span class="p">])</span>

<span class="c1"># Naive implementation (broken!)
</span><span class="n">exp_vals</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Exponentials:"</span><span class="p">,</span> <span class="n">exp_vals</span><span class="p">)</span>
<span class="c1"># Output: tensor([inf, inf, inf])
</span>
<span class="n">sum_exp</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">exp_vals</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Sum:"</span><span class="p">,</span> <span class="n">sum_exp</span><span class="p">)</span>
<span class="c1"># Output: tensor(inf)
</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">exp_vals</span> <span class="o">/</span> <span class="n">sum_exp</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Result:"</span><span class="p">,</span> <span class="n">result</span><span class="p">)</span>
<span class="c1"># Output: tensor([nan, nan, nan])  ‚Üê BROKEN!
</span></code></pre></div></div>

<h5 id="example-2-with-numerical-stability-trick">Example 2: With Numerical Stability Trick</h5>

<p>Same logits: <code class="language-plaintext highlighter-rouge">[1000, 1001, 1002]</code></p>

<p><strong>Step 1:</strong> Subtract the maximum value (1002):</p>

<p>$[1000, 1001, 1002] - 1002 = [-2, -1, 0]$</p>

<p><strong>Step 2:</strong> Compute exponentials on the stable values:</p>

<p>$e^{-2} \approx 0.135$,
$e^{-1} \approx 0.368$,
$e^{0} = 1.0$</p>

<p>These are manageable numbers with no overflow!</p>

<p><strong>Step 3:</strong> Normalize to get probabilities:</p>

<p>$\text{sum} = 0.135 + 0.368 + 1.0 = 1.503$</p>

<p>$\text{softmax} = \left[\frac{0.135}{1.503}, \frac{0.368}{1.503}, \frac{1.0}{1.503}\right] = [0.090, 0.245, 0.665]$</p>

<p><strong>Demonstration in PyTorch:</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch</span>

<span class="n">logits</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1000.0</span><span class="p">,</span> <span class="mf">1001.0</span><span class="p">,</span> <span class="mf">1002.0</span><span class="p">])</span>

<span class="c1"># Stable implementation
</span><span class="n">max_val</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nb">max</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Max value:"</span><span class="p">,</span> <span class="n">max_val</span><span class="p">)</span>
<span class="c1"># Output: tensor(1002.)
</span>
<span class="n">logits_stable</span> <span class="o">=</span> <span class="n">logits</span> <span class="o">-</span> <span class="n">max_val</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Stabilized logits:"</span><span class="p">,</span> <span class="n">logits_stable</span><span class="p">)</span>
<span class="c1"># Output: tensor([-2., -1.,  0.])
</span>
<span class="n">exp_vals</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="n">logits_stable</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Exponentials:"</span><span class="p">,</span> <span class="n">exp_vals</span><span class="p">)</span>
<span class="c1"># Output: tensor([0.1353, 0.3679, 1.0000])
</span>
<span class="n">sum_exp</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">exp_vals</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Sum:"</span><span class="p">,</span> <span class="n">sum_exp</span><span class="p">)</span>
<span class="c1"># Output: tensor(1.5032)
</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">exp_vals</span> <span class="o">/</span> <span class="n">sum_exp</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Result:"</span><span class="p">,</span> <span class="n">result</span><span class="p">)</span>
<span class="c1"># Output: tensor([0.0900, 0.2447, 0.6652])  ‚Üê WORKS!
</span>
<span class="c1"># Verify probabilities sum to 1
</span><span class="k">print</span><span class="p">(</span><span class="s">"Sum of probabilities:"</span><span class="p">,</span> <span class="n">torch</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">result</span><span class="p">))</span>
<span class="c1"># Output: tensor(1.0000)
</span></code></pre></div></div>
<h4 id="mathematical-proof-why-this-works">Mathematical Proof: Why This Works</h4>

<p>The stability trick is mathematically sound because:</p>

<p>$\text{softmax}(x) = \text{softmax}(x - c)$</p>

<p>for any constant $c$!</p>

<p><strong>Proof:</strong></p>

<p>$\frac{e^{x_i - c}}{\sum_{j} e^{x_j - c}} = \frac{e^{x_i} \cdot e^{-c}}{\sum_{j} e^{x_j} \cdot e^{-c}} = \frac{e^{x_i} \cdot e^{-c}}{e^{-c} \cdot \sum_{j} e^{x_j}} = \frac{e^{x_i}}{\sum_{j} e^{x_j}}$</p>

<p>The $e^{-c}$ terms cancel out! By choosing $c = \max(x)$, we ensure the largest exponent becomes 0, preventing overflow while maintaining mathematical correctness.</p>

<h4 id="the-underflow-problem">The Underflow Problem</h4>

<p>There‚Äôs also a potential underflow issue with very negative numbers:</p>

<p>$e^{-1000} \approx 0$</p>

<p>This underflows to zero in floating-point arithmetic. However, by subtracting the maximum, the largest value becomes 0 ($e^0 = 1$), and only smaller values might underflow. This is acceptable because extremely small exponentials contribute negligibly to the sum anyway.</p>

<p><strong>Demonstration:</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch</span>

<span class="c1"># Very negative logits
</span><span class="n">logits</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([</span><span class="o">-</span><span class="mf">1000.0</span><span class="p">,</span> <span class="o">-</span><span class="mf">999.0</span><span class="p">,</span> <span class="o">-</span><span class="mf">998.0</span><span class="p">])</span>

<span class="c1"># Without stability trick
</span><span class="n">exp_vals</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Exponentials:"</span><span class="p">,</span> <span class="n">exp_vals</span><span class="p">)</span>
<span class="c1"># Output: tensor([0., 0., 0.])  ‚Üê All underflow!
</span>
<span class="c1"># With stability trick
</span><span class="n">max_val</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nb">max</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span>
<span class="n">logits_stable</span> <span class="o">=</span> <span class="n">logits</span> <span class="o">-</span> <span class="n">max_val</span>
<span class="n">exp_vals_stable</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="n">logits_stable</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Stable exponentials:"</span><span class="p">,</span> <span class="n">exp_vals_stable</span><span class="p">)</span>
<span class="c1"># Output: tensor([0.1353, 0.3679, 1.0000])  ‚Üê Works perfectly!
</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">exp_vals_stable</span> <span class="o">/</span> <span class="n">torch</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">exp_vals_stable</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Result:"</span><span class="p">,</span> <span class="n">result</span><span class="p">)</span>
<span class="c1"># Output: tensor([0.0900, 0.2447, 0.6652])
</span></code></pre></div></div>
<h4 id="why-this-matters-in-deep-learning">Why This Matters in Deep Learning</h4>

<p>In deep learning, logits frequently reach magnitudes of hundreds or thousands, especially in:</p>

<ul>
  <li><strong>Language models</strong> with large vocabulary sizes (tens of thousands of classes)</li>
  <li><strong>Deep networks</strong> where activations accumulate through many layers</li>
  <li><strong>Unnormalized outputs</strong> before the final softmax layer</li>
  <li><strong>Training dynamics</strong> where gradients can push logits to extreme values</li>
</ul>

<p>Without the stability trick, your model would crash with <code class="language-plaintext highlighter-rouge">nan</code> values during training or inference, making it impossible to:</p>
<ul>
  <li>Train the model (gradients become <code class="language-plaintext highlighter-rouge">nan</code>)</li>
  <li>Make predictions (outputs become <code class="language-plaintext highlighter-rouge">nan</code>)</li>
  <li>Debug issues (everything breaks catastrophically)</li>
</ul>

<p>This simple technique‚Äîsubtracting the maximum‚Äîkeeps all exponentials in a safe computational range (approximately 0 to 1) while computing the exact same mathematical result.</p>

<p><strong>Real-world example from GPT-2:</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Typical logits from a language model
</span><span class="n">logits</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">50257</span><span class="p">)</span> <span class="o">*</span> <span class="mi">10</span>  <span class="c1"># vocab_size = 50,257
</span><span class="k">print</span><span class="p">(</span><span class="s">"Logit range:"</span><span class="p">,</span> <span class="n">logits</span><span class="p">.</span><span class="nb">min</span><span class="p">().</span><span class="n">item</span><span class="p">(),</span> <span class="s">"to"</span><span class="p">,</span> <span class="n">logits</span><span class="p">.</span><span class="nb">max</span><span class="p">().</span><span class="n">item</span><span class="p">())</span>
<span class="c1"># Output: Logit range: -28.3 to 31.7
</span>
<span class="c1"># Without stability trick (would overflow!)
# With stability trick (works perfectly)
</span><span class="n">probs</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Probability range:"</span><span class="p">,</span> <span class="n">probs</span><span class="p">.</span><span class="nb">min</span><span class="p">().</span><span class="n">item</span><span class="p">(),</span> <span class="s">"to"</span><span class="p">,</span> <span class="n">probs</span><span class="p">.</span><span class="nb">max</span><span class="p">().</span><span class="n">item</span><span class="p">())</span>
<span class="c1"># Output: Probability range: 1.2e-27 to 0.0043
</span><span class="k">print</span><span class="p">(</span><span class="s">"Sum:"</span><span class="p">,</span> <span class="n">probs</span><span class="p">.</span><span class="nb">sum</span><span class="p">().</span><span class="n">item</span><span class="p">())</span>
<span class="c1"># Output: Sum: 1.0
</span></code></pre></div></div>
<hr />

<h3 id="pytorch-fundamentals">PyTorch Fundamentals: <code class="language-plaintext highlighter-rouge">dim</code>, <code class="language-plaintext highlighter-rouge">keepdim</code>, and <code class="language-plaintext highlighter-rouge">view</code></h3>

<p>Now that we understand the importance of numerical stability, we need to master the essential PyTorch operations that enable us to implement these stable operations efficiently. Before diving into the implementation details, it is important to understand the three fundamental PyTorch operations on tensors.</p>

<h4 id="1-understanding-dim-dimensionaxis">1. Understanding <code class="language-plaintext highlighter-rouge">dim</code> (Dimension/Axis)</h4>

<p>In PyTorch, tensors can have multiple dimensions. The <code class="language-plaintext highlighter-rouge">dim</code> parameter specifies <strong>which dimension</strong> to operate along.</p>

<h5 id="dimension-indexing">Dimension Indexing</h5>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch</span>

<span class="c1"># A 2D tensor (matrix)
</span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
                  <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">]])</span>

<span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># torch.Size([2, 3])
# dim=0 ‚Üí rows (size 2)
# dim=1 ‚Üí columns (size 3)
</span></code></pre></div></div>

<p><strong>Visual representation:</strong></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>        dim=1 ‚Üí
       [1, 2, 3]
dim=0  [4, 5, 6]
  ‚Üì
</code></pre></div></div>

<h5 id="operations-along-different-dimensions">Operations Along Different Dimensions</h5>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Sum along dim=0 (collapse rows, keep columns)
</span><span class="n">result_dim0</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">result_dim0</span><span class="p">)</span>  <span class="c1"># tensor([5, 7, 9])
# Adds: [1+4, 2+5, 3+6]
</span>
<span class="c1"># Sum along dim=1 (collapse columns, keep rows)
</span><span class="n">result_dim1</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">result_dim1</span><span class="p">)</span>  <span class="c1"># tensor([6, 15])
# Adds: [1+2+3, 4+5+6]
</span></code></pre></div></div>

<p><strong>Visual:</strong></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Original:          Sum along dim=0:             Sum along dim=1:
[1, 2, 3]          [5, 7, 9]                    [6, 15]
[4, 5, 6]                                       
</code></pre></div></div>

<h5 id="3d-tensor-example">3D Tensor Example</h5>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Shape: [2, 3, 4] means 2 "matrices" of size 3x4
</span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># torch.Size([2, 3, 4])
# dim=0 ‚Üí first dimension (size 2)
# dim=1 ‚Üí second dimension (size 3)
# dim=2 ‚Üí third dimension (size 4)
# dim=-1 ‚Üí last dimension (same as dim=2)
# dim=-2 ‚Üí second to last (same as dim=1)
</span></code></pre></div></div>

<p><strong>Negative indexing:</strong></p>
<ul>
  <li><code class="language-plaintext highlighter-rouge">dim=-1</code> always refers to the <strong>last dimension</strong></li>
  <li><code class="language-plaintext highlighter-rouge">dim=-2</code> refers to the <strong>second to last</strong>, etc.</li>
</ul>

<h4 id="2-understanding-keepdimtrue">2. Understanding <code class="language-plaintext highlighter-rouge">keepdim=True</code></h4>

<p><code class="language-plaintext highlighter-rouge">keepdim</code> controls whether the reduced dimension is <strong>kept</strong> or <strong>removed</strong> after an operation.</p>

<h5 id="example-keepdimfalse-default">Example: <code class="language-plaintext highlighter-rouge">keepdim=False</code> (default)</h5>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
                  <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">]])</span>

<span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># torch.Size([2, 3])
</span>
<span class="c1"># Sum along dim=1 WITHOUT keepdim
</span><span class="n">result</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>        <span class="c1"># tensor([6, 15])
</span><span class="k">print</span><span class="p">(</span><span class="n">result</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># torch.Size([2])  ‚Üê dimension collapsed!
</span></code></pre></div></div>

<p>The dimension is <strong>removed</strong>, so shape goes from <code class="language-plaintext highlighter-rouge">[2, 3]</code> ‚Üí <code class="language-plaintext highlighter-rouge">[2]</code></p>

<h5 id="example-keepdimtrue">Example: <code class="language-plaintext highlighter-rouge">keepdim=True</code></h5>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
                  <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">]])</span>

<span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># torch.Size([2, 3])
</span>
<span class="c1"># Sum along dim=1 WITH keepdim
</span><span class="n">result</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>        <span class="c1"># tensor([[6], [15]])
</span><span class="k">print</span><span class="p">(</span><span class="n">result</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># torch.Size([2, 1])  ‚Üê dimension kept!
</span></code></pre></div></div>

<p>The dimension is <strong>preserved</strong> (but size becomes 1), so shape goes from <code class="language-plaintext highlighter-rouge">[2, 3]</code> ‚Üí <code class="language-plaintext highlighter-rouge">[2, 1]</code></p>

<h5 id="why-keepdimtrue-matters-broadcasting">Why <code class="language-plaintext highlighter-rouge">keepdim=True</code> Matters: Broadcasting</h5>

<p><code class="language-plaintext highlighter-rouge">keepdim=True</code> is crucial for <strong>broadcasting</strong> operations:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">],</span>
                  <span class="p">[</span><span class="mf">4.0</span><span class="p">,</span> <span class="mf">5.0</span><span class="p">,</span> <span class="mf">6.0</span><span class="p">]])</span>

<span class="c1"># Without keepdim
</span><span class="n">mean_no_keep</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">mean_no_keep</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># torch.Size([2])
</span>
<span class="c1"># This will fail! Shapes don't match for broadcasting
# x - mean_no_keep  # Error!
</span>
<span class="c1"># With keepdim
</span><span class="n">mean_keep</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">mean_keep</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># torch.Size([2, 1])
</span>
<span class="c1"># This works! Broadcasting happens correctly
</span><span class="n">normalized</span> <span class="o">=</span> <span class="n">x</span> <span class="o">-</span> <span class="n">mean_keep</span>
<span class="k">print</span><span class="p">(</span><span class="n">normalized</span><span class="p">)</span>
<span class="c1"># tensor([[-1., 0., 1.],
#         [-1., 0., 1.]])
</span></code></pre></div></div>

<p><strong>Visual explanation:</strong></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Original x:        mean (keepdim=True):    Broadcasting x - mean:
[1, 2, 3]          [2]                     [1, 2, 3]   [2]
[4, 5, 6]          [5]                     [4, 5, 6] - [5]
                                                        ‚Üì
Shape [2, 3]       Shape [2, 1]            [1-2, 2-2, 3-2]
                                           [4-5, 5-5, 6-5]
                                            = [[-1, 0, 1],
                                               [-1, 0, 1]]
</code></pre></div></div>

<h5 id="more-examples-with-different-operations">More Examples with Different Operations</h5>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span>
                   <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]],</span>
                  <span class="p">[[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">],</span>
                   <span class="p">[</span><span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">]]])</span>
<span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># torch.Size([2, 2, 2])
# x is a stack of two [2,2] tensors:
# Tensor #0:
# [[1, 2],
#  [3, 4]]
</span>
<span class="c1"># Tensor #1:
# [[5, 6],
#  [7, 8]]
</span>
<span class="c1"># Max along dim=0, we're taking the max across the first dimension (dim=0), meaning we compare corresponding elements between the two [2,2] matrices
# [[max(1,5), max(2,6)],
#  [max(3,7), max(4,8)]]
# That gives
# [[5, 6],
#  [7, 8]]
</span>
<span class="n">max_vals_no_keep</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nb">max</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">False</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
<span class="k">print</span><span class="p">(</span><span class="n">max_vals_no_keep</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># torch.Size([2, 2])
</span>
<span class="n">max_vals_keep</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nb">max</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
<span class="k">print</span><span class="p">(</span><span class="n">max_vals_keep</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># torch.Size([1, 2, 2])
</span></code></pre></div></div>

<h4 id="3-understanding-view---reshaping-tensors">3. Understanding <code class="language-plaintext highlighter-rouge">view()</code> - Reshaping Tensors</h4>

<p><code class="language-plaintext highlighter-rouge">view()</code> reshapes a tensor <strong>without changing its data</strong> : which means that the underlying values stored in memory stay exactly the same ‚Äî PyTorch just interprets that same block of memory in a different shape.</p>
<h5 id="basic-reshaping">Basic Reshaping</h5>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">])</span>
<span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># torch.Size([6])
</span>
<span class="c1"># Reshape to 2x3
</span><span class="n">x_2d</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">x_2d</span><span class="p">)</span>
<span class="c1"># tensor([[1, 2, 3],
#         [4, 5, 6]])
</span><span class="k">print</span><span class="p">(</span><span class="n">x_2d</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># torch.Size([2, 3])
</span>
<span class="c1"># Reshape to 3x2
</span><span class="n">x_3d</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">x_3d</span><span class="p">)</span>
<span class="c1"># tensor([[1, 2],
#         [3, 4],
#         [5, 6]])
</span></code></pre></div></div>

<p><strong>Important:</strong> The <strong>total number of elements must remain the same</strong>!</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">6</span><span class="p">)</span>
<span class="n">x</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>  <span class="c1"># ‚úì Works: 6 = 2 √ó 3
</span><span class="n">x</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>  <span class="c1"># ‚úì Works: 6 = 3 √ó 2
</span><span class="n">x</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span>  <span class="c1"># ‚úì Works: 6 = 1 √ó 6
# x.view(2, 2)  # ‚úó Error: 6 ‚â† 2 √ó 2
</span></code></pre></div></div>

<h5 id="using--1-for-automatic-dimension-inference">Using <code class="language-plaintext highlighter-rouge">-1</code> for Automatic Dimension Inference</h5>

<p>You can use <code class="language-plaintext highlighter-rouge">-1</code> to let PyTorch <strong>automatically calculate</strong> one dimension:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">12</span><span class="p">)</span>

<span class="c1"># -1 means "figure out this dimension automatically"
</span><span class="n">x</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>   <span class="c1"># torch.Size([3, 4]) - PyTorch calculates 12/3 = 4
</span><span class="n">x</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span>   <span class="c1"># torch.Size([2, 6]) - PyTorch calculates 12/6 = 2
</span><span class="n">x</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>   <span class="c1"># torch.Size([12, 1])
</span><span class="n">x</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>      <span class="c1"># torch.Size([12]) - flattens to 1D
</span></code></pre></div></div>

<p><strong>Rule:</strong> You can only use <code class="language-plaintext highlighter-rouge">-1</code> for <strong>one dimension</strong> at a time.</p>

<h5 id="flattening-tensors">Flattening Tensors</h5>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Common pattern: flatten all dimensions
</span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>  <span class="c1"># Shape: [2, 3, 4]
</span><span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># torch.Size([2, 3, 4])
</span>
<span class="c1"># Flatten to 1D
</span><span class="n">x_flat</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">x_flat</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># torch.Size([24])  (2*3*4 = 24)
</span>
<span class="c1"># Flatten batch dimensions but keep last dimension
</span><span class="n">x_partial_flat</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">x_partial_flat</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># torch.Size([6, 4])  (2*3 = 6)
</span></code></pre></div></div>

<h5 id="practical-example-batch-processing">Practical Example: Batch Processing</h5>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Batch of images: [batch_size, height, width, channels]
</span><span class="n">images</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">images</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># torch.Size([32, 28, 28, 3])
</span>
<span class="c1"># Flatten each image for a fully connected layer
# Keep batch dimension, flatten the rest
</span><span class="n">images_flat</span> <span class="o">=</span> <span class="n">images</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">images_flat</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># torch.Size([32, 2352])  (28*28*3 = 2352)
</span>
<span class="c1"># Or equivalently:
</span><span class="n">images_flat</span> <span class="o">=</span> <span class="n">images</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="n">images</span><span class="p">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">images_flat</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># torch.Size([32, 2352])
</span></code></pre></div></div>

<h5 id="view-vs-reshape"><code class="language-plaintext highlighter-rouge">view()</code> vs <code class="language-plaintext highlighter-rouge">reshape()</code></h5>

<p>PyTorch has both <code class="language-plaintext highlighter-rouge">view()</code> and <code class="language-plaintext highlighter-rouge">reshape()</code>:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>

<span class="c1"># view() - requires contiguous memory
</span><span class="n">y</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="mi">6</span><span class="p">)</span>

<span class="c1"># reshape() - works even if not contiguous (may copy data)
</span><span class="n">z</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">6</span><span class="p">)</span>
</code></pre></div></div>

<p><strong>Key difference:</strong></p>
<ul>
  <li><code class="language-plaintext highlighter-rouge">view()</code>: Only works if tensor is <strong>contiguous in memory</strong>, otherwise raises error</li>
  <li><code class="language-plaintext highlighter-rouge">reshape()</code> is more flexible:
    <ul>
      <li>If the tensor is contiguous, it behaves like .view() (no copy).</li>
      <li>If it isn‚Äôt contiguous, it will make a copy behind the scenes to ensure the new tensor has contiguous memory. That‚Äôs why <code class="language-plaintext highlighter-rouge">reshape()</code> is safer, but sometimes slightly slower (copying costs time &amp; memory).</li>
    </ul>
  </li>
</ul>

<h4 id="quick-reference-summary">Quick Reference Summary</h4>

<table>
  <thead>
    <tr>
      <th>Operation</th>
      <th>What it does</th>
      <th>Example</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">dim=0</code></td>
      <td>Operate along first dimension</td>
      <td><code class="language-plaintext highlighter-rouge">torch.sum(x, dim=0)</code></td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">dim=-1</code></td>
      <td>Operate along last dimension</td>
      <td><code class="language-plaintext highlighter-rouge">torch.max(x, dim=-1)</code></td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">keepdim=True</code></td>
      <td>Keep dimension (size‚Üí1)</td>
      <td>Shape <code class="language-plaintext highlighter-rouge">[2,3]</code> ‚Üí <code class="language-plaintext highlighter-rouge">[2,1]</code></td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">keepdim=False</code></td>
      <td>Remove dimension</td>
      <td>Shape <code class="language-plaintext highlighter-rouge">[2,3]</code> ‚Üí <code class="language-plaintext highlighter-rouge">[2]</code></td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">view(a, b)</code></td>
      <td>Change the view of the data to <code class="language-plaintext highlighter-rouge">[a, b]</code></td>
      <td><code class="language-plaintext highlighter-rouge">x.view(2, 3)</code></td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">view(-1)</code></td>
      <td>Flatten to 1D</td>
      <td>Shape <code class="language-plaintext highlighter-rouge">[2,3]</code> ‚Üí <code class="language-plaintext highlighter-rouge">[6]</code></td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">view(-1, n)</code></td>
      <td>Auto calculate first dim given <code class="language-plaintext highlighter-rouge">n</code></td>
      <td><code class="language-plaintext highlighter-rouge">x.view(-1, 4)</code></td>
    </tr>
  </tbody>
</table>

<hr />

<h3 id="the-implementation">The Implementation</h3>

<p>With a solid understanding of numerical stability concerns and the fundamental PyTorch operations, we‚Äôre now ready to see how these concepts come together in a complete implementation. Below is a sample code snippet showing the complete implementation.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch</span>

<span class="k">def</span> <span class="nf">softmax</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="s">"""
    Apply the softmax operation to a tensor along the specified dimension.

    Uses the numerical stability trick of subtracting the maximum value
    from all elements before applying exponential.

    Args:
        x: torch.Tensor - Input tensor
        dim: int - Dimension along which to apply softmax

    Returns:
        torch.Tensor - Output tensor with same shape as input, with normalized
                      probability distribution along the specified dimension
    """</span>
    <span class="c1"># Subtract maximum for numerical stability
</span>    <span class="c1"># keepdim=True ensures the shape is preserved for broadcasting
</span>    <span class="n">max_vals</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nb">max</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">dim</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">x_stable</span> <span class="o">=</span> <span class="n">x</span> <span class="o">-</span> <span class="n">max_vals</span>

    <span class="c1"># Apply exponential
</span>    <span class="n">exp_vals</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x_stable</span><span class="p">)</span>

    <span class="c1"># Compute sum along the specified dimension
</span>    <span class="n">sum_exp</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">exp_vals</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">dim</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

    <span class="c1"># Normalize to get probabilities
</span>    <span class="k">return</span> <span class="n">exp_vals</span> <span class="o">/</span> <span class="n">sum_exp</span>


<span class="k">def</span> <span class="nf">log_softmax</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="s">"""
    Apply the log-softmax operation to a tensor along the specified dimension.

    log_softmax(x) = log(softmax(x)) = x - log(sum(exp(x)))

    This is more numerically stable than computing log(softmax(x)) separately
    because it cancels the exp and log operations.

    Args:
        x: torch.Tensor - Input tensor
        dim: int - Dimension along which to apply log-softmax

    Returns:
        torch.Tensor - Output tensor with same shape as input, containing
                      log probabilities along the specified dimension
    """</span>
    <span class="c1"># Subtract maximum for numerical stability
</span>    <span class="n">max_vals</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nb">max</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">dim</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">x_stable</span> <span class="o">=</span> <span class="n">x</span> <span class="o">-</span> <span class="n">max_vals</span>

    <span class="c1"># Compute log(sum(exp(x_stable)))
</span>    <span class="n">log_sum_exp</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x_stable</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="n">dim</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">True</span><span class="p">))</span>

    <span class="c1"># log_softmax = x_stable - log(sum(exp(x_stable)))
</span>    <span class="k">return</span> <span class="n">x_stable</span> <span class="o">-</span> <span class="n">log_sum_exp</span>


<span class="k">def</span> <span class="nf">cross_entropy</span><span class="p">(</span><span class="n">logits</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">targets</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="s">"""
    Compute the cross-entropy loss given logits and targets.

    The cross-entropy loss is: -log(softmax(logits)[target])

    This implementation uses the log_softmax function which provides
    numerical stability by canceling log and exp operations.

    Args:
        logits: torch.Tensor of shape (..., vocab_size) - Unnormalized logits
        targets: torch.Tensor of shape (...,) - Target class indices

    Returns:
        torch.Tensor - Scalar tensor with average cross-entropy loss across all examples
    """</span>
    <span class="c1"># Compute log probabilities using log_softmax (numerically stable)
</span>    <span class="n">log_probs</span> <span class="o">=</span> <span class="n">log_softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

    <span class="c1"># Get the log probability for the target class for each example
</span>    <span class="c1"># Flatten batch dimensions to handle any number of batch dims
</span>    <span class="n">batch_shape</span> <span class="o">=</span> <span class="n">logits</span><span class="p">.</span><span class="n">shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">log_probs_flat</span> <span class="o">=</span> <span class="n">log_probs</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">log_probs</span><span class="p">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>
    <span class="n">targets_flat</span> <span class="o">=</span> <span class="n">targets</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>

    <span class="n">batch_indices</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="n">log_probs_flat</span><span class="p">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="n">logits</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>
    <span class="n">target_log_probs</span> <span class="o">=</span> <span class="n">log_probs_flat</span><span class="p">[</span><span class="n">batch_indices</span><span class="p">,</span> <span class="n">targets_flat</span><span class="p">]</span>

    <span class="c1"># Reshape back to original batch shape
</span>    <span class="n">target_log_probs</span> <span class="o">=</span> <span class="n">target_log_probs</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_shape</span><span class="p">)</span>

    <span class="c1"># Cross entropy: -log(softmax(o)[target]) = -log_prob[target]
</span>    <span class="n">cross_entropy_loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">target_log_probs</span>

    <span class="c1"># Return average across all batch dimensions
</span>    <span class="k">return</span> <span class="n">cross_entropy_loss</span><span class="p">.</span><span class="n">mean</span><span class="p">()</span>
</code></pre></div></div>

<hr />

<h3 id="detailed-explanation">Detailed Explanation of the Implementation</h3>

<p>Now that we‚Äôve seen the complete implementation, let‚Äôs break down each function to understand how they work.</p>

<h4 id="softmax-function">Softmax Function</h4>

<p><strong>Goal:</strong> Convert raw scores (logits) into probabilities that sum to 1.</p>

<p><strong>Formula:</strong> $\text{softmax}(x_i) = \frac{e^{x_i}}{\sum_{j} e^{x_j}}$</p>

<p><strong>Step-by-step:</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Step 1: Find maximum (for numerical stability)
</span><span class="n">max_vals</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nb">max</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">dim</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
<span class="c1"># keepdim=True keeps shape [2,1] instead of [2] ‚Üí enables broadcasting
# [0] gets values (not indices)
</span>
<span class="c1"># Step 2: Subtract maximum
</span><span class="n">x_stable</span> <span class="o">=</span> <span class="n">x</span> <span class="o">-</span> <span class="n">max_vals</span>
<span class="c1"># Prevents overflow: softmax(x) = softmax(x - c) mathematically
</span>
<span class="c1"># Step 3: Exponentiate
</span><span class="n">exp_vals</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x_stable</span><span class="p">)</span>
<span class="c1"># Apply e^x to each element
</span>
<span class="c1"># Step 4: Sum and normalize
</span><span class="n">sum_exp</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">exp_vals</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">dim</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="k">return</span> <span class="n">exp_vals</span> <span class="o">/</span> <span class="n">sum_exp</span>  <span class="c1"># Division broadcasts correctly
</span></code></pre></div></div>

<p><strong>Example:</strong> Input <code class="language-plaintext highlighter-rouge">[[1, 2, 3], [4, 5, 6]]</code> ‚Üí Output <code class="language-plaintext highlighter-rouge">[[0.09, 0.24, 0.67], [0.09, 0.24, 0.67]]</code></p>

<h4 id="log-softmax-function">Log-Softmax Function</h4>

<p><strong>Goal:</strong> Compute $\log(\text{softmax}(x))$ without numerical overflow.</p>

<p><strong>Formula:</strong> $\log(\text{softmax}(x_i)) = x_i - \log\left(\sum_{j} e^{x_j}\right)$</p>

<p><strong>Why not just <code class="language-plaintext highlighter-rouge">log(softmax(x))</code>?</strong> Computing <code class="language-plaintext highlighter-rouge">log(exp(large_number))</code> can overflow. Log-softmax avoids this by staying in log-space.</p>

<p><strong>Step-by-step:</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Step 1 &amp; 2: Subtract maximum (same as softmax)
</span><span class="n">max_vals</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nb">max</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">dim</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">x_stable</span> <span class="o">=</span> <span class="n">x</span> <span class="o">-</span> <span class="n">max_vals</span>

<span class="c1"># Step 3: Compute log(sum(exp(x_stable)))
</span><span class="n">log_sum_exp</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x_stable</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="n">dim</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">True</span><span class="p">))</span>
<span class="c1"># This is the log of the denominator in softmax
</span>
<span class="c1"># Step 4: Subtract to get log probabilities
</span><span class="k">return</span> <span class="n">x_stable</span> <span class="o">-</span> <span class="n">log_sum_exp</span>
</code></pre></div></div>

<p><strong>Example:</strong> Input <code class="language-plaintext highlighter-rouge">[[1, 2, 3], [4, 5, 6]]</code> ‚Üí Output <code class="language-plaintext highlighter-rouge">[[-2.41, -1.41, -0.41], [-2.41, -1.41, -0.41]]</code></p>

<h4 id="cross-entropy-function">Cross-Entropy Function</h4>

<p><strong>Goal:</strong> Measure how well predictions match the correct labels.</p>

<p><strong>Formula:</strong> $\mathcal{L} = -\log(\text{softmax}(\text{logits})[\text{target}])$</p>

<p><strong>Intuition:</strong> Penalize low probabilities assigned to the correct class. Lower loss = better prediction.</p>

<p><strong>Step-by-step:</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Step 1: Get log probabilities
</span><span class="n">log_probs</span> <span class="o">=</span> <span class="n">log_softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="c1"># Shape: [batch, seq, vocab] ‚Üí [batch, seq, vocab]
</span>
<span class="c1"># Step 2: Save batch shape for later
</span><span class="n">batch_shape</span> <span class="o">=</span> <span class="n">logits</span><span class="p">.</span><span class="n">shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>  <span class="c1"># e.g., [2, 2] for shape [2, 2, 3]
</span>
<span class="c1"># Step 3: Flatten to 2D for easier indexing
</span><span class="n">log_probs_flat</span> <span class="o">=</span> <span class="n">log_probs</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">log_probs</span><span class="p">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>  <span class="c1"># [batch*seq, vocab]
</span><span class="n">targets_flat</span> <span class="o">=</span> <span class="n">targets</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>                           <span class="c1"># [batch*seq]
# Example: [2, 2, 3] ‚Üí [4, 3] and [2, 2] ‚Üí [4]
</span>
<span class="c1"># Step 4: Extract log prob for the correct class of each example
</span><span class="n">batch_indices</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="n">log_probs_flat</span><span class="p">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="n">logits</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>
<span class="n">target_log_probs</span> <span class="o">=</span> <span class="n">log_probs_flat</span><span class="p">[</span><span class="n">batch_indices</span><span class="p">,</span> <span class="n">targets_flat</span><span class="p">]</span>
<span class="c1"># Advanced indexing: log_probs_flat[i, targets_flat[i]] for each i
# Gets the log probability that the model assigned to the correct class
</span>
<span class="c1"># Step 5: Reshape back to original batch dimensions
</span><span class="n">target_log_probs</span> <span class="o">=</span> <span class="n">target_log_probs</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_shape</span><span class="p">)</span>

<span class="c1"># Step 6: Apply negative and average
</span><span class="n">cross_entropy_loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">target_log_probs</span>
<span class="k">return</span> <span class="n">cross_entropy_loss</span><span class="p">.</span><span class="n">mean</span><span class="p">()</span>
</code></pre></div></div>

<p><strong>Understanding Advanced Indexing (Step 4):</strong></p>

<p>This is the trickiest part. Let‚Äôs break it down with a concrete example:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Suppose we have:
</span><span class="n">log_probs_flat</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([[</span><span class="o">-</span><span class="mf">2.4</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.4</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4</span><span class="p">],</span>  <span class="c1"># example 0: log probs for 3 classes
</span>                               <span class="p">[</span><span class="o">-</span><span class="mf">2.4</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.4</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4</span><span class="p">],</span>  <span class="c1"># example 1
</span>                               <span class="p">[</span><span class="o">-</span><span class="mf">2.4</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.4</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4</span><span class="p">],</span>  <span class="c1"># example 2
</span>                               <span class="p">[</span><span class="o">-</span><span class="mf">1.1</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.1</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.1</span><span class="p">]])</span> <span class="c1"># example 3
# Shape: [4, 3]
</span>
<span class="n">targets_flat</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>  <span class="c1"># correct class for each example
# Shape: [4]
</span>
<span class="c1"># We want to extract:
# - example 0, class 2 ‚Üí log_probs_flat[0, 2] = -0.4
# - example 1, class 1 ‚Üí log_probs_flat[1, 1] = -1.4
# - example 2, class 0 ‚Üí log_probs_flat[2, 0] = -2.4
# - example 3, class 1 ‚Üí log_probs_flat[3, 1] = -1.1
</span></code></pre></div></div>

<p><strong>How advanced indexing works:</strong></p>

<p>When we have <code class="language-plaintext highlighter-rouge">tensor[indices1, indices2]</code>, PyTorch pairs up elements from both index arrays:</p>
<ul>
  <li><code class="language-plaintext highlighter-rouge">tensor[indices1[0], indices2[0]]</code></li>
  <li><code class="language-plaintext highlighter-rouge">tensor[indices1[1], indices2[1]]</code></li>
  <li><code class="language-plaintext highlighter-rouge">tensor[indices1[2], indices2[2]]</code></li>
  <li>and so on‚Ä¶</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">batch_indices</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>  <span class="c1"># row indices
</span><span class="n">targets_flat</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>   <span class="c1"># column indices
</span>
<span class="c1"># This pairing happens:
</span><span class="n">result</span> <span class="o">=</span> <span class="n">log_probs_flat</span><span class="p">[</span><span class="n">batch_indices</span><span class="p">,</span> <span class="n">targets_flat</span><span class="p">]</span>
<span class="c1"># ‚Üí [log_probs_flat[0,2], log_probs_flat[1,1], log_probs_flat[2,0], log_probs_flat[3,1]]
# ‚Üí [-0.4, -1.4, -2.4, -1.1]
</span></code></pre></div></div>

<p><strong>Why we need <code class="language-plaintext highlighter-rouge">batch_indices</code>:</strong></p>

<p>Without it, we‚Äôd just have <code class="language-plaintext highlighter-rouge">log_probs_flat[:, targets_flat]</code>, which tries to select multiple columns for ALL rows‚Äînot what we want! We need to select ONE element per row (the correct class for that specific example).</p>

<p><strong>Visual representation:</strong></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>log_probs_flat:          targets_flat:         batch_indices:
[[-2.4, -1.4, -0.4]         [2                     [0
 [-2.4, -1.4, -0.4]          1                      1
 [-2.4, -1.4, -0.4]          0                      2
 [-1.1, -1.1, -1.1]]         1]                     3]

Pairing: [0,2] [1,1] [2,0] [3,1]
         ‚Üì     ‚Üì     ‚Üì     ‚Üì
Result: [-0.4, -1.4, -2.4, -1.1]
</code></pre></div></div>

<p><strong>Summary:</strong></p>
<ul>
  <li>Input: <code class="language-plaintext highlighter-rouge">log_probs_flat</code> shape <code class="language-plaintext highlighter-rouge">[4, 3]</code>, <code class="language-plaintext highlighter-rouge">targets_flat = [2, 1, 0, 1]</code></li>
  <li>Output: <code class="language-plaintext highlighter-rouge">target_log_probs</code> shape <code class="language-plaintext highlighter-rouge">[4]</code> with values <code class="language-plaintext highlighter-rouge">[-0.4, -1.4, -2.4, -1.1]</code></li>
  <li>Each value is the log probability of the correct class for that example</li>
</ul>

<h4 id="putting-it-all-together">Putting It All Together</h4>

<p>Here‚Äôs a complete workflow showing how these functions work together:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch</span>

<span class="c1"># Setup: batch=2, sequence=2, vocab=3
</span><span class="n">logits</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([[[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">],</span>
                        <span class="p">[</span><span class="mf">4.0</span><span class="p">,</span> <span class="mf">5.0</span><span class="p">,</span> <span class="mf">6.0</span><span class="p">]],</span>
                       <span class="p">[[</span><span class="mf">7.0</span><span class="p">,</span> <span class="mf">8.0</span><span class="p">,</span> <span class="mf">9.0</span><span class="p">],</span>
                        <span class="p">[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">]]])</span>
<span class="n">targets</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
                        <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span>

<span class="c1"># Compute loss
</span><span class="n">loss</span> <span class="o">=</span> <span class="n">cross_entropy</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Loss: </span><span class="si">{</span><span class="n">loss</span><span class="si">:</span><span class="p">.</span><span class="mi">3</span><span class="n">f</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>  <span class="c1"># Loss: 1.330
</span></code></pre></div></div>

<p><strong>What happens internally:</strong></p>
<ol>
  <li><code class="language-plaintext highlighter-rouge">log_softmax</code> converts logits to log probabilities</li>
  <li>Flatten everything to make indexing easier</li>
  <li>Extract log probability for each correct class</li>
  <li>Reshape back and compute mean loss</li>
</ol>

<p><strong>Key insight:</strong> The entire pipeline is designed to compute $-\log(P(\text{correct class}))$ efficiently and stably.</p>

<hr />

<h3 id="key-takeaways">Key Takeaways</h3>

<ol>
  <li>
    <p><strong>Numerical stability isn‚Äôt optional</strong>: It‚Äôs the difference between code that works and code that fails in production</p>
  </li>
  <li>
    <p><strong>Always subtract the maximum</strong> before computing softmax or log-softmax</p>
  </li>
  <li>
    <p><strong>Use log-softmax for cross-entropy</strong>: Computing <code class="language-plaintext highlighter-rouge">log(softmax(x))</code> separately is both slower and less stable than log-softmax</p>
  </li>
  <li>
    <p><strong>The math is equivalent</strong>: These tricks don‚Äôt change the results‚Äîthey just make them computable</p>
  </li>
  <li><strong>Modern frameworks do this automatically</strong>: PyTorch‚Äôs <code class="language-plaintext highlighter-rouge">torch.nn.functional.softmax()</code> and <code class="language-plaintext highlighter-rouge">torch.nn.functional.cross_entropy()</code>include these optimizations, but understanding them helps you:
    <ul>
      <li>Debug numerical issues</li>
      <li>Implement custom loss functions</li>
      <li>Appreciate the engineering behind deep learning libraries</li>
    </ul>
  </li>
  <li><strong>Test edge cases</strong>: Always test your implementations with extreme values (very large, very small, very negative) to ensure numerical stability</li>
</ol>

  </div><a class="u-url" href="/cs336/2025/10/19/cs336-implement-softmax-log_softmax-cross_entropy.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="http://localhost:4000/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>I chronicle my captivating journey through Generative AI, sharing insights,  breakthroughs, and learnings from my enthralling side projects in the field. 
</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"></ul>
</div>

  </div>

</footer>
</body>

</html>
